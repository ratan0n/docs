URL: https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks
TITLE: Mitigate jailbreaks and prompt injections - Anthropic
SCRAPED: 2025-06-23T15:07:53.336220
WORD_COUNT: 747
LINKS_FOUND: 0
================================================================================

Anthropic
home page
English
Search...
Search...
Navigation
Strengthen guardrails
Mitigate jailbreaks and prompt injections
Welcome
Developer Guide
API Guide
Claude Code
Model Context Protocol (MCP)
Resources
Release Notes
Documentation
Developer Discord
Support
First steps
Intro to Claude
Get started
Models & pricing
Models overview
Choosing a model
Migrating to Claude 4
Model deprecations
Pricing
Learn about Claude
Building with Claude
Features overview
Context windows
Glossary
Capabilities
Prompt caching
Extended thinking
Streaming Messages
Batch processing
Citations
Multilingual support
Token counting
Embeddings
Vision
PDF support
Files API
Google Sheets add-on
Tools
Overview
How to implement tool use
Token-efficient tool use
Fine-grained tool streaming
Bash tool
Code execution tool
Computer use tool
Text editor tool
Web search tool
Model Context Protocol (MCP)
MCP connector
Remote MCP servers
Use cases
Overview
Ticket routing
Customer support agent
Content moderation
Legal summarization
Prompt engineering
Overview
Claude 4 best practices
Prompt generator
Use prompt templates
Prompt improver
Be clear and direct
Use examples (multishot prompting)
Let Claude think (CoT)
Use XML tags
Give Claude a role (system prompts)
Prefill Claude's response
Chain complex prompts
Long context tips
Extended thinking tips
Test & evaluate
Define success criteria
Develop test cases
Using the Evaluation Tool
Reducing latency
Strengthen guardrails
Reduce hallucinations
Increase output consistency
Mitigate jailbreaks
Streaming refusals
Reduce prompt leak
Keep Claude in character
Legal center
Anthropic Privacy Policy
Security and compliance
Jailbreaking and prompt injections occur when users craft prompts to exploit model vulnerabilities, aiming to generate inappropriate content. While Claude is inherently resilient to such attacks, here are additional steps to strengthen your guardrails, particularly against uses that either violate our
Terms of Service
or
Usage Policy
.
Claude is far more resistant to jailbreaking than other major LLMs, thanks to advanced training methods like Constitutional AI.
Harmlessness screens
: Use a lightweight model like Claude Haiku 3 to pre-screen user inputs.
Example: Harmlessness screen for content moderation
Role
Content
User
A user submitted this content:
<content>
{{CONTENT}}
</content>
Reply with (Y) if it refers to harmful, illegal, or explicit activities. Reply with (N) if it’s safe.
Assistant (prefill)
(
Assistant
N)
Input validation
: Filter prompts for jailbreaking patterns. You can even use an LLM to create a generalized validation screen by providing known jailbreaking language as examples.
Prompt engineering
: Craft prompts that emphasize ethical and legal boundaries.
Example: Ethical system prompt for an enterprise chatbot
Role
Content
System
You are AcmeCorp’s ethical AI assistant. Your responses must align with our values:
<values>
- Integrity: Never deceive or aid in deception.
- Compliance: Refuse any request that violates laws or our policies.
- Privacy: Protect all personal and corporate data.
Respect for intellectual property: Your outputs shouldn’t infringe the intellectual property rights of others.
</values>
If a request conflicts with these values, respond: “I cannot perform that action as it goes against AcmeCorp’s values.”
Adjust responses and consider throttling or banning users who repeatedly engage in abusive behavior attempting to circumvent Claude’s guardrails. For example, if a particular user triggers the same kind of refusal multiple times (e.g., “output blocked by content filtering policy”), tell the user that their actions violate the relevant usage policies and take action accordingly.
Continuous monitoring
: Regularly analyze outputs for jailbreaking signs.
Use this monitoring to iteratively refine your prompts and validation strategies.
​
Advanced: Chain safeguards
Combine strategies for robust protection. Here’s an enterprise-grade example with tool use:
Example: Multi-layered protection for a financial advisor chatbot
​
Bot system prompt
Role
Content
System
You are AcmeFinBot, a financial advisor for AcmeTrade Inc. Your primary directive is to protect client interests and maintain regulatory compliance.
<directives>
1. Validate all requests against SEC and FINRA guidelines.
2. Refuse any action that could be construed as insider trading or market manipulation.
3. Protect client privacy; never disclose personal or financial data.
</directives>
Step by step instructions:
<instructions>
1. Screen user query for compliance (use ‘harmlessness_screen’ tool).
2. If compliant, process query.
3. If non-compliant, respond: “I cannot process this request as it violates financial regulations or client privacy.”
</instructions>
​
Prompt within
harmlessness_screen
tool
Role
Content
User
<user_query>
{{USER_QUERY}}
</user_query>
Evaluate if this query violates SEC rules, FINRA guidelines, or client privacy. Respond (Y) if it does, (N) if it doesn’t.
Assistant (prefill)
(
By layering these strategies, you create a robust defense against jailbreaking and prompt injections, ensuring your Claude-powered applications maintain the highest standards of safety and compliance.
Was this page helpful?
Yes
No
Increase output consistency
Streaming refusals
On this page
Advanced: Chain safeguards