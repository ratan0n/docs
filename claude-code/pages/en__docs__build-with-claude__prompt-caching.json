{
  "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching",
  "title": "Prompt caching - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nCapabilities\nPrompt caching\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nPrompt caching is a powerful feature that optimizes your API usage by allowing resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements.\nHere’s an example of how to implement prompt caching with the Messages API using a\ncache_control\nblock:\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n-H\n\"content-type: application/json\"\n\\\n-H\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n-H\n\"anthropic-version: 2023-06-01\"\n\\\n-d\n'\n{\n\"model\"\n:\n\"claude-opus-4-20250514\"\n,\n\"max_tokens\"\n:\n1024\n,\n\"system\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n\\n\n\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"<the entire contents of Pride and Prejudice>\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze the major themes in Pride and Prejudice.\"\n}\n]\n}\n'\n# Call the model again with the same inputs up to the cache checkpoint\ncurl\nhttps://api.anthropic.com/v1/messages\n# rest of input\nJSON\n{\n\"cache_creation_input_tokens\"\n:\n188086\n,\n\"cache_read_input_tokens\"\n:\n0\n,\n\"input_tokens\"\n:\n21\n,\n\"output_tokens\"\n:\n393\n}\n{\n\"cache_creation_input_tokens\"\n:\n0\n,\n\"cache_read_input_tokens\"\n:\n188086\n,\n\"input_tokens\"\n:\n21\n,\n\"output_tokens\"\n:\n393\n}\nIn this example, the entire text of “Pride and Prejudice” is cached using the\ncache_control\nparameter. This enables reuse of this large text across multiple API calls without reprocessing it each time. Changing only the user message allows you to ask various questions about the book while utilizing the cached content, leading to faster responses and improved efficiency.\n​\nHow prompt caching works\nWhen you send a request with prompt caching enabled:\nThe system checks if a prompt prefix, up to a specified cache breakpoint, is already cached from a recent query.\nIf found, it uses the cached version, reducing processing time and costs.\nOtherwise, it processes the full prompt and caches the prefix once the response begins.\nThis is especially useful for:\nPrompts with many examples\nLarge amounts of context or background information\nRepetitive tasks with consistent instructions\nLong multi-turn conversations\nBy default, the cache has a 5-minute lifetime. The cache is refreshed for no additional cost each time the cached content is used.\nPrompt caching caches the full prefix\nPrompt caching references the entire prompt -\ntools\n,\nsystem\n, and\nmessages\n(in that order) up to and including the block designated with\ncache_control\n.\n​\nPricing\nPrompt caching introduces a new pricing structure. The table below shows the price per million tokens for each supported model:\nModel\nBase Input Tokens\n5m Cache Writes\n1h Cache Writes\nCache Hits & Refreshes\nOutput Tokens\nClaude Opus 4\n$15 / MTok\n$18.75 / MTok\n$30 / MTok\n$1.50 / MTok\n$75 / MTok\nClaude Sonnet 4\n$3 / MTok\n$3.75 / MTok\n$6 / MTok\n$0.30 / MTok\n$15 / MTok\nClaude Sonnet 3.7\n$3 / MTok\n$3.75 / MTok\n$6 / MTok\n$0.30 / MTok\n$15 / MTok\nClaude Sonnet 3.5\n$3 / MTok\n$3.75 / MTok\n$6 / MTok\n$0.30 / MTok\n$15 / MTok\nClaude Haiku 3.5\n$0.80 / MTok\n$1 / MTok\n$1.6 / MTok\n$0.08 / MTok\n$4 / MTok\nClaude Opus 3\n$15 / MTok\n$18.75 / MTok\n$30 / MTok\n$1.50 / MTok\n$75 / MTok\nClaude Haiku 3\n$0.25 / MTok\n$0.30 / MTok\n$0.50 / MTok\n$0.03 / MTok\n$1.25 / MTok\nNote:\n5-minute cache write tokens are 1.25 times the base input tokens price\n1-hour cache write tokens are 2 times the base input tokens price\nCache read tokens are 0.1 times the base input tokens price\nRegular input and output tokens are priced at standard rates\n​\nHow to implement prompt caching\n​\nSupported models\nPrompt caching is currently supported on:\nClaude Opus 4\nClaude Sonnet 4\nClaude Sonnet 3.7\nClaude Sonnet 3.5\nClaude Haiku 3.5\nClaude Haiku 3\nClaude Opus 3\n​\nStructuring your prompt\nPlace static content (tool definitions, system instructions, context, examples) at the beginning of your prompt. Mark the end of the reusable content for caching using the\ncache_control\nparameter.\nCache prefixes are created in the following order:\ntools\n,\nsystem\n, then\nmessages\n. This order forms a hierarchy where each level builds upon the previous ones.\nUsing the\ncache_control\nparameter, you can define up to 4 cache breakpoints, allowing you to cache different reusable sections separately. For each breakpoint, the system will automatically check for cache hits at previous positions and use the longest matching prefix if one is found.\n​\nCache limitations\nThe minimum cacheable prompt length is:\n1024 tokens for Claude Opus 4, Claude Sonnet 4, Claude Sonnet 3.7, Claude Sonnet 3.5 and Claude Opus 3\n2048 tokens for Claude Haiku 3.5 and Claude Haiku 3\nShorter prompts cannot be cached, even if marked with\ncache_control\n. Any requests to cache fewer than this number of tokens will be processed without caching. To see if a prompt was cached, see the response usage\nfields\n.\nFor concurrent requests, note that a cache entry only becomes available after the first response begins. If you need cache hits for parallel requests, wait for the first response before sending subsequent requests.\nCurrently, “ephemeral” is the only supported cache type, which by default has a 5-minute lifetime.\n​\nWhat can be cached\nMost blocks in the request can be designated for caching with\ncache_control\n. This includes:\nTools: Tool definitions in the\ntools\narray\nSystem messages: Content blocks in the\nsystem\narray\nText messages: Content blocks in the\nmessages.content\narray, for both user and assistant turns\nImages & Documents: Content blocks in the\nmessages.content\narray, in user turns\nTool use and tool results: Content blocks in the\nmessages.content\narray, in both user and assistant turns\nEach of these elements can be marked with\ncache_control\nto enable caching for that portion of the request.\n​\nWhat cannot be cached\nWhile most request blocks can be cached, there are some exceptions:\nThinking blocks cannot be cached directly with\ncache_control\n. However, thinking blocks CAN be cached alongside other content when they appear in previous assistant turns. When cached this way, they DO count as input tokens when read from cache.\nSub-content blocks (like\ncitations\n) themselves cannot be cached directly. Instead, cache the top-level block.\nIn the case of citations, the top-level document content blocks that serve as the source material for citations can be cached. This allows you to use prompt caching with citations effectively by caching the documents that citations will reference.\nEmpty text blocks cannot be cached.\n​\nWhat invalidates the cache\nModifications to cached content can invalidate some or all of the cache.\nAs described in\nStructuring your prompt\n, the cache follows the hierarchy:\ntools\n→\nsystem\n→\nmessages\n. Changes at each level invalidate that level and all subsequent levels.\nThe following table shows which parts of the cache are invalidated by different types of changes. ✘ indicates that the cache is invalidated, while ✓ indicates that the cache remains valid.\nWhat changes\nTools cache\nSystem cache\nMessages cache\nImpact\nTool definitions\n✘\n✘\n✘\nModifying tool definitions (names, descriptions, parameters) invalidates the entire cache\nWeb search toggle\n✓\n✘\n✘\nEnabling/disabling web search modifies the system prompt\nCitations toggle\n✓\n✘\n✘\nEnabling/disabling citations modifies the system prompt\nTool choice\n✓\n✓\n✘\nChanges to\ntool_choice\nparameter only affect message blocks\nImages\n✓\n✓\n✘\nAdding/removing images anywhere in the prompt affects message blocks\nThinking parameters\n✓\n✓\n✘\nChanges to extended thinking settings (enable/disable, budget) affect message blocks\nNon-tool results passed to extended thinking requests\n✓\n✓\n✘\nWhen non-tool results are passed in requests while extended thinking is enabled, all previously-cached thinking blocks are stripped from context, and any messages in context that follow those thinking blocks are removed from the cache. For more details, see\nCaching with thinking blocks\n.\n​\nTracking cache performance\nMonitor cache performance using these API response fields, within\nusage\nin the response (or\nmessage_start\nevent if\nstreaming\n):\ncache_creation_input_tokens\n: Number of tokens written to the cache when creating a new entry.\ncache_read_input_tokens\n: Number of tokens retrieved from the cache for this request.\ninput_tokens\n: Number of input tokens which were not read from or used to create a cache.\n​\nBest practices for effective caching\nTo optimize prompt caching performance:\nCache stable, reusable content like system instructions, background information, large contexts, or frequent tool definitions.\nPlace cached content at the prompt’s beginning for best performance.\nUse cache breakpoints strategically to separate different cacheable prefix sections.\nRegularly analyze cache hit rates and adjust your strategy as needed.\n​\nOptimizing for different use cases\nTailor your prompt caching strategy to your scenario:\nConversational agents: Reduce cost and latency for extended conversations, especially those with long instructions or uploaded documents.\nCoding assistants: Improve autocomplete and codebase Q&A by keeping relevant sections or a summarized version of the codebase in the prompt.\nLarge document processing: Incorporate complete long-form material including images in your prompt without increasing response latency.\nDetailed instruction sets: Share extensive lists of instructions, procedures, and examples to fine-tune Claude’s responses.  Developers often include an example or two in the prompt, but with prompt caching you can get even better performance by including 20+ diverse examples of high quality answers.\nAgentic tool use: Enhance performance for scenarios involving multiple tool calls and iterative code changes, where each step typically requires a new API call.\nTalk to books, papers, documentation, podcast transcripts, and other longform content:  Bring any knowledge base alive by embedding the entire document(s) into the prompt, and letting users ask it questions.\n​\nTroubleshooting common issues\nIf experiencing unexpected behavior:\nEnsure cached sections are identical and marked with cache_control in the same locations across calls\nCheck that calls are made within the cache lifetime (5 minutes by default)\nVerify that\ntool_choice\nand image usage remain consistent between calls\nValidate that you are caching at least the minimum number of tokens\nWhile the system will attempt to use previously cached content at positions prior to a cache breakpoint, you may use an additional\ncache_control\nparameter to guarantee cache lookup on previous portions of the prompt, which may be useful for queries with very long lists of content blocks\nChanges to\ntool_choice\nor the presence/absence of images anywhere in the prompt will invalidate the cache, requiring a new cache entry to be created. For more details on cache invalidation, see\nWhat invalidates the cache\n.\n​\nCaching with thinking blocks\nWhen using\nextended thinking\nwith prompt caching, thinking blocks have special behavior:\nAutomatic caching alongside other content\n: While thinking blocks cannot be explicitly marked with\ncache_control\n, they get cached as part of the request content when you make subsequent API calls with tool results. This commonly happens during tool use when you pass thinking blocks back to continue the conversation.\nInput token counting\n: When thinking blocks are read from cache, they count as input tokens in your usage metrics. This is important for cost calculation and token budgeting.\nCache invalidation patterns\n:\nCache remains valid when only tool results are provided as user messages\nCache gets invalidated when non-tool-result user content is added, causing all previous thinking blocks to be stripped\nThis caching behavior occurs even without explicit\ncache_control\nmarkers\nFor more details on cache invalidation, see\nWhat invalidates the cache\n.\nExample with tool use\n:\nRequest 1: User: \"What's the weather in Paris?\"\nResponse: [thinking_block_1] + [tool_use block 1]\nRequest 2:\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True]\nResponse: [thinking_block_2] + [text block 2]\n# Request 2 caches its request content (not the response)\n# The cache includes: user message, thinking_block_1, tool_use block 1, and tool_result_1\nRequest 3:\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [thinking_block_2] + [text block 2],\nUser: [Text response, cache=True]\n# Non-tool-result user block causes all thinking blocks to be ignored\n# This request is processed as if thinking blocks were never present\nWhen a non-tool-result user block is included, it designates a new assistant loop and all previous thinking blocks are removed from context.\nFor more detailed information, see the\nextended thinking documentation\n.\n​\nCache storage and sharing\nOrganization Isolation\n: Caches are isolated between organizations. Different organizations never share caches, even if they use identical prompts.\nExact Matching\n: Cache hits require 100% identical prompt segments, including all text and images up to and including the block marked with cache control.\nOutput Token Generation\n: Prompt caching has no effect on output token generation. The response you receive will be identical to what you would get if prompt caching was not used.\n​\n1-hour cache duration\nIf you find that 5 minutes is too short, Anthropic also offers a 1-hour cache duration.\nThe 1-hour cache is currently in beta. To use the extended cache, add\nextended-cache-ttl-2025-04-11\nas a\nbeta header\nto your request, and then include\nttl\nin the\ncache_control\ndefinition like this:\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n,\n\"ttl\"\n:\n\"5m\"\n|\n\"1h\"\n}\nThe response will include detailed cache information like the following:\n{\n\"usage\"\n:\n{\n\"input_tokens\"\n:\n...\n,\n\"cache_read_input_tokens\"\n:\n...\n,\n\"cache_creation_input_tokens\"\n:\n...\n,\n\"output_tokens\"\n:\n...\n,\n\"cache_creation\"\n:\n{\n\"ephemeral_5m_input_tokens\"\n:\n456\n,\n\"ephemeral_1h_input_tokens\"\n:\n100\n,\n}\n}\n}\nNote that the current\ncache_creation_input_tokens\nfield equals the sum of the values in the\ncache_creation\nobject.\n​\nWhen to use the 1-hour cache\nIf you have prompts that are used at a regular cadence (i.e., system prompts that are used more frequently than every 5 minutes), continue to use the 5-minute cache, since this will continue to be refreshed at no additional charge.\nThe 1-hour cache is best used in the following scenarios:\nWhen you have prompts that are likely used less frequently than 5 minutes, but more frequently than every hour. For example, when an agentic side-agent will take longer than 5 minutes, or when storing a long chat conversation with a user and you generally expect that user may not respond in the next 5 minutes.\nWhen latency is important and your follow up prompts may be sent beyond 5 minutes.\nWhen you want to improve your rate limit utilization, since cache hits are not deducted against your rate limit.\nThe 5-minute and 1-hour cache behave the same with respect to latency. You will generally see improved time-to-first-token for long documents.\n​\nMixing different TTLs\nYou can use both 1-hour and 5-minute cache controls in the same request, but with an important constraint: Cache entries with longer TTL must appear before shorter TTLs (i.e., a 1-hour cache entry must appear before any 5-minute cache entries).\nWhen mixing TTLs, we determine three billing locations in your prompt:\nPosition\nA\n: The token count at the highest cache hit (or 0 if no hits).\nPosition\nB\n: The token count at the highest 1-hour\ncache_control\nblock after\nA\n(or equals\nA\nif none exist).\nPosition\nC\n: The token count at the last\ncache_control\nblock.\nIf\nB\nand/or\nC\nare larger than\nA\n, they will necessarily be cache misses, because\nA\nis the highest cache hit.\nYou’ll be charged for:\nCache read tokens for\nA\n.\n1-hour cache write tokens for\n(B - A)\n.\n5-minute cache write tokens for\n(C - B)\n.\nHere are 3 examples. This depicts the input tokens of 3 requests, each of which has different cache hits and cache misses. Each has a different calculated pricing, shown in the colored boxes, as a result.\n​\nPrompt caching examples\nTo help you get started with prompt caching, we’ve prepared a\nprompt caching cookbook\nwith detailed examples and best practices.\nBelow, we’ve included several code snippets that showcase various prompt caching patterns. These examples demonstrate how to implement caching in different scenarios, helping you understand the practical applications of this feature:\nLarge context caching example\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-opus-4-20250514\"\n,\n\"max_tokens\"\n:\n1024\n,\n\"system\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"You are an AI assistant tasked with analyzing legal documents.\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What are the key terms and conditions in this agreement?\"\n}\n]\n}\n'\nThis example demonstrates basic prompt caching usage, caching the full text of the legal agreement as a prefix while keeping the user instruction uncached.\nFor the first request:\ninput_tokens\n: Number of tokens in the user message only\ncache_creation_input_tokens\n: Number of tokens in the entire system message, including the legal document\ncache_read_input_tokens\n: 0 (no cache hit on first request)\nFor subsequent requests within the cache lifetime:\ninput_tokens\n: Number of tokens in the user message only\ncache_creation_input_tokens\n: 0 (no new cache creation)\ncache_read_input_tokens\n: Number of tokens in the entire cached system message\nCaching tool definitions\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-opus-4-20250514\"\n,\n\"max_tokens\"\n:\n1024\n,\n\"tools\"\n:\n[\n{\n\"name\"\n:\n\"get_weather\"\n,\n\"description\"\n:\n\"Get the current weather in a given location\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city and state, e.g. San Francisco, CA\"\n}\n,\n\"unit\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"enum\"\n:\n[\n\"celsius\"\n,\n\"fahrenheit\"\n]\n,\n\"description\"\n:\n\"The unit of temperature, either celsius or fahrenheit\"\n}\n}\n,\n\"required\"\n:\n[\n\"location\"\n]\n}\n}\n,\n# many more tools\n{\n\"name\"\n:\n\"get_time\"\n,\n\"description\"\n:\n\"Get the current time in a given time zone\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"timezone\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The IANA time zone name, e.g. America/Los_Angeles\"\n}\n}\n,\n\"required\"\n:\n[\n\"timezone\"\n]\n}\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather and time in New York?\"\n}\n]\n}\n'\nIn this example, we demonstrate caching tool definitions.\nThe\ncache_control\nparameter is placed on the final tool (\nget_time\n) to designate all of the tools as part of the static prefix.\nThis means that all tool definitions, including\nget_weather\nand any other tools defined before\nget_time\n, will be cached as a single prefix.\nThis approach is useful when you have a consistent set of tools that you want to reuse across multiple requests without re-processing them each time.\nFor the first request:\ninput_tokens\n: Number of tokens in the user message\ncache_creation_input_tokens\n: Number of tokens in all tool definitions and system prompt\ncache_read_input_tokens\n: 0 (no cache hit on first request)\nFor subsequent requests within the cache lifetime:\ninput_tokens\n: Number of tokens in the user message\ncache_creation_input_tokens\n: 0 (no new cache creation)\ncache_read_input_tokens\n: Number of tokens in all cached tool definitions and system prompt\nContinuing a multi-turn conversation\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-opus-4-20250514\"\n,\n\"max_tokens\"\n:\n1024\n,\n\"system\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"...long system prompt\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Hello, can you tell me more about the solar system?\"\n,\n}\n]\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Good to know.\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Tell me more about Mars.\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n}\n]\n}\n'\nIn this example, we demonstrate how to use prompt caching in a multi-turn conversation.\nDuring each turn, we mark the final block of the final message with\ncache_control\nso the conversation can be incrementally cached. The system will automatically lookup and use the longest previously cached prefix for follow-up messages. That is, blocks that were previously marked with a\ncache_control\nblock are later not marked with this, but they will still be considered a cache hit (and also a cache refresh!) if they are hit within 5 minutes.\nIn addition, note that the\ncache_control\nparameter is placed on the system message. This is to ensure that if this gets evicted from the cache (after not being used for more than 5 minutes), it will get added back to the cache on the next request.\nThis approach is useful for maintaining context in ongoing conversations without repeatedly processing the same information.\nWhen this is set up properly, you should see the following in the usage response of each request:\ninput_tokens\n: Number of tokens in the new user message (will be minimal)\ncache_creation_input_tokens\n: Number of tokens in the new assistant and user turns\ncache_read_input_tokens\n: Number of tokens in the conversation up to the previous turn\nPutting it all together: Multiple cache breakpoints\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-opus-4-20250514\"\n,\n\"max_tokens\"\n:\n1024\n,\n\"tools\"\n:\n[\n{\n\"name\"\n:\n\"search_documents\"\n,\n\"description\"\n:\n\"Search through the knowledge base\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"query\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Search query\"\n}\n}\n,\n\"required\"\n:\n[\n\"query\"\n]\n}\n}\n,\n{\n\"name\"\n:\n\"get_document\"\n,\n\"description\"\n:\n\"Retrieve a specific document by ID\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"doc_id\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Document ID\"\n}\n}\n,\n\"required\"\n:\n[\n\"doc_id\"\n]\n}\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"system\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"You are a helpful research assistant with access to a document knowledge base.\n\\n\n\\n\n# Instructions\n\\n\n- Always search for relevant documents before answering\n\\n\n- Provide citations for your sources\n\\n\n- Be objective and accurate in your responses\n\\n\n- If multiple documents contain relevant information, synthesize them\n\\n\n- Acknowledge when information is not available in the knowledge base\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"# Knowledge Base Context\n\\n\n\\n\nHere are the relevant documents for this conversation:\n\\n\n\\n\n## Document 1: Solar System Overview\n\\n\nThe solar system consists of the Sun and all objects that orbit it...\n\\n\n\\n\n## Document 2: Planetary Characteristics\n\\n\nEach planet has unique features. Mercury is the smallest planet...\n\\n\n\\n\n## Document 3: Mars Exploration\n\\n\nMars has been a target of exploration for decades...\n\\n\n\\n\n[Additional documents...]\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Can you search for information about Mars rovers?\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_use\"\n,\n\"id\"\n:\n\"tool_1\"\n,\n\"name\"\n:\n\"search_documents\"\n,\n\"input\"\n:\n{\n\"query\"\n:\n\"Mars rovers\"\n}\n}\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\n\"tool_1\"\n,\n\"content\"\n:\n\"Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)\"\n}\n]\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Yes, please tell me about the Perseverance rover specifically.\"\n}\n]\n}\n'\nThis comprehensive example demonstrates how to use all 4 available cache breakpoints to optimize different parts of your prompt:\nTools cache\n(cache breakpoint 1): The\ncache_control\nparameter on the last tool definition caches all tool definitions.\nReusable instructions cache\n(cache breakpoint 2): The static instructions in the system prompt are cached separately. These instructions rarely change between requests.\nRAG context cache\n(cache breakpoint 3): The knowledge base documents are cached independently, allowing you to update the RAG documents without invalidating the tools or instructions cache.\nConversation history cache\n(cache breakpoint 4): The assistant’s response is marked with\ncache_control\nto enable incremental caching of the conversation as it progresses.\nThis approach provides maximum flexibility:\nIf you only update the final user message, all four cache segments are reused\nIf you update the RAG documents but keep the same tools and instructions, the first two cache segments are reused\nIf you change the conversation but keep the same tools, instructions, and documents, the first three segments are reused\nEach cache breakpoint can be invalidated independently based on what changes in your application\nFor the first request:\ninput_tokens\n: Tokens in the final user message\ncache_creation_input_tokens\n: Tokens in all cached segments (tools + instructions + RAG documents + conversation history)\ncache_read_input_tokens\n: 0 (no cache hits)\nFor subsequent requests with only a new user message:\ninput_tokens\n: Tokens in the new user message only\ncache_creation_input_tokens\n: Any new tokens added to conversation history\ncache_read_input_tokens\n: All previously cached tokens (tools + instructions + RAG documents + previous conversation)\nThis pattern is especially powerful for:\nRAG applications with large document contexts\nAgent systems that use multiple tools\nLong-running conversations that need to maintain context\nApplications that need to optimize different parts of the prompt independently\n​\nFAQ\nWhat is the cache lifetime?\nThe cache’s default minimum lifetime (TTL) is 5 minutes. This lifetime is refreshed each time the cached content is used.\nIf you find that 5 minutes is too short, Anthropic also offers a\n1-hour cache TTL\n.\nHow many cache breakpoints can I use?\nYou can define up to 4 cache breakpoints (using\ncache_control\nparameters) in your prompt.\nIs prompt caching available for all models?\nNo, prompt caching is currently only available for Claude Opus 4, Claude Sonnet 4, Claude Sonnet 3.7, Claude Sonnet 3.5, Claude Haiku 3.5, Claude Haiku 3, and Claude Opus 3.\nHow does prompt caching work with extended thinking?\nCached system prompts and tools will be reused when thinking parameters change. However, thinking changes (enabling/disabling or budget changes) will invalidate previously cached prompt prefixes with messages content.\nFor more details on cache invalidation, see\nWhat invalidates the cache\n.\nFor more on extended thinking, including its interaction with tool use and prompt caching, see the\nextended thinking documentation\n.\nHow do I enable prompt caching?\nTo enable prompt caching, include at least one\ncache_control\nbreakpoint in your API request.\nCan I use prompt caching with other API features?\nYes, prompt caching can be used alongside other API features like tool use and vision capabilities. However, changing whether there are images in a prompt or modifying tool use settings will break the cache.\nFor more details on cache invalidation, see\nWhat invalidates the cache\n.\nHow does prompt caching affect pricing?\nPrompt caching introduces a new pricing structure where cache writes cost 25% more than base input tokens, while cache hits cost only 10% of the base input token price.\nCan I manually clear the cache?\nCurrently, there’s no way to manually clear the cache. Cached prefixes automatically expire after a minimum of 5 minutes of inactivity.\nHow can I track the effectiveness of my caching strategy?\nYou can monitor cache performance using the\ncache_creation_input_tokens\nand\ncache_read_input_tokens\nfields in the API response.\nWhat can break the cache?\nSee\nWhat invalidates the cache\nfor more details on cache invalidation, including a list of changes that require creating a new cache entry.\nHow does prompt caching handle privacy and data separation?\nPrompt caching is designed with strong privacy and data separation measures:\nCache keys are generated using a cryptographic hash of the prompts up to the cache control point. This means only requests with identical prompts can access a specific cache.\nCaches are organization-specific. Users within the same organization can access the same cache if they use identical prompts, but caches are not shared across different organizations, even for identical prompts.\nThe caching mechanism is designed to maintain the integrity and privacy of each unique conversation or context.\nIt’s safe to use\ncache_control\nanywhere in your prompts. For cost efficiency, it’s better to exclude highly variable parts (e.g., user’s arbitrary input) from caching.\nThese measures ensure that prompt caching maintains data privacy and security while offering performance benefits.\nCan I use prompt caching with the Batches API?\nYes, it is possible to use prompt caching with your\nBatches API\nrequests. However, because asynchronous batch requests can be processed concurrently and in any order, cache hits are provided on a best-effort basis.\nThe\n1-hour cache\ncan help improve your cache hits. The most cost effective way of using it is the following:\nGather a set of message requests that have a shared prefix.\nSend a batch request with just a single request that has this shared prefix and a 1-hour cache block. This will get written to the 1-hour cache.\nAs soon as this is complete, submit the rest of the requests. You will have to monitor the job to know when it completes.\nThis is typically better than using the 5-minute cache simply because it’s common for batch requests to take between 5 minutes and 1 hour to complete. We’re considering ways to improve these cache hit rates and making this process more straightforward.\nWhy am I seeing the error `AttributeError: 'Beta' object has no attribute 'prompt_caching'` in Python?\nThis error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:\nPython\npython client\n.\nbeta\n.\nprompt_caching\n.\nmessages\n.\ncreate\n(\n.\n.\n.\n)\nSimply use:\nPython\npython client\n.\nmessages\n.\ncreate\n(\n.\n.\n.\n)\nWhy am I seeing 'TypeError: Cannot read properties of undefined (reading 'messages')'?\nThis error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:\nTypeScript\nclient\n.\nbeta\n.\npromptCaching\n.\nmessages\n.\ncreate\n(\n...\n)\nSimply use:\nclient\n.\nmessages\n.\ncreate\n(\n...\n)\nWas this page helpful?\nYes\nNo\nGlossary\nExtended thinking\nOn this page\nHow prompt caching works\nPricing\nHow to implement prompt caching\nSupported models\nStructuring your prompt\nCache limitations\nWhat can be cached\nWhat cannot be cached\nWhat invalidates the cache\nTracking cache performance\nBest practices for effective caching\nOptimizing for different use cases\nTroubleshooting common issues\nCaching with thinking blocks\nCache storage and sharing\n1-hour cache duration\nWhen to use the 1-hour cache\nMixing different TTLs\nPrompt caching examples\nFAQ",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/multilingual-support",
      "text": "Multilingual support"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/token-counting",
      "text": "Token counting"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/embeddings",
      "text": "Embeddings"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/vision",
      "text": "Vision"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/pdf-support",
      "text": "PDF support"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/files",
      "text": "Files API"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-for-sheets",
      "text": "Google Sheets add-on"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/implement-tool-use",
      "text": "How to implement tool use"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/token-efficient-tool-use",
      "text": "Token-efficient tool use"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/fine-grained-tool-streaming",
      "text": "Fine-grained tool streaming"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/bash-tool",
      "text": "Bash tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool",
      "text": "Code execution tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool",
      "text": "Computer use tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/text-editor-tool",
      "text": "Text editor tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/web-search-tool",
      "text": "Web search tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/mcp-connector",
      "text": "MCP connector"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/remote-mcp-servers",
      "text": "Remote MCP servers"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/ticket-routing",
      "text": "Ticket routing"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/customer-support-chat",
      "text": "Customer support agent"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/content-moderation",
      "text": "Content moderation"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/legal-summarization",
      "text": "Legal summarization"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices",
      "text": "Claude 4 best practices"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator",
      "text": "Prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables",
      "text": "Use prompt templates"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver",
      "text": "Prompt improver"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct",
      "text": "Be clear and direct"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting",
      "text": "Use examples (multishot prompting)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "text": "Let Claude think (CoT)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "text": "Use XML tags"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts",
      "text": "Give Claude a role (system prompts)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response",
      "text": "Prefill Claude's response"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts",
      "text": "Chain complex prompts"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "text": "Long context tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips",
      "text": "Extended thinking tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/define-success",
      "text": "Define success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
      "text": "Develop test cases"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
      "text": "Using the Evaluation Tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency",
      "text": "Reducing latency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations",
      "text": "Reduce hallucinations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    },
    {
      "url": "https://docs.anthropic.com/_sites/docs.anthropic.com/en/docs/build-with-claude/prompt-caching",
      "text": "Structuring your prompt"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:37.745733",
    "word_count": 5543,
    "link_count": 48,
    "content_length": 34553
  }
}