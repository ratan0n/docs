{
  "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/ticket-routing",
  "title": "Ticket routing - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nUse cases\nTicket routing\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\n​\nDefine whether to use Claude for ticket routing\nHere are some key indicators that you should use an LLM like Claude  instead of traditional ML approaches for your classification task:\nYou have limited labeled training data available\nTraditional ML processes require massive labeled datasets. Claude’s pre-trained model can effectively classify tickets with just a few dozen labeled examples, significantly reducing data preparation time and costs.\nYour classification categories are likely to change or evolve over time\nOnce a traditional ML approach has been established, changing it is a laborious and data-intensive undertaking. On the other hand, as your product or customer needs evolve, Claude can easily adapt to changes in class definitions or new classes without extensive relabeling of training data.\nYou need to handle complex, unstructured text inputs\nTraditional ML models often struggle with unstructured data and require extensive feature engineering. Claude’s advanced language understanding allows for accurate classification based on content and context, rather than relying on strict ontological structures.\nYour classification rules are based on semantic understanding\nTraditional ML approaches often rely on bag-of-words models or simple pattern matching. Claude excels at understanding and applying underlying rules when classes are defined by conditions rather than examples.\nYou require interpretable reasoning for classification decisions\nMany traditional ML models provide little insight into their decision-making process. Claude can provide human-readable explanations for its classification decisions, building trust in the automation system and facilitating easy adaptation if needed.\nYou want to handle edge cases and ambiguous tickets more effectively\nTraditional ML systems often struggle with outliers and ambiguous inputs, frequently misclassifying them or defaulting to a catch-all category. Claude’s natural language processing capabilities allow it to better interpret context and nuance in support tickets, potentially reducing the number of misrouted or unclassified tickets that require manual intervention.\nYou need multilingual support without maintaining separate models\nTraditional ML approaches typically require separate models or extensive translation processes for each supported language. Claude’s multilingual capabilities allow it to classify tickets in various languages without the need for separate models or extensive translation processes, streamlining support for global customer bases.\n​\nBuild and deploy your LLM support workflow\n​\nUnderstand your current support approach\nBefore diving into automation, it’s crucial to understand your existing ticketing system. Start by investigating how your support team currently handles ticket routing.\nConsider questions like:\nWhat criteria are used to determine what SLA/service offering is applied?\nIs ticket routing used to determine which tier of support or product specialist a ticket goes to?\nAre there any automated rules or workflows already in place? In what cases do they fail?\nHow are edge cases or ambiguous tickets handled?\nHow does the team prioritize tickets?\nThe more you know about how humans handle certain cases, the better you will be able to work with Claude to do the task.\n​\nDefine user intent categories\nA well-defined list of user intent categories is crucial for accurate support ticket classification with Claude. Claude’s ability to route tickets effectively within your system is directly proportional to how well-defined your system’s categories are.\nHere are some example user intent categories and subcategories.\nTechnical issue\nHardware problem\nSoftware bug\nCompatibility issue\nPerformance problem\nAccount management\nPassword reset\nAccount access issues\nBilling inquiries\nSubscription changes\nProduct information\nFeature inquiries\nProduct compatibility questions\nPricing information\nAvailability inquiries\nUser guidance\nHow-to questions\nFeature usage assistance\nBest practices advice\nTroubleshooting guidance\nFeedback\nBug reports\nFeature requests\nGeneral feedback or suggestions\nComplaints\nOrder-related\nOrder status inquiries\nShipping information\nReturns and exchanges\nOrder modifications\nService request\nInstallation assistance\nUpgrade requests\nMaintenance scheduling\nService cancellation\nSecurity concerns\nData privacy inquiries\nSuspicious activity reports\nSecurity feature assistance\nCompliance and legal\nRegulatory compliance questions\nTerms of service inquiries\nLegal documentation requests\nEmergency support\nCritical system failures\nUrgent security issues\nTime-sensitive problems\nTraining and education\nProduct training requests\nDocumentation inquiries\nWebinar or workshop information\nIntegration and API\nIntegration assistance\nAPI usage questions\nThird-party compatibility inquiries\nIn addition to intent, ticket routing and prioritization may also be influenced by other factors such as urgency, customer type, SLAs, or language. Be sure to consider other routing criteria when building your automated routing system.\n​\nEstablish success criteria\nWork with your support team to\ndefine clear success criteria\nwith measurable benchmarks, thresholds, and goals.\nHere are some standard criteria and benchmarks when using LLMs for support ticket routing:\nClassification consistency\nThis metric assesses how consistently Claude classifies similar tickets over time. It’s crucial for maintaining routing reliability. Measure this by periodically testing the model with a set of standardized inputs and aiming for a consistency rate of 95% or higher.\nAdaptation speed\nThis measures how quickly Claude can adapt to new categories or changing ticket patterns. Test this by introducing new ticket types and measuring the time it takes for the model to achieve satisfactory accuracy (e.g., >90%) on these new categories. Aim for adaptation within 50-100 sample tickets.\nMultilingual handling\nThis assesses Claude’s ability to accurately route tickets in multiple languages. Measure the routing accuracy across different languages, aiming for no more than a 5-10% drop in accuracy for non-primary languages.\nEdge case handling\nThis evaluates Claude’s performance on unusual or complex tickets. Create a test set of edge cases and measure the routing accuracy, aiming for at least 80% accuracy on these challenging inputs.\nBias mitigation\nThis measures Claude’s fairness in routing across different customer demographics. Regularly audit routing decisions for potential biases, aiming for consistent routing accuracy (within 2-3%) across all customer groups.\nPrompt efficiency\nIn situations where minimizing token count is crucial, this criteria assesses how well Claude performs with minimal context. Measure routing accuracy with varying amounts of context provided, aiming for 90%+ accuracy with just the ticket title and a brief description.\nExplainability score\nThis evaluates the quality and relevance of Claude’s explanations for its routing decisions. Human raters can score explanations on a scale (e.g., 1-5), with the goal of achieving an average score of 4 or higher.\nHere are some common success criteria that may be useful regardless of whether an LLM is used:\nRouting accuracy\nRouting accuracy measures how often tickets are correctly assigned to the appropriate team or individual on the first try. This is typically measured as a percentage of correctly routed tickets out of total tickets. Industry benchmarks often aim for 90-95% accuracy, though this can vary based on the complexity of the support structure.\nTime-to-assignment\nThis metric tracks how quickly tickets are assigned after being submitted. Faster assignment times generally lead to quicker resolutions and improved customer satisfaction. Best-in-class systems often achieve average assignment times of under 5 minutes, with many aiming for near-instantaneous routing (which is possible with LLM implementations).\nRerouting rate\nThe rerouting rate indicates how often tickets need to be reassigned after initial routing. A lower rate suggests more accurate initial routing. Aim for a rerouting rate below 10%, with top-performing systems achieving rates as low as 5% or less.\nFirst-contact resolution rate\nThis measures the percentage of tickets resolved during the first interaction with the customer. Higher rates indicate efficient routing and well-prepared support teams. Industry benchmarks typically range from 70-75%, with top performers achieving rates of 80% or higher.\nAverage handling time\nAverage handling time measures how long it takes to resolve a ticket from start to finish. Efficient routing can significantly reduce this time. Benchmarks vary widely by industry and complexity, but many organizations aim to keep average handling time under 24 hours for non-critical issues.\nCustomer satisfaction scores\nOften measured through post-interaction surveys, these scores reflect overall customer happiness with the support process. Effective routing contributes to higher satisfaction. Aim for CSAT scores of 90% or higher, with top performers often achieving 95%+ satisfaction rates.\nEscalation rate\nThis measures how often tickets need to be escalated to higher tiers of support. Lower escalation rates often indicate more accurate initial routing. Strive for an escalation rate below 20%, with best-in-class systems achieving rates of 10% or less.\nAgent productivity\nThis metric looks at how many tickets agents can handle effectively after implementing the routing solution. Improved routing should increase productivity. Measure this by tracking tickets resolved per agent per day or hour, aiming for a 10-20% improvement after implementing a new routing system.\nSelf-service deflection rate\nThis measures the percentage of potential tickets resolved through self-service options before entering the routing system. Higher rates indicate effective pre-routing triage. Aim for a deflection rate of 20-30%, with top performers achieving rates of 40% or higher.\nCost per ticket\nThis metric calculates the average cost to resolve each support ticket. Efficient routing should help reduce this cost over time. While benchmarks vary widely, many organizations aim to reduce cost per ticket by 10-15% after implementing an improved routing system.\n​\nChoose the right Claude model\nThe choice of model depends on the trade-offs between cost, accuracy, and response time.\nMany customers have found\nclaude-3-5-haiku-20241022\nan ideal model for ticket routing, as it is the fastest and most cost-effective model in the Claude 3 family while still delivering excellent results. If your classification problem requires deep subject matter expertise or a large volume of intent categories complex reasoning, you may opt for the\nlarger Sonnet model\n.\n​\nBuild a strong prompt\nTicket routing is a type of classification task. Claude analyzes the content of a support ticket and classifies it into predefined categories based on the issue type, urgency, required expertise, or other relevant factors.\nLet’s write a ticket classification prompt. Our initial prompt should contain the contents of the user request and return both the reasoning and the intent.\nTry the\nprompt generator\non the\nAnthropic Console\nto have Claude write a first draft for you.\nHere’s an example ticket routing classification prompt:\ndef\nclassify_support_request\n(\nticket_contents\n)\n:\n# Define the prompt for the classification task\nclassification_prompt\n=\nf\"\"\n\"You will be acting\nas\na customer support ticket classification system\n.\nYour task\nis\nto analyze customer support requests\nand\noutput the appropriate classification intent\nfor\neach request\n,\nalong\nwith\nyour reasoning\n.\nHere\nis\nthe customer support request you need to classify\n:\n<\nrequest\n>\n{\nticket_contents\n}\n<\n/\nrequest\n>\nPlease carefully analyze the above request to determine the customer's core intent\nand\nneeds\n.\nConsider what the customer\nis\nasking\nfor\nhas concerns about\n.\nFirst\n,\nwrite out your reasoning\nand\nanalysis of how to classify this request inside\n<\nreasoning\n>\ntags\n.\nThen\n,\noutput the appropriate classification label\nfor\nthe request inside a\n<\nintent\n>\ntag\n.\nThe valid intents are\n:\n<\nintents\n>\n<\nintent\n>\nSupport\n,\nFeedback\n,\nComplaint\n<\n/\nintent\n>\n<\nintent\n>\nOrder Tracking\n<\n/\nintent\n>\n<\nintent\n>\nRefund\n/\nExchange\n<\n/\nintent\n>\n<\n/\nintents\n>\nA request may have ONLY ONE applicable intent\n.\nOnly include the intent that\nis\nmost applicable to the request\n.\nAs an example\n,\nconsider the following request\n:\n<\nrequest\n>\nHello! I had high\n-\nspeed fiber internet installed on Saturday\nand\nmy installer\n,\nKevin\n,\nwas absolutely fantastic! Where can I send my positive review? Thanks\nfor\nyour\nhelp\n!\n<\n/\nrequest\n>\nHere\nis\nan example of how your output should be formatted\n(\nfor\nthe above example request\n)\n:\n<\nreasoning\n>\nThe user seeks information\nin\norder to leave positive feedback\n.\n<\n/\nreasoning\n>\n<\nintent\n>\nSupport\n,\nFeedback\n,\nComplaint\n<\n/\nintent\n>\nHere are a few more examples\n:\n<\nexamples\n>\n<\nexample\n2\n>\nExample\n2\nInput\n:\n<\nrequest\n>\nI wanted to write\nand\npersonally thank you\nfor\nthe compassion you showed towards my family during my fathe\nr's funeral this past weekend. Your staff was so considerate and helpful throughout this whole process; it really took a load off our shoulders. The visitation brochures were beautiful. We'\nll never forget the kindness you showed us\nand\nwe are so appreciative of how smoothly the proceedings went\n.\nThank you\n,\nagain\n,\nAmarantha Hill on behalf of the Hill Family\n.\n<\n/\nrequest\n>\nExample\n2\nOutput\n:\n<\nreasoning\n>\nUser leaves a positive review of their experience\n.\n<\n/\nreasoning\n>\n<\nintent\n>\nSupport\n,\nFeedback\n,\nComplaint\n<\n/\nintent\n>\n<\n/\nexample\n2\n>\n<\nexample\n3\n>\n.\n.\n.\n<\n/\nexample\n8\n>\n<\nexample\n9\n>\nExample\n9\nInput\n:\n<\nrequest\n>\nYour website keeps sending ad\n-\npopups that block the entire screen\n.\nIt took me twenty minutes just to\nfinally\nfind the phone number to call\nand\ncomplain\n.\nHow can I possibly access my account information\nwith\nall\nof these popups? Can you access my account\nfor\nme\n,\nsince your website\nis\nbroken? I need to know what the address\nis\non\nfile\n.\n<\n/\nrequest\n>\nExample\n9\nOutput\n:\n<\nreasoning\n>\nThe user requests\nhelp\naccessing their web account information\n.\n<\n/\nreasoning\n>\n<\nintent\n>\nSupport\n,\nFeedback\n,\nComplaint\n<\n/\nintent\n>\n<\n/\nexample\n9\n>\nRemember to always include your classification reasoning before your actual intent output\n.\nThe reasoning should be enclosed\nin\n<\nreasoning\n>\ntags\nand\nthe intent\nin\n<\nintent\n>\ntags\n.\nReturn only the reasoning\nand\nthe intent\n.\n\"\"\n\"\nLet’s break down the key components of this prompt:\nWe use Python f-strings to create the prompt template, allowing the\nticket_contents\nto be inserted into the\n<request>\ntags.\nWe give  Claude a clearly defined role as a classification system that carefully analyzes the ticket content to determine the customer’s core intent and needs.\nWe instruct Claude on proper output formatting, in this case to provide its reasoning and analysis inside\n<reasoning>\ntags, followed by the appropriate classification label inside\n<intent>\ntags.\nWe specify the valid intent categories: “Support, Feedback, Complaint”, “Order Tracking”, and “Refund/Exchange”.\nWe include a few examples (a.k.a. few-shot prompting) to illustrate how the output should be formatted, which improves accuracy and consistency.\nThe reason we want to have Claude split its response into various XML tag sections is so that we can use regular expressions to separately extract the reasoning and intent from the output. This allows us to create targeted next steps in the ticket routing workflow, such as using only the intent to decide which person to route the ticket to.\n​\nDeploy your prompt\nIt’s hard to know how well your prompt works without deploying it in a test production setting and\nrunning evaluations\n.\nLet’s build the deployment structure. Start by defining the method signature for wrapping our call to Claude. We’ll take the method we’ve already begun to write, which has\nticket_contents\nas input, and now return a tuple of\nreasoning\nand\nintent\nas output. If you have an existing automation using traditional ML, you’ll want to follow that method signature instead.\nimport\nanthropic\nimport\nre\n# Create an instance of the Anthropic API client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Set the default model\nDEFAULT_MODEL\n=\n\"claude-3-5-haiku-20241022\"\ndef\nclassify_support_request\n(\nticket_contents\n)\n:\n# Define the prompt for the classification task\nclassification_prompt\n=\nf\"\"\n\"You will be acting\nas\na customer support ticket classification system\n.\n.\n.\n.\n.\n.\n.\nThe reasoning should be enclosed\nin\n<\nreasoning\n>\ntags\nand\nthe intent\nin\n<\nintent\n>\ntags\n.\nReturn only the reasoning\nand\nthe intent\n.\n\"\"\n\"\n# Send the prompt to the API to classify the support request.\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\nDEFAULT_MODEL\n,\nmax_tokens\n=\n500\n,\ntemperature\n=\n0\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nclassification_prompt\n}\n]\n,\nstream\n=\nFalse\n,\n)\nreasoning_and_intent\n=\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\n# Use Python's regular expressions library to extract `reasoning`.\nreasoning_match\n=\nre\n.\nsearch\n(\nr\"<reasoning>(.*?)</reasoning>\"\n,\nreasoning_and_intent\n,\nre\n.\nDOTALL\n)\nreasoning\n=\nreasoning_match\n.\ngroup\n(\n1\n)\n.\nstrip\n(\n)\nif\nreasoning_match\nelse\n\"\"\n# Similarly, also extract the `intent`.\nintent_match\n=\nre\n.\nsearch\n(\nr\"<intent>(.*?)</intent>\"\n,\nreasoning_and_intent\n,\nre\n.\nDOTALL\n)\nintent\n=\nintent_match\n.\ngroup\n(\n1\n)\n.\nstrip\n(\n)\nif\nintent_match\nelse\n\"\"\nreturn\nreasoning\n,\nintent\nThis code:\nImports the Anthropic library and creates a client instance using your API key.\nDefines a\nclassify_support_request\nfunction that takes a\nticket_contents\nstring.\nSends the\nticket_contents\nto Claude for classification using the\nclassification_prompt\nReturns the model’s\nreasoning\nand\nintent\nextracted from the response.\nSince we need to wait for the entire reasoning and intent text to be generated before parsing, we set\nstream=False\n(the default).\n​\nEvaluate your prompt\nPrompting often requires testing and optimization for it to be production ready. To determine the readiness of your solution, evaluate performance based on the success criteria and thresholds you established earlier.\nTo run your evaluation, you will need test cases to run it on. The rest of this guide assumes you have already\ndeveloped your test cases\n.\n​\nBuild an evaluation function\nOur example evaluation for this guide measures Claude’s performance along three key metrics:\nAccuracy\nCost per classification\nYou may need to assess Claude on other axes depending on what factors that are important to you.\nTo assess this, we first have to modify the script we wrote and add a function to compare the predicted intent with the actual intent and calculate the percentage of correct predictions. We also have to add in cost calculation and time measurement functionality.\nimport\nanthropic\nimport\nre\n# Create an instance of the Anthropic API client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Set the default model\nDEFAULT_MODEL\n=\n\"claude-3-5-haiku-20241022\"\ndef\nclassify_support_request\n(\nrequest\n,\nactual_intent\n)\n:\n# Define the prompt for the classification task\nclassification_prompt\n=\nf\"\"\n\"You will be acting\nas\na customer support ticket classification system\n.\n.\n.\n.\n.\n.\n.\nThe reasoning should be enclosed\nin\n<\nreasoning\n>\ntags\nand\nthe intent\nin\n<\nintent\n>\ntags\n.\nReturn only the reasoning\nand\nthe intent\n.\n\"\"\n\"\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\nDEFAULT_MODEL\n,\nmax_tokens\n=\n500\n,\ntemperature\n=\n0\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nclassification_prompt\n}\n]\n,\n)\nusage\n=\nmessage\n.\nusage\n# Get the usage statistics for the API call for how many input and output tokens were used.\nreasoning_and_intent\n=\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\n# Use Python's regular expressions library to extract `reasoning`.\nreasoning_match\n=\nre\n.\nsearch\n(\nr\"<reasoning>(.*?)</reasoning>\"\n,\nreasoning_and_intent\n,\nre\n.\nDOTALL\n)\nreasoning\n=\nreasoning_match\n.\ngroup\n(\n1\n)\n.\nstrip\n(\n)\nif\nreasoning_match\nelse\n\"\"\n# Similarly, also extract the `intent`.\nintent_match\n=\nre\n.\nsearch\n(\nr\"<intent>(.*?)</intent>\"\n,\nreasoning_and_intent\n,\nre\n.\nDOTALL\n)\nintent\n=\nintent_match\n.\ngroup\n(\n1\n)\n.\nstrip\n(\n)\nif\nintent_match\nelse\n\"\"\n# Check if the model's prediction is correct.\ncorrect\n=\nactual_intent\n.\nstrip\n(\n)\n==\nintent\n.\nstrip\n(\n)\n# Return the reasoning, intent, correct, and usage.\nreturn\nreasoning\n,\nintent\n,\ncorrect\n,\nusage\nLet’s break down the edits we’ve made:\nWe added the\nactual_intent\nfrom our test cases into the\nclassify_support_request\nmethod and set up a comparison to assess whether Claude’s intent classification matches our golden intent classification.\nWe extracted usage statistics for the API call to calculate cost based on input and output tokens used\n​\nRun your evaluation\nA proper evaluation requires clear thresholds and benchmarks to determine what is a good result. The script above will give us the runtime values for accuracy, response time, and cost per classification, but we still would need clearly established thresholds. For example:\nAccuracy:\n95% (out of 100 tests)\nCost per classification:\n50% reduction on average (across 100 tests) from current routing method\nHaving these thresholds allows you to quickly and easily tell at scale, and with impartial empiricism, what method is best for you and what changes might need to be made to better fit your requirements.\n​\nImprove performance\nIn complex scenarios, it may be helpful to consider additional strategies to improve performance beyond standard\nprompt engineering techniques\n&\nguardrail implementation strategies\n. Here are some common scenarios:\n​\nUse a taxonomic hierarchy for cases with 20+ intent categories\nAs the number of classes grows, the number of examples required also expands, potentially making the prompt unwieldy. As an alternative, you can consider implementing a hierarchical classification system using a mixture of classifiers.\nOrganize your intents in a taxonomic tree structure.\nCreate a series of classifiers at every level of the tree, enabling a cascading routing approach.\nFor example, you might have a top-level classifier that broadly categorizes tickets into “Technical Issues,” “Billing Questions,” and “General Inquiries.” Each of these categories can then have its own sub-classifier to further refine the classification.\nPros - greater nuance and accuracy:\nYou can create different prompts for each parent path, allowing for more targeted and context-specific classification. This can lead to improved accuracy and more nuanced handling of customer requests.\nCons - increased latency:\nBe advised that multiple classifiers can lead to increased latency, and we recommend implementing this approach with our fastest model, Haiku.\n​\nUse vector databases and similarity search retrieval to handle highly variable tickets\nDespite providing examples being the most effective way to improve performance, if support requests are highly variable, it can be hard to include enough examples in a single prompt.\nIn this scenario, you could employ a vector database to do similarity searches from a dataset of examples and retrieve the most relevant examples for a given query.\nThis approach, outlined in detail in our\nclassification recipe\n, has been shown to improve performance from 71% accuracy to 93% accuracy.\n​\nAccount specifically for expected edge cases\nHere are some scenarios where Claude may misclassify tickets (there may be others that are unique to your situation). In these scenarios,consider providing explicit instructions or examples in the prompt of how Claude should handle the edge case:\nCustomers make implicit requests\nCustomers often express needs indirectly. For example, “I’ve been waiting for my package for over two weeks now” may be an indirect request for order status.\nSolution:\nProvide Claude with some real customer examples of these kinds of requests, along with what the underlying intent is. You can get even better results if you include a classification rationale for particularly nuanced ticket intents, so that Claude can better generalize the logic to other tickets.\nClaude prioritizes emotion over intent\nWhen customers express dissatisfaction, Claude may prioritize addressing the emotion over solving the underlying problem.\nSolution:\nProvide Claude with directions on when to prioritize customer sentiment or not. It can be something as simple as “Ignore all customer emotions. Focus only on analyzing the intent of the customer’s request and what information the customer might be asking for.”\nMultiple issues cause issue prioritization confusion\nWhen customers present multiple issues in a single interaction, Claude may have difficulty identifying the primary concern.\nSolution:\nClarify the prioritization of intents so thatClaude can better rank the extracted intents and identify the primary concern.\n​\nIntegrate Claude into your greater support workflow\nProper integration requires that you make some decisions regarding how your Claude-based ticket routing script fits into the architecture of your greater ticket routing system.There are two ways you could do this:\nPush-based:\nThe support ticket system you’re using (e.g. Zendesk) triggers your code by sending a webhook event to your routing service, which then classifies the intent and routes it.\nThis approach is more web-scalable, but needs you to expose a public endpoint.\nPull-Based:\nYour code pulls for the latest tickets based on a given schedule and routes them at pull time.\nThis approach is easier to implement but might make unnecessary calls to the support ticket system when the pull frequency is too high or might be overly slow when the pull frequency is too low.\nFor either of these approaches, you will need to wrap your script in a service. The choice of approach depends on what APIs your support ticketing system provides.\nClassification cookbook\nVisit our classification cookbook for more example code and detailed eval guidance.\nAnthropic Console\nBegin building and evaluating your workflow on the Anthropic Console.\nWas this page helpful?\nYes\nNo\nOverview\nCustomer support agent\nOn this page\nDefine whether to use Claude for ticket routing\nBuild and deploy your LLM support workflow\nUnderstand your current support approach\nDefine user intent categories\nEstablish success criteria\nChoose the right Claude model\nBuild a strong prompt\nDeploy your prompt\nEvaluate your prompt\nBuild an evaluation function\nRun your evaluation\nImprove performance\nUse a taxonomic hierarchy for cases with 20+ intent categories\nUse vector databases and similarity search retrieval to handle highly variable tickets\nAccount specifically for expected edge cases\nIntegrate Claude into your greater support workflow",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices",
      "text": "Claude 4 best practices"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator",
      "text": "Prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables",
      "text": "Use prompt templates"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver",
      "text": "Prompt improver"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct",
      "text": "Be clear and direct"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting",
      "text": "Use examples (multishot prompting)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "text": "Let Claude think (CoT)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "text": "Use XML tags"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts",
      "text": "Give Claude a role (system prompts)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response",
      "text": "Prefill Claude's response"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts",
      "text": "Chain complex prompts"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "text": "Long context tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips",
      "text": "Extended thinking tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/define-success",
      "text": "Define success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
      "text": "Develop test cases"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
      "text": "Using the Evaluation Tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency",
      "text": "Reducing latency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations",
      "text": "Reduce hallucinations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/define-success",
      "text": "define clear success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/models",
      "text": "larger Sonnet model"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/prompt-generator",
      "text": "prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/develop-tests",
      "text": "running evaluations"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:46.562515",
    "word_count": 4388,
    "link_count": 27,
    "content_length": 28285
  }
}