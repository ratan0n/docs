{
  "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/content-moderation",
  "title": "Content moderation - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nUse cases\nContent moderation\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nVisit our\ncontent moderation cookbook\nto see an example content moderation implementation using Claude.\nThis guide is focused on moderating user-generated content within your application. If you’re looking for guidance on moderating interactions with Claude, please refer to our\nguardrails guide\n.\n​\nBefore building with Claude\n​\nDecide whether to use Claude for content moderation\nHere are some key indicators that you should use an LLM like Claude instead of a traditional ML or rules-based approach for content moderation:\nYou want a cost-effective and rapid implementation\nTraditional ML methods require significant engineering resources, ML expertise, and infrastructure costs. Human moderation systems incur even higher costs. With Claude, you can have a sophisticated moderation system up and running in a fraction of the time for a fraction of the price.\nYou desire both semantic understanding and quick decisions\nTraditional ML approaches, such as bag-of-words models or simple pattern matching, often struggle to understand the tone, intent, and context of the content. While human moderation systems excel at understanding semantic meaning, they require time for content to be reviewed. Claude bridges the gap by combining semantic understanding with the ability to deliver moderation decisions quickly.\nYou need consistent policy decisions\nBy leveraging its advanced reasoning capabilities, Claude can interpret and apply complex moderation guidelines uniformly. This consistency helps ensure fair treatment of all content, reducing the risk of inconsistent or biased moderation decisions that can undermine user trust.\nYour moderation policies are likely to change or evolve over time\nOnce a traditional ML approach has been established, changing it is a laborious and data-intensive undertaking. On the other hand, as your product or customer needs evolve, Claude can easily adapt to changes or additions to moderation policies without extensive relabeling of training data.\nYou require interpretable reasoning for your moderation decisions\nIf you wish to provide users or regulators with clear explanations behind moderation decisions, Claude can generate detailed and coherent justifications. This transparency is important for building trust and ensuring accountability in content moderation practices.\nYou need multilingual support without maintaining separate models\nTraditional ML approaches typically require separate models or extensive translation processes for each supported language. Human moderation requires hiring a workforce fluent in each supported language. Claude’s multilingual capabilities allow it to classify tickets in various languages without the need for separate models or extensive translation processes, streamlining moderation for global customer bases.\nYou require multimodal support\nClaude’s multimodal capabilities allow it to analyze and interpret content across both text and images. This makes it a versatile tool for comprehensive content moderation in environments where different media types need to be evaluated together.\nAnthropic has trained all Claude models to be honest, helpful and harmless. This may result in Claude moderating content deemed particularly dangerous (in line with our\nAcceptable Use Policy\n), regardless of the prompt used. For example, an adult website that wants to allow users to post explicit sexual content may find that Claude still flags explicit content as requiring moderation, even if they specify in their prompt not to moderate explicit sexual content. We recommend reviewing our AUP in advance of building a moderation solution.\n​\nGenerate examples of content to moderate\nBefore developing a content moderation solution, first create examples of content that should be flagged and content that should not be flagged. Ensure that you include edge cases and challenging scenarios that may be difficult for a content moderation system to handle effectively. Afterwards, review your examples to create a well-defined list of moderation categories.\nFor instance, the examples generated by a social media platform might include the following:\nallowed_user_comments\n=\n[\n'This movie was great, I really enjoyed it. The main actor really killed it!'\n,\n'I hate Mondays.'\n,\n'It is a great time to invest in gold!'\n]\ndisallowed_user_comments\n=\n[\n'Delete this post now or you better hide. I am coming after you and your family.'\n,\n'Stay away from the 5G cellphones!! They are using 5G to control you.'\n,\n'Congratulations! You have won a $1,000 gift card. Click here to claim your prize!'\n]\n# Sample user comments to test the content moderation\nuser_comments\n=\nallowed_user_comments\n+\ndisallowed_user_comments\n# List of categories considered unsafe for content moderation\nunsafe_categories\n=\n[\n'Child Exploitation'\n,\n'Conspiracy Theories'\n,\n'Hate'\n,\n'Indiscriminate Weapons'\n,\n'Intellectual Property'\n,\n'Non-Violent Crimes'\n,\n'Privacy'\n,\n'Self-Harm'\n,\n'Sex Crimes'\n,\n'Sexual Content'\n,\n'Specialized Advice'\n,\n'Violent Crimes'\n]\nEffectively moderating these examples requires a nuanced understanding of language. In the comment,\nThis movie was great, I really enjoyed it. The main actor really killed it!\n, the content moderation system needs to recognize that “killed it” is a metaphor, not an indication of actual violence. Conversely, despite the lack of explicit mentions of violence, the comment\nDelete this post now or you better hide. I am coming after you and your family.\nshould be flagged by the content moderation system.\nThe\nunsafe_categories\nlist can be customized to fit your specific needs. For example, if you wish to prevent minors from creating content on your website, you could append “Underage Posting” to the list.\n​\nHow to moderate content using Claude\n​\nSelect the right Claude model\nWhen selecting a model, it’s important to consider the size of your data. If costs are a concern, a smaller model like Claude Haiku 3 is an excellent choice due to its cost-effectiveness. Below is an estimate of the cost to moderate text for a social media platform that receives one billion posts per month:\nContent size\nPosts per month: 1bn\nCharacters per post: 100\nTotal characters: 100bn\nEstimated tokens\nInput tokens: 28.6bn (assuming 1 token per 3.5 characters)\nPercentage of messages flagged: 3%\nOutput tokens per flagged message: 50\nTotal output tokens: 1.5bn\nClaude Haiku 3 estimated cost\nInput token cost: 2,860 MTok * $0.25/MTok = $715\nOutput token cost: 1,500 MTok * $1.25/MTok = $1,875\nMonthly cost: $715 + $1,875 = $2,590\nClaude Sonnet 4 estimated cost\nInput token cost: 2,860 MTok * $3.00/MTok = $8,580\nOutput token cost: 1,500 MTok * $15.00/MTok = $22,500\nMonthly cost: $8,580 + $22,500 = $31,080\nActual costs may differ from these estimates. These estimates are based on the prompt highlighted in the section on\nbatch processing\n. Output tokens can be reduced even further by removing the\nexplanation\nfield from the response.\n​\nBuild a strong prompt\nIn order to use Claude for content moderation, Claude must understand the moderation requirements of your application. Let’s start by writing a prompt that allows you to define your moderation needs:\nimport\nanthropic\nimport\njson\n# Initialize the Anthropic client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nmoderate_message\n(\nmessage\n,\nunsafe_categories\n)\n:\n# Convert the list of unsafe categories into a string, with each category on a new line\nunsafe_category_str\n=\n'\\n'\n.\njoin\n(\nunsafe_categories\n)\n# Construct the prompt for Claude, including the message and unsafe categories\nassessment_prompt\n=\nf\"\"\n\"\nDetermine whether the following message warrants moderation\n,\nbased on the unsafe categories outlined below\n.\nMessage\n:\n<\nmessage\n>\n{\nmessage\n}\n<\n/\nmessage\n>\nUnsafe Categories\n:\n<\ncategories\n>\n{\nunsafe_category_str\n}\n<\n/\ncategories\n>\nRespond\nwith\nONLY a JSON\nobject\n,\nusing the\nformat\nbelow\n:\n{\n{\n\"violation\"\n:\n<\nBoolean field denoting whether the message should be moderated\n>\n,\n\"categories\"\n:\n[\nComma\n-\nseparated\nlist\nof violated categories\n]\n,\n\"explanation\"\n:\n[\nOptional\n.\nOnly include\nif\nthere\nis\na violation\n.\n]\n}\n}\n\"\"\n\"\n# Send the request to Claude for content moderation\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\n# Using the Haiku model for lower costs\nmax_tokens\n=\n200\n,\ntemperature\n=\n0\n,\n# Use 0 temperature for increased consistency\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nassessment_prompt\n}\n]\n)\n# Parse the JSON response from Claude\nassessment\n=\njson\n.\nloads\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n)\n# Extract the violation status from the assessment\ncontains_violation\n=\nassessment\n[\n'violation'\n]\n# If there's a violation, get the categories and explanation; otherwise, use empty defaults\nviolated_categories\n=\nassessment\n.\nget\n(\n'categories'\n,\n[\n]\n)\nif\ncontains_violation\nelse\n[\n]\nexplanation\n=\nassessment\n.\nget\n(\n'explanation'\n)\nif\ncontains_violation\nelse\nNone\nreturn\ncontains_violation\n,\nviolated_categories\n,\nexplanation\n# Process each comment and print the results\nfor\ncomment\nin\nuser_comments\n:\nprint\n(\nf\"\\nComment:\n{\ncomment\n}\n\"\n)\nviolation\n,\nviolated_categories\n,\nexplanation\n=\nmoderate_message\n(\ncomment\n,\nunsafe_categories\n)\nif\nviolation\n:\nprint\n(\nf\"Violated Categories:\n{\n', '\n.\njoin\n(\nviolated_categories\n)\n}\n\"\n)\nprint\n(\nf\"Explanation:\n{\nexplanation\n}\n\"\n)\nelse\n:\nprint\n(\n\"No issues detected.\"\n)\nIn this example, the\nmoderate_message\nfunction contains an assessment prompt that includes the unsafe content categories and the message we wish to evaluate. The prompt asks Claude to assess whether the message should be moderated, based on the unsafe categories we defined.\nThe model’s assessment is then parsed to determine if there is a violation. If there is a violation, Claude also returns a list of violated categories, as well as an explanation as to why the message is unsafe.\n​\nEvaluate your prompt\nContent moderation is a classification problem. Thus, you can use the same techniques outlined in our\nclassification cookbook\nto determine the accuracy of your content moderation system.\nOne additional consideration is that instead of treating content moderation as a binary classification problem, you may instead create multiple categories to represent various risk levels. Creating multiple risk levels allows you to adjust the aggressiveness of your moderation. For example, you might want to automatically block user queries that are deemed high risk, while users with many medium risk queries are flagged for human review.\nimport\nanthropic\nimport\njson\n# Initialize the Anthropic client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nassess_risk_level\n(\nmessage\n,\nunsafe_categories\n)\n:\n# Convert the list of unsafe categories into a string, with each category on a new line\nunsafe_category_str\n=\n'\\n'\n.\njoin\n(\nunsafe_categories\n)\n# Construct the prompt for Claude, including the message, unsafe categories, and risk level definitions\nassessment_prompt\n=\nf\"\"\n\"\nAssess the risk level of the following message warrants moderation\n,\nbased on the unsafe categories listed below\n.\nMessage\n:\n<\nmessage\n>\n{\nmessage\n}\n<\n/\nmessage\n>\nUnsafe Categories\n:\n<\ncategories\n>\n{\nunsafe_category_str\n}\n<\n/\ncategories\n>\nAssign a risk level based on your confidence that the user's message should be moderated\nbased on the defined unsafe categories\n:\n0\n-\nNo risk\n1\n-\nLow risk\n2\n-\nMedium risk\n3\n-\nHigh risk\nRespond\nwith\nONLY a JSON\nobject\n,\nusing the\nformat\nbelow\n:\n{\n{\n\"risk_level\"\n:\n<\nNumerical field denoting the risk level\n>\n,\n\"categories\"\n:\n[\nComma\n-\nseparated\nlist\nof violated categories\n]\n,\n\"explanation\"\n:\n<\nOptional\n.\nOnly include\nif\nrisk level\nis\ngreater than\n0\n>\n}\n}\n\"\"\n\"\n# Send the request to Claude for risk assessment\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\n# Using the Haiku model for lower costs\nmax_tokens\n=\n200\n,\ntemperature\n=\n0\n,\n# Use 0 temperature for increased consistency\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nassessment_prompt\n}\n]\n)\n# Parse the JSON response from Claude\nassessment\n=\njson\n.\nloads\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n)\n# Extract the risk level, violated categories, and explanation from the assessment\nrisk_level\n=\nassessment\n[\n\"risk_level\"\n]\nviolated_categories\n=\nassessment\n[\n\"categories\"\n]\nexplanation\n=\nassessment\n.\nget\n(\n\"explanation\"\n)\nreturn\nrisk_level\n,\nviolated_categories\n,\nexplanation\n# Process each comment and print the results\nfor\ncomment\nin\nuser_comments\n:\nprint\n(\nf\"\\nComment:\n{\ncomment\n}\n\"\n)\nrisk_level\n,\nviolated_categories\n,\nexplanation\n=\nassess_risk_level\n(\ncomment\n,\nunsafe_categories\n)\nprint\n(\nf\"Risk Level:\n{\nrisk_level\n}\n\"\n)\nif\nviolated_categories\n:\nprint\n(\nf\"Violated Categories:\n{\n', '\n.\njoin\n(\nviolated_categories\n)\n}\n\"\n)\nif\nexplanation\n:\nprint\n(\nf\"Explanation:\n{\nexplanation\n}\n\"\n)\nThis code implements an\nassess_risk_level\nfunction that uses Claude to evaluate the risk level of a message. The function accepts a message and a list of unsafe categories as inputs.\nWithin the function, a prompt is generated for Claude, including the message to be assessed, the unsafe categories, and specific instructions for evaluating the risk level. The prompt instructs Claude to respond with a JSON object that includes the risk level, the violated categories, and an optional explanation.\nThis approach enables flexible content moderation by assigning risk levels. It can be seamlessly integrated into a larger system to automate content filtering or flag comments for human review based on their assessed risk level. For instance, when executing this code, the comment\nDelete this post now or you better hide. I am coming after you and your family.\nis identified as high risk due to its dangerous threat. Conversely, the comment\nStay away from the 5G cellphones!! They are using 5G to control you.\nis categorized as medium risk.\n​\nDeploy your prompt\nOnce you are confident in the quality of your solution, it’s time to deploy it to production. Here are some best practices to follow when using content moderation in production:\nProvide clear feedback to users:\nWhen user input is blocked or a response is flagged due to content moderation, provide informative and constructive feedback to help users understand why their message was flagged and how they can rephrase it appropriately. In the coding examples above, this is done through the\nexplanation\ntag in the Claude response.\nAnalyze moderated content:\nKeep track of the types of content being flagged by your moderation system to identify trends and potential areas for improvement.\nContinuously evaluate and improve:\nRegularly assess the performance of your content moderation system using metrics such as precision and recall tracking. Use this data to iteratively refine your moderation prompts, keywords, and assessment criteria.\n​\nImprove performance\nIn complex scenarios, it may be helpful to consider additional strategies to improve performance beyond standard\nprompt engineering techniques\n. Here are some advanced strategies:\n​\nDefine topics and provide examples\nIn addition to listing the unsafe categories in the prompt, further improvements can be made by providing definitions and phrases related to each category.\nimport\nanthropic\nimport\njson\n# Initialize the Anthropic client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Dictionary of categories considered unsafe for content moderation, with their definitions\nunsafe_category_definitions\n=\n{\n'Child Exploitation'\n:\n'Content that depicts child nudity or that enables, encourages, excuses, or depicts the sexual abuse of children.'\n,\n'Conspiracy Theories'\n:\n\"\"\n\"Content that promotes\nor\nendorses unfounded\n,\nfalse\n,\nor\nmisleading theories about events\n,\nsituations\n,\nor\nindividuals\n,\nwhich can incite harm\nor\nundermine public trust\nin\ninstitutions\nor\nfacts\n.\n\"\"\n\"\n,\n'Hate'\n:\n\"\"\n\"Content that\nis\nhateful toward people on the basis of their protected characteristics\n(\nrace\n,\ncolor\n,\nethnicity\n,\nnational origin\n,\ndisability\n,\nreligious affiliation\n,\ncaste\n,\nsexual orientation\n,\nsex\n,\ngender identity\nand\nserious disease\n)\n,\nor\ncontent that perpetuates negative stereotypes\n.\n\"\"\n\"\n,\n'Indiscriminate Weapons'\n:\n'Content that enables the creation of weapons of mass destruction.'\n,\n'Intellectual Property'\n:\n'Content that violates the intellectual property rights of third parties.'\n,\n'Non-Violent Crimes'\n:\n'Content that enables, encourages, or excuses the commission of non-violent crimes.'\n,\n'Privacy'\n:\n'Content that contains sensitive, personal information about private individuals.'\n,\n'Self-Harm'\n:\n'Content that enables, encourages, or excuses acts of intentional self-harm.'\n,\n'Sex Crimes'\n:\n'Content that enables, encourages, or excuses the commission of sex crimes.'\n,\n'Sexual Content'\n:\n'Content that contains erotic descriptions or explicit depictions of sex acts.'\n,\n'Specialized Advice'\n:\n\"\"\n\"Content that contains financial\n,\nmedical\n,\nor\nlegal advice\n.\nFinancial advice includes guidance on investments\n,\nstocks\n,\nbonds\n,\nor\nany\nfinancial planning\n.\n\"\"\n\"\n,\n'Violent Crimes'\n:\n'Content that enables, encourages, or excuses the commission of violent crimes.'\n,\n}\ndef\nmoderate_message_with_definitions\n(\nmessage\n,\nunsafe_category_definitions\n)\n:\n# Format unsafe categories string, with each category and its definition on a new line\nunsafe_category_str\n=\n'\\n'\n.\njoin\n(\nf'\n{\ncategory\n}\n:\n{\ndefinition\n}\n'\nfor\ncategory\n,\ndefinition\nin\nunsafe_category_definitions\n.\nitems\n(\n)\n)\n# Construct the prompt for Claude, including the message and unsafe categories\nassessment_prompt\n=\nf\"\"\n\"Determine whether the following message warrants moderation\n,\nbased on the unsafe categories outlined below\n.\nMessage\n:\n<\nmessage\n>\n{\nmessage\n}\n<\n/\nmessage\n>\nUnsafe Categories\nand\nTheir Definitions\n:\n<\ncategories\n>\n{\nunsafe_category_str\n}\n<\n/\ncategories\n>\nIt's important that you remember\nall\nunsafe categories\nand\ntheir definitions\n.\nRespond\nwith\nONLY a JSON\nobject\n,\nusing the\nformat\nbelow\n:\n{\n{\n\"violation\"\n:\n<\nBoolean field denoting whether the message should be moderated\n>\n,\n\"categories\"\n:\n[\nComma\n-\nseparated\nlist\nof violated categories\n]\n,\n\"explanation\"\n:\n[\nOptional\n.\nOnly include\nif\nthere\nis\na violation\n.\n]\n}\n}\n\"\"\n\"\n# Send the request to Claude for content moderation\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\n# Using the Haiku model for lower costs\nmax_tokens\n=\n200\n,\ntemperature\n=\n0\n,\n# Use 0 temperature for increased consistency\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nassessment_prompt\n}\n]\n)\n# Parse the JSON response from Claude\nassessment\n=\njson\n.\nloads\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n)\n# Extract the violation status from the assessment\ncontains_violation\n=\nassessment\n[\n'violation'\n]\n# If there's a violation, get the categories and explanation; otherwise, use empty defaults\nviolated_categories\n=\nassessment\n.\nget\n(\n'categories'\n,\n[\n]\n)\nif\ncontains_violation\nelse\n[\n]\nexplanation\n=\nassessment\n.\nget\n(\n'explanation'\n)\nif\ncontains_violation\nelse\nNone\nreturn\ncontains_violation\n,\nviolated_categories\n,\nexplanation\n# Process each comment and print the results\nfor\ncomment\nin\nuser_comments\n:\nprint\n(\nf\"\\nComment:\n{\ncomment\n}\n\"\n)\nviolation\n,\nviolated_categories\n,\nexplanation\n=\nmoderate_message_with_definitions\n(\ncomment\n,\nunsafe_category_definitions\n)\nif\nviolation\n:\nprint\n(\nf\"Violated Categories:\n{\n', '\n.\njoin\n(\nviolated_categories\n)\n}\n\"\n)\nprint\n(\nf\"Explanation:\n{\nexplanation\n}\n\"\n)\nelse\n:\nprint\n(\n\"No issues detected.\"\n)\nThe\nmoderate_message_with_definitions\nfunction expands upon the earlier\nmoderate_message\nfunction by allowing each unsafe category to be paired with a detailed definition. This occurs in the code by replacing the\nunsafe_categories\nlist from the original function with an\nunsafe_category_definitions\ndictionary. This dictionary maps each unsafe category to its corresponding definition. Both the category names and their definitions are included in the prompt.\nNotably, the definition for the\nSpecialized Advice\ncategory now specifies the types of financial advice that should be prohibited. As a result, the comment\nIt's a great time to invest in gold!\n, which previously passed the\nmoderate_message\nassessment, now triggers a violation.\n​\nConsider batch processing\nTo reduce costs in situations where real-time moderation isn’t necessary, consider moderating messages in batches. Include multiple messages within the prompt’s context, and ask Claude to assess which messages should be moderated.\nimport\nanthropic\nimport\njson\n# Initialize the Anthropic client\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nbatch_moderate_messages\n(\nmessages\n,\nunsafe_categories\n)\n:\n# Convert the list of unsafe categories into a string, with each category on a new line\nunsafe_category_str\n=\n'\\n'\n.\njoin\n(\nunsafe_categories\n)\n# Format messages string, with each message wrapped in XML-like tags and given an ID\nmessages_str\n=\n'\\n'\n.\njoin\n(\n[\nf'<message id=\n{\nidx\n}\n>\n{\nmsg\n}\n</message>'\nfor\nidx\n,\nmsg\nin\nenumerate\n(\nmessages\n)\n]\n)\n# Construct the prompt for Claude, including the messages and unsafe categories\nassessment_prompt\n=\nf\"\"\n\"Determine the messages to moderate\n,\nbased on the unsafe categories outlined below\n.\nMessages\n:\n<\nmessages\n>\n{\nmessages_str\n}\n<\n/\nmessages\n>\nUnsafe categories\nand\ntheir definitions\n:\n<\ncategories\n>\n{\nunsafe_category_str\n}\n<\n/\ncategories\n>\nRespond\nwith\nONLY a JSON\nobject\n,\nusing the\nformat\nbelow\n:\n{\n{\n\"violations\"\n:\n[\n{\n{\n\"id\"\n:\n<\nmessage\nid\n>\n,\n\"categories\"\n:\n[\nlist\nof violated categories\n]\n,\n\"explanation\"\n:\n<\nExplanation of why there's a violation\n>\n}\n}\n,\n.\n.\n.\n]\n}\n}\nImportant Notes\n:\n-\nRemember to analyze every message\nfor\na violation\n.\n-\nSelect\nany\nnumber of violations that reasonably\napply\n.\n\"\"\n\"\n# Send the request to Claude for content moderation\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n,\n# Using the Haiku model for lower costs\nmax_tokens\n=\n2048\n,\n# Increased max token count to handle batches\ntemperature\n=\n0\n,\n# Use 0 temperature for increased consistency\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nassessment_prompt\n}\n]\n)\n# Parse the JSON response from Claude\nassessment\n=\njson\n.\nloads\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n)\nreturn\nassessment\n# Process the batch of comments and get the response\nresponse_obj\n=\nbatch_moderate_messages\n(\nuser_comments\n,\nunsafe_categories\n)\n# Print the results for each detected violation\nfor\nviolation\nin\nresponse_obj\n[\n'violations'\n]\n:\nprint\n(\nf\"\"\n\"Comment\n:\n{\nuser_comments\n[\nviolation\n[\n'id'\n]\n]\n}\nViolated Categories\n:\n{\n', '\n.\njoin\n(\nviolation\n[\n'categories'\n]\n)\n}\nExplanation\n:\n{\nviolation\n[\n'explanation'\n]\n}\n\"\"\n\"\n)\nIn this example, the\nbatch_moderate_messages\nfunction handles the moderation of an entire batch of messages with a single Claude API call.\nInside the function, a prompt is created that includes the list of messages to evaluate, the defined unsafe content categories, and their descriptions. The prompt directs Claude to return a JSON object listing all messages that contain violations. Each message in the response is identified by its id, which corresponds to the message’s position in the input list.\nKeep in mind that finding the optimal batch size for your specific needs may require some experimentation. While larger batch sizes can lower costs, they might also lead to a slight decrease in quality. Additionally, you may need to increase the\nmax_tokens\nparameter in the Claude API call to accommodate longer responses. For details on the maximum number of tokens your chosen model can output, refer to the\nmodel comparison page\n.\nContent moderation cookbook\nView a fully implemented code-based example of how to use Claude for content moderation.\nGuardrails guide\nExplore our guardrails guide for techniques to moderate interactions with Claude.\nWas this page helpful?\nYes\nNo\nCustomer support agent\nLegal summarization\nOn this page\nBefore building with Claude\nDecide whether to use Claude for content moderation\nGenerate examples of content to moderate\nHow to moderate content using Claude\nSelect the right Claude model\nBuild a strong prompt\nEvaluate your prompt\nDeploy your prompt\nImprove performance\nDefine topics and provide examples\nConsider batch processing",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices",
      "text": "Claude 4 best practices"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator",
      "text": "Prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables",
      "text": "Use prompt templates"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver",
      "text": "Prompt improver"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct",
      "text": "Be clear and direct"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting",
      "text": "Use examples (multishot prompting)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "text": "Let Claude think (CoT)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "text": "Use XML tags"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts",
      "text": "Give Claude a role (system prompts)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response",
      "text": "Prefill Claude's response"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts",
      "text": "Chain complex prompts"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "text": "Long context tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips",
      "text": "Extended thinking tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/define-success",
      "text": "Define success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
      "text": "Develop test cases"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
      "text": "Using the Evaluation Tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency",
      "text": "Reducing latency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations",
      "text": "Reduce hallucinations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    },
    {
      "url": "https://docs.anthropic.com/_sites/docs.anthropic.com/en/docs/about-claude/use-case-guides/content-moderation",
      "text": "batch processing"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/models",
      "text": "model comparison page"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:46.825934",
    "word_count": 4091,
    "link_count": 25,
    "content_length": 25576
  }
}