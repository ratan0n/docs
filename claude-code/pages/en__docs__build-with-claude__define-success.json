{
  "url": "https://docs.anthropic.com/en/docs/build-with-claude/define-success",
  "title": "Define your success criteria - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nTest & evaluate\nDefine your success criteria\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nBuilding a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?\nHaving clear success criteria ensures that your prompt engineering & optimization efforts are focused on achieving specific, measurable goals.\n​\nBuilding strong criteria\nGood success criteria are:\nSpecific\n: Clearly define what you want to achieve. Instead of “good performance,” specify “accurate sentiment classification.”\nMeasurable\n: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied\nalong\nwith quantitative measures.\nEven “hazy” topics such as ethics and safety can be quantified:\nSafety criteria\nBad\nSafe outputs\nGood\nLess than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter.\nExample metrics and measurement methods\nQuantitative metrics\n:\nTask-specific: F1 score, BLEU score, perplexity\nGeneric: Accuracy, precision, recall\nOperational: Response time (ms), uptime (%)\nQuantitative methods\n:\nA/B testing: Compare performance against a baseline model or earlier version.\nUser feedback: Implicit measures like task completion rates.\nEdge case analysis: Percentage of edge cases handled without errors.\nQualitative scales\n:\nLikert scales: “Rate coherence from 1 (nonsensical) to 5 (perfectly logical)”\nExpert rubrics: Linguists rating translation quality on defined criteria\nAchievable\n: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.\nRelevant\n: Align your criteria with your application’s purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.\nExample task fidelity criteria for sentiment analysis\nCriteria\nBad\nThe model should classify sentiments well\nGood\nOur sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable).\n*\nMore on held-out test sets in the next section\n​\nCommon success criteria to consider\nHere are some criteria that might be important for your use case. This list is non-exhaustive.\nTask fidelity\nHow well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.\nConsistency\nHow similar does the model’s responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?\nRelevance and coherence\nHow well does the model directly address the user’s questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?\nTone and style\nHow well does the model’s output style match expectations? How appropriate is its language for the target audience?\nPrivacy preservation\nWhat is a successful metric for how the model handles personal or sensitive information? Can it follow instructions not to use or share certain details?\nContext utilization\nHow effectively does the model use provided context? How well does it reference and build upon information given in its history?\nLatency\nWhat is the acceptable response time for the model? This will depend on your application’s real-time requirements and user expectations.\nPrice\nWhat is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.\nMost use cases will need multidimensional evaluation along several success criteria.\nExample multidimensional criteria for sentiment analysis\nCriteria\nBad\nThe model should classify sentiments well\nGood\nOn a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:\n- an F1 score of at least 0.85\n- 99.5% of outputs are non-toxic\n- 90% of errors are would cause inconvenience, not egregious error*\n- 95% response time < 200ms\n*\nIn reality, we would also define what “inconvenience” and “egregious” means.\n​\nNext steps\nBrainstorm criteria\nBrainstorm success criteria for your use case with Claude on claude.ai.\nTip\n: Drop this page into the chat as guidance for Claude!\nDesign evaluations\nLearn to build strong test sets to gauge Claude’s performance against your criteria.\nWas this page helpful?\nYes\nNo\nExtended thinking tips\nDevelop test cases\nOn this page\nBuilding strong criteria\nCommon success criteria to consider\nNext steps",
  "links": [],
  "metadata": {
    "scraped_at": "2025-06-23T15:08:04.217693",
    "word_count": 942,
    "link_count": 0,
    "content_length": 6303
  }
}