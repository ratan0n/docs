{
  "url": "https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking",
  "title": "Building with extended thinking - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nCapabilities\nBuilding with extended thinking\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nExtended thinking gives Claude enhanced reasoning capabilities for complex tasks, while providing varying levels of transparency into its step-by-step thought process before it delivers its final answer.\n​\nSupported models\nExtended thinking is supported in the following models:\nClaude Opus 4 (\nclaude-opus-4-20250514\n)\nClaude Sonnet 4 (\nclaude-sonnet-4-20250514\n)\nClaude Sonnet 3.7 (\nclaude-3-7-sonnet-20250219\n)\nAPI behavior differs across Claude 3.7 and Claude 4 models, but the API shapes remain exactly the same.\nFor more information, see\nDifferences in thinking across model versions\n.\n​\nHow extended thinking works\nWhen extended thinking is turned on, Claude creates\nthinking\ncontent blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.\nThe API response will include\nthinking\ncontent blocks, followed by\ntext\ncontent blocks.\nHere’s an example of the default response format:\n{\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"thinking\"\n,\n\"thinking\"\n:\n\"Let me analyze this step by step...\"\n,\n\"signature\"\n:\n\"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Based on my analysis...\"\n}\n]\n}\nFor more information about the response format of extended thinking, see the\nMessages API Reference\n.\n​\nHow to use extended thinking\nHere is an example of using extended thinking in the Messages API:\nShell\nPython\nTypeScript\nJava\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-sonnet-4-20250514\"\n,\n\"max_tokens\"\n:\n16000\n,\n\"thinking\"\n:\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n}\n]\n}\n'\nTo turn on extended thinking, add a\nthinking\nobject, with the\ntype\nparameter set to\nenabled\nand the\nbudget_tokens\nto a specified token budget for extended thinking.\nThe\nbudget_tokens\nparameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to\nthe summarized output\n. Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.\nbudget_tokens\nmust be set to a value less than\nmax_tokens\n. However, when using\ninterleaved thinking with tools\n, you can exceed this limit as the token limit becomes your entire context window (200k tokens).\n​\nSummarized thinking\nWith extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude’s full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.\nHere are some important considerations for summarized thinking:\nYou’re charged for the full thinking tokens generated by the original request, not the summary tokens.\nThe billed output token count will\nnot match\nthe count of tokens you see in the response.\nThe first few lines of thinking output are more verbose, providing detailed reasoning that’s particularly helpful for prompt engineering purposes.\nAs Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.\nSummarization preserves the key ideas of Claude’s thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude 3.7 models to Claude 4 models.\nSummarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.\nClaude Sonnet 3.7 continues to return full thinking output.\nIn rare cases where you need access to full thinking output for Claude 4 models,\ncontact our sales team\n.\n​\nStreaming thinking\nYou can stream extended thinking responses using\nserver-sent events (SSE)\n.\nWhen streaming is enabled for extended thinking, you receive thinking content via\nthinking_delta\nevents.\nFor more documention on streaming via the Messages API, see\nStreaming Messages\n.\nHere’s how to handle streaming with thinking:\nShell\nPython\nTypeScript\nJava\nTry in Console\ncurl\nhttps://api.anthropic.com/v1/messages\n\\\n--header\n\"x-api-key:\n$ANTHROPIC_API_KEY\n\"\n\\\n--header\n\"anthropic-version: 2023-06-01\"\n\\\n--header\n\"content-type: application/json\"\n\\\n--data\n\\\n'\n{\n\"model\"\n:\n\"claude-sonnet-4-20250514\"\n,\n\"max_tokens\"\n:\n16000\n,\n\"stream\"\n:\ntrue,\n\"thinking\"\n:\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is 27 * 453?\"\n}\n]\n}\n'\nExample streaming output:\nevent\n:\nmessage_start\ndata\n:\n{\n\"type\"\n:\n\"message_start\"\n,\n\"message\"\n:\n{\n\"id\"\n:\n\"msg_01...\"\n,\n\"type\"\n:\n\"message\"\n,\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\n]\n,\n\"model\"\n:\n\"claude-sonnet-4-20250514\"\n,\n\"stop_reason\"\n:\nnull\n,\n\"stop_sequence\"\n:\nnull\n}\n}\nevent\n:\ncontent_block_start\ndata\n:\n{\n\"type\"\n:\n\"content_block_start\"\n,\n\"index\"\n:\n0\n,\n\"content_block\"\n:\n{\n\"type\"\n:\n\"thinking\"\n,\n\"thinking\"\n:\n\"\"\n}\n}\nevent\n:\ncontent_block_delta\ndata\n:\n{\n\"type\"\n:\n\"content_block_delta\"\n,\n\"index\"\n:\n0\n,\n\"delta\"\n:\n{\n\"type\"\n:\n\"thinking_delta\"\n,\n\"thinking\"\n:\n\"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"\n}\n}\nevent\n:\ncontent_block_delta\ndata\n:\n{\n\"type\"\n:\n\"content_block_delta\"\n,\n\"index\"\n:\n0\n,\n\"delta\"\n:\n{\n\"type\"\n:\n\"thinking_delta\"\n,\n\"thinking\"\n:\n\"\\n2. 453 = 400 + 50 + 3\"\n}\n}\n// Additional thinking deltas...\nevent\n:\ncontent_block_delta\ndata\n:\n{\n\"type\"\n:\n\"content_block_delta\"\n,\n\"index\"\n:\n0\n,\n\"delta\"\n:\n{\n\"type\"\n:\n\"signature_delta\"\n,\n\"signature\"\n:\n\"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"\n}\n}\nevent\n:\ncontent_block_stop\ndata\n:\n{\n\"type\"\n:\n\"content_block_stop\"\n,\n\"index\"\n:\n0\n}\nevent\n:\ncontent_block_start\ndata\n:\n{\n\"type\"\n:\n\"content_block_start\"\n,\n\"index\"\n:\n1\n,\n\"content_block\"\n:\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"\"\n}\n}\nevent\n:\ncontent_block_delta\ndata\n:\n{\n\"type\"\n:\n\"content_block_delta\"\n,\n\"index\"\n:\n1\n,\n\"delta\"\n:\n{\n\"type\"\n:\n\"text_delta\"\n,\n\"text\"\n:\n\"27 * 453 = 12,231\"\n}\n}\n// Additional text deltas...\nevent\n:\ncontent_block_stop\ndata\n:\n{\n\"type\"\n:\n\"content_block_stop\"\n,\n\"index\"\n:\n1\n}\nevent\n:\nmessage_delta\ndata\n:\n{\n\"type\"\n:\n\"message_delta\"\n,\n\"delta\"\n:\n{\n\"stop_reason\"\n:\n\"end_turn\"\n,\n\"stop_sequence\"\n:\nnull\n}\n}\nevent\n:\nmessage_stop\ndata\n:\n{\n\"type\"\n:\n\"message_stop\"\n}\nWhen using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.\nThe streaming system needs to process content in batches for optimal performance, which can result in this “chunky” delivery pattern, with possible delays between streaming events. We’re continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.\n​\nExtended thinking with tool use\nExtended thinking can be used alongside\ntool use\n, allowing Claude to reason through tool selection and results processing.\nWhen using extended thinking with tool use, be aware of the following limitations:\nTool choice limitation\n: Tool use with thinking only supports\ntool_choice: {\"type\": \"auto\"}\n(the default) or\ntool_choice: {\"type\": \"none\"}\n. Using\ntool_choice: {\"type\": \"any\"}\nor\ntool_choice: {\"type\": \"tool\", \"name\": \"...\"}\nwill result in an error because these options force tool use, which is incompatible with extended thinking.\nPreserving thinking blocks\n: During tool use, you must pass\nthinking\nblocks back to the API for the last assistant message. Include the complete unmodified block back to the API to maintain reasoning continuity.\nExample: Passing thinking blocks with tool results\nHere’s a practical example showing how to preserve thinking blocks when providing tool results:\nPython\nTypeScript\nJava\nweather_tool\n=\n{\n\"name\"\n:\n\"get_weather\"\n,\n\"description\"\n:\n\"Get current weather for a location\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n}\n,\n\"required\"\n:\n[\n\"location\"\n]\n}\n}\n# First request - Claude responds with thinking and tool request\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\nweather_tool\n]\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather in Paris?\"\n}\n]\n)\nThe API response will include thinking, text, and tool_use blocks:\n{\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"thinking\"\n,\n\"thinking\"\n:\n\"The user wants to know the current weather in Paris. I have access to a function `get_weather`...\"\n,\n\"signature\"\n:\n\"BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs....\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"I can help you get the current weather information for Paris. Let me check that for you\"\n}\n,\n{\n\"type\"\n:\n\"tool_use\"\n,\n\"id\"\n:\n\"toolu_01CswdEQBMshySk6Y9DFKrfq\"\n,\n\"name\"\n:\n\"get_weather\"\n,\n\"input\"\n:\n{\n\"location\"\n:\n\"Paris\"\n}\n}\n]\n}\nNow let’s continue the conversation and use the tool\nPython\nTypeScript\nJava\n# Extract thinking block and tool use block\nthinking_block\n=\nnext\n(\n(\nblock\nfor\nblock\nin\nresponse\n.\ncontent\nif\nblock\n.\ntype\n==\n'thinking'\n)\n,\nNone\n)\ntool_use_block\n=\nnext\n(\n(\nblock\nfor\nblock\nin\nresponse\n.\ncontent\nif\nblock\n.\ntype\n==\n'tool_use'\n)\n,\nNone\n)\n# Call your actual weather API, here is where your actual API call would go\n# let's pretend this is what we get back\nweather_data\n=\n{\n\"temperature\"\n:\n88\n}\n# Second request - Include thinking block and tool result\n# No new thinking blocks will be generated in the response\ncontinuation\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\nweather_tool\n]\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather in Paris?\"\n}\n,\n# notice that the thinking_block is passed in as well as the tool_use_block\n# if this is not passed in, an error is raised\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\nthinking_block\n,\ntool_use_block\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\ntool_use_block\n.\nid\n,\n\"content\"\n:\nf\"Current temperature:\n{\nweather_data\n[\n'temperature'\n]\n}\n°F\"\n}\n]\n}\n]\n)\nThe API response will now\nonly\ninclude text\n{\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Currently in Paris, the temperature is 88°F (31°C)\"\n}\n]\n}\n​\nPreserving thinking blocks\nDuring tool use, you must pass\nthinking\nblocks back to the API, and you must include the complete unmodified block back to the API.  This is critical for maintaining the model’s reasoning flow and conversation integrity.\nWhile you can omit\nthinking\nblocks from prior\nassistant\nrole turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:\nAutomatically filter the provided thinking blocks\nUse the relevant thinking blocks necessary to preserve the model’s reasoning\nOnly bill for the input tokens for the blocks shown to Claude\nWhen Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:\nReasoning continuity\n: The thinking blocks capture Claude’s step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.\nContext maintenance\n: While tool results appear as user messages in the API structure, they’re part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls. For more information on context management, see our\nguide on context windows\n.\nImportant\n: When providing\nthinking\nblocks, the entire sequence of consecutive\nthinking\nblocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.\n​\nInterleaved thinking\nExtended thinking with tool use in Claude 4 models supports interleaved thinking, which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.\nWith interleaved thinking, Claude can:\nReason about the results of a tool call before deciding what to do next\nChain multiple tool calls with reasoning steps in between\nMake more nuanced decisions based on intermediate results\nTo enable interleaved thinking, add\nthe beta header\ninterleaved-thinking-2025-05-14\nto your API request.\nHere are some important considerations for interleaved thinking:\nWith interleaved thinking, the\nbudget_tokens\ncan exceed the\nmax_tokens\nparameter, as it represents the total budget across all thinking blocks within one assistant turn.\nInterleaved thinking is only supported for\ntools used via the Messages API\n.\nInterleaved thinking is supported for Claude 4 models only, with the beta header\ninterleaved-thinking-2025-05-14\n.\nDirect calls to Anthropic’s API allow you to pass\ninterleaved-thinking-2025-05-14\nin requests to any model, with no effect.\nOn 3rd-party platforms (e.g.,\nAmazon Bedrock\nand\nVertex AI\n), if you pass\ninterleaved-thinking-2025-05-14\nto any model aside from Claude Opus 4 or Sonnet 4, your request will fail.\nTool use without interleaved thinking\nPython\nTypeScript\nJava\nimport\nanthropic\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Define tools\ncalculator_tool\n=\n{\n\"name\"\n:\n\"calculator\"\n,\n\"description\"\n:\n\"Perform mathematical calculations\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"expression\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Mathematical expression to evaluate\"\n}\n}\n,\n\"required\"\n:\n[\n\"expression\"\n]\n}\n}\ndatabase_tool\n=\n{\n\"name\"\n:\n\"database_query\"\n,\n\"description\"\n:\n\"Query product database\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"query\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"SQL query to execute\"\n}\n}\n,\n\"required\"\n:\n[\n\"query\"\n]\n}\n}\n# First request - Claude thinks once before all tool calls\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\ncalculator_tool\n,\ndatabase_tool\n]\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?\"\n}\n]\n)\n# Response includes thinking followed by tool uses\n# Note: Claude thinks once at the beginning, then makes all tool decisions\nprint\n(\n\"First response:\"\n)\nfor\nblock\nin\nresponse\n.\ncontent\n:\nif\nblock\n.\ntype\n==\n\"thinking\"\n:\nprint\n(\nf\"Thinking (summarized):\n{\nblock\n.\nthinking\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"tool_use\"\n:\nprint\n(\nf\"Tool use:\n{\nblock\n.\nname\n}\nwith input\n{\nblock\n.\ninput\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"text\"\n:\nprint\n(\nf\"Text:\n{\nblock\n.\ntext\n}\n\"\n)\n# You would execute the tools and return results...\n# After getting both tool results back, Claude directly responds without additional thinking\nIn this example without interleaved thinking:\nClaude thinks once at the beginning to understand the task\nMakes all tool use decisions upfront\nWhen tool results are returned, Claude immediately provides a response without additional thinking\nTool use with interleaved thinking\nPython\nTypeScript\nJava\nimport\nanthropic\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Same tool definitions as before\ncalculator_tool\n=\n{\n\"name\"\n:\n\"calculator\"\n,\n\"description\"\n:\n\"Perform mathematical calculations\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"expression\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Mathematical expression to evaluate\"\n}\n}\n,\n\"required\"\n:\n[\n\"expression\"\n]\n}\n}\ndatabase_tool\n=\n{\n\"name\"\n:\n\"database_query\"\n,\n\"description\"\n:\n\"Query product database\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"query\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"SQL query to execute\"\n}\n}\n,\n\"required\"\n:\n[\n\"query\"\n]\n}\n}\n# First request with interleaved thinking enabled\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\ncalculator_tool\n,\ndatabase_tool\n]\n,\n# Enable interleaved thinking with beta header\nextra_headers\n=\n{\n\"anthropic-beta\"\n:\n\"interleaved-thinking-2025-05-14\"\n}\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?\"\n}\n]\n)\nprint\n(\n\"Initial response:\"\n)\nthinking_blocks\n=\n[\n]\ntool_use_blocks\n=\n[\n]\nfor\nblock\nin\nresponse\n.\ncontent\n:\nif\nblock\n.\ntype\n==\n\"thinking\"\n:\nthinking_blocks\n.\nappend\n(\nblock\n)\nprint\n(\nf\"Thinking:\n{\nblock\n.\nthinking\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"tool_use\"\n:\ntool_use_blocks\n.\nappend\n(\nblock\n)\nprint\n(\nf\"Tool use:\n{\nblock\n.\nname\n}\nwith input\n{\nblock\n.\ninput\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"text\"\n:\nprint\n(\nf\"Text:\n{\nblock\n.\ntext\n}\n\"\n)\n# First tool result (calculator)\ncalculator_result\n=\n\"7500\"\n# 150 * 50\n# Continue with first tool result\nresponse2\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\ncalculator_tool\n,\ndatabase_tool\n]\n,\nextra_headers\n=\n{\n\"anthropic-beta\"\n:\n\"interleaved-thinking-2025-05-14\"\n}\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\nthinking_blocks\n[\n0\n]\n,\ntool_use_blocks\n[\n0\n]\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\ntool_use_blocks\n[\n0\n]\n.\nid\n,\n\"content\"\n:\ncalculator_result\n}\n]\n}\n]\n)\nprint\n(\n\"\\nAfter calculator result:\"\n)\n# With interleaved thinking, Claude can think about the calculator result\n# before deciding to query the database\nfor\nblock\nin\nresponse2\n.\ncontent\n:\nif\nblock\n.\ntype\n==\n\"thinking\"\n:\nthinking_blocks\n.\nappend\n(\nblock\n)\nprint\n(\nf\"Interleaved thinking:\n{\nblock\n.\nthinking\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"tool_use\"\n:\ntool_use_blocks\n.\nappend\n(\nblock\n)\nprint\n(\nf\"Tool use:\n{\nblock\n.\nname\n}\nwith input\n{\nblock\n.\ninput\n}\n\"\n)\n# Second tool result (database)\ndatabase_result\n=\n\"5200\"\n# Example average monthly revenue\n# Continue with second tool result\nresponse3\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\ntools\n=\n[\ncalculator_tool\n,\ndatabase_tool\n]\n,\nextra_headers\n=\n{\n\"anthropic-beta\"\n:\n\"interleaved-thinking-2025-05-14\"\n}\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n[\nthinking_blocks\n[\n0\n]\n,\ntool_use_blocks\n[\n0\n]\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\ntool_use_blocks\n[\n0\n]\n.\nid\n,\n\"content\"\n:\ncalculator_result\n}\n]\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nthinking_blocks\n[\n1\n:\n]\n+\ntool_use_blocks\n[\n1\n:\n]\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\ntool_use_blocks\n[\n1\n]\n.\nid\n,\n\"content\"\n:\ndatabase_result\n}\n]\n}\n]\n)\nprint\n(\n\"\\nAfter database result:\"\n)\n# With interleaved thinking, Claude can think about both results\n# before formulating the final response\nfor\nblock\nin\nresponse3\n.\ncontent\n:\nif\nblock\n.\ntype\n==\n\"thinking\"\n:\nprint\n(\nf\"Final thinking:\n{\nblock\n.\nthinking\n}\n\"\n)\nelif\nblock\n.\ntype\n==\n\"text\"\n:\nprint\n(\nf\"Final response:\n{\nblock\n.\ntext\n}\n\"\n)\nIn this example with interleaved thinking:\nClaude thinks about the task initially\nAfter receiving the calculator result, Claude can think again about what that result means\nClaude then decides how to query the database based on the first result\nAfter receiving the database result, Claude thinks once more about both results before formulating a final response\nThe thinking budget is distributed across all thinking blocks within the turn\nThis pattern allows for more sophisticated reasoning chains where each tool’s output informs the next decision.\n​\nExtended thinking with prompt caching\nPrompt caching\nwith thinking has several important considerations:\nThinking block context removal\nThinking blocks from previous turns are removed from context, which can affect cache breakpoints\nWhen continuing conversations with tool use, thinking blocks are cached and count as input tokens when read from cache\nThis creates a tradeoff: while thinking blocks don’t consume context window space visually, they still count toward your input token usage when cached\nIf thinking becomes disabled, requests will fail if you pass thinking content in the current tool use turn. In other contexts, thinking content passed to the API is simply ignored\nCache invalidation patterns\nChanges to thinking parameters (enabled/disabled or budget allocation) invalidate message cache breakpoints\nInterleaved thinking\namplifies cache invalidation, as thinking blocks can occur between multiple\ntool calls\nSystem prompts and tools remain cached despite thinking parameter changes or block removal\nWhile thinking blocks are removed for caching and context calculations, they must be preserved when continuing conversations with\ntool use\n, especially with\ninterleaved thinking\n.\n​\nUnderstanding thinking block caching behavior\nWhen using extended thinking with tool use, thinking blocks exhibit specific caching behavior that affects token counting:\nHow it works:\nCaching only occurs when you make a subsequent request that includes tool results\nWhen the subsequent request is made, the previous conversation history (including thinking blocks) can be cached\nThese cached thinking blocks count as input tokens in your usage metrics when read from the cache\nWhen a non-tool-result user block is included, all previous thinking blocks are ignored and stripped from context\nDetailed example flow:\nRequest 1:\nUser: \"What's the weather in Paris?\"\nResponse 1:\n[thinking_block_1] + [tool_use block 1]\nRequest 2:\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True]\nResponse 2:\n[thinking_block_2] + [text block 2]\nRequest 2 writes a cache of the request content (not the response). The cache includes the original user message, the first thinking block, tool use block, and the tool result.\nRequest 3:\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [thinking_block_2] + [text block 2],\nUser: [Text response, cache=True]\nBecause a non-tool-result user block was included, all previous thinking blocks are ignored. This request will be processed the same as:\nUser: [\"What's the weather in Paris?\"],\nAssistant: [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [text block 2],\nUser: [Text response, cache=True]\nKey points:\nThis caching behavior happens automatically, even without explicit\ncache_control\nmarkers\nThis behavior is consistent whether using regular thinking or interleaved thinking\nSystem prompt caching (preserved when thinking changes)\nPython\nTypeScript\nfrom\nanthropic\nimport\nAnthropic\nimport\nrequests\nfrom\nbs4\nimport\nBeautifulSoup\nclient\n=\nAnthropic\n(\n)\ndef\nfetch_article_content\n(\nurl\n)\n:\nresponse\n=\nrequests\n.\nget\n(\nurl\n)\nsoup\n=\nBeautifulSoup\n(\nresponse\n.\ncontent\n,\n'html.parser'\n)\n# Remove script and style elements\nfor\nscript\nin\nsoup\n(\n[\n\"script\"\n,\n\"style\"\n]\n)\n:\nscript\n.\ndecompose\n(\n)\n# Get text\ntext\n=\nsoup\n.\nget_text\n(\n)\n# Break into lines and remove leading and trailing space on each\nlines\n=\n(\nline\n.\nstrip\n(\n)\nfor\nline\nin\ntext\n.\nsplitlines\n(\n)\n)\n# Break multi-headlines into a line each\nchunks\n=\n(\nphrase\n.\nstrip\n(\n)\nfor\nline\nin\nlines\nfor\nphrase\nin\nline\n.\nsplit\n(\n\"  \"\n)\n)\n# Drop blank lines\ntext\n=\n'\\n'\n.\njoin\n(\nchunk\nfor\nchunk\nin\nchunks\nif\nchunk\n)\nreturn\ntext\n# Fetch the content of the article\nbook_url\n=\n\"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content\n=\nfetch_article_content\n(\nbook_url\n)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT\n=\nbook_content\n[\n:\n5000\n]\nSYSTEM_PROMPT\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\"\n,\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\nLARGE_TEXT\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\nMESSAGES\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze the tone of this passage.\"\n}\n]\n# First request - establish cache\nprint\n(\n\"First request - establishing cache\"\n)\nresponse1\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n4000\n}\n,\nsystem\n=\nSYSTEM_PROMPT\n,\nmessages\n=\nMESSAGES\n)\nprint\n(\nf\"First response usage:\n{\nresponse1\n.\nusage\n}\n\"\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse1\n.\ncontent\n}\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze the characters in this passage.\"\n}\n)\n# Second request - same thinking parameters (cache hit expected)\nprint\n(\n\"\\nSecond request - same thinking parameters (cache hit expected)\"\n)\nresponse2\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n4000\n}\n,\nsystem\n=\nSYSTEM_PROMPT\n,\nmessages\n=\nMESSAGES\n)\nprint\n(\nf\"Second response usage:\n{\nresponse2\n.\nusage\n}\n\"\n)\n# Third request - different thinking parameters (cache miss for messages)\nprint\n(\n\"\\nThird request - different thinking parameters (cache miss for messages)\"\n)\nresponse3\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n8000\n# Changed thinking budget\n}\n,\nsystem\n=\nSYSTEM_PROMPT\n,\n# System prompt remains cached\nmessages\n=\nMESSAGES\n# Messages cache is invalidated\n)\nprint\n(\nf\"Third response usage:\n{\nresponse3\n.\nusage\n}\n\"\n)\nMessages caching (invalidated when thinking changes)\nPython\nTypeScript\nJava\nfrom\nanthropic\nimport\nAnthropic\nimport\nrequests\nfrom\nbs4\nimport\nBeautifulSoup\nclient\n=\nAnthropic\n(\n)\ndef\nfetch_article_content\n(\nurl\n)\n:\nresponse\n=\nrequests\n.\nget\n(\nurl\n)\nsoup\n=\nBeautifulSoup\n(\nresponse\n.\ncontent\n,\n'html.parser'\n)\n# Remove script and style elements\nfor\nscript\nin\nsoup\n(\n[\n\"script\"\n,\n\"style\"\n]\n)\n:\nscript\n.\ndecompose\n(\n)\n# Get text\ntext\n=\nsoup\n.\nget_text\n(\n)\n# Break into lines and remove leading and trailing space on each\nlines\n=\n(\nline\n.\nstrip\n(\n)\nfor\nline\nin\ntext\n.\nsplitlines\n(\n)\n)\n# Break multi-headlines into a line each\nchunks\n=\n(\nphrase\n.\nstrip\n(\n)\nfor\nline\nin\nlines\nfor\nphrase\nin\nline\n.\nsplit\n(\n\"  \"\n)\n)\n# Drop blank lines\ntext\n=\n'\\n'\n.\njoin\n(\nchunk\nfor\nchunk\nin\nchunks\nif\nchunk\n)\nreturn\ntext\n# Fetch the content of the article\nbook_url\n=\n\"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content\n=\nfetch_article_content\n(\nbook_url\n)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT\n=\nbook_content\n[\n:\n5000\n]\n# No system prompt - caching in messages instead\nMESSAGES\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\nLARGE_TEXT\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}\n,\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Analyze the tone of this passage.\"\n}\n]\n}\n]\n# First request - establish cache\nprint\n(\n\"First request - establishing cache\"\n)\nresponse1\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n4000\n}\n,\nmessages\n=\nMESSAGES\n)\nprint\n(\nf\"First response usage:\n{\nresponse1\n.\nusage\n}\n\"\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse1\n.\ncontent\n}\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze the characters in this passage.\"\n}\n)\n# Second request - same thinking parameters (cache hit expected)\nprint\n(\n\"\\nSecond request - same thinking parameters (cache hit expected)\"\n)\nresponse2\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n4000\n# Same thinking budget\n}\n,\nmessages\n=\nMESSAGES\n)\nprint\n(\nf\"Second response usage:\n{\nresponse2\n.\nusage\n}\n\"\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse2\n.\ncontent\n}\n)\nMESSAGES\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze the setting in this passage.\"\n}\n)\n# Third request - different thinking budget (cache miss expected)\nprint\n(\n\"\\nThird request - different thinking budget (cache miss expected)\"\n)\nresponse3\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-sonnet-4-20250514\"\n,\nmax_tokens\n=\n20000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n8000\n# Different thinking budget breaks cache\n}\n,\nmessages\n=\nMESSAGES\n)\nprint\n(\nf\"Third response usage:\n{\nresponse3\n.\nusage\n}\n\"\n)\nHere is the output of the script (you may see slightly different numbers)\nFirst request - establishing cache\nFirst response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }\nSecond request - same thinking parameters (cache hit expected)\nSecond response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }\nThird request - different thinking budget (cache miss expected)\nThird response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }\nThis example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget_tokens increased from 4000 to 8000)\ninvalidates the cache\n. The third request shows no cache hit with\ncache_creation_input_tokens=1370\nand\ncache_read_input_tokens=0\n, proving that message-based caching is invalidated when thinking parameters change.\n​\nMax tokens and context window size with extended thinking\nIn older Claude models (prior to Claude Sonnet 3.7), if the sum of prompt tokens and\nmax_tokens\nexceeded the model’s context window, the system would automatically adjust\nmax_tokens\nto fit within the context limit. This meant you could set a large\nmax_tokens\nvalue and the system would silently reduce it as needed.\nWith Claude 3.7 and 4 models,\nmax_tokens\n(which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens +\nmax_tokens\nexceeds the context window size.\nYou can read through our\nguide on context windows\nfor a more thorough deep dive.\n​\nThe context window with extended thinking\nWhen calculating context window usage with thinking enabled, there are some considerations to be aware of:\nThinking blocks from previous turns are stripped and not counted towards your context window\nCurrent turn thinking counts towards your\nmax_tokens\nlimit for that turn\nThe diagram below demonstrates the specialized token management when extended thinking is enabled:\nThe effective context window is calculated as:\ncontext window =\n(current input tokens - previous thinking tokens) +\n(thinking tokens + encrypted thinking tokens + text output tokens)\nWe recommend using the\ntoken counting API\nto get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.\n​\nThe context window with extended thinking and tool use\nWhen using extended thinking with tool use, thinking blocks must be explicitly preserved and returned with the tool results.\nThe effective context window calculation for extended thinking with tool use becomes:\ncontext window =\n(current input tokens + previous thinking tokens + tool use tokens) +\n(thinking tokens + encrypted thinking tokens + text output tokens)\nThe diagram below illustrates token management for extended thinking with tool use:\n​\nManaging tokens with extended thinking\nGiven the context window and\nmax_tokens\nbehavior with extended thinking Claude 3.7 and 4 models, you may need to:\nMore actively monitor and manage your token usage\nAdjust\nmax_tokens\nvalues as your prompt length changes\nPotentially use the\ntoken counting endpoints\nmore frequently\nBe aware that previous thinking blocks don’t accumulate in your context window\nThis change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.\n​\nThinking encryption\nFull thinking content is encrypted and returned in the\nsignature\nfield. This field is used to verify that thinking blocks were generated by Claude when passed back to the API.\nIt is only strictly necessary to send back thinking blocks when using\ntools with extended thinking\n. Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back.\nIf sending back thinking blocks, we recommend passing everything back as you received it for consistency and to avoid potential issues.\nHere are some important considerations on thinking encryption:\nWhen\nstreaming responses\n, the signature is added via a\nsignature_delta\ninside a\ncontent_block_delta\nevent just before the\ncontent_block_stop\nevent.\nsignature\nvalues are significantly longer in Claude 4 than in previous models.\nThe\nsignature\nfield is an opaque field and should not be interpreted or parsed - it exists solely for verification purposes.\nsignature\nvalues are compatible across platforms (Anthropic APIs,\nAmazon Bedrock\n, and\nVertex AI\n). Values generated on one platform will be compatible with another.\n​\nThinking redaction\nOccasionally Claude’s internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the\nthinking\nblock and return it to you as a\nredacted_thinking\nblock.\nredacted_thinking\nblocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.\nWhen building customer-facing applications that use extended thinking:\nBe aware that redacted thinking blocks contain encrypted content that isn’t human-readable\nConsider providing a simple explanation like: “Some of Claude’s internal reasoning has been automatically encrypted for safety reasons. This doesn’t affect the quality of responses.”\nIf showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks\nBe transparent that using extended thinking features may occasionally result in some reasoning being encrypted\nImplement appropriate error handling to gracefully manage redacted thinking without breaking your UI\nHere’s an example showing both normal and redacted thinking blocks:\n{\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"thinking\"\n,\n\"thinking\"\n:\n\"Let me analyze this step by step...\"\n,\n\"signature\"\n:\n\"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n}\n,\n{\n\"type\"\n:\n\"redacted_thinking\"\n,\n\"data\"\n:\n\"EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ...\"\n}\n,\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Based on my analysis...\"\n}\n]\n}\nSeeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.\nIf you need to test redacted thinking handling in your application, you can use this special test string as your prompt:\nANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\nWhen passing\nthinking\nand\nredacted_thinking\nblocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn. This is critical for maintaining the model’s reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the\nPreserving thinking blocks\nsection above.\nExample: Working with redacted thinking blocks\nThis example demonstrates how to handle\nredacted_thinking\nblocks that may appear in responses when Claude’s internal reasoning contains content flagged by safety systems:\nPython\nTypeScript\nJava\nTry in Console\nimport\nanthropic\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\n# Using a special prompt that triggers redacted thinking (for demonstration purposes only)\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-7-sonnet-20250219\"\n,\nmax_tokens\n=\n16000\n,\nthinking\n=\n{\n\"type\"\n:\n\"enabled\"\n,\n\"budget_tokens\"\n:\n10000\n}\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n}\n]\n)\n# Identify redacted thinking blocks\nhas_redacted_thinking\n=\nany\n(\nblock\n.\ntype\n==\n\"redacted_thinking\"\nfor\nblock\nin\nresponse\n.\ncontent\n)\nif\nhas_redacted_thinking\n:\nprint\n(\n\"Response contains redacted thinking blocks\"\n)\n# These blocks are still usable in subsequent requests\n# Extract all blocks (both redacted and non-redacted)\nall_thinking_blocks\n=\n[\nblock\nfor\nblock\nin\nresponse\n.\ncontent\nif\nblock\n.\ntype\nin\n[\n\"thinking\"\n,\n\"redacted_thinking\"\n]\n]\n# When passing to subsequent requests, include all blocks without modification\n# This preserves the integrity of Claude's reasoning\nprint\n(\nf\"Found\n{\nlen\n(\nall_thinking_blocks\n)\n}\nthinking blocks total\"\n)\nprint\n(\nf\"These blocks are still billable as output tokens\"\n)\n​\nDifferences in thinking across model versions\nThe Messages API handles thinking differently across Claude Sonnet 3.7 and Claude 4 models, primarily in redaction and summarization behavior.\nSee the table below for a condensed comparison:\nFeature\nClaude Sonnet 3.7\nClaude 4 Models\nThinking Output\nReturns full thinking output\nReturns summarized thinking\nInterleaved Thinking\nNot supported\nSupported with\ninterleaved-thinking-2025-05-14\nbeta header\n​\nPricing\nExtended thinking uses the standard token pricing scheme:\nModel\nBase Input Tokens\nCache Writes\nCache Hits\nOutput Tokens\nClaude Opus 4\n$15 / MTok\n$18.75 / MTok\n$1.50 / MTok\n$75 / MTok\nClaude Sonnet 4\n$3 / MTok\n$3.75 / MTok\n$0.30 / MTok\n$15 / MTok\nClaude Sonnet 3.7\n$3 / MTok\n$3.75 / MTok\n$0.30 / MTok\n$15 / MTok\nThe thinking process incurs charges for:\nTokens used during thinking (output tokens)\nThinking blocks from the last assistant turn included in subsequent requests (input tokens)\nStandard text output tokens\nWhen extended thinking is enabled, a specialized system prompt is automatically included to support this feature.\nWhen using summarized thinking:\nInput tokens\n: Tokens in your original request (excludes thinking tokens from previous turns)\nOutput tokens (billed)\n: The original thinking tokens that Claude generated internally\nOutput tokens (visible)\n: The summarized thinking tokens you see in the response\nNo charge\n: Tokens used to generate the summary\nThe billed output token count will\nnot\nmatch the visible token count in the response. You are billed for the full thinking process, not the summary you see.\n​\nBest practices and considerations for extended thinking\n​\nWorking with thinking budgets\nBudget optimization:\nThe minimum budget is 1,024 tokens. We suggest starting at the minimum and increasing the thinking budget incrementally to find the optimal range for your use case. Higher token counts enable more comprehensive reasoning but with diminishing returns depending on the task. Increasing the budget can improve response quality at the tradeoff of increased latency. For critical tasks, test different settings to find the optimal balance. Note that the thinking budget is a target rather than a strict limit—actual token usage may vary based on the task.\nStarting points:\nStart with larger thinking budgets (16k+ tokens) for complex tasks and adjust based on your needs.\nLarge budgets:\nFor thinking budgets above 32k, we recommend using\nbatch processing\nto avoid networking issues. Requests pushing the model to think above 32k tokens causes long running requests that might run up against system timeouts and open connection limits.\nToken usage tracking:\nMonitor thinking token usage to optimize costs and performance.\n​\nPerformance considerations\nResponse times:\nBe prepared for potentially longer response times due to the additional processing required for the reasoning process. Factor in that generating thinking blocks may increase overall response time.\nStreaming requirements:\nStreaming is required when\nmax_tokens\nis greater than 21,333. When streaming, be prepared to handle both thinking and text content blocks as they arrive.\n​\nFeature compatibility\nThinking isn’t compatible with\ntemperature\nor\ntop_k\nmodifications as well as\nforced tool use\n.\nWhen thinking is enabled, you can set\ntop_p\nto values between 1 and 0.95.\nYou cannot pre-fill responses when thinking is enabled.\nChanges to the thinking budget invalidate cached prompt prefixes that include messages. However, cached system prompts and tool definitions will continue to work when thinking parameters change.\n​\nUsage guidelines\nTask selection:\nUse extended thinking for particularly complex tasks that benefit from step-by-step reasoning like math, coding, and analysis.\nContext handling:\nYou do not need to remove previous thinking blocks yourself. The Anthropic API automatically ignores thinking blocks from previous turns and they are not included when calculating context usage.\nPrompt engineering:\nReview our\nextended thinking prompting tips\nif you want to maximize Claude’s thinking capabilities.\n​\nNext steps\nTry the extended thinking cookbook\nExplore practical examples of thinking in our cookbook.\nExtended thinking prompting tips\nLearn prompt engineering best practices for extended thinking.\nWas this page helpful?\nYes\nNo\nPrompt caching\nStreaming Messages\nOn this page\nSupported models\nHow extended thinking works\nHow to use extended thinking\nSummarized thinking\nStreaming thinking\nExtended thinking with tool use\nPreserving thinking blocks\nInterleaved thinking\nExtended thinking with prompt caching\nUnderstanding thinking block caching behavior\nMax tokens and context window size with extended thinking\nThe context window with extended thinking\nThe context window with extended thinking and tool use\nManaging tokens with extended thinking\nThinking encryption\nThinking redaction\nDifferences in thinking across model versions\nPricing\nBest practices and considerations for extended thinking\nWorking with thinking budgets\nPerformance considerations\nFeature compatibility\nUsage guidelines\nNext steps",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/multilingual-support",
      "text": "Multilingual support"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/token-counting",
      "text": "Token counting"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/embeddings",
      "text": "Embeddings"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/vision",
      "text": "Vision"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/pdf-support",
      "text": "PDF support"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/files",
      "text": "Files API"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/claude-for-sheets",
      "text": "Google Sheets add-on"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/implement-tool-use",
      "text": "How to implement tool use"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/token-efficient-tool-use",
      "text": "Token-efficient tool use"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/fine-grained-tool-streaming",
      "text": "Fine-grained tool streaming"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/bash-tool",
      "text": "Bash tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool",
      "text": "Code execution tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool",
      "text": "Computer use tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/text-editor-tool",
      "text": "Text editor tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/web-search-tool",
      "text": "Web search tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/mcp-connector",
      "text": "MCP connector"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/agents-and-tools/remote-mcp-servers",
      "text": "Remote MCP servers"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/ticket-routing",
      "text": "Ticket routing"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/customer-support-chat",
      "text": "Customer support agent"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/content-moderation",
      "text": "Content moderation"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/legal-summarization",
      "text": "Legal summarization"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
      "text": "Overview"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices",
      "text": "Claude 4 best practices"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator",
      "text": "Prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables",
      "text": "Use prompt templates"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver",
      "text": "Prompt improver"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct",
      "text": "Be clear and direct"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting",
      "text": "Use examples (multishot prompting)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "text": "Let Claude think (CoT)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "text": "Use XML tags"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts",
      "text": "Give Claude a role (system prompts)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response",
      "text": "Prefill Claude's response"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts",
      "text": "Chain complex prompts"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "text": "Long context tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips",
      "text": "Extended thinking tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/define-success",
      "text": "Define success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
      "text": "Develop test cases"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
      "text": "Using the Evaluation Tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency",
      "text": "Reducing latency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations",
      "text": "Reduce hallucinations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    },
    {
      "url": "https://docs.anthropic.com/_sites/docs.anthropic.com/en/docs/build-with-claude/extended-thinking",
      "text": "Differences in thinking across model versions"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:38.061619",
    "word_count": 7029,
    "link_count": 48,
    "content_length": 44032
  }
}