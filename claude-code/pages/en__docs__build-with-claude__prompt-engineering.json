{
  "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering",
  "title": "Prompt engineering overview - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nPrompt engineering\nPrompt engineering overview\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nWhile these tips apply broadly to all Claude models, you can find prompting tips specific to extended thinking models\nhere\n.\n​\nBefore prompt engineering\nThis guide assumes that you have:\nA clear definition of the success criteria for your use case\nSome ways to empirically test against those criteria\nA first draft prompt you want to improve\nIf not, we highly suggest you spend time establishing that first. Check out\nDefine your success criteria\nand\nCreate strong empirical evaluations\nfor tips and guidance.\nPrompt generator\nDon’t have a first draft prompt? Try the prompt generator in the Anthropic Console!\n​\nWhen to prompt engineer\nThis guide focuses on success criteria that are controllable through prompt engineering.\nNot every success criteria or failing eval is best solved by prompt engineering. For example, latency and cost can be sometimes more easily improved by selecting a different model.\nPrompting vs. finetuning\nPrompt engineering is far faster than other methods of model behavior control, such as finetuning, and can often yield leaps in performance in far less time. Here are some reasons to consider prompt engineering over finetuning:\nResource efficiency\n: Fine-tuning requires high-end GPUs and large memory, while prompt engineering only needs text input, making it much more resource-friendly.\nCost-effectiveness\n: For cloud-based AI services, fine-tuning incurs significant costs. Prompt engineering uses the base model, which is typically cheaper.\nMaintaining model updates\n: When providers update models, fine-tuned versions might need retraining. Prompts usually work across versions without changes.\nTime-saving\n: Fine-tuning can take hours or even days. In contrast, prompt engineering provides nearly instantaneous results, allowing for quick problem-solving.\nMinimal data needs\n: Fine-tuning needs substantial task-specific, labeled data, which can be scarce or expensive. Prompt engineering works with few-shot or even zero-shot learning.\nFlexibility & rapid iteration\n: Quickly try various approaches, tweak prompts, and see immediate results. This rapid experimentation is difficult with fine-tuning.\nDomain adaptation\n: Easily adapt models to new domains by providing domain-specific context in prompts, without retraining.\nComprehension improvements\n: Prompt engineering is far more effective than finetuning at helping models better understand and utilize external content such as retrieved documents\nPreserves general knowledge\n: Fine-tuning risks catastrophic forgetting, where the model loses general knowledge. Prompt engineering maintains the model’s broad capabilities.\nTransparency\n: Prompts are human-readable, showing exactly what information the model receives. This transparency aids in understanding and debugging.\n​\nHow to prompt engineer\nThe prompt engineering pages in this section have been organized from most broadly effective techniques to more specialized techniques. When troubleshooting performance, we suggest you try these techniques in order, although the actual impact of each technique will depend on your use case.\nPrompt generator\nBe clear and direct\nUse examples (multishot)\nLet Claude think (chain of thought)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude’s response\nChain complex prompts\nLong context tips\n​\nPrompt engineering tutorial\nIf you’re an interactive learner, you can dive into our interactive tutorials instead!\nGitHub prompting tutorial\nAn example-filled tutorial that covers the prompt engineering concepts found in our docs.\nGoogle Sheets prompting tutorial\nA lighter weight version of our prompt engineering tutorial via an interactive spreadsheet.\nWas this page helpful?\nYes\nNo\nLegal summarization\nClaude 4 best practices\nOn this page\nBefore prompt engineering\nWhen to prompt engineer\nHow to prompt engineer\nPrompt engineering tutorial",
  "links": [],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:59.127484",
    "word_count": 778,
    "link_count": 0,
    "content_length": 5473
  }
}