{
  "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
  "title": "Using the Evaluation Tool - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nTest & evaluate\nUsing the Evaluation Tool\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\n​\nAccessing the Evaluate Feature\nTo get started with the Evaluation tool:\nOpen the Anthropic Console and navigate to the prompt editor.\nAfter composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\n​\nGenerating Prompts\nThe Console offers a built-in\nprompt generator\npowered by Claude Opus 4:\n1\nClick 'Generate Prompt'\nClicking the ‘Generate Prompt’ helper tool will open a modal that allows you to enter your task information.\n2\nDescribe your task\nDescribe your desired task (e.g., “Triage inbound customer support requests”) with as much or as little detail as you desire. The more context you include, the more Claude can tailor its generated prompt to your specific needs.\n3\nGenerate your prompt\nClicking the orange ‘Generate Prompt’ button at the bottom will have Claude generate a high quality prompt for you. You can then further improve those prompts using the Evaluation screen in the Console.\nThis feature makes it easier to create prompts with the appropriate variable syntax for evaluation.\n​\nCreating Test Cases\nWhen you access the Evaluation screen, you have several options to create test cases:\nClick the ’+ Add Row’ button at the bottom left to manually add a case.\nUse the ‘Generate Test Case’ feature to have Claude automatically generate test cases for you.\nImport test cases from a CSV file.\nTo use the ‘Generate Test Case’ feature:\n1\nClick on 'Generate Test Case'\nClaude will generate test cases for you, one row at a time for each time you click the button.\n2\nEdit generation logic (optional)\nYou can also edit the test case generation logic by clicking on the arrow dropdown to the right of the ‘Generate Test Case’ button, then on ‘Show generation logic’ at the top of the Variables window that pops up. You may have to click `Generate’ on the top right of this window to populate initial generation logic.\nEditing this allows you to customize and fine tune the test cases that Claude generates to greater precision and specificity.\nHere’s an example of a populated Evaluation screen with several test cases:\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\n​\nTips for Effective Evaluation\nPrompt Structure for Evaluation\nTo make the most of the Evaluation tool, structure your prompts with clear input and output formats. For example:\nIn this task, you will generate a cute one sentence story that incorporates two elements: a color and a sound.\nThe color to include in the story is:\n<color>\n{{COLOR}}\n</color>\nThe sound to include in the story is:\n<sound>\n{{SOUND}}\n</sound>\nHere are the steps to generate the story:\n1. Think of an object, animal, or scene that is commonly associated with the color provided. For example, if the color is \"blue\", you might think of the sky, the ocean, or a bluebird.\n2. Imagine a simple action, event or scene involving the colored object/animal/scene you identified and the sound provided. For instance, if the color is \"blue\" and the sound is \"whistle\", you might imagine a bluebird whistling a tune.\n3. Describe the action, event or scene you imagined in a single, concise sentence. Focus on making the sentence cute, evocative and imaginative. For example: \"A cheerful bluebird whistled a merry melody as it soared through the azure sky.\"\nPlease keep your story to one sentence only. Aim to make that sentence as charming and engaging as possible while naturally incorporating the given color and sound.\nWrite your completed one sentence story inside <story> tags.\nThis structure makes it easy to vary inputs ({{COLOR}} and {{SOUND}}) and evaluate outputs consistently.\nUse the ‘Generate a prompt’ helper tool in the Console to quickly create prompts with the appropriate variable syntax for evaluation.\n​\nUnderstanding and comparing results\nThe Evaluation tool offers several features to help you refine your prompts:\nSide-by-side comparison\n: Compare the outputs of two or more prompts to quickly see the impact of your changes.\nQuality grading\n: Grade response quality on a 5-point scale to track improvements in response quality per prompt.\nPrompt versioning\n: Create new versions of your prompt and re-run the test suite to quickly iterate and improve results.\nBy reviewing results across test cases and comparing different prompt versions, you can spot patterns and make informed adjustments to your prompt more efficiently.\nStart evaluating your prompts today to build more robust AI applications with Claude!\nWas this page helpful?\nYes\nNo\nDevelop test cases\nReducing latency\nOn this page\nAccessing the Evaluate Feature\nGenerating Prompts\nCreating Test Cases\nTips for Effective Evaluation\nUnderstanding and comparing results",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:51.801027",
    "word_count": 1026,
    "link_count": 5,
    "content_length": 6504
  }
}