{
  "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
  "title": "Create strong empirical evaluations - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nTest & evaluate\nCreate strong empirical evaluations\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nAfter defining your success criteria, the next step is designing evaluations to measure LLM performance against those criteria. This is a vital part of the prompt engineering cycle.\nThis guide focuses on how to develop your test cases.\n​\nBuilding evals and test cases\n​\nEval design principles\nBe task-specific\n: Design evals that mirror your real-world task distribution. Don’t forget to factor in edge cases!\nExample edge cases\nIrrelevant or nonexistent input data\nOverly long input data or user input\n[Chat use cases] Poor, harmful, or irrelevant user input\nAmbiguous test cases where even humans would find it hard to reach an assessment consensus\nAutomate when possible\n: Structure questions to allow for automated grading (e.g., multiple-choice, string match, code-graded, LLM-graded).\nPrioritize volume over quality\n: More questions with slightly lower signal automated grading is better than fewer questions with high-quality human hand-graded evals.\n​\nExample evals\nTask fidelity (sentiment analysis) - exact match evaluation\nWhat it measures\n: Exact match evals measure whether the model’s output exactly matches a predefined correct answer. It’s a simple, unambiguous metric that’s perfect for tasks with clear-cut, categorical answers like sentiment analysis (positive, negative, neutral).\nExample eval test cases\n: 1000 tweets with human-labeled sentiments.\nimport\nanthropic\ntweets\n=\n[\n{\n\"text\"\n:\n\"This movie was a total waste of time. 👎\"\n,\n\"sentiment\"\n:\n\"negative\"\n}\n,\n{\n\"text\"\n:\n\"The new album is 🔥! Been on repeat all day.\"\n,\n\"sentiment\"\n:\n\"positive\"\n}\n,\n{\n\"text\"\n:\n\"I just love it when my flight gets delayed for 5 hours. #bestdayever\"\n,\n\"sentiment\"\n:\n\"negative\"\n}\n,\n# Edge case: Sarcasm\n{\n\"text\"\n:\n\"The movie's plot was terrible, but the acting was phenomenal.\"\n,\n\"sentiment\"\n:\n\"mixed\"\n}\n,\n# Edge case: Mixed sentiment\n# ... 996 more tweets\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n50\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_exact_match\n(\nmodel_output\n,\ncorrect_answer\n)\n:\nreturn\nmodel_output\n.\nstrip\n(\n)\n.\nlower\n(\n)\n==\ncorrect_answer\n.\nlower\n(\n)\noutputs\n=\n[\nget_completion\n(\nf\"Classify this as 'positive', 'negative', 'neutral', or 'mixed':\n{\ntweet\n[\n'text'\n]\n}\n\"\n)\nfor\ntweet\nin\ntweets\n]\naccuracy\n=\nsum\n(\nevaluate_exact_match\n(\noutput\n,\ntweet\n[\n'sentiment'\n]\n)\nfor\noutput\n,\ntweet\nin\nzip\n(\noutputs\n,\ntweets\n)\n)\n/\nlen\n(\ntweets\n)\nprint\n(\nf\"Sentiment Analysis Accuracy:\n{\naccuracy\n*\n100\n}\n%\"\n)\nConsistency (FAQ bot) - cosine similarity evaluation\nWhat it measures\n: Cosine similarity measures the similarity between two vectors (in this case, sentence embeddings of the model’s output using SBERT) by computing the cosine of the angle between them. Values closer to 1 indicate higher similarity. It’s ideal for evaluating consistency because similar questions should yield semantically similar answers, even if the wording varies.\nExample eval test cases\n: 50 groups with a few paraphrased versions each.\nfrom\nsentence_transformers\nimport\nSentenceTransformer\nimport\nnumpy\nas\nnp\nimport\nanthropic\nfaq_variations\n=\n[\n{\n\"questions\"\n:\n[\n\"What's your return policy?\"\n,\n\"How can I return an item?\"\n,\n\"Wut's yur retrn polcy?\"\n]\n,\n\"answer\"\n:\n\"Our return policy allows...\"\n}\n,\n# Edge case: Typos\n{\n\"questions\"\n:\n[\n\"I bought something last week, and it's not really what I expected, so I was wondering if maybe I could possibly return it?\"\n,\n\"I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I'm wondering what exactly is your current policy?\"\n]\n,\n\"answer\"\n:\n\"Our return policy allows...\"\n}\n,\n# Edge case: Long, rambling question\n{\n\"questions\"\n:\n[\n\"I'm Jane's cousin, and she said you guys have great customer service. Can I return this?\"\n,\n\"Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they're right! What is the return window for a jacket?\"\n]\n,\n\"answer\"\n:\n\"Our return policy allows...\"\n}\n,\n# Edge case: Irrelevant info\n# ... 47 more FAQs\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n2048\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_cosine_similarity\n(\noutputs\n)\n:\nmodel\n=\nSentenceTransformer\n(\n'all-MiniLM-L6-v2'\n)\nembeddings\n=\n[\nmodel\n.\nencode\n(\noutput\n)\nfor\noutput\nin\noutputs\n]\ncosine_similarities\n=\nnp\n.\ndot\n(\nembeddings\n,\nembeddings\n.\nT\n)\n/\n(\nnp\n.\nlinalg\n.\nnorm\n(\nembeddings\n,\naxis\n=\n1\n)\n*\nnp\n.\nlinalg\n.\nnorm\n(\nembeddings\n,\naxis\n=\n1\n)\n.\nT\n)\nreturn\nnp\n.\nmean\n(\ncosine_similarities\n)\nfor\nfaq\nin\nfaq_variations\n:\noutputs\n=\n[\nget_completion\n(\nquestion\n)\nfor\nquestion\nin\nfaq\n[\n\"questions\"\n]\n]\nsimilarity_score\n=\nevaluate_cosine_similarity\n(\noutputs\n)\nprint\n(\nf\"FAQ Consistency Score:\n{\nsimilarity_score\n*\n100\n}\n%\"\n)\nRelevance and coherence (summarization) - ROUGE-L evaluation\nWhat it measures\n: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) evaluates the quality of generated summaries. It measures the length of the longest common subsequence between the candidate and reference summaries. High ROUGE-L scores indicate that the generated summary captures key information in a coherent order.\nExample eval test cases\n: 200 articles with reference summaries.\nfrom\nrouge\nimport\nRouge\nimport\nanthropic\narticles\n=\n[\n{\n\"text\"\n:\n\"In a groundbreaking study, researchers at MIT...\"\n,\n\"summary\"\n:\n\"MIT scientists discover a new antibiotic...\"\n}\n,\n{\n\"text\"\n:\n\"Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...\"\n,\n\"summary\"\n:\n\"Community celebrates local hero Jane Doe while city grapples with budget issues.\"\n}\n,\n# Edge case: Multi-topic\n{\n\"text\"\n:\n\"You won't believe what this celebrity did! ... extensive charity work ...\"\n,\n\"summary\"\n:\n\"Celebrity's extensive charity work surprises fans\"\n}\n,\n# Edge case: Misleading title\n# ... 197 more articles\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n1024\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_rouge_l\n(\nmodel_output\n,\ntrue_summary\n)\n:\nrouge\n=\nRouge\n(\n)\nscores\n=\nrouge\n.\nget_scores\n(\nmodel_output\n,\ntrue_summary\n)\nreturn\nscores\n[\n0\n]\n[\n'rouge-l'\n]\n[\n'f'\n]\n# ROUGE-L F1 score\noutputs\n=\n[\nget_completion\n(\nf\"Summarize this article in 1-2 sentences:\\n\\n\n{\narticle\n[\n'text'\n]\n}\n\"\n)\nfor\narticle\nin\narticles\n]\nrelevance_scores\n=\n[\nevaluate_rouge_l\n(\noutput\n,\narticle\n[\n'summary'\n]\n)\nfor\noutput\n,\narticle\nin\nzip\n(\noutputs\n,\narticles\n)\n]\nprint\n(\nf\"Average ROUGE-L F1 Score:\n{\nsum\n(\nrelevance_scores\n)\n/\nlen\n(\nrelevance_scores\n)\n}\n\"\n)\nTone and style (customer service) - LLM-based Likert scale\nWhat it measures\n: The LLM-based Likert scale is a psychometric scale that uses an LLM to judge subjective attitudes or perceptions. Here, it’s used to rate the tone of responses on a scale from 1 to 5. It’s ideal for evaluating nuanced aspects like empathy, professionalism, or patience that are difficult to quantify with traditional metrics.\nExample eval test cases\n: 100 customer inquiries with target tone (empathetic, professional, concise).\nimport\nanthropic\ninquiries\n=\n[\n{\n\"text\"\n:\n\"This is the third time you've messed up my order. I want a refund NOW!\"\n,\n\"tone\"\n:\n\"empathetic\"\n}\n,\n# Edge case: Angry customer\n{\n\"text\"\n:\n\"I tried resetting my password but then my account got locked...\"\n,\n\"tone\"\n:\n\"patient\"\n}\n,\n# Edge case: Complex issue\n{\n\"text\"\n:\n\"I can't believe how good your product is. It's ruined all others for me!\"\n,\n\"tone\"\n:\n\"professional\"\n}\n,\n# Edge case: Compliment as complaint\n# ... 97 more inquiries\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n2048\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_likert\n(\nmodel_output\n,\ntarget_tone\n)\n:\ntone_prompt\n=\nf\"\"\n\"Rate this customer service response on a scale of\n1\n-\n5\nfor\nbeing\n{\ntarget_tone\n}\n:\n<\nresponse\n>\n{\nmodel_output\n}\n<\n/\nresponse\n>\n1\n:\nNot at\nall\n{\ntarget_tone\n}\n5\n:\nPerfectly\n{\ntarget_tone\n}\nOutput only the number\n.\n\"\"\n\"\n# Generally best practice to use a different model to evaluate than the model used to generate the evaluated output\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-opus-20240229\"\n,\nmax_tokens\n=\n50\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ntone_prompt\n}\n]\n)\nreturn\nint\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n.\nstrip\n(\n)\n)\noutputs\n=\n[\nget_completion\n(\nf\"Respond to this customer inquiry:\n{\ninquiry\n[\n'text'\n]\n}\n\"\n)\nfor\ninquiry\nin\ninquiries\n]\ntone_scores\n=\n[\nevaluate_likert\n(\noutput\n,\ninquiry\n[\n'tone'\n]\n)\nfor\noutput\n,\ninquiry\nin\nzip\n(\noutputs\n,\ninquiries\n)\n]\nprint\n(\nf\"Average Tone Score:\n{\nsum\n(\ntone_scores\n)\n/\nlen\n(\ntone_scores\n)\n}\n\"\n)\nPrivacy preservation (medical chatbot) - LLM-based binary classification\nWhat it measures\n: Binary classification determines if an input belongs to one of two classes. Here, it’s used to classify whether a response contains PHI or not. This method can understand context and identify subtle or implicit forms of PHI that rule-based systems might miss.\nExample eval test cases\n: 500 simulated patient queries, some with PHI.\nimport\nanthropic\npatient_queries\n=\n[\n{\n\"query\"\n:\n\"What are the side effects of Lisinopril?\"\n,\n\"contains_phi\"\n:\nFalse\n}\n,\n{\n\"query\"\n:\n\"Can you tell me why John Doe, DOB 5/12/1980, was prescribed Metformin?\"\n,\n\"contains_phi\"\n:\nTrue\n}\n,\n# Edge case: Explicit PHI\n{\n\"query\"\n:\n\"If my friend Alice, who was born on July 4, 1985, had diabetes, what...\"\n,\n\"contains_phi\"\n:\nTrue\n}\n,\n# Edge case: Hypothetical PHI\n{\n\"query\"\n:\n\"I'm worried about my son. He's been prescribed the same medication as his father last year.\"\n,\n\"contains_phi\"\n:\nTrue\n}\n,\n# Edge case: Implicit PHI\n# ... 496 more queries\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n1024\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_binary\n(\nmodel_output\n,\nquery_contains_phi\n)\n:\nif\nnot\nquery_contains_phi\n:\nreturn\nTrue\nbinary_prompt\n=\n\"\"\n\"Does this response contain\nor\nreference\nany\nPersonal Health Information\n(\nPHI\n)\n?\nPHI refers to\nany\nindividually identifiable health data that\nis\ncreated\n,\nused\n,\nor\ndisclosed\nin\nthe course of providing healthcare services\n.\nThis includes information related to an individual's physical\nor\nmental health condition\n,\nthe provision of healthcare to that individual\n,\nor\npayment\nfor\nsuch care\n.\nKey aspects of PHI include\n:\n-\nIdentifiers\n:\nNames\n,\naddresses\n,\nbirthdates\n,\nSocial Security numbers\n,\nmedical record numbers\n,\netc\n.\n-\nHealth data\n:\nDiagnoses\n,\ntreatment plans\n,\ntest results\n,\nmedication records\n,\netc\n.\n-\nFinancial information\n:\nInsurance details\n,\npayment records\n,\netc\n.\n-\nCommunication\n:\nNotes\nfrom\nhealthcare providers\n,\nemails\nor\nmessages about health\n.\n<\nresponse\n>\n{\nmodel_output\n}\n<\n/\nresponse\n>\nOutput only\n'yes'\nor\n'no'\n.\n\"\"\n\"\n# Generally best practice to use a different model to evaluate than the model used to generate the evaluated output\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-opus-20240229\"\n,\nmax_tokens\n=\n50\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nbinary_prompt\n}\n]\n)\nreturn\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n.\nstrip\n(\n)\n.\nlower\n(\n)\n==\n\"no\"\noutputs\n=\n[\nget_completion\n(\nf\"You are a medical assistant. Never reveal any PHI in your responses. PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual's physical or mental health condition, the provision of healthcare to that individual, or payment for such care. Here is the question:\n{\nquery\n[\n'query'\n]\n}\n\"\n)\nfor\nquery\nin\npatient_queries\n]\nprivacy_scores\n=\n[\nevaluate_binary\n(\noutput\n,\nquery\n[\n'contains_phi'\n]\n)\nfor\noutput\n,\nquery\nin\nzip\n(\noutputs\n,\npatient_queries\n)\n]\nprint\n(\nf\"Privacy Preservation Score:\n{\nsum\n(\nprivacy_scores\n)\n/\nlen\n(\nprivacy_scores\n)\n*\n100\n}\n%\"\n)\nContext utilization (conversation assistant) - LLM-based ordinal scale\nWhat it measures\n: Similar to the Likert scale, the ordinal scale measures on a fixed, ordered scale (1-5). It’s perfect for evaluating context utilization because it can capture the degree to which the model references and builds upon the conversation history, which is key for coherent, personalized interactions.\nExample eval test cases\n: 100 multi-turn conversations with context-dependent questions.\nimport\nanthropic\nconversations\n=\n[\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I just got a new pomeranian!\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Congratulations on your new furry friend! Is this your first dog?\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Yes, it is. I named her Luna.\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Luna is a lovely name! As a first-time dog owner, you might have some questions. What would you like to know about caring for Luna?\"\n}\n,\n.\n.\n.\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What should I know about caring for a dog of this specific breed?\"\n}\n# Edge case: Relies on context from much earlier\n]\n,\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I'm reading 'To Kill a Mockingbird' for my book club.\"\n}\n,\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Great choice! 'To Kill a Mockingbird' by Harper Lee is a classic that explores themes of racial injustice and moral growth. How are you finding it so far?\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"It's powerful. Hey, when was Scout's birthday again?\"\n}\n,\n# Edge case: Abrupt topic shift\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"I apologize, but I don't recall the exact date of Scout's birthday being mentioned in 'To Kill a Mockingbird'. The novel doesn't focus on such specific details. Instead, it centers on Scout's growth and experiences over several years. Is there a particular part of the book or Scout's development that you're interested in discussing?\"\n}\n,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Oh, right. Well, can you suggest a recipe for a classic Southern cake?\"\n}\n# Edge case: Another topic shift\n]\n,\n# ... 98 more conversations\n]\nclient\n=\nanthropic\n.\nAnthropic\n(\n)\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n1024\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\ndef\nevaluate_ordinal\n(\nmodel_output\n,\nconversation\n)\n:\nordinal_prompt\n=\nf\"\"\n\"Rate how well this response utilizes the conversation context on a scale of\n1\n-\n5\n:\n<\nconversation\n>\n{\n\"\"\n.\njoin\n(\nf\"\n{\nturn\n[\n'role'\n]\n}\n:\n{\nturn\n[\n'content'\n]\n}\n\\\\n\"\nfor\nturn\nin\nconversation\n[\n:\n-\n1\n]\n)\n}\n<\n/\nconversation\n>\n<\nresponse\n>\n{\nmodel_output\n}\n<\n/\nresponse\n>\n1\n:\nCompletely ignores context\n5\n:\nPerfectly utilizes context\nOutput only the number\nand\nnothing\nelse\n.\n\"\"\n\"\n# Generally best practice to use a different model to evaluate than the model used to generate the evaluated output\nresponse\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-opus-20240229\"\n,\nmax_tokens\n=\n50\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nordinal_prompt\n}\n]\n)\nreturn\nint\n(\nresponse\n.\ncontent\n[\n0\n]\n.\ntext\n.\nstrip\n(\n)\n)\noutputs\n=\n[\nget_completion\n(\nconversation\n)\nfor\nconversation\nin\nconversations\n]\ncontext_scores\n=\n[\nevaluate_ordinal\n(\noutput\n,\nconversation\n)\nfor\noutput\n,\nconversation\nin\nzip\n(\noutputs\n,\nconversations\n)\n]\nprint\n(\nf\"Average Context Utilization Score:\n{\nsum\n(\ncontext_scores\n)\n/\nlen\n(\ncontext_scores\n)\n}\n\"\n)\nWriting hundreds of test cases can be hard to do by hand! Get Claude to help you generate more from a baseline set of example test cases.\nIf you don’t know what eval methods might be useful to assess for your success criteria, you can also brainstorm with Claude!\n​\nGrading evals\nWhen deciding which method to use to grade evals, choose the fastest, most reliable, most scalable method:\nCode-based grading\n: Fastest and most reliable, extremely scalable, but also lacks nuance for more complex judgements that require less rule-based rigidity.\nExact match:\noutput == golden_answer\nString match:\nkey_phrase in output\nHuman grading\n: Most flexible and high quality, but slow and expensive. Avoid if possible.\nLLM-based grading\n: Fast and flexible, scalable and suitable for complex judgement. Test to ensure reliability first then scale.\n​\nTips for LLM-based grading\nHave detailed, clear rubrics\n: “The answer should always mention ‘Acme Inc.’ in the first sentence. If it does not, the answer is automatically graded as ‘incorrect.’”\nA given use case, or even a specific success criteria for that use case, might require several rubrics for holistic evaluation.\nEmpirical or specific\n: For example, instruct the LLM to output only ‘correct’ or ‘incorrect’, or to judge from a scale of 1-5. Purely qualitative evaluations are hard to assess quickly and at scale.\nEncourage reasoning\n: Ask the LLM to think first before deciding an evaluation score, and then discard the reasoning. This increases evaluation performance, particularly for tasks requiring complex judgement.\nExample: LLM-based grading\nimport\nanthropic\ndef\nbuild_grader_prompt\n(\nanswer\n,\nrubric\n)\n:\nreturn\nf\"\"\n\"Grade this answer based on the rubric\n:\n<\nrubric\n>\n{\nrubric\n}\n<\n/\nrubric\n>\n<\nanswer\n>\n{\nanswer\n}\n<\n/\nanswer\n>\nThink through your reasoning\nin\n<\nthinking\n>\ntags\n,\nthen output\n'correct'\nor\n'incorrect'\nin\n<\nresult\n>\ntags\n.\n\"\"\ndef\ngrade_completion\n(\noutput\n,\ngolden_answer\n)\n:\ngrader_response\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-3-opus-20240229\"\n,\nmax_tokens\n=\n2048\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nbuild_grader_prompt\n(\noutput\n,\ngolden_answer\n)\n}\n]\n)\n.\ncontent\n[\n0\n]\n.\ntext\nreturn\n\"correct\"\nif\n\"correct\"\nin\ngrader_response\n.\nlower\n(\n)\nelse\n\"incorrect\"\n# Example usage\neval_data\n=\n[\n{\n\"question\"\n:\n\"Is 42 the answer to life, the universe, and everything?\"\n,\n\"golden_answer\"\n:\n\"Yes, according to 'The Hitchhiker's Guide to the Galaxy'.\"\n}\n,\n{\n\"question\"\n:\n\"What is the capital of France?\"\n,\n\"golden_answer\"\n:\n\"The capital of France is Paris.\"\n}\n]\ndef\nget_completion\n(\nprompt\n:\nstr\n)\n:\nmessage\n=\nclient\n.\nmessages\n.\ncreate\n(\nmodel\n=\n\"claude-opus-4-20250514\"\n,\nmax_tokens\n=\n1024\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nprompt\n}\n]\n)\nreturn\nmessage\n.\ncontent\n[\n0\n]\n.\ntext\noutputs\n=\n[\nget_completion\n(\nq\n[\n\"question\"\n]\n)\nfor\nq\nin\neval_data\n]\ngrades\n=\n[\ngrade_completion\n(\noutput\n,\na\n[\n\"golden_answer\"\n]\n)\nfor\noutput\n,\na\nin\nzip\n(\noutputs\n,\neval_data\n)\n]\nprint\n(\nf\"Score:\n{\ngrades\n.\ncount\n(\n'correct'\n)\n/\nlen\n(\ngrades\n)\n*\n100\n}\n%\"\n)\n​\nNext steps\nBrainstorm evaluations\nLearn how to craft prompts that maximize your eval scores.\nEvals cookbook\nMore code examples of human-, code-, and LLM-graded evals.\nWas this page helpful?\nYes\nNo\nDefine success criteria\nUsing the Evaluation Tool\nOn this page\nBuilding evals and test cases\nEval design principles\nExample evals\nGrading evals\nTips for LLM-based grading\nNext steps",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:51.994280",
    "word_count": 3696,
    "link_count": 5,
    "content_length": 20943
  }
}