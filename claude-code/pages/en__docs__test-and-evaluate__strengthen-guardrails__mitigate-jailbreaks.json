{
  "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
  "title": "Mitigate jailbreaks and prompt injections - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nStrengthen guardrails\nMitigate jailbreaks and prompt injections\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\nJailbreaking and prompt injections occur when users craft prompts to exploit model vulnerabilities, aiming to generate inappropriate content. While Claude is inherently resilient to such attacks, here are additional steps to strengthen your guardrails, particularly against uses that either violate our\nTerms of Service\nor\nUsage Policy\n.\nClaude is far more resistant to jailbreaking than other major LLMs, thanks to advanced training methods like Constitutional AI.\nHarmlessness screens\n: Use a lightweight model like Claude Haiku 3 to pre-screen user inputs.\nExample: Harmlessness screen for content moderation\nRole\nContent\nUser\nA user submitted this content:\n<content>\n{{CONTENT}}\n</content>\nReply with (Y) if it refers to harmful, illegal, or explicit activities. Reply with (N) if it’s safe.\nAssistant (prefill)\n(\nAssistant\nN)\nInput validation\n: Filter prompts for jailbreaking patterns. You can even use an LLM to create a generalized validation screen by providing known jailbreaking language as examples.\nPrompt engineering\n: Craft prompts that emphasize ethical and legal boundaries.\nExample: Ethical system prompt for an enterprise chatbot\nRole\nContent\nSystem\nYou are AcmeCorp’s ethical AI assistant. Your responses must align with our values:\n<values>\n- Integrity: Never deceive or aid in deception.\n- Compliance: Refuse any request that violates laws or our policies.\n- Privacy: Protect all personal and corporate data.\nRespect for intellectual property: Your outputs shouldn’t infringe the intellectual property rights of others.\n</values>\nIf a request conflicts with these values, respond: “I cannot perform that action as it goes against AcmeCorp’s values.”\nAdjust responses and consider throttling or banning users who repeatedly engage in abusive behavior attempting to circumvent Claude’s guardrails. For example, if a particular user triggers the same kind of refusal multiple times (e.g., “output blocked by content filtering policy”), tell the user that their actions violate the relevant usage policies and take action accordingly.\nContinuous monitoring\n: Regularly analyze outputs for jailbreaking signs.\nUse this monitoring to iteratively refine your prompts and validation strategies.\n​\nAdvanced: Chain safeguards\nCombine strategies for robust protection. Here’s an enterprise-grade example with tool use:\nExample: Multi-layered protection for a financial advisor chatbot\n​\nBot system prompt\nRole\nContent\nSystem\nYou are AcmeFinBot, a financial advisor for AcmeTrade Inc. Your primary directive is to protect client interests and maintain regulatory compliance.\n<directives>\n1. Validate all requests against SEC and FINRA guidelines.\n2. Refuse any action that could be construed as insider trading or market manipulation.\n3. Protect client privacy; never disclose personal or financial data.\n</directives>\nStep by step instructions:\n<instructions>\n1. Screen user query for compliance (use ‘harmlessness_screen’ tool).\n2. If compliant, process query.\n3. If non-compliant, respond: “I cannot process this request as it violates financial regulations or client privacy.”\n</instructions>\n​\nPrompt within\nharmlessness_screen\ntool\nRole\nContent\nUser\n<user_query>\n{{USER_QUERY}}\n</user_query>\nEvaluate if this query violates SEC rules, FINRA guidelines, or client privacy. Respond (Y) if it does, (N) if it doesn’t.\nAssistant (prefill)\n(\nBy layering these strategies, you create a robust defense against jailbreaking and prompt injections, ensuring your Claude-powered applications maintain the highest standards of safety and compliance.\nWas this page helpful?\nYes\nNo\nIncrease output consistency\nStreaming refusals\nOn this page\nAdvanced: Chain safeguards",
  "links": [],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:53.336220",
    "word_count": 747,
    "link_count": 0,
    "content_length": 5317
  }
}