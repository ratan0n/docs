{
  "url": "https://docs.anthropic.com/en/docs/about-claude/use-case-guides/customer-support-chat",
  "title": "Customer support agent - Anthropic",
  "text": "Anthropic\nhome page\nEnglish\nSearch...\nSearch...\nNavigation\nUse cases\nCustomer support agent\nWelcome\nDeveloper Guide\nAPI Guide\nClaude Code\nModel Context Protocol (MCP)\nResources\nRelease Notes\nDocumentation\nDeveloper Discord\nSupport\nFirst steps\nIntro to Claude\nGet started\nModels & pricing\nModels overview\nChoosing a model\nMigrating to Claude 4\nModel deprecations\nPricing\nLearn about Claude\nBuilding with Claude\nFeatures overview\nContext windows\nGlossary\nCapabilities\nPrompt caching\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb search tool\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nLegal center\nAnthropic Privacy Policy\nSecurity and compliance\n​\nBefore building with Claude\n​\nDecide whether to use Claude for support chat\nHere are some key indicators that you should employ an LLM like Claude to automate portions of your customer support process:\nHigh volume of repetitive queries\nClaude excels at handling a large number of similar questions efficiently, freeing up human agents for more complex issues.\nNeed for quick information synthesis\nClaude can quickly retrieve, process, and combine information from vast knowledge bases, while human agents may need time to research or consult multiple sources.\n24/7 availability requirement\nClaude can provide round-the-clock support without fatigue, whereas staffing human agents for continuous coverage can be costly and challenging.\nRapid scaling during peak periods\nClaude can handle sudden increases in query volume without the need for hiring and training additional staff.\nConsistent brand voice\nYou can instruct Claude to consistently represent your brand’s tone and values, whereas human agents may vary in their communication styles.\nSome considerations for choosing Claude over other LLMs:\nYou prioritize natural, nuanced conversation: Claude’s sophisticated language understanding allows for more natural, context-aware conversations that feel more human-like than chats with other LLMs.\nYou often receive complex and open-ended queries: Claude can handle a wide range of topics and inquiries without generating canned responses or requiring extensive programming of permutations of user utterances.\nYou need scalable multilingual support: Claude’s multilingual capabilities allow it to engage in conversations in over 200 languages without the need for separate chatbots or extensive translation processes for each supported language.\n​\nDefine your ideal chat interaction\nOutline an ideal customer interaction to define how and when you expect the customer to interact with Claude. This outline will help to determine the technical requirements of your solution.\nHere is an example chat interaction for car insurance customer support:\nCustomer\n: Initiates support chat experience\nClaude\n: Warmly greets customer and initiates conversation\nCustomer\n: Asks about insurance for their new electric car\nClaude\n: Provides relevant information about electric vehicle coverage\nCustomer\n: Asks questions related to unique needs for electric vehicle insurances\nClaude\n: Responds with accurate and informative answers and provides links to the sources\nCustomer\n: Asks off-topic questions unrelated to insurance or cars\nClaude\n: Clarifies it does not discuss unrelated topics and steers the user back to car insurance\nCustomer\n: Expresses interest in an insurance quote\nClaude\n: Ask a set of questions to determine the appropriate quote, adapting to their responses\nClaude\n: Sends a request to use the quote generation API tool along with necessary information collected from the user\nClaude\n: Receives the response information from the API tool use, synthesizes the information into a natural response, and presents the provided quote to the user\nCustomer\n: Asks follow up questions\nClaude\n: Answers follow up questions as needed\nClaude\n: Guides the customer to the next steps in the insurance process and closes out the conversation\nIn the real example that you write for your own use case, you might find it useful to write out the actual words in this interaction so that you can also get a sense of the ideal tone, response length, and level of detail you want Claude to have.\n​\nBreak the interaction into unique tasks\nCustomer support chat is a collection of multiple different tasks, from question answering to information retrieval to taking action on requests, wrapped up in a single customer interaction. Before you start building, break down your ideal customer interaction into every task you want Claude to be able to perform. This ensures you can prompt and evaluate Claude for every task, and gives you a good sense of the range of interactions you need to account for when writing test cases.\nCustomers sometimes find it helpful to visualize this as an interaction flowchart of possible conversation inflection points depending on user requests.\nHere are the key tasks associated with the example insurance interaction above:\nGreeting and general guidance\nWarmly greet the customer and initiate conversation\nProvide general information about the company and interaction\nProduct Information\nProvide information about electric vehicle coverage\nThis will require that Claude have the necessary information in its context, and might imply that a\nRAG integration\nis necessary.\nAnswer questions related to unique electric vehicle insurance needs\nAnswer follow-up questions about the quote or insurance details\nOffer links to sources when appropriate\nConversation Management\nStay on topic (car insurance)\nRedirect off-topic questions back to relevant subjects\nQuote Generation\nAsk appropriate questions to determine quote eligibility\nAdapt questions based on customer responses\nSubmit collected information to quote generation API\nPresent the provided quote to the customer\n​\nEstablish success criteria\nWork with your support team to\ndefine clear success criteria\nand write\ndetailed evaluations\nwith measurable benchmarks and goals.\nHere are criteria and benchmarks that can be used to evaluate how successfully Claude performs the defined tasks:\nQuery comprehension accuracy\nThis metric evaluates how accurately Claude understands customer inquiries across various topics. Measure this by reviewing a sample of conversations and assessing whether Claude has the correct interpretation of customer intent, critical next steps, what successful resolution looks like, and more. Aim for a comprehension accuracy of 95% or higher.\nResponse relevance\nThis assesses how well Claude’s response addresses the customer’s specific question or issue. Evaluate a set of conversations and rate the relevance of each response (using LLM-based grading for scale). Target a relevance score of 90% or above.\nResponse accuracy\nAssess the correctness of general company and product information provided to the user, based on the information provided to Claude in context. Target 100% accuracy in this introductory information.\nCitation provision relevance\nTrack the frequency and relevance of links or sources offered. Target providing relevant sources in 80% of interactions where additional information could be beneficial.\nTopic adherence\nMeasure how well Claude stays on topic, such as the topic of car insurance in our example implementation. Aim for 95% of responses to be directly related to car insurance or the customer’s specific query.\nContent generation effectiveness\nMeasure how successful Claude is at determining when to generate informational content and how relevant that content is. For example, in our implementation, we would be determining how well Claude understands when to generate a quote and how accurate that quote is. Target 100% accuracy, as this is vital information for a successful customer interaction.\nEscalation efficiency\nThis measures Claude’s ability to recognize when a query needs human intervention and escalate appropriately. Track the percentage of correctly escalated conversations versus those that should have been escalated but weren’t. Aim for an escalation accuracy of 95% or higher.\nHere are criteria and benchmarks that can be used to evaluate the business impact of employing Claude for support:\nSentiment maintenance\nThis assesses Claude’s ability to maintain or improve customer sentiment throughout the conversation. Use sentiment analysis tools to measure sentiment at the beginning and end of each conversation. Aim for maintained or improved sentiment in 90% of interactions.\nDeflection rate\nThe percentage of customer inquiries successfully handled by the chatbot without human intervention. Typically aim for 70-80% deflection rate, depending on the complexity of inquiries.\nCustomer satisfaction score\nA measure of how satisfied customers are with their chatbot interaction. Usually done through post-interaction surveys. Aim for a CSAT score of 4 out of 5 or higher.\nAverage handle time\nThe average time it takes for the chatbot to resolve an inquiry. This varies widely based on the complexity of issues, but generally, aim for a lower AHT compared to human agents.\n​\nHow to implement Claude as a customer service agent\n​\nChoose the right Claude model\nThe choice of model depends on the trade-offs between cost, accuracy, and response time.\nFor customer support chat,\nclaude-opus-4-20250514\nis well suited to balance intelligence, latency, and cost. However, for instances where you have conversation flow with multiple prompts including RAG, tool use, and/or long-context prompts,\nclaude-3-haiku-20240307\nmay be more suitable to optimize for latency.\n​\nBuild a strong prompt\nUsing Claude for customer support requires Claude having enough direction and context to respond appropriately, while having enough flexibility to handle a wide range of customer inquiries.\nLet’s start by writing the elements of a strong prompt, starting with a system prompt:\nIDENTITY\n=\n\"\"\n\"You are Eva\n,\na friendly\nand\nknowledgeable AI assistant\nfor\nAcme Insurance\nCompany\n.\nYour role\nis\nto warmly welcome customers\nand\nprovide information on\nAcme's insurance offerings\n,\nwhich include car insurance\nand\nelectric car\ninsurance\n.\nYou can also\nhelp\ncustomers get quotes\nfor\ntheir insurance needs\n.\n\"\"\n\"\nWhile you may be tempted to put all your information inside a system prompt as a way to separate instructions from the user conversation, Claude actually works best with the bulk of its prompt content written inside the first\nUser\nturn (with the only exception being role prompting). Read more at\nGiving Claude a role with a system prompt\n.\nIt’s best to break down complex prompts into subsections and write one part at a time. For each task, you might find greater success by following a step by step process to define the parts of the prompt Claude would need to do the task well. For this car insurance customer support example, we’ll be writing piecemeal all the parts for a prompt starting with the “Greeting and general guidance” task. This also makes debugging your prompt easier as you can more quickly adjust individual parts of the overall prompt.\nWe’ll put all of these pieces in a file called\nconfig.py\n.\nSTATIC_GREETINGS_AND_GENERAL\n=\n\"\"\n\"\n<\nstatic_context\n>\nAcme Auto Insurance\n:\nYour Trusted Companion on the Road\nAbout\n:\nAt Acme Insurance\n,\nwe understand that your vehicle\nis\nmore than just a mode of transportation—it\n's your ticket to life'\ns adventures\n.\nSince\n1985\n,\nwe've been crafting auto insurance policies that give drivers the confidence to explore\n,\ncommute\n,\nand\ntravel\nwith\npeace of mind\n.\nWhether you're navigating city streets\nor\nembarking on cross\n-\ncountry road trips\n,\nAcme\nis\nthere to protect you\nand\nyour vehicle\n.\nOur innovative auto insurance policies are designed to adapt to your unique needs\n,\ncovering everything\nfrom\nfender benders to major collisions\n.\nWith Acme's award\n-\nwinning customer service\nand\nswift claim resolution\n,\nyou can focus on the joy of driving\nwhile\nwe handle the rest\n.\nWe\n're not just an insurance provider—we'\nre your co\n-\npilot\nin\nlife's journeys\n.\nChoose Acme Auto Insurance\nand\nexperience the assurance that comes\nwith\nsuperior coverage\nand\ngenuine care\n.\nBecause at Acme\n,\nwe don't just\ninsure your car—we fuel your adventures on the\nopen\nroad\n.\nNote\n:\nWe also offer specialized coverage\nfor\nelectric vehicles\n,\nensuring that drivers of\nall\ncar types can benefit\nfrom\nour protection\n.\nAcme Insurance offers the following products\n:\n-\nCar insurance\n-\nElectric car insurance\n-\nTwo\n-\nwheeler insurance\nBusiness hours\n:\nMonday\n-\nFriday\n,\n9\nAM\n-\n5\nPM EST\nCustomer service number\n:\n1\n-\n800\n-\n123\n-\n4567\n<\n/\nstatic_context\n>\n\"\"\n\"\nWe’ll then do the same for our car insurance and electric car insurance information.\nSTATIC_CAR_INSURANCE\n=\n\"\"\n\"\n<\nstatic_context\n>\nCar Insurance Coverage\n:\nAcme's car insurance policies typically cover\n:\n1.\nLiability coverage\n:\nPays\nfor\nbodily injury\nand\nproperty\ndamage you cause to others\n.\n2.\nCollision coverage\n:\nPays\nfor\ndamage to your car\nin\nan accident\n.\n3.\nComprehensive coverage\n:\nPays\nfor\ndamage to your car\nfrom\nnon\n-\ncollision incidents\n.\n4.\nMedical payments coverage\n:\nPays\nfor\nmedical expenses after an accident\n.\n5.\nUninsured\n/\nunderinsured motorist coverage\n:\nProtects you\nif\nyou're hit by a driver\nwith\ninsufficient insurance\n.\nOptional coverages include\n:\n-\nRental car reimbursement\n-\nRoadside assistance\n-\nNew car replacement\n<\n/\nstatic_context\n>\n\"\"\n\"\nSTATIC_ELECTRIC_CAR_INSURANCE\n=\n\"\"\n\"\n<\nstatic_context\n>\nElectric Car Insurance\n:\nOur specialized electric car insurance goes beyond traditional auto coverage\n,\noffering tailored protection\nfor\nyour high\n-\ntech investment\n.\nIn addition to standard coverages\n,\nwe offer\n:\n-\nSafeguarding expensive battery systems\n-\nCovering home charging equipment\n-\nAdapting to the rapidly evolving world of electric vehicles\nWhether you're driving a Rivian\n,\nNissan Leaf\n,\nChevrolet Bolt\n,\nor\nany\nother electric vehicle\n,\nour knowledgeable agents are ready to craft a policy\nthat gives you the confidence to enjoy every electron\n-\npowered mile\n.\n<\n/\nstatic_context\n>\n\"\"\n\"\nNow that we have our static content, let’s add at least 4-5 sample “good” interactions to guide Claude’s responses. These examples should be representative of your ideal customer interaction and can include guardrails, tool calls, etc.\nEXAMPLES\n=\n\"\"\n\"\nHere are a few examples of how you can interact\nwith\ncustomers\n:\n<\nexample\n1\n>\nH\n:\nHi\n,\ndo you offer commercial insurance\nfor\nsmall businesses?\nA\n:\nAh! Unfortunately\n,\nwe don't offer commercial insurance at this time\n.\nHowever\n,\nwe do provide a\nrange\nof personal insurance products including car insurance\nand\nelectric car insurance\n.\nWould you like to know more about these offerings?\n<\n/\nexample\n1\n>\n<\nexample\n2\n>\nH\n:\nHi\n,\nwhat are your business hours?\nA\n:\nOur business hours are Monday to Friday\n,\n9\nAM to\n5\nPM EST\n.\nIf you have\nany\nquestions\nor\nneed assistance\n,\nfeel free to reach out during those times\n.\nHow\nelse\ncan I assist you today?\n<\n/\nexample\n2\n>\n<\nexample\n3\n>\nH\n:\nI'm shopping\nfor\ncar insurance\n.\nWhat does Acme offer?\nA\n:\nGreat choice! Acme's car insurance policies typically include\n:\n-\nLiability coverage\n:\nProtects you\nif\nyou're at fault\nin\nan accident\n-\nCollision coverage\n:\nPays\nfor\ndamage to your car\nin\nan accident\n-\nComprehensive coverage\n:\nCovers non\n-\ncollision incidents like theft\nor\nvandalism\n-\nMedical payments coverage\n:\nPays\nfor\nmedical expenses after an accident\n-\nUninsured\n/\nunderinsured motorist coverage\n:\nProtects you\nfrom\ndrivers\nwith\ninsufficient insurance\nWe also offer optional coverages like rental car reimbursement\n,\nroadside assistance\n,\nand\nnew car replacement\n.\nWould you like more details on\nany\nof these coverages\n,\nor\ndo you have specific questions about insuring your vehicle?\n<\n/\nexample\n3\n>\n<\nexample\n4\n>\nH\n:\nI'd like to hear about your electric car insurance\n.\nA\n:\nCertainly! Our electric car insurance\nis\nspecially designed to meet the unique needs of electric vehicle owners\n.\nIn addition to\nall\nthe standard coverages we offer\nfor\nregular cars\n,\nour electric car policies include\n:\n-\nBattery coverage\n:\nProtection against damage\nor\ndegradation of your vehicle's expensive battery system\n-\nTech package\n:\nCovers repairs\nor\nreplacement of advanced electronics\nand\nonboard computer systems\n-\nCharging station coverage\n:\nProtects both home\nand\nportable charging equipment\n-\nCybersecurity protection\n:\nCoverage against hacking\nor\ndigital threats\n-\nSpecialized roadside assistance\n:\nIncludes flatbed towing to protect the vehicle's undercarriage\nWe cover a wide\nrange\nof electric vehicles\n,\nincluding popular models\nfrom\nTesla\n,\nRivian\n,\nNissan\n,\nChevrolet\n,\nand\nmore\n.\nIs there a specific electric vehicle you're looking to insure\n,\nor\nwould you like more details on\nany\nof these coverages?\n<\n/\nexample\n4\n>\n<\nexample\n5\n>\nH\n:\nI'd like to get a quote\nfor\nmy car insurance\n.\nA\n:\nCertainly! I'd be happy to\nhelp\nyou get a quote\nfor\nyour car insurance\n.\nTo provide you\nwith\nan accurate quote\n,\nI'll need to collect some information about your vehicle\nand\nthe primary driver\n.\nLet's start\nwith\nthe basics\n:\n1.\nWhat\nis\nthe make\nand\nmodel of your vehicle?\n2.\nWhat year was it manufactured?\n3.\nApproximately how many miles have you driven?\n4.\nWhat\nis\nthe age of the primary driver?\nOnce you provide this information\n,\nI'll use our quoting tool to generate a personalized insurance quote\nfor\nyou\n.\n<\n/\nexample\n5\n>\n\"\"\n\"\nYou will also want to include any important instructions outlining Do’s and Don’ts for how Claude should interact with the customer.\nThis may draw from brand guardrails or support policies.\nADDITIONAL_GUARDRAILS\n=\n\"\"\n\"Please adhere to the following guardrails\n:\n1.\nOnly provide information about insurance types listed\nin\nour offerings\n.\n2.\nIf asked about an insurance\ntype\nwe don't offer\n,\npolitely state\nthat we don't provide that service\n.\n3.\nDo\nnot\nspeculate about future product offerings\nor\ncompany plans\n.\n4.\nDon\n't make promises or enter into agreements it'\ns\nnot\nauthorized to make\n.\nYou only provide information\nand\nguidance\n.\n5.\nDo\nnot\nmention\nany\ncompetitor's products\nor\nservices\n.\n\"\"\n\"\nNow let’s combine all these sections into a single string to use as our prompt.\nTASK_SPECIFIC_INSTRUCTIONS\n=\n' '\n.\njoin\n(\n[\nSTATIC_GREETINGS_AND_GENERAL\n,\nSTATIC_CAR_INSURANCE\n,\nSTATIC_ELECTRIC_CAR_INSURANCE\n,\nEXAMPLES\n,\nADDITIONAL_GUARDRAILS\n,\n]\n)\n​\nAdd dynamic and agentic capabilities with tool use\nClaude is capable of taking actions and retrieving information dynamically using client-side tool use functionality. Start by listing any external tools or APIs the prompt should utilize.\nFor this example, we will start with one tool for calculating the quote.\nAs a reminder, this tool will not perform the actual calculation, it will just signal to the application that a tool should be used with whatever arguments specified.\nExample insurance quote calculator:\nTOOLS\n=\n[\n{\n\"name\"\n:\n\"get_quote\"\n,\n\"description\"\n:\n\"Calculate the insurance quote based on user input. Returned value is per month premium.\"\n,\n\"input_schema\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"make\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The make of the vehicle.\"\n}\n,\n\"model\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The model of the vehicle.\"\n}\n,\n\"year\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The year the vehicle was manufactured.\"\n}\n,\n\"mileage\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The mileage on the vehicle.\"\n}\n,\n\"driver_age\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The age of the primary driver.\"\n}\n}\n,\n\"required\"\n:\n[\n\"make\"\n,\n\"model\"\n,\n\"year\"\n,\n\"mileage\"\n,\n\"driver_age\"\n]\n}\n}\n]\ndef\nget_quote\n(\nmake\n,\nmodel\n,\nyear\n,\nmileage\n,\ndriver_age\n)\n:\n\"\"\"Returns the premium per month in USD\"\"\"\n# You can call an http endpoint or a database to get the quote.\n# Here, we simulate a delay of 1 seconds and return a fixed quote of 100.\ntime\n.\nsleep\n(\n1\n)\nreturn\n100\n​\nDeploy your prompts\nIt’s hard to know how well your prompt works without deploying it in a test production setting and\nrunning evaluations\nso let’s build a small application using our prompt, the Anthropic SDK, and streamlit for a user interface.\nIn a file called\nchatbot.py\n, start by setting up the ChatBot class, which will encapsulate the interactions with the Anthropic SDK.\nThe class should have two main methods:\ngenerate_message\nand\nprocess_user_input\n.\nfrom\nanthropic\nimport\nAnthropic\nfrom\nconfig\nimport\nIDENTITY\n,\nTOOLS\n,\nMODEL\n,\nget_quote\nfrom\ndotenv\nimport\nload_dotenv\nload_dotenv\n(\n)\nclass\nChatBot\n:\ndef\n__init__\n(\nself\n,\nsession_state\n)\n:\nself\n.\nanthropic\n=\nAnthropic\n(\n)\nself\n.\nsession_state\n=\nsession_state\ndef\ngenerate_message\n(\nself\n,\nmessages\n,\nmax_tokens\n,\n)\n:\ntry\n:\nresponse\n=\nself\n.\nanthropic\n.\nmessages\n.\ncreate\n(\nmodel\n=\nMODEL\n,\nsystem\n=\nIDENTITY\n,\nmax_tokens\n=\nmax_tokens\n,\nmessages\n=\nmessages\n,\ntools\n=\nTOOLS\n,\n)\nreturn\nresponse\nexcept\nException\nas\ne\n:\nreturn\n{\n\"error\"\n:\nstr\n(\ne\n)\n}\ndef\nprocess_user_input\n(\nself\n,\nuser_input\n)\n:\nself\n.\nsession_state\n.\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nuser_input\n}\n)\nresponse_message\n=\nself\n.\ngenerate_message\n(\nmessages\n=\nself\n.\nsession_state\n.\nmessages\n,\nmax_tokens\n=\n2048\n,\n)\nif\n\"error\"\nin\nresponse_message\n:\nreturn\nf\"An error occurred:\n{\nresponse_message\n[\n'error'\n]\n}\n\"\nif\nresponse_message\n.\ncontent\n[\n-\n1\n]\n.\ntype\n==\n\"tool_use\"\n:\ntool_use\n=\nresponse_message\n.\ncontent\n[\n-\n1\n]\nfunc_name\n=\ntool_use\n.\nname\nfunc_params\n=\ntool_use\n.\ninput\ntool_use_id\n=\ntool_use\n.\nid\nresult\n=\nself\n.\nhandle_tool_use\n(\nfunc_name\n,\nfunc_params\n)\nself\n.\nsession_state\n.\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse_message\n.\ncontent\n}\n)\nself\n.\nsession_state\n.\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"tool_result\"\n,\n\"tool_use_id\"\n:\ntool_use_id\n,\n\"content\"\n:\nf\"\n{\nresult\n}\n\"\n,\n}\n]\n,\n}\n)\nfollow_up_response\n=\nself\n.\ngenerate_message\n(\nmessages\n=\nself\n.\nsession_state\n.\nmessages\n,\nmax_tokens\n=\n2048\n,\n)\nif\n\"error\"\nin\nfollow_up_response\n:\nreturn\nf\"An error occurred:\n{\nfollow_up_response\n[\n'error'\n]\n}\n\"\nresponse_text\n=\nfollow_up_response\n.\ncontent\n[\n0\n]\n.\ntext\nself\n.\nsession_state\n.\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse_text\n}\n)\nreturn\nresponse_text\nelif\nresponse_message\n.\ncontent\n[\n0\n]\n.\ntype\n==\n\"text\"\n:\nresponse_text\n=\nresponse_message\n.\ncontent\n[\n0\n]\n.\ntext\nself\n.\nsession_state\n.\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\nresponse_text\n}\n)\nreturn\nresponse_text\nelse\n:\nraise\nException\n(\n\"An error occurred: Unexpected response type\"\n)\ndef\nhandle_tool_use\n(\nself\n,\nfunc_name\n,\nfunc_params\n)\n:\nif\nfunc_name\n==\n\"get_quote\"\n:\npremium\n=\nget_quote\n(\n**\nfunc_params\n)\nreturn\nf\"Quote generated: $\n{\npremium\n:\n.2f\n}\nper month\"\nraise\nException\n(\n\"An unexpected tool was used\"\n)\n​\nBuild your user interface\nTest deploying this code with Streamlit using a main method. This\nmain()\nfunction sets up a Streamlit-based chat interface.\nWe’ll do this in a file called\napp.py\nimport\nstreamlit\nas\nst\nfrom\nchatbot\nimport\nChatBot\nfrom\nconfig\nimport\nTASK_SPECIFIC_INSTRUCTIONS\ndef\nmain\n(\n)\n:\nst\n.\ntitle\n(\n\"Chat with Eva, Acme Insurance Company's Assistant🤖\"\n)\nif\n\"messages\"\nnot\nin\nst\n.\nsession_state\n:\nst\n.\nsession_state\n.\nmessages\n=\n[\n{\n'role'\n:\n\"user\"\n,\n\"content\"\n:\nTASK_SPECIFIC_INSTRUCTIONS\n}\n,\n{\n'role'\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Understood\"\n}\n,\n]\nchatbot\n=\nChatBot\n(\nst\n.\nsession_state\n)\n# Display user and assistant messages skipping the first two\nfor\nmessage\nin\nst\n.\nsession_state\n.\nmessages\n[\n2\n:\n]\n:\n# ignore tool use blocks\nif\nisinstance\n(\nmessage\n[\n\"content\"\n]\n,\nstr\n)\n:\nwith\nst\n.\nchat_message\n(\nmessage\n[\n\"role\"\n]\n)\n:\nst\n.\nmarkdown\n(\nmessage\n[\n\"content\"\n]\n)\nif\nuser_msg\n:=\nst\n.\nchat_input\n(\n\"Type your message here...\"\n)\n:\nst\n.\nchat_message\n(\n\"user\"\n)\n.\nmarkdown\n(\nuser_msg\n)\nwith\nst\n.\nchat_message\n(\n\"assistant\"\n)\n:\nwith\nst\n.\nspinner\n(\n\"Eva is thinking...\"\n)\n:\nresponse_placeholder\n=\nst\n.\nempty\n(\n)\nfull_response\n=\nchatbot\n.\nprocess_user_input\n(\nuser_msg\n)\nresponse_placeholder\n.\nmarkdown\n(\nfull_response\n)\nif\n__name__\n==\n\"__main__\"\n:\nmain\n(\n)\nRun the program with:\nstreamlit run app.py\n​\nEvaluate your prompts\nPrompting often requires testing and optimization for it to be production ready. To determine the readiness of your solution, evaluate the chatbot performance using a systematic process combining quantitative and qualitative methods. Creating a\nstrong empirical evaluation\nbased on your defined success criteria will allow you to optimize your prompts.\nThe\nAnthropic Console\nnow features an Evaluation tool that allows you to test your prompts under various scenarios.\n​\nImprove performance\nIn complex scenarios, it may be helpful to consider additional strategies to improve performance beyond standard\nprompt engineering techniques\n&\nguardrail implementation strategies\n. Here are some common scenarios:\n​\nReduce long context latency with RAG\nWhen dealing with large amounts of static and dynamic context, including all information in the prompt can lead to high costs, slower response times, and reaching context window limits. In this scenario, implementing Retrieval Augmented Generation (RAG) techniques can significantly improve performance and efficiency.\nBy using\nembedding models like Voyage\nto convert information into vector representations, you can create a more scalable and responsive system. This approach allows for dynamic retrieval of relevant information based on the current query, rather than including all possible context in every prompt.\nImplementing RAG for support use cases\nRAG recipe\nhas been shown to increase accuracy, reduce response times, and reduce API costs in systems with extensive context requirements.\n​\nIntegrate real-time data with tool use\nWhen dealing with queries that require real-time information, such as account balances or policy details, embedding-based RAG approaches are not sufficient. Instead, you can leverage tool use to significantly enhance your chatbot’s ability to provide accurate, real-time responses. For example, you can use tool use to look up customer information, retrieve order details, and cancel orders on behalf of the customer.\nThis approach,\noutlined in our tool use: customer service agent recipe\n, allows you to seamlessly integrate live data into your Claude’s responses and provide a more personalized and efficient customer experience.\n​\nStrengthen input and output guardrails\nWhen deploying a chatbot, especially in customer service scenarios, it’s crucial to prevent risks associated with misuse, out-of-scope queries, and inappropriate responses. While Claude is inherently resilient to such scenarios, here are additional steps to strengthen your chatbot guardrails:\nReduce hallucination\n: Implement fact-checking mechanisms and\ncitations\nto ground responses in provided information.\nCross-check information: Verify that the agent’s responses align with your company’s policies and known facts.\nAvoid contractual commitments: Ensure the agent doesn’t make promises or enter into agreements it’s not authorized to make.\nMitigate jailbreaks\n: Use methods like harmlessness screens and input validation to prevent users from exploiting model vulnerabilities, aiming to generate inappropriate content.\nAvoid mentioning competitors: Implement a competitor mention filter to maintain brand focus and not mention any competitor’s products or services.\nKeep Claude in character\n: Prevent Claude from changing their style of context, even during long, complex interactions.\nRemove Personally Identifiable Information (PII): Unless explicitly required and authorized, strip out any PII from responses.\n​\nReduce perceived response time with streaming\nWhen dealing with potentially lengthy responses, implementing streaming can significantly improve user engagement and satisfaction. In this scenario, users receive the answer progressively instead of waiting for the entire response to be generated.\nHere is how to implement streaming:\nUse the\nAnthropic Streaming API\nto support streaming responses.\nSet up your frontend to handle incoming chunks of text.\nDisplay each chunk as it arrives, simulating real-time typing.\nImplement a mechanism to save the full response, allowing users to view it if they navigate away and return.\nIn some cases, streaming enables the use of more advanced models with higher base latencies, as the progressive display mitigates the impact of longer processing times.\n​\nScale your Chatbot\nAs the complexity of your Chatbot grows, your application architecture can evolve to match. Before you add further layers to your architecture, consider the following less exhaustive options:\nEnsure that you are making the most out of your prompts and optimizing through prompt engineering. Use our\nprompt engineering guides\nto write the most effective prompts.\nAdd additional\ntools\nto the prompt (which can include\nprompt chains\n) and see if you can achieve the functionality required.\nIf your Chatbot handles incredibly varied tasks, you may want to consider adding a\nseparate intent classifier\nto route the initial customer query. For the existing application, this would involve creating a decision tree that would route customer queries through the classifier and then to specialized conversations (with their own set of tools and system prompts). Note, this method requires an additional call to Claude that can increase latency.\n​\nIntegrate Claude into your support workflow\nWhile our examples have focused on Python functions callable within a Streamlit environment, deploying Claude for real-time support chatbot requires an API service.\nHere’s how you can approach this:\nCreate an API wrapper: Develop a simple API wrapper around your classification function. For example, you can use Flask API or Fast API to wrap your code into a HTTP Service. Your HTTP service could accept the user input and return the Assistant response in its entirety. Thus, your service could have the following characteristics:\nServer-Sent Events (SSE): SSE allows for real-time streaming of responses from the server to the client. This is crucial for providing a smooth, interactive experience when working with LLMs.\nCaching: Implementing caching can significantly improve response times and reduce unnecessary API calls.\nContext retention: Maintaining context when a user navigates away and returns is important for continuity in conversations.\nBuild a web interface: Implement a user-friendly web UI for interacting with the Claude-powered agent.\nRetrieval Augmented Generation (RAG) cookbook\nVisit our RAG cookbook recipe for more example code and detailed guidance.\nCitations cookbook\nExplore our Citations cookbook recipe for how to ensure accuracy and explainability of information.\nWas this page helpful?\nYes\nNo\nTicket routing\nContent moderation\nOn this page\nBefore building with Claude\nDecide whether to use Claude for support chat\nDefine your ideal chat interaction\nBreak the interaction into unique tasks\nEstablish success criteria\nHow to implement Claude as a customer service agent\nChoose the right Claude model\nBuild a strong prompt\nAdd dynamic and agentic capabilities with tool use\nDeploy your prompts\nBuild your user interface\nEvaluate your prompts\nImprove performance\nReduce long context latency with RAG\nIntegrate real-time data with tool use\nStrengthen input and output guardrails\nReduce perceived response time with streaming\nScale your Chatbot\nIntegrate Claude into your support workflow",
  "links": [
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices",
      "text": "Claude 4 best practices"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator",
      "text": "Prompt generator"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables",
      "text": "Use prompt templates"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-improver",
      "text": "Prompt improver"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct",
      "text": "Be clear and direct"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting",
      "text": "Use examples (multishot prompting)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought",
      "text": "Let Claude think (CoT)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
      "text": "Use XML tags"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts",
      "text": "Give Claude a role (system prompts)"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response",
      "text": "Prefill Claude's response"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts",
      "text": "Chain complex prompts"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips",
      "text": "Long context tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips",
      "text": "Extended thinking tips"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/define-success",
      "text": "Define success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/develop-tests",
      "text": "Develop test cases"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool",
      "text": "Using the Evaluation Tool"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency",
      "text": "Reducing latency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations",
      "text": "Reduce hallucinations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency",
      "text": "Increase output consistency"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks",
      "text": "Mitigate jailbreaks"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals",
      "text": "Streaming refusals"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-prompt-leak",
      "text": "Reduce prompt leak"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/keep-claude-in-character",
      "text": "Keep Claude in character"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/define-success",
      "text": "define clear success criteria"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/develop-tests",
      "text": "detailed evaluations"
    },
    {
      "url": "https://docs.anthropic.com/en/docs/build-with-claude/tool-use",
      "text": "tools"
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-23T15:07:46.488934",
    "word_count": 5164,
    "link_count": 26,
    "content_length": 32076
  }
}