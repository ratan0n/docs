# RATANON/MZ93-DOCUMENTATION - Part 24/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 24 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.8 KB
---

With the System Exporter, you can export data from your system into a Configuration Export (ZIP file) or a Workflow Package Export (MZP file). In Legacy Desktop, you can also export to a directory. The export can contain your system's data, configurations, and runtime data. You can send this export data to another system, using the System Importer to import it and embed its contents locally. Example - How System Exporter can be used You can import a tested and exported ZIP or MZP file of configurations from a test system and use it safely in production. In System Exporter you can select data from the following folder types: Configuration: Workflow configurations, Profiles, Workflow groups, Ultra formats, alarm detectors, etc. Workflow packages: Workflow Packages are compiled versions of configuration, and are created in the Workflow Package mode in the System Exporter. See Workflow Package Export Type When exporting External References profiles of type Properties Database the External Reference Values will also be exported. In System Importer it can be selected if they should be imported or not. This section describes the following: 1 Export Types 2 Configuration Export Type 2.1 Exporting to Local Destination 2.2 Exporting to Remote Destination 3 Workflow Package Export Type 3.1 Exporting to Local Destination 3.2 Exporting to Remote Destination 4 How To Perform an Export Export Types There are two export types. They result in different behaviors when imported later. Select the one that suits your requirements: Configuration : The configurations must be compatible with the software packages of the system to which you import the configurations This is normally not a problem as long as you are using the same release. The export will be downloaded as a ZIP file. See the section below Configuration Export Type. Workflow Package : Export in Workflow Packages format. A Workflow Package is read-only and self-contained, with the same binaries as in the exporting system. A Workflow Package is not affected by, nor will it affect, any existing configurations when imported, since the used binaries are not replaced. When selecting Workflow Package Export Type, you compile a Workflow Package by selecting workflows in the Available Entries table. All dependent configurations will also be selected. See the section below Workflow Package Export Type . Note! See File System Profile to learn how to configure a File System Profile when exporting to Git or AWS S3 Bucket. Configuration Export Type You can export a configuration to your local browser or remote with a File System Profile of type Git or AWS S3. Currently, Git is available in the Desktop interface only. Open Configuration Export type - Local Destination Open Configuration Export type - Remote Destination When the Export Type is Configuration you can select data from the following types: Configuration: Workflow configurations, agent profiles, workflow groups, Ultra formats, or alarm detectors. Run-time: Some data that is generated by the system during workflow run-time. Clear the Option Exclude Runtime Data checkbox. System: Other customized parts such as; Data Veracity, Event Category, Folder (structure), Pico Host, Ultra, User, or Workflow Alarm Value. Workflow packages: Workflow Packages are compiled versions of configuration, and are created in the Export type Workflow Package in the System Exporter. The following options can be found on the toolbar: Option Description Option Description Export This will export the selected configuration(s). The behavior is different depending on your choice of Destination. Described in detail below. Refresh Refreshes the list of shown configurations. Options This will open the Options menu. Each available option can be toggled by the user. See the section below Configurations Export Type Options Expand all Expands all folders in the Available entries section. Collapse all Minimizes all folders in the Available entries section. Configurations Export Type Options Based on the chosen options in the export window, the running system will execute the export operations differently. Open Configurations export type options The following options can be toggled by the users using the Options menu. Export Option Description Export Option Description Abort on error Select this option to abort the export process if an error occurs. If an error occurs and you do not select this option, the export will be completed, but the exported data might contain erroneous components. Invalid Ultra and APL definitions are considered erroneous and result in aborting the export. Select Dependencies Select this option to have dependencies follow along with the entries that you actively select. Encryption Activates encryption on the chosen export. Exclude Runtime Data When enabled it excludes locally stored runtime data from the export. The runtime data that can be included is, Aggregation Session Data and Archive data Export as XML Exporting configuration objects with this option enabled will create compressed XML files instead of JSON. Exporting to Local Destination When the Export to Local Destination is complete, a dialog showing the exact time of the export appears, and the export is automatically downloaded and saved in your default download location. Open Dialog after Successful Export Exporting to Remote Destination When you click the Export button, the following dialog opens and you need to select a File System Profile . You can select the File System Profile either as Git or AWS S3 . Open Select a File System Profile When a profile is selected more fields will appear. When a Git File System Profile is selected it will look like this. Open File System Profile - Git Option Description Option Description Commit message The message will be visible in the Git log. Repository/Branch Read-Only Field, showing the Remote repository name and the branch selected in the profile. Select Target Folder A table showing the folders in the current repository, see GIT Support . New Folder Button Create a new folder on the root level. New Folder Icon on each row Open Create a new folder under the selected row When an AWS S3 File System Profile is selected it will look like this. Open File System Profile - AWS S3 Option Description Option Description File Name The file name of the export. It will automatically become a Zip file. Select Target A table showing the folders in AWS S3 Bucket. Select a folder to export to, when no folder or file is selected the export will be uploaded to the top level of the Bucket. Workflow Package Export Type When this type is selected, Workflow Packages can be created and exported as a MZP file. A Workflow Package is read-only and self-contained, with the same binaries as in the exporting system. A Workflow Package is not affected by, nor will it affect, any existing configurations when imported, since the used binaries are not replaced. When the Export Type is Workflow Package you can select data from the following types: Configuration: Workflow configurations, agent profiles, workflow groups, Ultra formats, or alarm detectors. System: Other customized parts such as; Data Veracity, Event Category, Folder (structure), Pico Host, Ultra, User, or Workflow Alarm Value. Open When one configuration is selected all dependent configurations will also be selected. Export Option Description Export Option Description Export The behavior is different depending on your choice of Destination. Described in detail below. Refresh Refreshes the list of shown configurations. Options This option is not available in workflow package selection mode. Expand all Expands all folders. Collapse all Collapses all folders. Exporting to Local Destination When you select to export to Local Destination, the following dialog box opens. In this dialog box, specify the Package name and Package version . The Output option can also be chosen  the file can either be downloaded to your browser as an MZP file or committed (inserted) to the system. Open Local Workflow Package Export Specify the following information in the Workflow package export options: Package name : The name of the workflow package. Package version : The version of the workflow package. Output option : Specify the output option of the workflow package. Select the Download option if you want to download the file to your browser as an MZP file or select the Commit option to create the Workflow package and import into the current system. Exporting to Remote Destination When you select to export to Remote Destination , the following dialog box appears. Select the File System Profile of type AWS S3. When the profile is selected more fields will show. Open Remote Workflow Package Export Specify the following information in the Workflow package export options: Package name : The name of the workflow package. Package version : The version of the workflow package. Output option : Only Download is valid when the destination is Remote Select Export Folder: A table showing the folders in AWS S3 Bucket. Select a folder to export to, when no folder is selected the export will be uploaded to Bucket top level. How To Perform an Export Note! Since no workflow state, i.e. File Sequence Number, is included in the exported data and only the initial value is exported, you need to take note of information such as file sequence numbers in Collector agents. Select Export Type, Configuration, or Workflow Package . Select Destination, Local or Remote In the System Exporter, select options according to your preferences in the Options dialog. Expand the folders in Available Entries and select the checkboxes in the Include column for the entries you want to export. Click the Export button to start the export process or to fill in the information regarding the Remote destination or Workflow Package name.

---

# Document 515: Request UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643542/Request+UDR
**Categories:** chunks_index.json

The Request UDR is used to send requests from the UI Builder Agent. The following fields are included in the Request UDR : Field Description Field Description body (bytearray) This field contains the HTTP message body. clientHost (string) This field contains the client IP. clientPort (int) This field contains the client port. contextID (long) This field contains the context ID files (map<string,bytearray>) This field may contain files sent in a POST request with Encoding as MULTIPART. It will consist of a key-value pair, where the key is the file name and the value is the byte array of the file. headerFields (map<string,list<string>>) This field may contain an HTTP header. The header fields are stored as key-value pairs. httpMethod (string) This field must contain the HTTP method. requestedUri (string) This field contains a requested URI. params (map<string,list<string>>) This field may contain HTTP query parameters and/or POST parameters parsed from request body. username (string) This field contains username of logged in user.

---

# Document 516: Duplicate Batch Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032712/Duplicate+Batch+Agent+Configuration
**Categories:** chunks_index.json

To open the Duplicate Batch agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and click the Processing tab. Select Dup Batch from the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. Open Duplicate Batch Detection agent configuration dialog Setting Description Setting Description Profile Click Brows to select from a list of already defined Duplicate Batch profiles. All workflows in the same workflow configuration can use separate Duplicate Batch profiles, however it is not possible to map MIM Values with different names via different profiles. The mapping of MIM values for the Duplicate Batch agent is done in the agent for the entire workflow configuration. In order to appoint different workflow profiles, the Field Settings found in the Workflow Properties dialog must be set to Default . When this is done each workflow in the Workflow Table can be appointed the correct profile. Only MIM check data passing this agent Select this check box to only perform MIM duplicate checks for data passing the Duplicate Batch agent. If this checkbox is not selected, MIM duplicate checks will be performed for all data, regardless of how it is routed in the workflow. Named MIM/Source Timestamp Click the MIM button to select from a list of user defined MIMs to be defined for the profile. MIM Resource A list of existing MIM values to be mapped against the user defined Named MIMs. This list of MIMs is defined in the selected Profile . In a workflow, you need to configure which MIM should correspond to each Named MIM . Add Click Add to display the MIM Browser from which to select MIM values to populate the Logged MIMs table. Logged MIMs Selected MIM values to be used in duplicate detection messages.

---

# Document 517: Logging - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611574/Logging
**Categories:** chunks_index.json

Problems related to submission of a Spark application are logged in the Platform log. Errors that occur after submission of a Spark application, i e runtime errors, are logged in the the Spark environment. Error information related to Kafka and Zookeeper services can be found in the SC-logs for the respective service. Runtime Errors Cluster Runtime errors that occur in the cluster are logged in SPARK_HOME/logs . Spark Application Runtime errors that occur in the Spark application when it is running are logged in the file SPARK_HOME/work/driver-<number>-<number>/stderr . Runtime errors on the executor level are logged in the file SPARK_HOME/work/app-<number>-<number>/stderr You can also access these logs from the Spark Master Web UI: Click a Worker id under Running Drivers . Open Spark UI - Master Click stderr under Logs . Open Spark UI - Worker KPI Processing Accumulators When a Spark batch has finished processing, a set of accumulators are logged in the file SPARK_HOME/work/driver-<number>/stdout . These accumulators serve as a summary of what has been collected and calculated within the batch. The following accumulators are logged: Accumulator Description Accumulator Description CalculatedKPIs This accumulator includes GeneratedKPIOutputs and calculated KPIs that are not closed yet. DiscardedKPIs This accumulator is incremented by one for each calculated KPI that belongs to a previously closed period . FailedMetricCalculations This accumulator is incremented by one for each metric calculation that fails, e g due to invalid data in the input records . If there are several nodes in the node tree(s) that contain the metric, one input record may affect several metric calculations. FailedKPICalculations This accumulator is incremented by one for a KPI calculation that fails due to undefined metrics in the KPI expression. In order for the accumulator to be incremented, the following conditions must apply: - The period for the KPI ends during the Spark batch. - The KPI expression uses multiple metrics and one or more of these are undefined. GeneratedKPIOutputs This accumulator is incremented by one for each successfully calculated and delivered KPI. MissingExpressionForInputType This accumulator is increased by one for each input record that does not match a metric and a dimension object in the service model. Example - Counters in stdout The example below indicates that 20 input records failed to match both a metric and dimension expression in the service model. ============= SPARK BATCH: 2023-10-19 12:35:20:0 =============== CalculatedKPIs = 222 GeneratedKPIOutputs = 200 MissingExpressionForInputType = 20 DiscardedKPIs = 0 FailedMetricCalculations = 0 FailedKPICalculations = 0 You can also access these accumulators from the Spark Master Web UI: Click a Worker id under Running Drivers . Open Click stdout under Logs . Note! The accumulators are logged using log4j, meaning that the configured log level will decide whether or not the accumulators will be logged. The log level is specified in submit.sh by assigning log4j_setting and supply --conf spark.driver.extraJavaOptions=$log4j_setting. The default log level in Spark is WARNING and the log level for the accumulators is INFO . Note! It is possible to log the accumulators to a separate log file by adding the log4j.properties log4j.appender.accumulatorlog=org.apache.log4j.RollingFileAppender log4j.appender.accumulatorlog.File=accumulators.log log4j.appender.accumulatorlog.layout=org.apache.log4j.PatternLayout log4j.appender.accumulatorlog.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n log4j.logger.com.digitalroute.mz.spark.StreamOperations$=INFO, accumulatorlog log4j.additivity.com.digitalroute.mz.spark.StreamOperations$=false

---

# Document 518: Data Veracity Task Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032375/Data+Veracity+Task+Agent
**Categories:** chunks_index.json

The Data Veracity Task agent configures and schedules automated repair tasks for UDRs located in Data Veracity tables. When triggered by a workflow group scheduler, the agent will execute all repair rules that were added into the task agent itself. To open the Data Veracity task agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button Open Data Veracity Task agent configuration Data Veracity Tab Setting Description Setting Description Profile Click Browse to select a predefined Data Veracity Profile. The profile contains the configuration of the Data Veracity connection details and the Data Veracity table schema sql generation. For further information, see Data Veracity Profile . Repair Configurations Adding a repair configuration into the table will allow the agent to determine which UDR to repair, which saved filter to use to source out the UDRs that needed to be repaired as well as which repair rule to execute on the chosen UDRs. Error Behaviour On Error This combo box displays two options that determine how the workflow will behave when the repair job encounters an issue during execution. When set to Ignore, the agent will ignore any errors by a repair job and continue executing the next repair job in the Repair Configurations table. When it is set to Abort, the agent will trigger the workflow to abort when a repair job hits a single error. The default value is Ignore.

---

# Document 519: Aggregation Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998848
**Categories:** chunks_index.json

Take into account the following behaviors when using the Aggregation profile: You can apply an Aggregation profile to any number of workflow configurations. Aggregation sessions created in the storage that is specified by the profile can be accessed by multiple active workflows simultaneously. When you have selected file storage and are using an Aggregation profile across several workflow configurations, you must consider the read-and-write lock mechanisms that are applied to the stored sessions. For further information about read-and-write locks, see Aggregation Agent Configuration - Batch and Aggregation Agent Configuration - Real-Time . The Aggregation profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Session UDR Each Aggregation profile stores sessions of a specific Session UDR type that you define in ultra. This means that your Aggregation profile configuration must include a session UDR type. See the example below: Example - Defining a Session UDR type in an Ultra Configuration session SessionUDRType { int intField; string strField; list<drudr> udrList; }; It is recommended that you keep the session UDR as small as possible. A larger UDR decreases performance compared to a small one. Note! Take particular care when updating the Ultra formats. It is not possible to collect data from the Aggregation session storage if the corresponding UDR has been renamed. However, if you change the format definition, you can still collect the data. Changes to the formats are handled as follows: Default values are assigned to fields that are added or renamed. Fields that have been removed are ignored. Default values are assigned to fields with data types that have been changed. For further information on Ultra formats, see Ultra Reference Guide . Configuration The contents of the buttons in the menu bar may change depending on which configuration type has been opened in the currently active tab. The Aggregation profile uses the standard buttons that are visible for all configurations, and these are described in Build View . The profile consists of four tabs: Session Tab Association Tab Storage Tab Advanced Tab Session Tab In the Session tab you can browse and select a Session UDR Type and configure the Storage selection settings. Open Aggregation profile configuration - Session tab Setting Description Setting Description Session UDR Type Click on the Browse... button and select the Session UDR Type. The Session UDR is defined in Ultra see Aggregation Profile | Session UDR above for more information. Storage Select the type of storage for aggregation sessions. The available settings are: Couchbase Storage Elasticsearch Storage File Storage Memory Only Redis Storage SQL Storage File Storage and Memory Only can be used in batch and real-time workflows. Elasticsearch Storage and SQL Storage can only be used in batch workflows. Couchbase Storage and Redis Storage can only be used in real-time workflows. These storage types allow highly available systems with geographic redundancy. The session data that is replicated within the storage is available across workflows, EC Groups, and systems. This serves to minimize data loss in failover scenarios. Note! Data stored in Couchbase or Redis is not available in the Aggregation Session Inspector . Open Session tab in the Aggregation profile Association Tab You use the Association tab to configure rules that are used to match an incoming UDR with a session. Every UDR type requires a set of rules that are processed in a certain order. In most cases, only one rule per incoming UDR type is defined. You can use a primary expression to filter out UDRs that are candidates for a specific rule. If the UDR is filtered out by the primary expression, it is matched with the existing sessions by using one or several ID fields as a key. For UDRs with ID Fields matching an existing session, an additional expression may be used to specify additional matching criteria. For example, if dynamic IP addresses are provided to customers based on time intervals, the field that contains the IP address could be used in ID Fields while the actual time could be compared in Additional Expression . Open Aggregation profile configuration - Association tab Setting Description Setting Description UDR Types Click on the Add button to select a UDR Type in the UDR Internal Format dialog. The selected UDR type will then appear in this field. Each UDR type may have a list of rules attached to it. Selecting the UDR type will display its rules as separate tabs to the right in the Aggregation profile configuration. Primary Expression The Primary Expression is optional. Enter an APL code expression that is going to be evaluated before the ID Fields are evaluated. If the evaluation result is false the rule is ignored and the evaluation continues with the next rule. Use the input variable to write this filtering expression. ID Fields Click on the Add button to select additional ID Fields in the ID Fields dialog. These fields, along with the Additional Expression settings enable the system to determine whether a UDR belongs to an existing session or not. If the contents of the selected fields match the contents of a session, and an Additional Expression evaluation results in true , the UDR belongs to the session. Note! Ensure that the selected fields are of the same type and appear in the same order for all the rules that are defined for the agent. Additional Expression The Additional Expression is optional. Enter an APL code expression that is going to be evaluated along with the ID Fields. Use the input variable to write this filtering expression. The Additional Expression is useful when you have several UDR types with a varying number of ID Fields, that are about to be consolidated. Having several UDR types requires the ID fields to be equal in number and type. If one of the types requires additional fields that do not have any counterpart in the other type or types, these must be evaluated in the Additional Expression field. Save the field contents as a session variable, and compare the new UDRs with it. For an example, see Association - Radius UDRs in Aggregation Example - Association of IP Data . Note! When using Additional Expressions for Aggregation the caching mechanism only takes into account the primary and secondary rules when creating the session CRC. This means that if the number of sessions that cannot be told apart without the use of an Additional Expression is high, the performance of the Aggregation Agent decreases due to cache read/write operations. This is especially true if the Max Cached Sessions property is low compared to the number of sessions. For this reason, it is recommended that Max Cached Sessions is set to a high value when using Additional Expressions. Create Session on Failure Select this check box to create a new session if no matching session is found. If the check box is not selected, a new session will not be created when no matching session is found. Note! If you provide a primary expression, and it evaluates to false , the rule is ignored and no new session is created. If the order of the input UDRs is not important, all the rules should have this check box checked. This means that the session object is going to be created regardless of the order in which the UDRs arrive. However, if the UDRs are expected to arrive in a particular sequence, Create Session on Failure must only be selected for the UDR type or field that is considered to be the master UDR, i e the UDR that marks the beginning of the sequence. In this case, all the slave UDR types or fields are targeted for error handling if they arrive before their master UDR. Note! At least one of all defined rules must have this check box selected. Otherwise, no session will ever be created. For further information about all available system properties, see System Properties . Add Rule Click this button to add a new rule for the selected UDR Type. The rule will appear as a new folder to the right of the UDR Types in the Aggregation profile configuration. Usually, only one rule is required. However, in a situation where a session is based on an IP number, stored in either a target or source IP field, two rules are required. The source IP field can be listed in the ID Fields of the first rule and the target IP field listed in the ID Fields of the second rule. Remove Rule Click this button to remove the currently displayed rule. Storage Tab The Storage tab contains settings that are specific for File Storage, Couchbase Storage, Redis Storage, Elasticsearch Storage, and SQL Storage. Couchbase Storage Open Aggregation profile configuration - Couchbase Storage Setting Description Setting Description Profile Select a Couchbase Profile . This profile is used to access the primary storage for aggregation sessions. Mirror Profile Selecting this Couchbase profile is optional. It is used to access secondary storage, providing read-only access for aggregation sessions. Typically, the Mirror Profile is identically configured to a (primary) Profile , that is used by workflows on a different EC Group or other MediationZone system. This is useful to minimize data loss in various failover scenarios. The read-only sessions can be retrieved with APL commands. For more information and examples, see Aggregation Functions . Open Mirror profile concept Elasticsearch Storage Open Aggregation profile configuration - Elasticsearch Storage Setting Description Setting Description Elasticsearch Select an Elasticsearch Profile . This profile is used to access the storage for aggregation sessions. File Storage Open Aggregation profile configuration - File Storage settings Setting Description Setting Description Storage Host Select a Storage Host from the drop-down list. For storage of aggregation sessions select either a specific EC Group or Automatic . If you select Automatic , the same EC Group that has been used by the running workflow will be applied. Alternatively, if the Aggregation Session Inspector is used, a storage host is selected automatically. Refer to Aggregation Session Inspector for further information on the Aggregation Session Inspector. Note! It is recommended that you configure the aggregation workflow to run on the same EC Group that you have selected as Storage Host . Directory Enter the directory on the Storage Host where you want the aggregation data to be stored. Note! If the Storage Host above, is configured to be Automatic , the corresponding Directory has to be a shared file system between all the EC Groups. If this field is greyed out with a stated directory, it means that the directory path has been hard-coded using the mz.present.aggregation.storage.path property. This property is set to false by default. Example - Using the mz.preset.aggregation.storage.path property To enable the property and state the directory to be used: mzsh topo set val:common.mz.preset.aggregation.storage.path '/mydirectory/agg' To disable the property: mzsh topo unset val:common.mz.preset.aggregation.storage.path Partial File Count In this field, you can enter the maximum number of partial files that you want to store. Consider the following: Startup: All the files are read at startup. It takes longer if there are many partial files. Transaction commitment: When the transactions are committed, many small files (large Partial File Count) increase performance. In a batch workflow, use this variable to tune performance. Note! In a real-time workflow, updates to sessions are saved on disk only if the Storage tab is configured with Storage Commit Conditions . Max Cached Sessions Enter the maximum number of sessions to keep in the memory cache. This is a performance-tuning parameter that determines the memory usage of the Aggregation agent. Set this value to be low enough so that there is still enough space for the cache in memory, but not too low, as this will cause performance to deteriorate, see Performance Tuning with File Storage for more information. Enable Separate Storage Per Workflow This option enables each workflow to have a separate session storage. Multiple workflows are allowed to run simultaneously using the same Aggregation profile. If this checkbox is selected, a workflow will never see a session from another workflow. Note! Sometimes, you may notice that file storage takes up more space than expected. This is expected behavior. Read through this note for an overall understanding of the way file storage in Aggregation works. When session data is stored, it is appended to the session file. This means that old session data from the session file is still present in the storage and the current version is added to the file. Removal of old data is done only under certain conditions because otherwise, aggregation handling would be too slow. This is why file storage takes up more space than calculated with session number and single session object size. The session files on the disk grow up to a certain threshold ( 50MB by default) and then a new file is created and used. The old session file will be deleted when no more active sessions are stored in it. The accepted size of a session file can be adjusted by using aggregation.min_session_file_size parameter. For instance, aggregation.min_session_file_size=20000000 will set it to 20MB. This parameter is set with the mzsh topo command on EC, cell, or container level. Old files are removed during the storage commit. Also, since there is a possibility that there will be old session files present because of some long-lived sessions stored there, a defragmentation algorithm is implemented. It runs occasionally and moves those long-lived sessions to new session files so that old session files can be deleted. This is why aggregation storage takes up a lot of disk space. It is designed to provide higher performance at the expense of higher disk space consumption. Memory Only Open Aggregation profile configuration dialog - Memory Only storage When you have selected Memory Only as storage, there are no additional settings in the Storage tab. Redis Storage Open Aggregation profile configuration dialog - Redis Storage Setting Description Setting Description Profile Select a Redis Profile . This profile is used to access the storage for aggregation sessions. SQL Storage Open Aggregation profile configuration dialog - SQL Storage Setting Description Setting Description Profile Select a Database Profile configured with the SQL database type. This profile is used to access the storage for aggregation sessions. Note! Currently the SQL storage only supports PostgreSQL and SAP HANA databases. Storage sharing functionality is currently not supported. Index Fields Click the Add button to select the UDR type. Table SQL Script This text box will generate the SQL statements for the selected UDRs' table schema and indexes for Id, TxId. The schema will be generated based on the number of UDRs in the UDR Type Mapping table. Note! Users will have to copy the SQL script generated in the text box to create the PostgreSQL and SAP HANA tables on their own in the database listed in the Database profile. The Aggregation profile will not automatically create the tables for you. Note! The following table columns are mandatory when creating the database: Column Name Data Type Id VARCHAR(24) TxId BIGINT Deleted BOOLEAN Timeout BIGINT Session BYTEA Advanced Tab The Advanced tab is available when you have selected Couchbase Storage , Redis Storage , Elasticsearch Storage or SQL Storage in the Session tab . It contains properties that can be used for performance tuning. For information about performance tuning, see Aggregation Performance Tuning . These fields supports parameterization using ${} syntax, see Appendix 1 - Profiles for more information on how parameterization works. Couchbase Storage Open The Aggregation profile configuration dialog - Advanced tab for Couchbase Storage You can also set the properties listed in the Advanced tab as Execution properties in the STR. This will override the values that are set in the profile, including default values. Example - Overriding the Advanced properties $ mzsh topo set topo://container:container1/pico:ec1/val:  config.properties.mz.cb.agg.json_serializer.format MZ-BIN Note! See the Note at the end of this page for more information when using pessimistic or optimistic locking mechanisms for Couchbase aggregation storage. Elasticsearch Storage Open The Aggregation profile configuration dialog - Advanced tab for Elasticsearch Storage For Elasticsearch Storage, you can modify the properties listed as shown above in the Advanced tab. Redis Storage Open The Aggregation profile configuration dialog - Advanced tab for Redis Storage For Redis Storage, you can only modify the properties in the Advanced tab. Note! See the Note at the end of this page for more information when using optimistic locking for Redis aggregation storage. SQL Storage Open The Aggregation profile configuration dialog - Advanced tab for SQL Storage For SQL Storage, you can modify the properties listed as shown above in the Advanced tab. Note! When using Couchbase or Redis aggregation storage, it is important to take note of the concept of locking mechanisms when configuring workflows. Locking mechanisms are of two types: Pessimistic and Optimistic. Redis aggregation storage only has an Optimistic lock whereas Couchbase aggregation storage has both Optimistic and Pessimistic locks. Pessimistic Lock When a workflow thread is working on a session, it is considered to be fully locked. No other thread can work on that particular session. Once the first thread is finished, the lock is released and another thread can take the lock and work on the session. Optimistic Lock Instead of acquiring a traditional lock for a session, a workflow thread obtains a CAS (Compare And Swap) for that session. The CAS serves as a kind of hash or fingerprint of the session data. When the consume block is done and the session is ready to be updated, an error occurs if the CAS no longer matches. In scenarios where multiple threads have made updates to the same session, only the changes from the first thread to complete its work are accepted. Any other thread(s) attempting to update will encounter failure and need to restart their work from the beginning. This process ensures that only changes from one workflow at a time can be committed, akin to the principles of pessimistic locking. It's essential to understand a key distinction: the consume and sessionInit blocks may be invoked multiple times due to the retry mechanism mentioned earlier. As a result, it's advisable to avoid using global variables within the aggregation APL. However, the udrRoute function can be safely utilized within these blocks since it is executed only when the Optimistic lock succeeds. If global variables are necessary, they can be relocated to an analysis agent and updated through the udrRoute function. It is important to note that the threads specified in the locks above may live in multiple processes on multiple machines. Note! Threads may live in multiple processes on multiple machines.

---

# Document 520: GCP BigQuery Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033273
**Categories:** chunks_index.json

The GCP BigQuery agent is a batch forwarding agent that inserts UDR data into the BigQuery Data Warehouse in a GCP Project specified by the agent configuration, based on user-defined mappings between UDR fields and BigQuery database table columns. A special column in the target data table is also assigned a unique Transaction ID, generated for each batch. In relation to this, a batch status table is utilized to indicate the batch status for the unique Transaction ID. Open GCP BigQuery Agent Workflow Note! Due to the nature of BigQuery, the order of insertion of the UDR data into the BigQuery table is done out of sequence and will appear as such in the target table. Sorting will have to be done from BigQuery itself. Note! The GCP BigQuery batch forwarding agent is implemented using a streaming API called the tabledata.insertAll method. You will need to refer to the GCP Bigquery documentation to ensure it is supported for your BigQuery tier. This section includes the following subsections: GCP BigQuery Agent Configuration GCP BigQuery Agent Transaction Behavior GCP BigQuery Agent Input/Output Data and MIM GCP BigQuery Agent Events

---

# Document 521: System Topology Registry - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647037/System+Topology+Registry
**Categories:** chunks_index.json

The STR is used for managing pico configurations, service configurations, and attributes that control the behavior of MediationZone. The STR data is stored in MZ_HOME/common/config/cell and consists of the following: Master registry - The master registry is used for staging changes in the STR. You can update the files in the master registry manually in an editor or using an mzsh command. Active registry - When you are using the mzsh command topo to update the master registry, the changes are automatically validated and propagated to the active registry. The active registry files are used when you start a pico process. These files are hidden and should not be edited directly by users. Backup registry - When the active registry is updated the previous data is stored in the backup registry files. These files are hidden and should not be edited directly by users. Templates - Templates contain pico definitions and default properties. A template have a namespace based on its parent directory. For instance the namespace of the predefined templates is mz . Containers - Containers are used by the STR to reference a MediationZone installation on a host. Container Groups - A container group contains the configuration of picos that can be started on a set of containers. The STR currently supports a single pre-defined container group named default . Pico configurations in this group can be started in any container in the system. Cells - A cell contains a set of Container Groups and can be considered synonymous with "system". At the time of writing, only one predefined cell ( default ) is available. Open STR structure - MZ_HOME/common/config/cell This chapter includes the following sections: HOCON Format STR File Structure STR Replication Working with STR

---

# Document 522: IPDR Compliance - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646956/IPDR+Compliance
**Categories:** chunks_index.json

MediationZone is fully IPDR compliant. UFDL also supports IPDR compact decoding and encoding. To apply decoding or encoding specify the option ipdr_compact in the map definition.

---

# Document 523: Java - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657773/Java
**Categories:** chunks_index.json

Oracle JDK 17 or OpenJDK 17 must be installed on the server running Platform Container and Execution Containers.

---

# Document 524: Editing a UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646780/Editing+a+UDR
**Categories:** chunks_index.json

Double-click a UDR in the UDR File Editor to open the Edit UDR dialog where you can edit the UDR. Open Edit UDR dialog Column Description Column Description Name Displays the names of the UDR fields. A mandatory field can be edited by clicking the Edit button, but the Present checkbox cannot be cleared. An optional field can be edited, and the Present checkbox can be cleared. The value of a field can be cleared by clicking Clear . Note that removing a field containing sub-fields and then creating it again does not recreate the sub-fields. The same applies to elements in a list field. The special field OriginalData holds the raw UDR input data which cannot be edited, only viewed by clicking the View button. Value Displays the field values. To edit a value, click Edit to open a dialog with the name of the field where you can change the value of the field. Present Shows whether the field is present in the UDR View or not. Action Contains the avilable Action buttons: Edit - Opens a dialog with the name of the field where you can enter a different value. New - Is available if you have cleared a field, and allows you to enter a new value in the Value field. Clear - Clears the value in the Value field View - Opens up the bytearray in the dialog OriginalData (bytearray) dialog Show Only Present Select this checkbox to only display present fields (as specified by the Present flag)

---

# Document 525: SAP JCo Uploader Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674597/SAP+JCo+Uploader+Agent
**Categories:** chunks_index.json

The SAP JCo Uploader agent is a batch forwarding agent, which sends records to SAP Convergent Invoicing (SAP CI) using the JCo protocol. The workflow which includes the SAP JCo Uploader agent, processes files in CSV format with a large number of rows, e g 100,000 rows each, in sub batches, the size of which you determine in the SAP Jco Uploader agent configuration dialog. When a sub-batch of records has been committed, the next sub-batch is processed, however, sub-batches can be processed simultaneously, depending on the No of Threads option is determined in the agent configuration dialog. The SAP JCo Uploader agent inserts states into a remote database, and once the file has been processed, the information is removed from the database. If processing of a file is interrupted for example, if the workflow aborts or an error occurs, processing continues from an offset corresponding to where in the file it left off when interrupted. To determine the offset, the agent performs a database lookup and gets the list of records that are already processed. In the file that contains the records to be sent to SAP CI, the first record contains the RemoteFunctionCallName and FileFormat , and the rest of the file is the content so that a HeaderUDR must be sent, and then for the content, a RecordUDR must be sent. For further information, see SAP JCo Uploader Agent UDRs . This section describes the SAP JCo Uploader agent. The agent is a forwarding agent for batch workflow configurations. Loading

---

# Document 526: Agent Selection Dialog - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604724
**Categories:** chunks_index.json

The Agent Selection dialog contains all the available agents, represented as icons. Different icons are available, depending on which kind of workflow type that you have selected. There are different agents for Batch, Realtime, or Task workflow types. You open the Agent Selection dialog from a workflow configuration. To open the Agent Selection dialog, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select either Batch, Realtime Or Task . Click Add Agent to open the Agent Selection dialog. Agent Selection dialog - Processing tab Real-time and batch workflow configurations contain three types of agents, collection, processing, and forwarding. These are sorted into different tabs, depending on the type of data an agent expects and delivers. The tabs are labelled Collection , Processing, and Forwarding . Task Workflow configurations have only one agent type and always contain a single agent. The different agent types Batch and Real-Time Workflow Agents Collection Agents A collection agent is responsible for the gathering of data from external systems or devices, such as opening a file and reading it byte-by-byte and then passing it into the workflow. A collection agent must produce one or several data types. The workflow configuration validates that the data types between the collection agent and the connected agents are compatible. Note! Real-time collection agents may also receive data from the workflow in order to send responses to the external system or device. Processing Agents A processing agent expects to be fed with data and to deliver data on one or several outgoing routes. Inside a workflow, data propagates between agents as streams, that is, as a flow of bytes or UDRs. A simple processing agent can, for example, be a counter counting the throughput. There are also more complex types, for instance, agents that, depending on the processed result, deliver data on different routes. The processing agent decodes (translates) an incoming byte stream into a UDR object and the encoding agent does the opposite. Forwarding Agents A forwarding agent is responsible for distributing data from the workflow to other systems or devices. An example would be to create a file from a data stream and transfer it to another system using FTP. Different agents act differently on the Begin Batch and End Batch messages. The forwarding agents, for instance, need a set of boundaries in order to close a file or commit a database transaction. A forwarding agent must receive one or several data types. The workflow configuration validates that the data types between the forwarding agent and the connected agents are compatible. For detailed information about specific collection-, processing-, and forwarding agents, see Appendix 2 - Batch and Real-Time Workflow Agents . Task Workflow Agents Task workflow agents are used to execute system housekeeping tasks such as removing files or cleaning up database tables. These agents do not receive any data from other agents nor do they publish data to other agents. For detailed information about specific task agents, see Appendix 3 - Task Workflow Agents .

---

# Document 527: Google Secret Manager Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/227311617/Google+Secret+Manager+Profile
**Categories:** chunks_index.json

You can use the Google Secret Manager profile to setup up the access credentials and properties for connecting to a Google Secret Manager environment. Currently, the profile can be used in the following Profiles: Secrets Profile Security Profile Buttons The contents of the buttons bar may change depending on which configuration type has been opened in the currently displayed tab. The Google Secret Manager profile uses the standard buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit menu is specific for the Google Secret Manager Profile configurations. Button Description Button Description External References Click on this button to enable External References in the Google Secret Manager profile configuration. This can be used to configure the following fields: Project Id Private Key Id Private Key Client Email Client Id Other Information For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . Note! If there is a proxy in your network environment, it will work with a proxy that does not require authentication. Currently, it does not work with a proxy that requires authentication. Refer to HTTP Proxy Support for more details. Configuration Open Google Secret Manager Profile Setting Description Setting Description Environment-Provided Service Account When MediationZone is deployed in a GCP environment, such as in Compute Engine, enable this option to retrieve the Service Account credentials provided by the environment. Project Id The GCP Project Id that hosts the GCP service that MediationZone should access. Private Key Id The Private Key Id to be used for the service account. Private Key The full content of the private key. Client Email The email address given to the service account. Client Id The Id for the service account client. Other Information The Auth URI, Token URI and info about the certs are to be added into this field.

---

# Document 528: FTP Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607534/FTP+Forwarding+Agent
**Categories:** chunks_index.json

The FTP forwarding agent sends files to a remote host by using the standard FTP (RFC 959) protocol. Files are created when a Begin Batch message is received and closed when an End Batch message is received. In addition, the agent offers the possibility to compress (gzip) the files. To ensure that downstream systems will not use the files until they are closed, they are maintained in a temporary directory on the remote host until the End Batch message is received. This behavior is also used for Cancel Batch messages. If a Cancel Batch is received, file creation is canceled. The FTP fo rwarding ag ent su ppor ts IPv4 and IPv6 environments. This section contains the following subsections: FTP Forwarding Agent Configuration FTP Forwarding Agent Memory Management FTP Forwarding Agent MultiForwardingUDR Input FTP Forwarding Agent Input/Output Data and MIM FTP Forwarding Agent Events FTP Forwarding Agent Transaction Behavior

---

# Document 529: Archive Inspector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640167
**Categories:** chunks_index.json

The Archive Inspector is used to locate files in an archive. The respective access group user can launch and purge files found in the archive. As soon as the target file is located, it is treated as a regular UNIX and standard commands can be used to interact with it. Note! It is not encouraged to alter or remove a file from the archive using UNIX commands. If altering is desired, make a copy of the file. If removal is desired, use the Archive Inspector. To open the Archive Inspector, click the Manage button from the top bar and select Archive Inspector from the list. Open The Archive Inspector Main Window Info Sensitive information is blurred on the screen. You will see the actual path of the file when using the Archive Inspector. Searching the Archive Initially, the dialog is empty and must be populated with data using the corresponding Search Archive dialog. When you click on the Search button, the Search Archive dialog appears where you can select the number of rows you want to view in the Archive Inspector . Each row represents information about a data batch (file). Open The Search Archive dialog Settings Description Settings Description Archive Profile Select the Archive profile that corresponds to the data of interest. If no profile is selected archive entries for all profiles will be shown. Workflow Option to narrow the search with respect to the workflow that archived the file. Agent Option to narrow the search with respect to the agent that archived the file. Period Option to search for data archived during a certain period. You can either select the User Defined option in the drop-down list and then enter date and time in the From and To fields, or you can select one of the predefined time intervals in the drop-down list; Last Hour, Today, Yesterday, This Week, Previous Week, Last 7 Days, This Month or Previous Month. Accessing the Inspector The Archive Inspector table shows the file(s) based on the search criteria specified in the Archive Inspector#Search Archive dialog. Open Archive Inspector Table - Selection Each file (row) in the table has a set of properties associated with it. Property Description Property Description ID Refers to the ID of the archived entry. Workflow Full name of the archiving workflow. The format is <folder>.<configuration>.<workflow name> Agent Name of the archiving agent. File path The full path of the file stored on disk. Timestamp The time when the entry was inserted in the archive. MIM Values Adherent MIM resources are defined as Logged MIM Data in the Archiving agent configuration dialog. Profile Name of the profile used to archive the file. Note! If the profile is not available, < Profile not found > message is displayed. An example screen: Open Table Toolbar The table toolbar for the Archive Inspector presents with more options to manipulate data within the table. Table Toolbar - Default View The following are the buttons found on the toolbar: Default options Option Description Search Displays the Search Archive dialog where search criteria may be defined to identify the files to be displayed, see Archive Inspector#Search Archive for more information. Refresh Refreshes the table. Delete All Deletes all shown items. With one file selected Clear Selection(s) Clears the currently selected item(s). View Raw Data Opens the Raw Data Viewer screen. An example of a Raw Data Viewer dialog : Open View MIM Values Shows the associated MIM values. For further information, see the figure below: MIM Values Dialog Note! The dialog also appears when you double-click on a row of the table. Delete Deletes the currently selected file. With multiple file(s) selected Clear Selection(s) Clears the current selection. Delete Delete selected files. Delete If Keep Files is disabled in the Archive profile, all selected files are removed from the archive, including their corresponding references in the database. If Keep Files is enabled or if the Archive profile that archived the entry is not available, only the references are removed while the files shown in Archive Inspector are still kept on disk. Refer to the documentation on the Keep Files option to know more.

---

# Document 530: Amazon SQS Forwarding Agent Input/Output and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/299663596/Amazon+SQS+Forwarding+Agent+Input+Output+and+MIM
**Categories:** chunks_index.json

Input/Output Data The SQS Forwarding Agent receives UDRs of type SqsForwarderCycleUDR and forwards them as messages to Amazon SQS. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop | MIM . Publishes The SQS Forwarding Agent does not publish any MIMs. Accesses The SQS Forwarding Agent does not access any MIM parameters.

---

# Document 531: Agent Plugin Examples - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742433/Agent+Plugin+Examples
**Categories:** chunks_index.json

You will find several plugin examples in the core/dtk/examples/ directory. The Disk Collection agent plugin example also includes an example documentation package.

---

# Document 532: Workflow Bridge Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675555/Workflow+Bridge+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events

---

# Document 533: Building Blocks - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657307/Building+Blocks
**Categories:** chunks_index.json

UFDL uses the following building blocks: Block Description Block Description External formats An external format describes the physical layout of the raw data. It can be declared in multiple ways such as through an asn_block (for BER or PER encoded formats) or an external block. Internal formats An internal format describes the structure of the internal, working UDR type. These formats are used in workflow activities (such as APL processing) and are declared through an internal block or generated from an automatic in-map (through a target_internal specification). In-map A mapping from an external type to an internal type. This is the basic component used to describe a decoder, and it is declared through the in_map specification. Note! Mapping between different Ultra formats may work, but is not supported, for example, between xml format and ASN.1 formats. Instead you need to add an Analysis agent to execute the conversion. Out-map A mapping from an internal type to an external type. This is the basic component used to describe an encoder, and it is declared through the out_map specification. Note! Mapping between different Ultra formats may work, but is not supported, for example, between xml format and ASN.1 formats. Instead you need to add an Analysis agent to execute the conversion. Decoder A decoder uses one or more in-maps to specify a set of decoding rules to be used when decoding input data. Blocking specification as well as other file structure specifications can also be added. Encoder An encoder uses one or more out-maps to specify a set of encoding rules to be used when decoding input data. Blocking specifications can also be added. As an example, consider a incoming call detail record containing information on the type of call, who made the call and which number was dialed. That means that the record contains three fields. Two fields and new termination marks ( ; ) for each field in the record in order to conform to the expected output format must be added. Concerning both incoming and outgoing external formats, each record (the last field in the record) is terminated by a newline character. Open An example of two external format definitions As can be seen in the image above, An example of two external format definitions , an external specification is used to describe the raw data formats. The UDR format used by the processing agents is generated through the target_internal specification. This format is automatically generated based on the incoming external data and includes any fields in the internal specification of the in-map. In this case, the generated internal format consists of the fields from the external incoming format with two additional fields. Hence, we introduce an internal containing the new fields only. Open Relation between declared and generated internal formats Finally, the in_map and out_map definitions are used when creating decoder and encoder definitions to be used. UFDL code of the example is described as follows: external recA { int type : static_size(1); ascii Anum : static_size(8); ascii Bnum : terminated_by(0xA); }; internal recA_int { int field1; int field2; }; external recB { int type : terminated_by(";"); ascii Anum : terminated_by(";"); ascii Bnum : terminated_by(";"); int field1 : terminated_by(";"); int field2 : terminated_by(0xA); }; in_map A_map : external(recA), internal(recA_int), target_internal(recA_TI) { automatic; }; decoder decode_A : in_map(A_map); out_map B_map : internal(recA_TI), external(recB) { automatic; }; encoder encode_B : out_map(B_map); External Network elements delivering data to the system produce records in various physical formats. An external format describes the physical structure of the incoming data, as well as the data delivered by the system. It can be any type of ASCII, XML, or binary data. Typically, one external block per record type is defined. File headers and trailers are also considered to be records. The external formats are described through a number of different methods with different syntax. The external formats, currently supported by the MediationZone Platform, are: Sequential formats: ASCII or binary based on static or dynamic sizes for both records and fields (see External - Sequential Format ). The generic external specifications are defined via the external block. Ericsson IOG/IN formats: (see External - Ericsson IOG/IN Records ). This special format type is described through the inw variant of the external block. ASN.1 based formats: A subset of ASN.1 is supported (see External - ASN.1 Formats ). These formats can be used to specify either BER or PER encodings. XML formats: A subset of the standard XML schema syntax is supported and can be used to handle different XML formats. For further information, see XML Schema Support . Records of different format types may exist in the same record. An example of this is a BER (ASN.1) record that contains a field whose format is a sequential record (specified through an external block). The image below, A definition of a sequential external record , illustrates the syntax for a sequential record type. Open A definition of a sequential external record Internal and In-Map There are two ways to define an internal UDR format in UFDL. Either by using the automatic generation through an in_map specification, or through an internal declaration block. For more complex formats it is often impractical to manually write an internal format if all that is desired is a direct mapping from the fields in the external format. In these cases the target_internal specification in the in_map is used. An additional internal block can be specified in the in_map if additional processing fields are needed. The in_map manages the mapping of external data into the internal UDR structure. An external must be defined, and one or both of the internal and target_internal options must be used to specify the internal format to map to. For information about how to specify an internal format, see Internal Formats . For information about in_map specifications, see In-maps . Example - An internal type declaration internal AXE_DAM { string ANumberAreaCode; string ANumberClass : optional; string BR_IncomingPQR; string BR_NumberClass; intLen ANumber; intLen BRNumber; intLen BSNumber; string PreProcRecordClass; }; An internal type declared through target_internal is automatically generated based on the fields of the external and internal formats. The target_internal is the format used by the processing agents. It can be a direct representation of the external incoming format, or a combination of the external and additional fields from an internal definition. An internal definition is needed in case any changes to the default mapping from the external format are required or if several externals are mapped to the same UDR. To understand the structure of the generated target_internal format, it is necessary to understand how the automatic mapping of an in_map is performed. When automatically mapping an external format (that is, using the automatic keyword), all external fields are mapped to an internal field unless the external format specifies a different automatic mapping behavior (for example through the external_only keyword). The automatic in-map generation performs the following steps for each external field: If the field has an explicit mapping specified, use that mapping (explicit field mapping specifications overrides automatic mapping). If there is a field in the internal format with the same name as in the external , the external format field is mapped to the internal field. Otherwise, a new field is created in the generated internal format with the same name as the external field, and a type is chosen according to the external format (the default mapping type). If the automatic mapping algorithm requires new fields, a new internal type is first created. This type gets the name specified in the target_internal , and is a subtype of the internal type (if specified). Decoders using this in_map then produce records of that internal type. The external field default mapping type depends on the external format specification and if the external field is of a constructed type (a list or record type), the automatic mapping logic is applied recursively to any sub-formats. Other than automatic mappings, it is possible to specify explicit type mappings, where the corresponding external and internal fields are specified directly. Open A resulting target_internal In the image above, a resulting target_internal , a combination of explicit (the e: and i: specifications), and automatic mapping are used to create the resulting target_internal CDR_TI . Event Types Event format types are special internal types that can be sent to the Event Server. Events store additional internal information used during the event processing. For performance reasons, and in order to avoid confusion, event types should only be used as message UDRs dispatched to the Event Server. In other words, do not use event UDRs to transport ordinary UDR data. Event types are declared in the same way as internal types, except for the keyword event , which is used instead of internal . For further information, see Internal Formats . The following code results in the events shown in the image below, Event format showing added fields : event AMA_Event { int anum; int bnum; ... } Open Event format showing added fields For further information about Event handling, see the Desktop User's Guide . Decoder A Decoder defines what external records to accept, and describes any additional file structure information, such as whether file blocking is used in the input data. A decoder definition consists of in_maps or other decoders. The most basic decoder definition uses the single in-map argument. In the following example, myDecoder appears in the configuration of a Decoder agent: Example - Decoder The decoder myDecoder is created by taking one in-map. The incoming data is blocked in 2048 bytes, with the block padding 0x00 . decoder myDecoder : in_map( my_in_map ), block_size(2048), terminated_by(0x00); A decoder definition can consist of several in-maps, that is, the data stream received may contain several record type definitions. Another possible definition is combining several decoders, called constructed decoder . The image below, Two ways of defining a decoder , introduces some validation logic to the incoming stream of data. Open Two ways of defining a decoder The first case, TTFile1 , requires a batch to start with a header, end with a trailer, and contain any number of UDRs in between. The second decoder definition ( TTFile2 ) ignores the order and occurrence of arriving records. For further information about decoder specifications, see Decoders . Out-map An out-map defines how the internal data is mapped to the outgoing format. It requires an internal and an external definition as arguments and maps fields either automatically or through explicit field mappings. Open The minimum required to define an out-map In the image above, The minimum required to define an out-map , it is shown how an external Access_Reject_Ext is created using the internal Access_Reject_Int . The automatic keyword map all fields with the same name, while field names present in the internal , but not the external , are ignored. For further details about how to specify an out_map see Out-maps . Encoder Encoders define the rules for mapping internal UDRs to external data structures (raw data). An encoder must be defined in order to make any out_map usable, that is, selectable from the dialog. The most basic definition has one out_map as its only argument. In the following example, myEncoder appears in the configuration dialog of an Encoder agent: Example - Encoder encoder myEncoder : out_map( my_out_map ); Similar to a decoder, an encoder takes one or several out-maps to define the structure of the outgoing data. For detailed information about encoder specifications, see Encoders .

---

# Document 534: Workflow Bridge Profile Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675515/Workflow+Bridge+Profile+Configuration
**Categories:** chunks_index.json



---
**End of Part 24** - Continue to next part for more content.
