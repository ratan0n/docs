# RATANON/MZ93-DOCUMENTATION - Part 26/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 26 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.8 KB
---

The BatchData APL functions are used for managing the work orders and work logs used in the Usage Management Workflow Templates. The BatchData Support functions include: 1 pccBatchDataAddWorkorder 2 pccBatchDataBeginWorklogs 3 pccBatchDataAddWorklog 4 pccBatchDataAddWorklogs 5 pccBatchDataSuspendWorklogs 6 pccBatchDataResumeWorklogs 7 pccBatchDataGetWorkorders 8 pccBatchDataWorklogs 9 pccBatchDataRemove pccBatchDataBeginWorkorders This function enables a list with work orders to be created for a session context. When this function has been called, the session context field in the UDR can be populated with the list of work orders. void pccBatchDataBeginWorkorders( long txnId ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which you want to create a list of work orders Example pccBatchDataBeginWorkorders(cycle.TxnId); will allow the session context field in the cycle UDR with the stated transaction ID to be populated with a list of work orders. pccBatchDataAddWorkorder This function adds a work order to a session context. In the real-time template, this function is used for adding work orders that have been validated successfully to the session context, for example. void pccBatchDataAddWorkorder( long txnId, Workorder data ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context in which a work order should be added. data The work order to add. Example pccBatchDataAddWorkorder(txnId, wo); will add the wo work order for the session context with the transaction ID determined by the txnID variable. pccBatchDataBeginWorklogs This function enables a list with work logs to be created for a session context. When this function has been called, the session context field in the UDR can be populated with the list of work logs. void pccBatchDataBeginWorklog ( long txnId ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which you want to create a list of work logs. Example pccBatchDataBeginWorklogs(cycle.TxnId); will create a list of work logs for the session context with the transaction ID given in the TxnId field in the cycle UDR. pccBatchDataAddWorklog This function adds a work log to a session context. void pccBatchDataAddWorklog( long txnId, Worklog data ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which you want to add a work log. data The work logs you want to add. Example pccBatchDataAddWorklog(txnId, wl); will add the wl work order for the session context with the transaction ID determined by the txnID variable. pccBatchDataAddWorklogs This function adds a list of work logs to a session context. In the real-time template, this function is used for adding work logs for successfully updated work orders to the session context, for example. void pccBatchDataAddWorklogs( long txnId, List<Worklog> data ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which you want to add a list of work logs. data The list of work logs you want to add. Example pccBatchDataAddWorklogs(txnId, logs); will add the logs list with work orders for the session context with the transaction ID determined by the txnID variable. pccBatchDataSuspendWorklogs This function will suspend the work logs for a session context, this may be useful in case of an abnormal stop, for example. void pccBatchDataSuspendWorklogs( long txnId ) Parameters Parameter Description Parameter Description txnID The transaction ID of the session context for which to suspend work logs. Example pccBatchDataSuspendWorklogs((long) mapGet(sc, S_TXNID)); will suspend the work logs for the session context with the transaction ID returned by the mapGet function. pccBatchDataResumeWorklogs Resumes the work logs for a session context. This function is used after the pccBatchDataSupsendWorklogs function has been called. void pccBatchDataResumeWorklogs( long txnId ) Parameters Parameter Description Parameter Description txnID The transaction ID of the session context for which to resume work logs. Example pccBatchDataResumeWorklogs(cycle.TxnId); will resume the work logs for the session contexts with the transaction ID given in the TxnId field in the cycle UDR. pccBatchDataGetWorkorders This function retrieves a list of work orders for a certain session context. In the real-time workflow template, this is used for retrieving the list of work orders to compare with the list of work logs, for example. list<Workorder> pccBatchDataGetWorkorders( long txnId ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which to retrieve a list with work orders. Returns: A list with work logs. Example list<Workorder> orders = pccBatchDataGetWorkorders(cycle.TxnId); will return a list named orders containing the work orders for the session context with the transaction ID given in the TxnId field in the cycle UDR. pccBatchDataWorklogs This functions returns a list with work logs for a certain session context. In the real-time workflow template, this is used for retrieving the list of work logs to compare with the list of work orders, for example. list<Worklog> pccBatchDataGetWorklogs( long txnId ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which to retrieve a list with work logs. Returns: A list with work logs. Example list<Worklog> logs = pccBatchDataGetWorklogs(cycle.TxnId); will return a list named logs containing the work logs for the session context with the transaction ID given in the TxnId field in the cycle UDR. pccBatchDataRemove This function removes the batch data for a session context. In the real-time workflow template, this function is used for removing all work orders and work logs that have been successfully validated, updated and compared. void pccBatchDataRemove( long >txnId ) Parameters Parameter Description Parameter Description txnId The transaction ID of the session context for which you want to remove the batch Example pccBatchDataRemove((long) mapGet(sc, S_TXNID)); will remove the batch data for the session context with the transaction ID returned by the mapGet function.

---

# Document 557: SFTP Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740545
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file: filename Reported along with the name of the source file that has been collected and inserted into the workflow. File cancelled: filename Reported along with the name of the current file, each time a Cancel Batch message is received. This assumes the workflow is not aborted. For further information, see Retrieves in SFTP Collection Agent Transaction Behavior . Debug Events Debug messages are dispatched when debug is used. During execution, the messages are shown in the Workflow Monitor and can also be stated according to the configuration done in the Event Notification Editor . For further information about the debug event type, see Debug Event .

---

# Document 558: Port Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611591/Port+Configuration
**Categories:** chunks_index.json

You can set the Spark ports directly in the Spark. The available Spark ports are defined in the table below. When you are running the Spark cluster, the following network ports will be used: Port Name Default Port Number Description Port Name Default Port Number Description Spark Master Port 7077 This port is used by the Spark master for communication with Spark workers, and MediationZone. To change port number, set the property master-port in the Spark service configuration. Spark Worker Port Dynamic Port Range/Random This port is used by the Spark worker. To change port number,set the property SPARK_WORKER_PORT in the spark-environment block in the Spark service configuration Spark Master Web UI Port 8080 This port is used by the Spark master Web UI. To change port number, set the property MASTER_WEBUI_PORT in the spark-environment block in the Spark service configuration Spark Worker Web UI Port 8081 This port is used by the Spark worker Web UI. To change port number,set the property WORKER_WEBUI_PORT in the spark-environment block in the Spark service configuration Spark UI Port 4040 Every SparkContext launches a web UI that displays useful information about the application. If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc)". To change that initial default port set the SPARK_UI_PORT in the spark-environment block of the spark service context. To change the ui port for specific application set the property spark.ui.port in the spark app block Spark Driver Port Dynamic Port Range/Random By default, the Spark drivers use random ports for each submitted Spark application, to communicate with the Spark masters and the executors. To bind the drivers to a specific port number, set the driver port in the spark-conf block of the specific spark app block. Example of a port setting: spark.driver.port=7345 Block Manager Port Dynamic Port Range/Random By default, the block managers that runs the drivers on each executor, use random ports to communicate with each other. To bind the block managers to a specific port number, set the block manager port in the spark-conf block of the specific spark app block. Example of a port setting: spark.blockManager.port=7456 Spark REST Port 6066 This port is used by the Spark REST server interface, which is currently not included in the Apache Spark documentation. Note! The Spark REST server interface is required by KPI Management and submitted applications will fail to start if the interface is disabled.

---

# Document 559: mzcli - pico - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979600/mzcli+-+pico
**Categories:** chunks_index.json

Usage pico <command> [<args>] Add a Group Usage: -add [options] GroupName Delete a Group Usage: -delete GroupName List Groups Usage: -list GroupName View Running Groups & Processes Usage: -view Pico Name This command is used to: View, add, or delete EC groups. List running EC groups and processes Options Option Description Option Description [-add] Use pico -add to add an EC group to the system. [-delete <group name>] Use pico -delete to delete an EC group from the system. [ -list ] Use pico -list to list configured EC groups. [-view] Use pico -view to view the status of running groups and processes. Example - How to add an EC group java -jar mzcli.jar --port 9024 --password dr pico -add ecgroup -t ec See also EC Groups . Return Codes Listed below are the different return codes for the pico command: Code Description Code Description 0 Will be returned if the command was successful, or if arguments are missing. 1 Will be returned if arguments can not be parsed 2 Will be returned if the communication with the Platform fails. 3 Will be returned if checking of user privileges failed, or if pico already exists when trying to add a new pico. 4 Will be returned if the user does not have permission to add or delete picos. 5 Will be returned if an unexpected error occurs.

---

# Document 560: SFTP Agents Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740522/SFTP+Agents+Preparations
**Categories:** chunks_index.json

Prior to configuring an SFTP agent, consider the following preparation notes: Server Identification Attributes Authentication Server Keys Server Identficiation The SFTP agent uses a file with known host keys to validate the server identity during connection setup. The location and naming of this file is managed through the Execution Context property: mz.ssh.known_hosts_file It is set in the <pico name> .conf file of the relevant EC to manage where the file is saved. The default value is ${mz.home}/etc/ssh/known_hosts . The SSH implementation uses JCE (Java Cryptography Extension), which means that there may be limitations on key sizes for your Java distribution. This is usually not a problem. However, there may be some cases where the unlimited strength cryptography policy is needed. For instance, if the host RSA keys are larger than 2048 bits (depending on the SSH server configuration). This may require that you update the Java Platform that runs the EC. For unlimited strength cryptography on the Oracle JRE, download the JCE Unlimited Strength Jurisdiction Policy Files from http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html . Replace the jar files in $JAVA_HOME/jre/lib/security with the files in this package. The OpenJDK JRE does not require special handling of the JCE policy files for unlimited strength cryptography. Attributes he SFTP collection agent and the SFTP forwarding agent share a number of common attributes. They are both supported by a number of algorithms: 3des-cbc, 3des-ctr, blowfish-cbc, aes128-cbc, aes192-cbc, aes256-cbc, aes128-ctr, aes192-ctr, aes256-ctr. Authentication The SFTP agents support authentication through either username/password or private key. Private keys can optionally be protected by a Key password. Most commonly used private key files, can be imported into the system. Typical command line syntax (most systems): ssh-keygen -t <keyType> -f <directoryPath> Setting Description Setting Description keyType The type of key to be generated. Both RSA and DSA key types are supported. directoryPath The directory in which you want to save the generated keys. Example - Creating a private key The private key may be created using the following command line: > ssh-keygen -t rsa -f /tmp/keystore Enter passphrase: xxxxxx Enter same passphrase again: xxxxxx Then the following is stated: Your identification key has been saved in /tmp/keystore Your public key has been saved in /tmp/keystore.pub When the keys are created the private key may be imported to the SFTP agent. To import the private key into the SFTP agent: Open the Agent Configuration dialog for the SFTP collection agent as described in SFTP Collection Agent Configuration . Authenticate with Private Key . Click on the Select... button to open the Edit Private Key dialog. Paste your private key into the text field. Enter your password in the Password field and click OK . Close the Agent Configuration dialog by clicking OK . Open Finally, on the SFTP server host, append /tmp/keystore.pub to $HOME/.ssh/authorized_keys . If the $HOME/.ssh/authorized_keys is not there it must be created. Server Keys The SSH protocol uses host verification as protection against attacks where an attacker manages to reroute the TCP connection from the correct server to another machine. Since the password is sent directly over the encrypted connection, it is critical for security that an incorrect public key is not accepted by the client. The agent uses a file with the known hosts and keys. It will accept the key supplied by the server if either of the following is fulfilled: The host is previously unknown. In this case the public key will be registered in the file. The host is known and the public key matches the old data. The host is known however has a new key and the user has been configured to accept the new key. For further information, see the Advanced tab. If the host key changes for some reason, the file will have to be removed (or edited) in order for the new key to be accepted.

---

# Document 561: Amazon S3 Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606217/Amazon+S3+Agents
**Categories:** chunks_index.json

This section describes the Amazon S3 collection and forwarding agents. These agents are available in batch and real-time workflow configurations. The Amazon S3 agents collect and forward batches of files from specified buckets and regions in Amazon. Open The section contains the following subsections: Amazon Profile Amazon S3 Collection Agent Amazon S3 Forwarding Agent

---

# Document 562: Known Issues - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881617
**Categories:** chunks_index.json

Open In MediationZone 9.3, there are the following known issues: 1 Installation and Upgrade Issues 1.1 Unable to Change Platform Database After Installation 1.2 Installation is Reported as Successful Even if the Platform Fails to Start 1.3 Warning Messages Displayed During Installation 1.4 Error Occurs When Performing Multiple Installations on a Host with Different UNIX User IDs 1.5 Configuration History Loss After Upgrade from MediationZone 8 1.6 Deleted Configuration History Unavailable After Upgrade from MediationZone 8 1.7 Only the first EC is started when multiple ECs are specified 2 User Interface issues 2.1 New Desktop Issues 2.1.1 Database Agent Assignment Tab Value Are Not Cleared 2.1.2 Database Agent MIM Browser Need To Click Twice To Close It 2.1.3 External Reference Profile Datalist Can Be Altered in Read-Only Mode 2.1.4 Login Web UI Will Go To Last Page That Last User Accessed 2.1.5 Type Assigned Indication not Shown in MIM Browser 2.1.6 Idle Timeout Only Warns the User Once 2.1.7 Data Veracity Repair Jobs Show in One Line 2.2 Accessibility Issues 2.3 Code Editor Issues 2.4 Known Differences Between Desktops 3 Data Veracity issues 3.1 Data Veracity Search Filtering For Some Fields Are Case-Sensitive 3.2 Data Veracity Filter's Full Query Is Not Available on Desktop 3.3 Adding an Empty Query Group With A Parent Condition In Data Veracity Search & Repair Filter Will Result In An Error 3.4 Importing Old Data Veracity Collection Agent Workflow Configurations Returned Validation Errors 3.5 Data Veracity Filters Are Not Refreshed Automatically 3.6 Data Veracity New Restricted Fields That Were Used For An Existing Repair Rule Is Not Disabled 3.7 Data Veracity Not Working Well in Workflow Packages 4 Duplicate UDR issues 4.1 Duplicate UDR Inspector Stuck Loading When Attempting To Delete Records 4.2 System Log Shows Unknown User On Deleting A Record In Duplicate UDR Inspector 4.3 Duplicate UDR Inspector Allows To Delete Records Without Profile Locking 4.4 Duplicate UDR Inspector Didnt Return Records Properly 4.5 Duplicate UDR MIMs Shows 0 4.6 Incorrect Handling When Duplicate UDR Process Old Data With System Arrival Time 4.7 Duplicate UDR Inspector Table Header Row Count Is Not Updated When Records Found Is 0 4.8 Rename Duplicate UDR Profile Doesnt Auto Reflect In Duplicate UDR Agent 4.9 Exception Thrown When Truncating Previous Tables After Changing DB Profile In Duplicate UDR Profile 4.10 File Directory Is Not Empty When Delete Duplicate UDR File Storage Profile 5 Reference Data Management 5.1 Reference Data Management Table Shows No Data And All Functions Are Locked 5.2 Import/Export Ongoing Process Are Not Aborted When Navigating Away From Reference Data Management 6 SAPCC Rest Agent 6.1 Unable To Add Additional Parameter In Desktop Online Workflow Page 6.2 Workflow Unable To Rollback To Default Values 7 Other issues 7.1 Logging Issue 7.2 Error Thrown When SAPCC Agent Not Added With Any Host 7.3 Inappropriate Validation Handling On Workflow Group Scheduling 7.4 Database Forwarding Agent Validation Message 7.5 External References Can Be Removed when a Workflow is Dependent on It 7.6 Keystore Information Is Gone After Imported Notification Workflow From MZ8.3 7.7 Overwrite After Collection Disabled in Workflow Table of Workflow Properties 7.8 ECS Tables Missing Indexes 7.9 SAP RFC Workflow Hangs when Setting Date Data Type 7.10 ORA-1000 For Oracle Database 7.11 Incorrect OpenAPI Specifications in Operations REST Interface Installation and Upgrade Issues Unable to Change Platform Database After Installation It is not possible to change platform database after the installation is completed which used to be a feature that available since 8.x. This issue will be fixed in future releases. Installation is Reported as Successful Even if the Platform Fails to Start In some circumstances during installation the installer may report as successful when the platform has not started. Warning Messages Displayed During Installation When the user performs  setup.sh prepare , a warning message  setSecurityManager will be removed in a future release " is displayed. This does not impact functionality of MZ 9 installation and will be handled in future versions of MZ. Error Occurs When Performing Multiple Installations on a Host with Different UNIX User IDs The /tmp/syslog/syslog-debug.log file is created when a host machine contains an MZ 9 installation. This file will be overwritten by any subsequent MZ 9 installations that are performed on the same host machine with a different UNIX user ID. However, the attempt to overwrite this file will fail and cause errors to the subsequent installations. To solve this issue, modify the platform.xml file setting by using the mzsh topo open platform command. Edit the platform.xml file, and change the directory to use another directory using the following command: <property name="mz.syslog.debuglogfile.filedir" value="[the directory]"/> . Configuration History Loss After Upgrade from MediationZone 8 After upgrading from MediationZone 8, existing configuration history will no longer be available. This issue will be fixed in future releases. Deleted Configuration History Unavailable After Upgrade from MediationZone 8 After upgrading from MediationZone 8, previously deleted configuration history in MediationZone 8 is no longer accessible in the Configuration Tracer. This issue will be fixed in future releases. Only the first EC is started when multiple ECs are specified When specifying multiple ECs during installation using a comma-delimited format (e.g., ec1,ec2,ec3), the installation completes successfully. However, only the first EC (ec1) is started post-installation. The remaining ECs are not started as expected. User Interface issues New Desktop Issues Database Agent Assignment Tab Value Are Not Cleared When Value Type is set to "To UDR" or "NULL", the Value column are not cleared. Database Agent MIM Browser Need To Click Twice To Close It The MIM browser open from database agent, need to close it by hitting twice on cancel button or the X button to close it. External Reference Profile Datalist Can Be Altered in Read-Only Mode When certain procedures are made, it is possible to alter the data list found in the external reference profile while the read-only mode is used. This causes a system-wide DRRCPException to be thrown. Login Web UI Will Go To Last Page That Last User Accessed User A log out from web UI, and User B login, it will automatically go to the page last accessed by User A. Type Assigned Indication not Shown in MIM Browser This can be viewed in the Legacy Desktop if required. Idle Timeout Only Warns the User Once Inactive users will be logged out after a configurable period, following a warning to that effect. If a user is active after the warning they won't be logged out, but if they are inactive again then they will be logged out with no warning. Data Veracity Repair Jobs Show in One Line When viewing the repair jobs in the new UI they are displayed in one line which is hard to read. Accessibility Issues The new web-native desktop aims to be fully accessible, but there are currently some outstanding issues regarding this. These include: In some interfaces, the keyboard navigation doesn't follow the ideal order or convention Incorrect behavior of radio buttons Code Editor Issues The code editor in the desktop provides code completion and syntax highlighting there are some known issues with syntax highlighting not being correct, it is not possible to add tabs to code and code completion cannot be activated on a Mac if multiple keyboard layouts are enabled. When attempting to use the code completion function in agents or profiles, for example, the Analysis agent , the built-in keyboard shortcut of Command + Space on a Mac computer will not trigger the code completion dialog box. Known Differences Between Desktops There are some known, minor issues with differences between the two Desktops. Data Veracity issues Data Veracity Search Filtering For Some Fields Are Case-Sensitive The filtering for the source_node and message fields in the Data Veracity Search & Repair table are case-sensitive. Data Veracity Filter's Full Query Is Not Available on Desktop The full filter query for Data Veracity is not available for viewing on the Desktop. A partial display of the query can be seen in the filter listing under Data Veracity > Filter listing. Open Adding an Empty Query Group With A Parent Condition In Data Veracity Search & Repair Filter Will Result In An Error Attempting to add an empty query group with a parent condition will cause an error to be thrown upon clicking OK . A sample of the query setup is shown below. Open Importing Old Data Veracity Collection Agent Workflow Configurations Returned Validation Errors If you have an old Data Veracity Collection Agent Workflow configuration from version 8.x, it is possible that validation errors about invalid Analysis Agents may occur. Example error during import: The following agents returned validation errors. Analysis_1 is invalid. The UDR type DataVeracityUDR (DataVeracity) doesn't exist Example error when viewing the invalid workflow Analysis Agent configuration: Error: Failed to set UDR Types Message: Unable to create UDR type description from DataVeracityUDR (DataVeracity). To resolve this, you will need to re-add the Data Veracity UDR for Analysis Agent input UDR Types or use the Set To Input button if applicable. After adding the appropriate Analysis UDR Types, save the workflow configuration. Data Veracity Filters Are Not Refreshed Automatically The Filter page is not always the latest, after creating a new Filter from the Data Veracity Search page, you may need to click the Refresh button to view it. Data Veracity New Restricted Fields That Were Used For An Existing Repair Rule Is Not Disabled If setting a new restricted field that was used in a previously created Repair Rule, it is expected that the rule on the said field should be grayed out when viewing the saved Repair Rule and be ignored when applying repair tasks. This is not the case as the rule on the said field still appears to be editable in the old Saved Repair Rule. Data Veracity Not Working Well in Workflow Packages There are classloading issues in both inspectors and for Data Veracity it is not even possible to access the data since the profile is not selectable. For both ECS Inspector and DV Inspector, it is impossible to view the UDRs containing the Request (generated from Diameter Application Profile) due to classloading issues. Duplicate UDR issues Duplicate UDR Inspector Stuck Loading When Attempting To Delete Records When a Duplicate UDR profile is being used by a running workflow, attempting to delete any records for that profile in Duplicate UDR Inspector would cause the Inspector to be stuck loading. System Log Shows Unknown User On Deleting A Record In Duplicate UDR Inspector After deleting any records in Duplicate UDR Inspector, the log message in the System Log, will show the Username as <Unknown>. Duplicate UDR Inspector Allows To Delete Records Without Profile Locking Currently, when executing a workflow using the Duplicate UDR profile, the profile remains unlocked, allowing for the deletion of records from the Duplicate UDR Inspector. Duplicate UDR Inspector Didnt Return Records Properly With certain search date criteria in Duplicate UDR Inspector, some records are not returned. Duplicate UDR MIMs Shows 0 Duplicate UDR Inspector shows these MIMs as 0 even when there are Duplicate UDR caught. Dup_UDR_1.dups UDRs Dup_UDR_1.Inbound UDRs Dup_UDR_1.Outbound UDRs Dup_UDR_1.r_3 UDRs Incorrect Handling When Duplicate UDR Process Old Data With System Arrival Time With the SQL Storage Duplicate UDR profile set to use System Arrival Time , UDRs with dates older than the system time on the Platform will not be handled properly. The Duplicate UDR Inspector will not display any data despite the UDRs being processed. When using File Storage Duplicate UDR profile with System Arrival Time as well, you might see Records and Duplicates as 0 in the legacy desktop Duplicate UDR Inspector. Duplicate UDR Inspector Table Header Row Count Is Not Updated When Records Found Is 0 When searching a profile that has no records after previously searching with a profile that has any records, the table header would still display previous row count. Rename Duplicate UDR Profile Doesnt Auto Reflect In Duplicate UDR Agent When renaming a Duplicate UDR profile in the configuration browser, the Duplicate UDR agent that is using this profile is does not refresh with the new name automatically. This does not affect the workflow when executing with the profile. Exception Thrown When Truncating Previous Tables After Changing DB Profile In Duplicate UDR Profile Upon saving a copy of Duplicate UDR profile A to another Duplicate UDR profile. Changing the DB profile to a different schema and clicking on Yes at the confirmation message to truncate previous tables would lead to an exception error. File Directory Is Not Empty When Delete Duplicate UDR File Storage Profile When deleting a Duplicate UDR File Storage profile and choosing Yes to delete relational data, the system will fail to empty the File Directory. Reference Data Management Reference Data Management Table Shows No Data And All Functions Are Locked After searching in Reference Data Management with any number of results on the table, navigate away from Reference Data Management to another page. Navigate back to Reference Data Management and click on Get Started. Once the Query dialog is opened, refresh the page, and click on Query to select the same profile and table and the apply, you would find that table is now empty, and all functions are locked. Import/Export Ongoing Process Are Not Aborted When Navigating Away From Reference Data Management Running import/export processes are not aborted when navigating away from the Reference Data Management page. SAPCC Rest Agent Unable To Add Additional Parameter In Desktop Online Workflow Page In Desktop Online Add/Edit Workflow page, click on the Add button for Additional Parameters field, we will see the pop up missing Key and Value fields, so currently user is unable to add Additional Parameters in Workflow page. However, user can add Additional Parameters in Legacy Desktop. Workflow Unable To Rollback To Default Values Set agent fields to Default in Workflow Properties, and enter some data into lets say a textfield and save it. Then edit again to remove data in this textfield, workflow template will throws validation error that the workflow is invalid. This happened to checkbox and some other fields as well. Other issues Logging Issue When logging out of the legacy desktop desktop_current.log has unnecessary logs added Error Thrown When SAPCC Agent Not Added With Any Host In the SAP CC Batch agent, do not add any host, click the Save As button, and you will find the error shown below: java.lang.NullPointerException: Cannot invoke "String.matches(String)" because "hosts" is null Inappropriate Validation Handling On Workflow Group Scheduling In Workflow Group Scheduling, when adding a duplicate execution day plan, it prompts the wrong validation message content and if the user continues to save, scheduling data will be wiped off. Database Forwarding Agent Validation Message In Database Forwarding Agent, when we have a field NULL/To UDR, the validation message that prompted does not properly describe the problem. External References Can Be Removed when a Workflow is Dependent on It External references of the type Database Properties can be removed even if they are in use. Any workflows using these references will become invalid. Keystore Information Is Gone After Imported Notification Workflow From MZ8.3 System Import Notification Workflow from MZ8.3, found that Keystore information in the SAP CC Notification agent is gone. Overwrite After Collection Disabled in Workflow Table of Workflow Properties Users might encounter this issue when using a workflow configuration that uses SFTP Agent exported from MediationZone versions prior to MediationZone 9.3.0.4. This impacts SFTP Agents configured with the Default Collection Strategy and with the After Collection option set to either Move To or Rename. This issue occurs for workflow configurations that have not been modified since import. To resolve this, edit the workflow configuration (e.g. move an agent slightly) and then save the configuration. The Overwrite After Collection workflow property will now be configurable. ECS Tables Missing Indexes ECS Tables have found some indexes were missing during the migration from mz8 to mz9. We are planning to add it back in MZ 9.4.0.0 which will be release at 28th April 2025. Follow by MZ 9.3.2.0 SAP RFC Workflow Hangs when Setting Date Data Type Users may encounter suspended workflows when setting SAP RFC functions with the Date data type using the SAP Java Connector (JCo) library release 3.1.4. This issue happens specifically with the handling of date fields. It is recommended to upgrade to SAP Java Connector (JCo) library release 3.1.11 or later to avoid this issue. The newer releases contain fixes that prevent the workflow from hanging when handling Date data types. ORA-1000 For Oracle Database You will encounter an ORA-01000: maximum open cursors exceeded error for versions 9.3.0.0 onwards until 9.3.0.4 and 9.3.1.3 when using MediationZone with Oracle database. The issue is fixed in MediationZone versions 9.3.0.4 and 9.3.1.3 onwards. Incorrect OpenAPI Specifications in Operations REST Interface The OpenAPI specification files generated for the Operations REST Interface contain inaccuracies. This affects the following endpoints: http(s)://<platform server>:<platform port>/ops/mz/wf/v1/openapi.yaml  Workflows API http(s)://<platform server>:<platform port>/ops/mz/wfg/v1/openapi.yaml  Workflow Groups API http(s)://<platform server>:<platform port>/ops/extref/v1/openapi.yaml  External References API http(s)://<platform server>:<platform port>/ops/mz/host/v1/openapi.yaml - Host API http(s)://<platform server>:<platform port>/ops/mz/pico/v1/openapi.yaml - Pico API These inaccuracies may lead to failed validation when creating an Open API Profile based on the provided OpenAPI specification. The issue will be fixed in MediationZone 9.4.0.0.

---

# Document 563: HDFS Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033406
**Categories:** chunks_index.json

The HDFS forwarding agent creates files on the remote file system containing the received data. Files are created when a Begin Batch message is received and closed when an End Batch message is received. In addition, the Filename Template service offers the possibility to compress (gzip) the files or to further process them, using commands. To ensure that downstream systems will not use the files until they are closed, they are stored in a temporary directory until the End Batch message is received. This behavior also applies to Cancel Batch messages. If a Cancel Batch is received, file creation is canceled. The chapter contains the following sections: HDFS Forwarding Agent Events HDFS Forwarding Agent Transaction Behavior HDFS Forwarding Agent MultiForwardingUDR Input HDFS Forwarding Agent Configuration HDFS Forwarding Agent Input/Output Data and MIM

---

# Document 564: Workflow Bridge Example Real-Time to Real-Time Scenario with Load Balancing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002887
**Categories:** chunks_index.json

This section will show an example of a scenario where a real-time forwarding workflow will split the execution of UDRs between three real-time collection workflows depending on the incoming data. Each collection workflow will add information and send back the consumeCycleUDR to the real-time forwarding workflow for further execution. The following configurations will be created: An Ultra Format A Workflow Bridge profile A Workflow Bridge real-time forwarding workflow A Workflow Bridge real-time collection workflow Open Example of a real-time to real-time scenario Define an Ultra Format A simple Ultra Format needs to be created in order to forward the incoming data and enable the collection workflows to populate it with more information. For more information about the Ultra Format Editor and the UFDL syntax, r efer to the Ultra Reference Guide . Create an Ultra Format as defined below: internal myInternal { string inputValue; string executingWF; }; Define a Profile The profile is used to connect the forwarding workflow towards the three collection workflows. See Workflow Bridge Profile for information how to open the Workflow Bridge profile dialog. Open Example of a profile configuration In this dialog, the following settings have been made: The Send Reply Over Bridge is selected which means that all ConsumeCycleUDR s will be returned to the Workflow Bridge forwarding agent. Force serialization is not used since there will be no configuration changes during workflow execution. The Workflow Bridge real-time collection agent must always respond to the WorkflowState UDRs. The Response Timeout (s) has been set to "60" and this means that the Workflow Bridge real-time forwarding agent that is waiting for a WorkflowState UDR reply will timeout and abort (stop) after 60 seconds if no reply has been received from the real-time collection workflow. Enter the appropriate timeout value to set the timeout for the Workflow Bridge real-time forwarding agent. The Bulk Size has been set to "0". This means that the UDRs will be sent from the Workflow Bridge real-time forwarding agent one by one, and not in a bulk. Enter the appropriate bulk size if you wish to use bulk forwarding of UDRs. The Bulk Timeout (ms) has been set to "0" since there will be no bulk forwarding. Enter the appropriate bulk timeout if you wish to use bulk forwarding of UDRs. Bulk timeout can only be specified if the bulk functionality has been enabled in the Bulk size setting. Since the UDRs in this example will be split between three different workflows, the Number of Collectors has been set to "3". Create a Real-Time Forwarding Workflow In this workflow, a TCP/IP agent collects data that is forwarded to an Analysis agent. The Analysis agent will define the receiving real-time collection workflow before the ConsumeCycleUDR is sent to the Workflow Bridge forwarding agent. The Workflow Bridge forwarding agent will distribute the UDRs to the correct collection workflow and forward the returning ConsumeCycleUDR to another Analysis agent for further execution. Open Example of a real-time forwarding workflow The workflow consists of a TCP/IP agent, an Analysis agent named Analysis , a Workflow Bridge real-time forwarding agent named Workflow_Bridge_FW and a second Analysis agent named Result . TCP/IP TCP/IP is a collection agent that collects data using the standard TCP/IP protocol and forwards it to the Analysis agent. Double-click on the TCP_IP agent to display the configuration dialog for the agent: Open Example of a TCP/IP Agent Configuration In this dialog, the following settings have been made: Host has been set to "10.46.20.136". This is the IP address or hostname to which the TCP/IP agent will bind. Port has been set to "3210". This is the port number from which the data is received. Allow Multiple Connections has been selected and Number of Connections Allowed has been set to "2". This is the number of TCP/IP connections that are allowed simultaneously. Analysis The Analysis agent is an Analysis agent that receives the input data from the TCP/IP agent. It defines which real-time collection workflow should be chosen and forwards the ConsumeCycleUDR to the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { wfb.ConsumeCycleUDR ccUDR = udrCreate(wfb.ConsumeCycleUDR); WFBridge.myFormat.myInternal data = udrCreate(WFBridge.myFormat.myInternal); data.inputValue = baToStr(input); debug("First character is: " + strSubstring(data.inputValue,0,1)); if (strStartsWith(data.inputValue,"1") || strStartsWith(data.inputValue,"2")) { int wfId; strToInt(wfId,strSubstring(data.inputValue,0,1)); ccUDR.LoadId = wfId; } else { ccUDR.LoadId = 3; } ccUDR.Data = data; udrRoute(ccUDR); } In this dialog, the APL code for handling input data is written. In the example, the incoming data is analyzed and depending on the first character in the incoming data, the receiving real-time collection workflow is chosen by setting the LoadId in the ConsumeCycleUDR , which is sent to the Workflow_Bridge_FWD agent. Adapt the code according to your requirements. Workflow_Bridge_FWD Workflow_Bridge_FWD is the Workflow Bridge real-time forwarding agent that sends data to the Workflow Bridge real-time collection agent. Double-click on Workflow_Bridge_FWD to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. Result The Result agent is an Analysis agent that receives the returning ConsumeCycleUDR s and potential ErrorCycleUDR s from the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, wfb.ErrorCycleUDR)) { debug("Something went wrong"); } else if (instanceOf(input, wfb.ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; string msg = ("Value " + data.inputValue + " was executed by " + data.executingWF); debug(msg); } } } In this dialog, the APL code for further handling of the UDRs is written. In the example, only simple debug messages are used as output. Adapt the code according to your requirements. Create the Real-Time Collection Workflows In this workflow, a Workflow Bridge real-time collection agent collects the data that has been sent in a ConsumeCycleUDR from the Workflow Bridge real-time forwarding agent and returns an updated ConsumeCycleUDR . Open Example of a real-time collection workflow Workflow_Bridge_Coll Workflow_Bridge_Coll is the Workflow Bridge real-time collection agent that receives the data that the Workflow Bridge real-time forwarding agent has sent over the bridge. Double-click on the Workflow_Bridge_Coll agent to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. The default port that the collector server will listen on for incoming requests has been set to default value "3299". Analysis The Analysis agent is the Analysis agent that receives and analyses the data originally sent from the Workflow Bridge real-time forwarding agent in the ConsumeCycleUDR , as well as the workflow state information delivered in the WorkflowStateUDR . Double-click on the agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, WorkflowStateUDR )) { udrRoute((WorkflowStateUDR)input, "response"); } else if (instanceOf(input, ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; debug("Incoming data: " + data.inputValue); data.executingWF = (string)mimGet("Workflow","Workflow Name"); ccUDR.Data = data; udrRoute(ccUDR, "response"); } else { debug(input); } } In this example, each ConsumeCycleUDR will populate the data field executingWF with the name of the executing workflow. Also WorkflowStateUDR s are routed back. Adapt the code according to your requirements. Instance Table Since this example will load balance between three workflows, additional workflows is added in the workflow table. Right-click in the workflow template and choose Workflow Properties to display the Workflow Properties dialog. Open Example of Workflow Properties In this dialog, the following settings have been made: Workflow - Execution - Execution Settings and Workflow_Bridge_Coll - WFB_Collector - Port have default checked, which means they will use the configured value in the template unless a new value is given in the Workflow Table. Workflow_Bridge_Coll - WEB_Collector - loadID has Per Workflow set, which means that the value must be specified in the Workflow Table. Number of Workflows to Add has been set to "2", since one is already existing and the example needs two additional workflows. The Workflow Table will contain three workflows, that all will communicate with the real-time forwarding workflow. Populate the Workflow Table with the correct settings for each workflow: Name should be set to a unique name for each workflow. Set which EC each workflow will execute on in Execution Settings . Each workflow needs to have a unique port assigned for communication with the Workflow_Bridge_FWD agent. In case workflows are configured to run on different ECs (configured on a different server), then the workflows can share the same ports. The loadID need to correspond with the APL code and should be "1", "2" and "3" in this example Load Balancing There are two types of load-balancing strategies that can be used. Only one type may be active at a time. Static  This load balancing method allows for a specific number of LoadIDs to be defined in the workflow instances. Dynamic  The dynamic balancing method will automatically distribute the load.

---

# Document 565: Viewing Pico Configurations and Attributes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744491/Viewing+Pico+Configurations+and+Attributes
**Categories:** chunks_index.json

View existing configurations or specific attributes by opening the file pico.conf or use the mzsh topo set command. Viewing Pico Configurations Run the following command to view one or more pico configurations: mzsh topo get <path expression> Example. Viewing a pico configuration using a full path $ mzsh topo get topo://container:main1/pico:ec2 You can view multiple pico configurations by replacing the full path with a regular expression. Example. Viewing all pico configurations using a regular expression $ mzsh topo get topo://container:main1/pico:.* Viewing Pico Attributes Pico Level Attributes Run the following command to view a specific attribute in a pico configuration: mzsh topo get topo://container:main1/pico:<pico>/val:<attribute> Example - Viewing a specific attribute in a pico configuration mzsh topo get topo://container:main1/pico:ec1/val:config.properties.ec.httpd.port Output: { "data" : "9090", "file" : "/home/user/mz/common/config/cell/default/master/containergroups/default/containers/main1/ec1.conf", "ref" : "topo://cell:default/group:default/container:main1/pico:ec1/val:config.properties.ec.httpd.port", "result" : 0 } You can exclude the metadata from the command output by using the --format flag. Example - Viewing a specific attribute in a pico configuration $ mzsh topo get --format data-only topo://container:main1/pico:ec1/val:config.properties.ec.httpd.port You can retrieve the attribute of multiple pico processes by replacing the full path with a regular expression. Example - Viewing a specific attribute of multiple pico configurations $ mzsh topo get --format data-only topo://container:main1/pico:.*/val:config.properties.ec.httpd.port Container Level Attributes Run the following command to view an attribute on container level: $ mzsh topo get topo://container:<container>/val:common.<attribute> Cell Level Attributes Run the following command to view an attribute on cell level: $ mzsh topo get topo://val:common.<attribute>

---

# Document 566: 5G Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671367/5G+Profile
**Categories:** chunks_index.json

If you want to use 5G with HTTP/2 agents, you require a 5G profile configuration. You can select the profile that you configure in the HTTP/2 Server agent configuration. In the 5G profile configuration, enter the details required to register an NF (Network Function) instance in the NRF (NF Repository Function). Configuration The 5G profile consists of the following tabs: 1 General Tab 2 Advanced Parameters Tab General Tab Open 5G Profile - General tab Setting Description Setting Description 3GPP Version Version Select Select the version of 3GPP to apply to the 5G Profile. Note! 3GPP Release 15 is the current default version supported by MediationZone. Release 16 was added to support the handling of 3xx status codes. Release 17 is added to support the enhanced specifications and the new UDRs that are generated when coding with APLs. Enable Custom Specification Click this to use the custom version of the specification of NRF NFManagement Service. See Custom Specification Enabled for more information. Info! When enabling custom specification, all fields except for OpenAPI Profile and Enable Validation will be disabled. You need to use the Analysis agent or APL Code to create the NRFspecificationUDR that will contain all the values for the disabled fields. See Custom Specification Enabled for more information. Custom error handling for 5G during the startup can be configured at APL when the Route to APL option is enabled. See Route Error to APL for more information. OpenAPI Profile Select the OpenAPI profile for specification of NRF NFManagement Service. Enable Validation Validate the request for NF (Network Function) registration, heartbeat and de-registration against the OpenAPI schema provided from the OpenAPI profile. Configure Server Settings Path Enter the exact path to where the HTTP request will register, de-register and update the NF profile in the NRF. Example: /nnrf-nfm/v1/nf-instances/ Configure NF Settings NF Instance name Enter the name of the NF instance for the NF type that you want to use. This field supports parameterization using ${} syntax, see Appendix 1 - Profiles for more information on how parameterization works. NF type A list of various NF types is available for selection. The default is set to CHF (charging function). For more information on each NF type, refer to the specification in 3GPP TS 29.510, https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3345 . Heartbeat timer Enter the frequency (in seconds) for the NF registration to be updated in the NRF. NF Registration status Select the registration status that you want to have updated. The following status options are available: REGISTERED: This is to register your NF instance to the NRF UNDISCOVERABLE: This is to register your NF Instance without it being discovered by other NFs SUSPENDED: This is to temporary suspend the NF, so that it is unable to respond to any request FQDN Enter the Fully Qualified Domain Name (FQDN) for the NF instance. Support NRF Change Select this option to allow the NRF server to modify the heartbeat timer provided by the NRF client upon registration. IPv4 Address List You can add the IPv4 address(es) for the NF instance. IPv6 Address List You can add the IPv6 address(es) for the NF instance. Note! You must enter at least an FQDN, an IPv4 address or an IPv6 address, but you are not required to make an entry for all of these fields. In addition, you can enter any combination of the three fields that you require. Advanced Parameters Tab Open 5G Profile - Advanced parameters tab In this tab, you can add settings in a JSON configuration in accordance with 6.2.6.2.3 "Type: NFProfile" as defined the specification 3GPP TS 29.510, https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3345 . See the example below: Examples for Advanced Parameters Note! Values set in the Advanced Parameters tab will override those set in the General tab. Json configuration of advanced parameter { "nfProfileChangesSupportInd": true, "nfProfileChangesInd": true, "allowedPlmns": [ { "mcc":"262-01", "mnc":"302-720" } ], "allowedNfTypes": [ "PCF", "CHF" ], "allowedNfDomains": [ "www.domainname.com" ] } This field supports parameterization using ${} syntax. For more information on parameterization see Appendix 1 - Profiles . Parameterized Json Configuration of Advanced Parameter { "nfProfileChangesSupportInd": true, "nfProfileChangesInd": true, "allowedPlmns": [ { "mcc":"${5g.mcc}", "mnc":"${5g.mnc}" } ], "allowedNfTypes": [ "PCF", "CHF" ], "allowedNfDomains": [ "${5g.nfDomains}" ] } This example would result in three dynamic fields being generated and configurable per workflow: 5g - mcc 5g - mnc 5g - nfDomains Custom Specification Enabled When custom specification is enabled in 5G Profile, HTTP/2 Server agent will not perform registration of NF (Network Function) automatically during the startup of the workflow. The HTTP/2 Server agent will instead, wait to receive the NRFspecificationUDR in order to perform NF registration, heartbeat and de-registration. You can refer to the example below: Open An example workflow of HTTP/2 Server Agent with 5G Profile enabled and an Analysis Agent Example - HTTP/2 Server agent with 5G custom enabled The example workflow consists of HTTP/2 Server Agent with 5G Profile enabled and Analysis Agent to construct the NRFSpecificationUDR . When the workflow starts, the HTTP/2 Server Agent will route the NRFSpecificationUDR to the Analysis agent to construct the NRFSpecificationUDR as shown below. The constructed UDR is then routed back to the HTTP/2 Server Agent in order to perform the NF (Network Function) registration, heartbeat and de-registration. consume { if (instanceOf(input, NRFSpecificationUDR)) { nrf.NRFSpecificationUDR udr = udrCreate(nrf.NRFSpecificationUDR); udr.registerCycleUDR = constructRegUdr(); udr.heartbeatCycleUDR = constructHbUDR(); udr.deregisterCycleUDR = constructDeRegUDR(); udr.heartBeatFieldName = "heartBeatTimer"; udr.nfInstanceIdFieldName = "nfInstanceId"; udr.nfTypeFieldName = "nfType"; udr.nfProfileChangesSupportIndFieldName = "nfProfileChangesSupportInd"; udrRoute(udr); } } RequestCycle constructRegUdr() { // construct register UDR NFProfile nfUdr = udrCreate(NFProfile); nfUdr.nfInstanceId = "94d7f196-9d03-11eb-a8b3-0242ac130003"; nfUdr.heartBeatTimer = 5; nfUdr.fqdn = "domain.my"; nfUdr.nfType = "CHF"; nfUdr.nfStatus = "REGISTERED"; nfUdr.nfProfileChangesSupportInd = false; list<string> ipv4List = listCreate(string); listAdd(ipv4List, "10.60.10.111"); list<string> ipv6List = listCreate(string); listAdd(ipv6List, "2001:db8:85a3::8a2e:370:7334"); nfUdr.ipv4Addresses = ipv4List; nfUdr.ipv6Addresses = ipv6List; string jsonString = jsonEncodeUdr(nfUdr); bytearray ba; strToBA(ba, jsonString); http.RequestCycle regUDR = udrCreate(http.RequestCycle); map<string,list<string>> headersMap = mapCreate(string, list<string>); list<string> contentList = listCreate(string); listAdd(contentList, "application/json"); mapSet(headersMap, "Content-Type", contentList); regUDR.headers = headersMap; regUDR.method = "PUT"; regUDR.requestTimeout = 10000; regUDR.body = ba; regUDR.openAPIUDR = nfUdr; regUDR.path = "/nnrf-nfm/v1/nf-instances/94d7f196-9d03-11eb-a8b3-0242ac130003"; return regUDR; } RequestCycle constructHbUDR() { // construct heartbeat UDR PatchItem patchUDR = udrCreate(PatchItem); patchUDR.op = "replace"; patchUDR.path = "/nfstatus1"; patchUDR.value = "REGISTERED"; string jsonString = "[" + jsonEncodeUdr(patchUDR) + "]"; bytearray ba; strToBA(ba, jsonString); http.RequestCycle hbUDR = udrCreate(http.RequestCycle); map<string,list<string>> hbHeadersMap = mapCreate(string, list<string>); list<string> hbContentList = listCreate(string); listAdd(hbContentList, "application/json-patch+json"); mapSet(hbHeadersMap, "Content-Type", hbContentList); hbUDR.headers = hbHeadersMap; hbUDR.method = "PATCH"; hbUDR.requestTimeout = 10000; hbUDR.body = ba; hbUDR.openAPIUDR = patchUDR; hbUDR.path = "/nnrf-nfm/v1/nf-instances/94d7f196-9d03-11eb-a8b3-0242ac130003"; return hbUDR; } RequestCycle constructDeRegUDR() { // construct de-regsitration UDR http.RequestCycle udr = udrCreate(http.RequestCycle); udr.method = "DELETE"; udr.requestTimeout = 10000; udr.path = "/nnrf-nfm/v1/nf-instances/94d7f196-9d03-11eb-a8b3-0242ac130003"; return udr; }

---

# Document 567: REST Client_Deprecated UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642589/REST+Client_Deprecated+UDR+Types
**Categories:** chunks_index.json

RESTCycleUDR The RESTCycleUDR type is used to pass data between the workflow and REST Client_Deprecated agent. Field Description Field Description Context (any) This field that can be used in the workflow configuration to keep track of, and use, internal workflow information related to a request from the REST Client_Deprecated agent. For instance, before you route a RestCycle UDR to the REST Client_Deprecated agent, store the TCPIPUDR from a TCP/IP collection agent in the Context field. You can then read the TCPIPUDR from the Context field in the RESTCycle UDR that contains the response from the REST Client_Deprecated agent, and use it to send back a response to the TCP/IP collection agent. Error (RESTError(RESTClient)) This field contains information related to internal processing errors. Request (RESTRequest(RESTClient)) This field contains the request from the REST Client_Deprecated agent to the server. RequestSentTime (long) This field contains the timestamp from before the REST request was sent. Response(RESTResponse(RESTClient)) This field contains the response from the server to the REST Client_Deprecated agent. ResponseReceivedTime (long) This field contains the timestamp from when the REST response was generated. SessionId (string) This field contains a string representation of a random UUID. The nested UDR types of RESTCycleUDR are described below. RESTRequest Field Description Field Description Body (bytearray) This field contains the HTTP message body. Header (map<string,string>) This field may contain an HTTP header. The header fields are stored as key-value pairs. The REST Client_Deprecated agent will override any values that you set in the Authorization field of the header. HeaderV2 (map<string,list<string>>) This field may contain an HTTP header. The header fields are stored as key-value pairs. The REST Client agent will override any values that you set in the Authorization field of the header. Note! If both Header and HeaderV2 are set, HeaderV2 will override Header . Method (string) This field must contain the HTTP method. The REST Client_Deprecated agent does not validate the specified method, and any string value is considered valid. Params (map<string,string>) This field may contain HTTP parameters that are passed in the query string. The parameters are stored as key-value pairs. ResourceURI (string) This field contains a resource URI, relative to the base URL that is configured in the REST Client_Deprecated agent. If the base URL does not end with slash character, you must include it in ResourceURI , e g "/search". Example - Creating a REST request in APL import ultra.RESTClient; consume { RESTCycleUDR aUDR = udrCreate(RESTCycleUDR); RESTRequest req = udrCreate(RESTRequest); //Create and set header map<string,string> header = mapCreate(string,string); mapSet(header,"Accept","*/*" ); req.Header = header; //Set HTTP method req.Method = "GET"; //Set resource URI req.ResourceURI="/get"; aUDR.Request = req; udrRoute(aUDR); } RESTResponse Field Description Field Description Body (bytearray) This field contains the HTTP message body. Header (map<string,string>) This field may contain an HTTP header. The header is stored as key-value pairs. HeaderV2 (map<string,list<string>>) This field may contain an HTTP header. The header is stored as key-value pairs. ResponseCode (int) This field contains the response code from the server. Example - Receiving a REST response in APL import ultra.RESTClient; consume { if (instanceOf(input,RESTCycleUDR)) { RESTResponse resp = udrCreate(RESTResponse); resp = ((RESTCycleUDR)input).Response; debug("Response Code:" + resp.ResponseCode); list<string> mKeys = mapKeys(resp.Header); for(string i:mKeys) { debug("Header:" + i + "=" + mapGet(resp.Header,i)); } debug(baToStr(resp.Body)); } } Note! HTTP redirect is currently not supported. When the response code is 301 (Moved Permanently) or 302 (Found), the REST Client_Deprecated agent will not follow the URL in the header. In order to handle redirects, you must check the ResponseCode field in the RESTResponse UDR. If the value is 301 or 302, you must configure the APL code to send a new request to the URL that is specified in the Location field of the header. Hint! When the body field contains a JSON formatted string, you can use the APL function jsonDecode to decode the contents. For further information about this function, see 21. JSON Functions in the APL Reference Guide . When the body field contains XML data, you can use the XML schema support in Ultra to decode the contents. For further information about XML and Ultra, see 18. XML Schema Support in the Ultra Reference Guide . RESTError Field Description Field Description ErrorCode (int) This field contains an internal error code: 0 - No error 1 - The response time from the remote server exceeded the timeout value of the agent. 2 - The number of outstanding requests exceeded the maximum value of the agent. 4 - Unable to refresh token. This error will be returned if the agent has tried to refresh the token 10 times without success. 5 - The response payload size exceeded limit. To resolve this error, you can configure the max content length through topo configuration. 99 - Miscellaneous error. ErrorMessage (string) This field contains a description of the error. Example - Handling errors APL import ultra.RESTClient; consume { if (instanceOf(input,RESTCycleUDR)) { if(((RESTCycleUDR)input).Error.ErrorCode!=0) { udrRoute((RESTCycleUDR)input,"RESTCycleUDR_error"); } else { //... } } } OAuth2UDR Field Description Field Description accessTokenURI (string) The URI where the access token can be obtained. bodyParams (map,<string,string>) Some authentication servers may require additional parameters in the body of the token requests, these go into this field in a map format. clientId (string) The unique client identifier issued by the authorization server. clientSecret (string) The client secret. grantType (string) The grant type; either client credentials or resource owner password credentials. overrideBaseURL (string) Determines whether the BaseURL may be overridden or not. password (string) The password associated with the username.This is mandatory when grant type is resource owner password credentials. username (string) The resource owner username, i e end-user granting access to a protected resource. This is mandatory when grant type is resource owner password credentials. If the REST Client_Deprecated agent consumes a new UDR, it will execute Authentication.

---

# Document 568: Common Configuration Buttons - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638256/Common+Configuration+Buttons
**Categories:** chunks_index.json

Config urati ons that are created in the platform share a button bar that allows interactions and operations to be accessed. These common configuration buttons are applicable to all configuration types except Workflows. Some configuration types may have buttons and those are described in detail in each separate section of the documentation. For more information, see the documentation for each configuration type. Button Description Open Edit Click this button to edit the configuration. Note! Each configuration will only allow 1 user to edit it at any one time. While editing the configuration, it will be locked to the user's session. To release the lock, you must navigate away from the configuration to another page. It is advised not to close the browser tab without navigating away from the configuration first as this will cause the lock to stay in place until timeout. Open New Click this button to create a new configuration of the same type. Open Open Click this button to open another configuration. Open Save Click this button to save changes to an existing configuration. Open Save As Click this button to open the Save As dialog where you can select in which folder you want to save the configuration and enter the name you want to save it as. Open Permissions Click this button to define permissions for the configuration. Open Validate Click this button to check if the configuration is valid. Open References Click this button to see which other configurations the configuration uses, or is used by. Open History Click this button to view the version history of the configuration. Open Help Click this button to access the user documentation. See the different tabs b elow for examples of the Open , Save As , Permissions , References , and History dialogs when using the previous buttons in the table above. Open Dialog Box This window is displayed when the users click the Open button in the configurations screen. It allows previously saved configurations to be loaded in the current instance.  the name permissions and owner are displayed in individual rows. Open You can use the Filter Name field to easily find a given instance. Save As Dialog Box This window is displayed when the users click the Save As button in the configurations screen. it allows the current configuration to be saved to a designated folder. Open You assign a Name and an optional version comment in the fields. Permissions Dialog Box This window is displayed when the users click the Permissions button in the configurations screen. It is used to set individual permissions based on the users' role  the owner itself, and the associated access groups in both read, write and execute permissions. Open Open The Browser options allow the users to select the designated user from a dialog box called User Selection . References Dialog Box This window is displayed when the users click the References button in the configurations screen. Open It displays three tabs that show important information in relation to all references that are part of the current configuration. The Used By tab lists all references that are currently used in the configuration, and the Users list the active users that have access to these references. The Access shows the access group and the listed users under them. History Dialog Box This window is displayed when the users click the History button in the configurations screen. Open It shows the available Version digit, the Modified date, Modified by , and the Comment which lists an optional comment for the listed version.

---

# Document 569: MQTT Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653265/MQTT+Agent
**Categories:** chunks_index.json

This section describes the MQTT agent. The agent is a collection agent for real-time workflow configurations. Prerequisites The user of this information should be familiar with: MQTT OASIS Standard MQTT Version 3.1.1 MQTT Protocol The MQ Telemetry Transport (MQTT) is a Client Server publish/subscribe transport messaging protocol that runs on TCP/IP. MQTT is simple and lightweight, designed to have a small footprint on clients with limited capacity. MQTT is meant to be used in low bandwidth, high latency networks, designed to minimize the use of the network bandwidth to ensure higher quality of service and reliability when transporting the message. MQTT is mostly used for Machine- to-Machine (M2M) and Internet of Things (IOT). MQTT Specification Description The MQTT Specifications can be found in OASIS Standard MQTT Version 3.1.1 . The section contains the following subsections: MQTT Agent Configuration MQTT Agent Input/Output Data and MIM MQTT Agent Events MQTT UDRs MQTT Example

---

# Document 570: Platform Container Installation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669841
**Categories:** chunks_index.json

This chapter describes how to prepare and install the Platform Container. This is done in three steps: General Preparations which include configuration of the software environment and setting values for predefined parameters that may vary between installations. For further information, see General Preparations Platform . Database-specific preparations. Depending on the software you have, either Oracle, PostgreSQL, SAP HANA, or Derby should be installed. For further information, see Oracle Preparations , PostgreSQL Preparations , SAP HANA Preparations , or Derby Preparations . Software installation. For further information, see Platform Software Installation . Post Installation configuration. For further information, see Post Platform Installation Configuration . Hint! This section covers a small installation with both Platform and EC in the same installation. For separate Platform and EC container installations, first perform a Platform installation and then refer to Execution Container Installation . This chapter includes the following sections: General Preparations Platform Platform Database Preparations Preparing JDBC Drivers Platform Software Installation Post Platform Installation Configuration

---

# Document 571: High Availability in PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736380/High+Availability+in+PCC
**Categories:** chunks_index.json



---
**End of Part 26** - Continue to next part for more content.
