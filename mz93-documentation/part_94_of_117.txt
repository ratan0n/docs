# RATANON/MZ93-DOCUMENTATION - Part 94/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 94 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.4 KB
---

To open the SFTP collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select SFTP in the Collection tab in the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. You can configure part of the parameters in the Filename Sequence or Sort Order service tabs, see Workflow Template for more information. The Configuration view consists of the following tabs: Connection Source Advanced Security Connection Tab The Connection tab contains configuration settings related to the remote host and authentication. Open The SFTP collection agent configuration - Connection tab Setting Description Setting Description Connection Information Settings Host Primary host name or IP-address of the remote host to be connected. If a connection cannot be established to this host, the Additional Hosts, specified in the Advanced tab, are tried. File System Type Type of file system on the remote host. This information is used to construct the remote filenames. Unix - remote host using Unix file system. Default setting. Windows NT - remote host using Windows NT file system. Enable Bind Address Select this checkbox to explicitly bind a specific virtual network IP as the source of the SFTP agents connection. Bind Address This mandatory field is enabled only when the Enable Bind Address checkbox is selected. Authentication Mechanism Settings Authenticate With Choice of authentication mechanism. Both password and private key authentication are supported. Username Username for an account on the remote host, enabling the SFTP session to login. Password Password related to the specified Username. This option only applies when password authentication is enabled. Private Key When you select this option, a Select... button will appear, which opens a window where the private key may be inserted. If the private key is protected by a passphrase, the passphrase must be provided as well. This option only applies when private key authentication is enabled. For further information, see Authentication in SFTP Agents Preparations . Collection Retries Settings Enable Select this check box to enable repetitive attempts to connect and start a file transfer. When this option is selected, the agent will attempt to connect to the host as many times as is stated in the Max Retries field described below. If the connection fails, a new attempt will be made after the number of seconds entered in the Retry Interval (s) field described below. Retry Interval (s) Enter the time interval in seconds, between retries. If a connection problem occurs, the actual time interval before the first attempt to reconnect will be the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval (s) field. For the remaining attempts, the actual time interval will be the number seconds entered in this field. Max Retries Enter the maximum number of retries to connect. In case more than one connection attempt has been made, the number of used retries will be reset as soon as a file transfer is completed successfully. Note! This number does not include the original connection attempt. Restart Retries Settings Enable Select this check box to enable the agent to send a RESTART command if the connection has been broken during a file transfer. The RESTART command contains information about where in the file you want to resume the file transfer. When this option is selected, the agent will attempt to re-establish the connection, and resume the file transfer from the point in the file stated in the RESTART command, as many times as is entered in the Max RESTARTS field described below. When a connection has been re-established, a RESTART command will be sent after the number of seconds entered in the Retry RESTART Interval (s) field described below. Note! The RESTART Retries settings will not work if you have selected to decompress the files in the Source tab, see the section below, Source Tab. Retry Restarts Interval (s) Enter the time interval, in seconds, you want to wait before initiating a restart in this field. This time interval will be applied for all restart retries. If a connection problem occurs, the actual time interval before the first attempt to send a RESTART command will be the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval (s) field. For the remaining attempts, the actual time interval will be the number seconds entered in this field. Max Restarts Enter the maximum number of restarts per file you want to allow. In case more than one attempt to send the RESTART command has been made, the number of used retries will be reset as soon as a file transfer is completed successfully. Source Tab The Source tab contains configurations related to the remote host, source directories and source files. The configuration available can be modified by creating and selecting a customized Collection Strategy. The following text describes the configuration options available when no customized Collection Strategy has been selected. Open The SFTP collection agent configuration - Source tab Setting Description Setting Description Collection Strategy If there are more than one collection strategy available in the system a Collection Strategy drop down list will also be visible containing the available strategies. For further information about the nature of the collection strategy, see Appendix 4 - Collection Strategies . File Information Settings Directory Absolute pathname of the source directory on the remote host, where the source files reside. The pathname might also be given relative to the home directory of the Username account. Include Subfolders Select this check box if you have subfolders in the source directory from which you want files to be collected. Note! Subfolders that are in the form of a link are not supported. If you select Enable Sort Order in the Sort Order tab, the sort order selected will also apply to subfolders. Filename Name of the source files on the remote host. Regular expressions according to Java syntax applies. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILES , type: TTFILES.* . Compression Compression type of the source files. Determines whether the agent will decompress the files before passing them on in the workflow or not. No Compression - the agent will not decompress the files. Gzip - the agent decompresses the files using gzip. Before Collection Settings Move to Temporary Directory If enabled, the source files will be moved to the automatically created subdirectory DR_TMP_DIR in the source directory, prior to collection. This option supports safe collection of a source file reusing the same name. Append Suffix to Filename Enter the suffix that you want added to the file name prior to collecting it. Warning! Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (h) If enabled, when the configured number of hours have passed without any file being available for collection, a warning message (event) will appear in the System Log and Event Area: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Settings Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ) after collection, to the directory specified in the Destination field. If Prefix or Suffix are set, the file will be renamed as well. Note! By default, if a file with the same filename already exists in the target directory, the file will be overwritten only if the contents of the file are the same. If the contents are not the same, the workflow will abort for SFTP Batch agent and log an error for SFTP batch-based real-time agent. If you want the workflow to overwrite the files unconditionally, select the Overwrite checkbox described in this table below. Destination Absolute pathname of the directory on the remote host into which the source files will be moved after the collection. This field is only available if Move to is enabled. Note! The Directory has to be located in the same file system as the collected files at the remote host. Also, absolute pathnames must be defined. Relative pathnames cannot be used. Prefix and Suffix Prefix and/or suffix that will be appended to the beginning and/or the end, respectively, of the source files after the collection. These fields are only available if Move to or Rename is enabled. Search and Replace To apply Search and Replace , select either Move to or Rename . Search : Enter the part of the filename that you want to replace. Replace : Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Enter Replace with characters and metacharacters that define the pattern and content of the replacement text. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Note that the search value divides the file name into two parts by using brackets. The replace value applies to the second part by using the place holder $2. Keep (days) Number of days to keep moved or renamed source files on the remote host after the collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, determining when the file may be removed. This field is only available if Move to or Rename is selected. Overwrite If selected, the existing file with the same name will be overwritten unconditionally. This field is only available for Default Collection Strategy and if Move to or Rename is enabled. Rename If enabled, the source files will be renamed after the collection, remaining (or moved back from the directory DR_TMP_DIR , if using Move to Temporary Directory ) in the source directory from which they were collected. Note! You must avoid creating new file names still matching the criteria for what files to be collected by the agent, or else the files will be collected over and over again. Note! By default, if a file with the modified name already exists, this file will be overwritten only if the contents of the file are the same. If the contents are not the same, the workflow will abort for SFTP Batch agent and log an error for SFTP batch-based real-time agent. If you want the workflow to overwrite the files unconditionally, select the Overwrite checkbox described in this table above. Remove If enabled, the source files will be removed from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ), after the collection. Ignore If enabled, the source files will remain in the source directory after the collection. This option is not available if Move to Temporary Directory is enabled. UDR Type Settings Route FileReferenceUDR Select this check box if you want to forward the data to an SQL Loader agent. See the description of the SQL Loader agent in SQL Loader Agent for further information Advanced Tab The Advanced tab contains configurations related to more specific use of the SFTP Advanced service. Open The SFTP collection agent configuration - Advanced tab Setting Description Setting Description Advanced Settings Port The port number the SFTP service will use on the remote host. Timeout (s) The maximum time, in seconds, to wait for response from the server. 0 (zero) means to wait forever. Accept New Host Keys If selected, the agent overwrites the existing host key when the host is represented with a new key. The default behavior is to abort when the key mismatches. Warning! Selecting this option causes a security risk since the agent will accept new keys regardless if they might belong to another machine. Enable Key Re-Exchange Used to enable and disable automatic re-exchange of session keys during ongoing connections. This can be useful if you have long lived sessions since you may experience connection problems for some SFTP servers if one of the sides initiates a key re-exchange during the session. Additional Hosts Settings Additional Hosts List of additional host names or IP-addresses that may be used to establish a connection. These hosts are tried, in sequence from top to bottom, if the agents fail to connect to the remote host set in their Connection tabs. Use the Add, Edit, Remove, Move up and Move down buttons to configure the host list. Security Tab The Security tab contains configurations related to the Advanced Security Options for SFTP. The Configuration available can be modified by selecting the Advanced Security Option check box. If the advanced security is not enabled, the Cipher Mode will default to aes128-ctr and the HMac Type will default to hmac-sha2-256 . If advanced security is enabled but the combo box fields are left empty, the Cipher Mode will default to aes128-ctr and the HMac Type will default to hmac-sha2-256 . Open The SFTP collection agent configuration - Advanced tab The SFTP collection agent configuration - Security tab Note! Due to an upgrade of the Maverick library for MediationZone version 8.1.5.0, the default handling of the advanced security has changed. Users should take note of the behaviour change for the Advanced Security Option for the SFTP agents. The Advanced Security Option will be disabled by default. Users will have to enable it on their own accord from the Security Tab in the SFTP agents configuration. With Advanced Security Option disabled, Maverick will manage the connection between the SFTP agent and the server. Maverick will attempt to connect with the STRONG security level. Failing to do so, it will auto downgrade the security level to WEAK and attempt to connect, this behaviour will allow our agents to work well with backwards compatibility for servers with older instances of the Maverick library. Furthermore, having STRONG security level will result in a performance degradation. However, when a user manually enables the Advanced Security Option from the security tab, Maverick will instead assign the WEAK security level, which will not be as strict or resource intensive as the STRONG security level. For more information about security levels, you can refer to this page: https://www.jadaptive.com/managed-security-in-our-java-ssh-apis/ Setting Description Setting Description Cipher Mode Algorithms for the Block Cipher Modes supported by the SFTP agent. This allows the agent to determine which algorithm for the block cipher to be used when communicating with the SFTP servers. 3des-cbc 3des-ctr blowfish-cbc aes128-cbc aes192-cbc aes256-cbc aes128-ctr aes192-ctr aes256-ctr HMac Type Methods of encryption for Key Exchange. This allows the agent to determine the method of encryption to be used when the keys are exchanged between the SFTP servers and the SFTP agent. Note! All Sha-1 encryption (such as hmac-sha1, hmac-sha1-96 and etc) are considered weak in terms of security and it may be deprecated in the future. We do not recommend you use this encryption type but we advise caution if you choose to use it. hmac-sha1 hmac-sha1-96 hmac-sha1-etm@openssh.com hmac-md5 hmac-md5-96 hmac-md5-etm@openssh.com hmac-sha2-256 hmac-sha2-256-96 hmac-sha2-256-etm@openssh.com hmac-sha2-512 hmac-sha2-512-96 hmac-sha2-512-etm@openssh.com hmac-ripemd160 hmac-ripemd160-etm@openssh.com

---

# Document 2206: Starting and Stopping the System - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205815975/Starting+and+Stopping+the+System
**Categories:** chunks_index.json

Starting the System Login as mzadmin on the host where the Platform Container is installed. Ensure that the environment variables are set correctly: Variable Description Variable Description MZ_CONTAINER Example value: container0 This environment variable specifies an identifier for the installed container. Each container in your MediationZone system must have a unique identifier. MZ_HOME Example value: /opt/mz This environment variable specifies where the MediationZone software is installed. JAVA_HOME Example value: /opt/jdk/jdk-17.0.2 This environment variable specifies where the JDK is installed. PATH This environment variable specifies the search path. The search path must contain the following directories: $JAVA_HOME/bin:$MZ_HOME/bin Start the Platform by entering the mzsh startup command: $ mzsh startup platform If you have configured SCs or ECs in the Platform container, you can add the names of these pico instances to the startup command. The Platform must be started before any ECs or SCs. Example - How to Start the System Enter the following command to start the Platform and an EC (ec1) in the Platform Container: $ mzsh startup platform ec1 Start other pico instances that are required for execution, i e ECs or SCs. If you have enabled remote access to the containers in your system, you can start pico instances on other containers with the the mzsh system start command: $ mzsh system start container:<container regexp>/pico:<pico name regexp> Hint! The Platform will be started if it is not already running. Note! Set your desired environment variables in $MZ_HOME/bin/mzshr.env as this file will be loaded with local variables. Add the desired profiles, such as ". /home/mzadmin/.profile_mz". Example - mzsh system start Enter the following command to start all pico instances in a container: $ mzsh system start container:<container>/pico:.* Repeat this command for all containers in the system to start all pico instances. You can also start all pico instances by omitting the target path: $ mzsh system start This is equivavalent to: $ mzsh system start container:.*/pico:.* Add the --tag option to the system command to start a group of pico instances: $ mzsh system start --tag tag1 container:.*/pico:.* For further information about the system command and how to add tags to pico instances, see system . If you have not enabled remote access, you must login to each container host and start the pico instances with the mzsh startup command. $ mzsh startup <pico-name-1> <pico-name-2> <...> Stopping the System Log in as mzadmin on the host where the Platform Container is installed. Make sure that the environment variables are set correctly: Variable Description Variable Description MZ_CONTAINER Example value: container0 This environment variable specifies an identifier for the installed container. Each container in your MediationZone system must have a unique identifier. MZ_HOME Example value: /opt/mz This environment variable specifies where the MediationZone software is installed. JAVA_HOME Example value: /opt/jdk/jdk-17.0.2 This environment variable specifies where the JDK is installed. PATH This environment variable specifies the search path. The search path must contain the following directories: $JAVA_HOME/bin:$MZ_HOME/bin Stop all ECs, and SCs. Note! It is recommended that you stop workflows that depend on the specified pico instances before you run the mzsh shutdown command. Workflows that are active during shutdown, may remain in an active state or enter an unreachable state. In both these cases, the workflows may need manual attention to ensure that they are deactivated.Set your desired environment variables in $MZ_HOME/bin/mzshr.env as this file will be loaded with local variables. Add the desired profiles, such as ". /home/mzadmin/.profile_mz". If you have enabled remote access to the containers in your system, you can stop pico instances on other containers with the the mzsh system stop command: $ mzsh system stop container:<container regexp>/pico:<pico regexp> Example - mzsh system stop Enter the following command to stop all pico instances in a container: $ mzsh system stop container:<container>/pico:.* Repeat this command for all containers in the system to stop all pico instances. You can also stop all pico instances by omitting the target path: $ mzsh system stop This is equivavalent to: $ mzsh system stop container:.*/pico:.* Add the --tag option to the system command to stop a group of pico instances: $ mzsh system stop --tag tag1 container:.*/pico:.* For further information about the the system command and how to add tags to pico instances, see system . If you have not enabled remote access, you must login to each container and stop the pico instances with the mzsh shutdown command. $ mzsh shutdown <pico-name-1> <pico-name-2> <...> Stop the Platform by entering the mzsh shutdown command: $ mzsh shutdown platform The Platform must be the last of the pico instances to shut down. The reason for this is that the mzsh shutdown and mzsh system stop commands are issued through the Platform. Note! The Unix command kill should only be used when the shutdown command is not working, since it will force interruption of all processing instead of waiting until the next End Batch message is issued for each workflow. The use of this command may be necessary if the Platform is accidentally shut down prior to an Execution Context, for example.It is recommended that you stop workflows that depend on the specified pico instances before you run the mzsh shutdown command. Workflows that are active during shutdown, may remain in an active state or enter an unreachable state. In both these cases, the workflows may need manual attention to ensure that they are deactivated.Set your desired environment variables in $MZ_HOME/bin/mzshr.env as this file will be loaded with local variables. Add the desired profiles, such as ". /home/mzadmin/.profile_mz". Hint! Use the following mzsh command to retrieve the running status of pico instances in the system: $ mzsh topo show status

---

# Document 2207: FTP NMSC Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641422/FTP+NMSC+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent consumes bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Published MIMs MIM Parameters Description MIM Parameters Description File Creation Timestamp A parameter that contains a time stamp, that indicates when the file has been created. The value originates from the Data Storage Control File and is expressed in local time. Data type date , set as defined in the MIM type header . File Retrieval Timestamp A parameter that contains a time stamp that indicates when the file transfer starts. Data type date , set as defined in the MIM type header . Source Filename A parameter that contains the name of the file that is currently being processed, as defined at the source. Type string , set in the MIM type header . Source Host The IP address or host name of the switch. Source Username The name of the logged-in user. Source Pathname The value of the directory where the control files will be read. Accessed MIMs No MIM values are accessed by this agent.

---

# Document 2208: Introduction to the Ultra Format Definition Language - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678205/Introduction+to+the+Ultra+Format+Definition+Language
**Categories:** chunks_index.json

Ultra Format Definition Language (UFDL) is the language used to configure the MediationZone format handling subsystem. It describes the physical structure of incoming and outgoing (external) data, internal (working) formats, as well as decoding and encoding rules. The agents responsible for translations are the Decoder and Encoder agents. The Decoder converts external raw data (byte arrays) into internal data (UDRs). The Encoder works the other way around, converting internal formats into raw output data for the forwarding agents. Open Decoders and Encoders processing data UFDL syntax is entered in the Ultra Format Configuration . This chapter includes the following sections: Building Blocks The Ultra Module

---

# Document 2209: Log Files - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031026/Log+Files
**Categories:** chunks_index.json

The system can produce the following types of log files during run-time: Pico logs: Logs generated from each Pico Debug logs: Debug logs are generated when workflows have turned on logging to file. See Log and Notification Functions . APL logs: When some log commands are used in APL they are logged to a file, see Log and Notification Functions . See Log Properties for more information. To view the Log Files, go to Manage  Tools & Monitoring and then select Log Files . Open Log Files page Filter Dialog Before anything is visible in the tables a selection needs to be made in the filter dialog. Click on the Get Started button to see the Filter dialog. Open The Pico Log File Filter Field Description Field Description Pico* This dropdown lists all available Picos/ECs from which logs can be retrieved. You must select at least one Pico to view logs, as this field cannot be left blank. It filters capture logs from both platform-level and application-level components, as they all operate within the Pico framework. As a result, applying filters like Platform , EC , or Transport may include logs from related services, such as Derby or Execution Contexts, due to their interaction with the core platform. Note! For each selected Pico, a request will be sent to that Pico to list all files in a specific folder. If you select Picos in bulk, the response time might be slow depending on the network of each Pico. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 File Name Use this field to filter by file name. If you enter multiple file names or parts of names, the filter will show logs that match any of the values you enter. Each Pico lists log files from specific directories based on the following properties: pico.stderr points to the directory containing standard error logs. pico.stdout points to the directory containing standard output logs. log4j.configurationFile specifies the log configuration file. If not defined, it defaults to $MZ_HOME/etc/log4j2.xml , which manages both regular and history log directories. If multiple Picos point to the same log directories, identical files may appear across different Picos. The UI automatically filters duplicates, showing only one file with the same name and timestamp. Debug Log File Tab On each EC it will list files from the debug folder decided by the parameter mz.wf.debugdir . If this parameter is not set it will list files from the debug folder inside the temporary folder decided by the parameter pico.tmpdir . Open The Debug Log File Filter Field Description Field Description EC This dropdown lists all available Execution Contexts (ECs) from which debug logs can be retrieved. You must select at least one EC to view logs, as this field cannot be left blank. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 APL Log File Tab On each EC it will read the log4j configuration described here, log4j APL Logging Configurations , to list the files in the directory specified by log4j. Open The APL Log File Filter Field Description Field Description EC* This dropdown lists all available Execution Contexts (ECs) from which APL logs can be retrieved. You must select at least one EC to view logs, as this field cannot be left blank. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 Log Files Table All three tabs have the same table layout. Open Pico Log Files table with 1 filter applied Table Item Description Table Item Description Filter Button Displays the filter dialog Refresh Button Updates the file list Name Column The name of the log file Last Modified Column The date and time when the file was last modified Pico/EC Column The Pico/EC file is present Size Column Size of the log file Actions Opens the Actions menubar with the option Download and Pre view. Open Preview You can view the first 10 Mb of a file, by clicking Preview in the Actions column. You can also download the log file from the Preview dialog. Open Preview log file dialogue

---

# Document 2210: Python Collection Agent Events - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686531/Python+Collection+Agent+Events+-+Real-Time
**Categories:** chunks_index.json

Agent Message Events

---

# Document 2211: ECS Statistics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641153/ECS+Statistics
**Categories:** chunks_index.json

To open the ECS Statistics dialog, click the Inspection button in the upper left part of the Desktop window, and then double-click ECS Statistics in the menu. The system gathers the current number of New and Reprocessed UDRs and batches per Error Code from the ECS. The ECS Statistics window allows for inspection of these calculated values, as well as more specific views since the data may be inspected down to the Error Code level. The data may also be exported and printed. Note! The ECS Statistics data is gathered and calculated using a system task called ECS_Maintenance, see ECS Maintenance System Task for more information. If you want to change the scheduling of the task, this is done in the ECS_Maintenance_grp configuration. If the ECS_Maintenance system task is scheduled to be executed with a time interval of less than an hour, the statistical data will be gathered every hour. Initially, the ECS Statistics dialog is empty. It is populated by performing a search. Setting Description Setting Description Search Shows the Search ECS Statistics dialog. For further information, see the section below, Searching the ECS Statistics. Export Shows the Export dialog, allowing the statistics to be exported. Print Shows the Print dialog, allowing the statistics to be printed. Searching the ECS Statistics To populate the ECS Statistics window, click Search and specify the search criteria. If no limitations are entered in the Search ECS Statistics dialog, a basic search is performed. For further information, see the section below, Basic Search. Open Search ECS Statistics dialog Setting Description Setting Description Data Type Determines if the search is made for Batches or UDRs. Error Code A list of available Error Codes, as defined in the ECS. To search for more Error Codes, click the Add button below the Error Code list. Period Select Period to search for a specific time period when the data was entered into the ECS. Note! Only 100 000 entries at a time can be browsed. If the search results in more than 100 000 entries, bulk operations must be repeated for each multiple of 100 000. Basic Search If no limitations are entered in the Search ECS Statistics dialog, a basic search table is shown. Select one row in the table to display the spread of Error Codes into a pie chart. If up to four Error Code types for the same date are named, these are all shown in the graph. If five or more Error Code types are present, the three most common Error Code types are shown, and the rest are grouped into the Other category. Open ECS Statistics dialog - Search without limitations Item Description Item Description Date The date and time when the values were calculated. New The amount of new errors current in the ECS on the given date. Reprocessed The amount of reprocessed errors current in the ECS on the given date. Value Type Select to display statistics for either New or Reprocessed UDRs or batches. Error Code Search If a specific Error Code search is done, three new columns related to the Error Codes are added to the table. Open ECS Statistics dialog - Search of error codes Item Description Item Description Error Code The Error Code column is visible when the search is made based on Error Codes. It displays the error code name. Newest The last time the error occurred. Oldest The first time the error occurred. Error Code Report Shows the number of UDRs attached to the selected Error Code.

---

# Document 2212: SQL Collection Agent Events - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740724/SQL+Collection+Agent+Events+-+Batch
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with batch after X UDRs. Reported when a complete batch is collected. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: SQL: XXX The debug message is sent when the SQL agent creates its SQL string to send to the database.

---

# Document 2213: generate_pcc_classes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743869/generate_pcc_classes
**Categories:** chunks_index.json

usage: generate_pcc_classes <Output dir> <XML data model dir> [ <XML data reference dir> ] [ -resource <resource dir> ] When having defined your own PCC datamodel, this command is used to generate the pcc classes for the extension. Return Codes Listed below are the different return codes for the generate_pcc_classes command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the syntax is incorrect. 2 Will be returned if any of the stated directories are missing.

---

# Document 2214: IPDR SP Examples - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653066/IPDR+SP+Examples
**Categories:** chunks_index.json

This section contains one example for the IPDR SP agent. IPDR SP Agent In this workflow example for the IPDR SP agent IPDR SP workflow example The IPDR SP agent sends any of these 4 UDR types: Data, SAMIS, SAMIS_TYPE_1 or SAMIS_TYPE_2 to the Analysis agent, which contains the following code: consume { //Debug the incoming UDR in Workflow Monitor if (instanceOf (input, SAMIS_TYPE_1)) { debug("SAMIS_TYPE_1 record DSN=" + input.SeqNo + " from " + input.ConnectionKey); } else if (instanceOf (input, SAMIS_TYPE_2)) { debug("SAMIS-TYPE-2 record"); } else if (instanceOf (input, SAMIS)) { debug("SAMIS record DSN=" + input.SeqNo + " from " + input.ConnectionKey); } else if (instanceOf (input, Data)) { debug("Data record"); } //Route the incoming udr to the Encoder udrRoute( input ); } With this code, the Analysis agent will: Check the input from the IPDR SP agent and print the input to the Events panel. Route the input back into the IPDR SP agent. This is done so that the agent is able to generate an ACK to be returned to the exporter. IPDR Response ACK It is mandatory to configure the Analysis agent to return the input UDR back to the IPDR SP agent. This will notify the IPDR SP agent that the particular IPDR record has been completed successfully by the Analysis agent and enables it to prepare for a data acknowledgement message to be sent back to exporter.

---

# Document 2215: Preparing the Database (To be removed) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204614015/Preparing+the+Database+To+be+removed
**Categories:** chunks_index.json

Follow these steps to prepare the Impala database: Open a browser and and enter URL of the Hue interface. Create a staging directory. Open the file browser in Hue. Select a directory in the file browser, e g /user/impala/uploads Click the New button and then select directory. Enter the name of the new directory, e g staging and then click Create . Select the directory in the file browser. Click the Actions button and then select Change Permissions . Update the permissions to make the new directory available to the UNIX user(s) that is used to start the ECs. Create a database and a table to be used by Data Hub. Select Impala from Query Editors . Enter a CREATE DATBASE statement in the editor and then click the Execute button. Example - Creating a database CREATE DATABASE test; Click the Refresh button. Enter a CREATE TABLE statement in the editor and then click the Execute button. The CREATE TABLE statement may contain the following data types: Note! A PARTIONED BY clause is optional. However, it is highly recommended since it will improve the performance of queries that restrict results by the partitioned column. A partition column of INT type also make it possible to use the Data Hub task agent to automatically remove old data from the table. For further information about the Data Hub task agent, see Data Hub Task Agent - int . Data Hub is limited to handle one partition column. A STORED AS PARQUET clause is required. If you omit this clause, Data Hub will fail to update the table. Example - Creating a table in Impala CREATE TABLE IF NOT EXISTS mytable (id STRING, start BIGINT, stop BIGINT) PARTITIONED BY (yearmonthday INT) STORED AS PARQUET When you run the Data Hub agent, temporary tables will be created in the same schema. These table will be visible in Hue but hidden in , e g in the Desktop and Web UI.

---

# Document 2216: FTAM IOG Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738880/FTAM+IOG+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file: filename Reported along with the name of the source file, when the file given in Filename has been collected and inserted into the workflow. File cancelled: filename Reported along with the name of the source file, each time a Cancel Batch message is received. This assumes the workflow is not aborted. For further information, see FTAM IOG Agent Transaction Behavior . Debug Events There are no debug events for this agent.

---

# Document 2217: kpi - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645468
**Categories:** chunks_index.json

The kpi objects describe how metrics are linked to dimensions and threshold levels. You may configure kpi objects to perform additional calculations based on metrics, e g ratios or sums of ratios. The following JSON schema describes the data format of the kpi object type: Loading The kpi object type has the following properties: Property Description node node must contain an array of strings that constitutes a path to a node in a tree object. The first value in the array must match the name of a tree object, and the subsequent values must match the name of the descendant nodes in the tree object. immediateAlarm When immediateAlarm is set to false , exceeded threshold levels will be indicated in a KPIOutput UDR at the end of a period. The UDR field outputType will be set to 0. This is the default behavior. When immediateAlarm is set to true , a KPIOutput UDR with outputType 1 will be generated at the time of detection. Exceeded threshold levels will also be indicated in the output that is generated at the end of the period. Immediate alarm data are sent by Spark to the Kafka topic kpi-alarm . windowSize The incoming data is sorted into time periods based on the timestamp value of the KDR UDRs. The KPIs are calculated and generated according to these periods. windowSize defines the length of the periods in the same time unit as the KDR timestamp. A flooring algorithm is applied to timestamps in order to select the start time of a period: period start = kdr.timestamp - (kdr.timestamp mod windowSize) Note! When the value of aggregated-output is false , kpi objects in your service model may have different window sizes. However, w hen the value of aggregated-output is true, all kpi objects in the model must have the same window size. threshold threshold may contain the name of a threshold object. expr expr must contain an arithmetic expression based on metric objects. You can use the expression to calculate e.g. ratios or sums of ratios. Example - Expressions "Plain" metric: "expr": "AvgLength" Ratio: "expr": "TotalAttempts/TotalSuccessful" Sum of Ratios: "expr": "(Type1Attempts/Type1Successful)+(Type2Attempts/Type2Successful)" In the case of division by zero, the value of the output of the expression will be positive infinity, negative infinity, or NaN (Not a Number) as defined in the JVM specification. Example - JSON Representation "kpi": { "Region.AL": { "node": [ "tree1", "Region" ], "windowSize": 60, "threshold": "Region.AL", "expr": "AvgLength" } }

---

# Document 2218: Aggregation, Consolidation and Correlation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849178/Aggregation+Consolidation+and+Correlation
**Categories:** chunks_index.json

The Aggregation agent can correlate, consolidate and aggregate records of different record types simultaneously, in real-time or batch-mode, based on configurable criteria. Among other things, it includes support for the following: Automatic creation of session on first UDR arrival Association of UDRs, potentially of different record types Script execution on association Configurable timeout management for sessions Session variables, including lists of associated records Storage of associated data in either memory, file or Couchbase database Sessions stored in a Couchbase database can be replicated to multiple servers Transaction safe for batch mode Creation of new records based on aggregated information using scripting An Aggregation agent can simultaneously correlate and aggregate multiple data streams from multiple network elements / network technologies. Any aggregation, consolidation or correlation logic can be configured through the GUI. An Aggregation profile is used to select the UDR types to correlate, and the fields on which to base the correlation. A rule identifies UDRs based on the content of the chosen UDR fields for each input type. One or more fields can be used to find the matching UDRs. In the example below, records from two sources are matched to create joint sessions. Any number of different records can be matched, as long as they have the same matching criteria. The Aggregation Profile is then selected when configuring the Aggregation agent within this workflow. Open Example of an Aggregation agent in a real-time workflow Aggregation/Correlation sessions can be viewed through the Aggregation Session Inspection GUI. The sessions can be searched for and released for further down-stream systems for further processing. Aggregation Flush Functionality The APL function aggregationHintFlushSessions is available in Analysis and Aggregation APL code in Batch Workflows. It is used to indicate that the timeout function block, in the APL code for the specified Aggregation Agent, will be executed for all the sessions in the storage that have a timeout value. The timeout block execution will be performed after the drain function block has been executed in the specified Aggregation Agent. This can be useful before system upgrades or maintenance windows where it is desirable to keep temporary data at a minimum. Couchbase Aggregation Couchbase can be used to store aggregation session data in real-time workflows. By selecting a mirror profile you may also acquire read only access to sessions in a secondary Couchbase cluster. This is useful in various failover scenarios. The mirror session data can be extracted via APL code. Storing aggregation sessions in Couchbase improves scalability and enables sharing of data across workflows and geographic locations.

---

# Document 2219: GTP' LGU Collection Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033358/GTP+LGU+Collection+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data that an agent expects and delivers. The agent produces UDR types in accordance with the Decoder tab settings. If you have not configured this tab, the agent produces UDRs of the type GTPCollectionUDR. The agent also produces GTPEchoRequestUDR. Meta Information Model For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Value Description Cancel Data Count (long) The number of received Cancel Data Record Packet requests. Data Record Count (long) The number of received Send Data Record Packet requests. Duplicate Message Count (long) The number of received duplicates. Last Request Timestamp (long) This MIM value contains the timestamp for the last received packet. Message Error Count (long) The number of received errorneous messages. Out of Sequence Count (long) The number of received records which are not in sequence. Possible Duplicate Count (long) The number of received Send possibly duplicated Data Record Packet requests. Release Data Count (long) The number of received Release Data Record Packet requests. Accesses The agent does not access any MIM parameters.

---

# Document 2220: TCP/IP Collection Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609691/TCP+IP+Collection+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The agent produces UDRs in accordance with the Decoder tab. If Send Response is selected, the agent consumes bytearray types for single connections and TCPIPUDR for multiple connections. MIM For information about the MIM and a list of the general MIM parameters, see Meta Information Model in Administration and Management in Legacy Desktop . The agent does not publish nor access any MIM parameters.

---

# Document 2221: Data Veracity for Correcting Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881930/Data+Veracity+for+Correcting+Data
**Categories:** chunks_index.json

Data Veracity provides a repository for erroneous records and allows users to search, view, modify and reprocess the stored data. Data Veracity is an alternative to the Error Correction System (ECS): Data Veracity can handle larger volumes of data and has higher performance when searching, filtering, and processing data. Data Veracity can store erroneous batches as well as erroneous UDRs. No additional installation is required. For demo and development systems, the integrated Derby database can be used. Data Veracity will contain a new web UI to support data exploration, classification and modification of records. this is accompanied by 3 new agents and APL functions to offload data flows from invalid, corrupt or incomplete records. The APL functions will work much like the ECS functions to facilitate the automation of detecting, qualifying and correcting the invalid records. Data Veracity Profile The Data Veracity Profile contains settings for selecting the database profile, mapping of UDR types to be used, mapping information for MIM and generation of the SQL script for the generation of tables in the external database. Open Data Veracity Profile Data Veracity is supported for use with the following databases: Oracle PostgreSQL The Data Veracity forwarding agent commits received UDRs to the external database, including any related metadata. Open Example of Workflow with the Data Veracity forwarding agent The Data Veracity collection agent is used to collect erroneous UDRs from the database using the search filters or error codes defined in the Data Veracity web UI. Only UDRs with the status "NEW" will be collected. The state will be changed from "NEW" to "UPDATED" after collection. Data Veracity UI The Data Veracity UI contains functions for examination and manipulation of the stored erroneous data. Open Data Veracity UI

---

# Document 2222: HTTP/2 Client Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739302
**Categories:** chunks_index.json

To open the HTTP/2 Client agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button The Agent Configuration consists of the following tabs: 1 Client Tab 2 OpenAPI Tab 3 Authentication Tab Client Tab Open The Client tab contains the following settings: Setting Description Setting Description Settings Use SSL Select this option to use the SSL. Security Profile Min Connections Per Destination Specify the minimum number of connections towards an endpoint (identified by host:port) that the agent will try to maintain. A value of 0 indicates "no minimum". This is not a hard limit as there could be more connections created during the spike in traffic. For more details see Algorithm for Minimum Connections . Min Connections Per Destination Check Interval (s) Specify how often you wish to check the established connections. A value of 0 indicates "no check will be performed". After each interval, the agent will try to create additional connections to achieve number set in the Min Connections Per Destination parameter. No connections will be shut down if the current number of connections is greater than the number set in Min Connections Per Destination . For more details, see Algorithm for Minimum Connections . Default connection concurrent streams The initial value of concurrent streams that the client would like to handle per connection. The default value is 1000. Note! This value may be overridden by any value sent by server. Max Response Content Length (MB) Specify the maximum length for response content. The default value is 8MB. Timeout Request Timeout (sec) Enter the timeout period in seconds for the request to the HTTP/2 client to wait for a response before timing out. Queue Settings Max Requests Queued Per Destination The size of the message queue in the Jetty server. The default value is 20000. Note! Use this property to manage the memory usage. It is recommended that the EC or ECD running the workflow have xmx that is at the minimum (message size X queue size), otherwise there is a risk for out of memory errors. Retry Interval For full Queues (ms) A millisecond value to indicate the time it will take for the request to try again when the queue is full. Max Retries For Full Queue When the Route Error UDR option is checked, this field will be enabled. This field will indicate how many times the request will attempt to retry before routing it to Error UDR. Route Error UDR If this option is selected, the request will attempt to be received by the agent until it reached the maximum amount of retries. Once the threshold is reached, the request will be stored in an Error UDR and sent as an output from the HTTP/2 client agent. If this option is not selected, the request will attempt the retries for an indefinite amount of time, until the workflow is terminated. Server Monitor Use HTTP2 Server Monitor Select this option to monitor the connection status of all servers that the agent has sent requests to. The monitoring is done by sending regular ping messages to the servers. If the servers are not responding, or there are other communication errors, they will be indicated as Unavailable. A list of the Available and Unavailable servers are available in two MIM values: Available Servers and Unreachable Servers . Ping Interval (s) Define the ping message interval for the Server Monitor. The time unit is seconds and 10 seconds is default. Cookies Add Cookies to Request Headers When this option is selected (default), the agent stores cookies from previous HTTP responses in a cookie store. There will be separate cookie storage maintained per agent for both HTTP/1 and HTTP/2 client instances. For each new request, the agent checks the cookie store and automatically adds any matching cookies to the request headers, provided they are not expired and match the requests destination path. Algorithm for Minimum Connections In case there is a need to run more than one connection towards an endpoint (identified by host:port) irrespective of current traffic, there is an option to configure that using Min Connections Per Destination and Min Connections Per Destination Check Interval(s) parameters. At the beginning, the agent will try to establish the number of connections set in Min Connections Per Destination and then later try to keep number of open connections for each endpoint. As some connections can be shut down due to any reason, the agent will try to check its status at regular intervals. These check intervals are configured in Min Connections Per Destination Check Interval(s) parameter. Note! The connections are used by the agent in Round Robin manner. The agent will never try to close a valid connection. OpenAPI Tab Open HTTP/2 Client Agent Configuration - OpenAPI tab The OpenAPI tab contains the following settings: Setting Description Setting Description Use OpenAPI Profile Select this option if you want the agent to use the OpenAPI profile(s). OpenAPI Profile Browse and select the profile to be used. This field is enabled when the Use OpenAPI Profile option is selected. Click Browse to search for the available OpenAPI profiles. Warning! There are no limits to the number of profiles users can select. However, by selecting a large number of OpenAPI profiles will have significant impact on the overall performance of the workflow. Enable Validation Select this option if you want to validate the OpenAPI profile. Warning! Turning this option ON will have a very significant performance impact on the overall performance of the flow. When validation is enabled, each payload will be validated against the Open API schema, an operation that can be very resource-intensive. We recommend to only enable this setting during development and testing and to disable it in a stable production environment. Note! Strict validation is applied against the OpenAPI specification due to the upgrade of third party libraries. For Example, if the response contains the body but the schema doesn't expect the response to contain body then it will causing validation failure. Refer to this link for further information https://bitbucket.org/atlassian/swagger-request-validator/issues/246/validator-does-not-check-a-response-body Connect your account . Authentication Tab The Authentication tab contains settings for the following Authentication types: 1 Basic Authentication Type 2 OAuth 2.0 Authentication Type 3 Nnrf Access Token Authentication Type If None is selected, the authentication is not enabled. Basic Authentication Type Open HTTP/2 Client Agent Configuration - Basic authentication type Setting Description Setting Description Username Enter a username for an account on the remote server. The username must not include colon (:) characters. Password Enter the password associated with the username. OAuth 2.0 Authentication Type Open HTTP/2 Client Agent Configuration - OAuth 2.0 authentication type Setting Description Setting Description Grant Type Select the grant type: Client Credentials The agent fetches the access token from the Access Token URI during initialization, using client id and client secret for basic authentication. The credentials are base64 encoded and sent in the header of the request. The response contains an access token, which is then used in subsequent requests. Resource Owner Password Credentials The agent fetches the access token from the Access Token URI during initialization, using the following credentials for authentication: Client ID Client Secret Username Password The credentials are sent in the request body. The response contains an access token, which is then used in subsequent requests. Client Authentication Type Select the client authentication method: client_secret_basic - The credentials are encoded in base64 and sent in the request header. client_secret_post - The credentials are sent in the request body. Client ID Enter the unique client identifier issued by the authorization server. Client Secret Enter the client's secret. Username Enter the resource owner username, this can be the end-user granting access to a protected resource. This field is required when you have selected Resource Owner Password Credentials from the drop-down list Grant Type . Password Enter the password associated with the username. This field is required when you have selected Resource Owner Password Credentials from the drop-down list Grant Type . Access Token URI Enter the URI where the access token can be obtained. Base URL Field Some authentication servers may provide a base URL in the response body that the HTTP/2 client agent must use for subsequent requests. If applicable, specify the key name in the JSON formatted string where the base URL is located. The value extracted from the response will override the host field of the RequestCycle UDR configured in the APL. In the following example, the base URL is available in instance_url . Example - Base URL in response from authentication server {"access_token":"00D5E0000008lbR...", "instance_url":"https://cs84.example.com", "id":"https://example.com/id/00D5E0000008lbRUAQ/0055E000000HRCHQA4", "token_type":"Bearer","issued_at":"1490699031149", "signature":"tWccV/a3r0y/JoMRTUbpiviwmslJD2J29yTtSz7yDHE="} Token expiration override (sec) Enter a time in seconds when you would like to refresh the access token prior to the expiration. This allows the application to obtain a new access token without the user's interaction. Additional Parameters Some authentication servers may require additional parameters in the body of the token requests. To add a parameter, click the Add button and then enter the name of the parameter in the Key field and the value of the parameter in the Value field. Do not use escape characters in the value field, these will be added automatically by the HTTP2 Client agent. For instance, " Example Domain " will be sent as "https%3A%2F% 2example.com %2F". Note! If an agent is configured on the Authentication Tab to use OAuth 2.0, an additional step may be required if the token needs to be obtained via HTTPS protocol. The HTTP/2 Client internally uses the Java built-in HTTP Client to obtain a token. If a certificate is required to contact the authentication server, the proper certificate has to be put into the default certificates file for the respective JDK distribution used. Example for OpenJDK 64-Bit Server VM Zulu17.40+19-CA cd $JAVA_HOME/lib/security keytool -import -alias mycert -keystore cacerts -file oauth2Host.cert Nnrf Access Token Authentication Type Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type Setting Description Setting Description Authorization Server (NRF) Enter the full URL path for the authorization server. If you enter an https URL, you must select the checkbox Use SSL . Use SSL If you want to use encryption, select this checkbox. Security Profile If you prefer to use a secure connection, click Browse to select a security profile with certificate and configuration. Refer to Security Profile for more information. NF Instance ID Enter the NF instance ID of the server that you want to send requests from. Scope Enter a string for the service(s) that you want to use, separated by whitespaces. Optional Parameters NF Type The NF type available for selection is CHF (charging function). If you select this setting, you must also select the Target NF Type . This is included in an access token request for an NF Type but not for a specific NF/NF service instance. Target NF Instance ID Enter the NF instance ID of the server for which the access token is requested. This is included in an access token request for a specific NF service provider and shall contain NF Instance ID of the specific NF service provider. Target NF Type The NF type available for selection is CHF (charging function). If you select this setting, you must also select NF Type . This is included in an access token request for an NF Type but not for a specific NF/NF service instance. Requester FQDN Enter the Requester FQDN (Fully Qualified Domain Name). This is used by the NRF (Network Repository Function) to validate that the requester NF service consumer is allowed to access the target NF service provider. Target NF Set ID Enter the Target NF Set ID of the consumer profile. This is included in the access token request of the NF type. Target NF Service Set ID Enter the Target NF Service Set ID of the consumer profile. This is used by the NRF (Network Repository Function) to validate that the requester NF service consumer is allowed to access the target NF service instance. Hnrf Access Token URI Enter the Hnrf URI where the access token can be obtained. Source NF Instance ID Enter the Source NF Instance ID of the service provider. This contains the NF Instance ID of the source NF to collect data from the NF service provider. Requester PLMN Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Requester PLMN) PLMN ID MCC Enter the mobile country code (MCC) for the Requester PLMN (Public Land Mobile Network) ID If you enter values for the PLMN ID, you must enter values for the Target PLMN ID - MCC . MNC Enter the mobile network code (MNC) for the Requester PLMN (Public Land Mobile Network) ID. If you enter values for the PLMN ID, you must enter values for the Target PLMN ID - MNC . Customize Format Select this option to enter customized formats for the Requester PLMN ID. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above MCC and MNC fields will be disabled. Target PLMN Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Target PLMN) Target PLMN ID MCC Enter the mobile country code (MCC) for the Target PLMN (Public Land Mobile Network) ID. This value is required if you have opted to enter the value for Requester PLMN ID - MCC . MNC Enter the mobile network code (MNC) for the Target PLMN (Public Land Mobile Network) ID. This value is required if you have opted to enter the value for Requester PLMN ID - MNC . Customize Format Select this option to enter customized formats for the Target PLMN ID. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above MCC and MNC fields will be disabled. Target SNPN Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Target SNPN) MCC Enter the mobile country code (MCC) for the Target SNPN (Standalone Non-Public Network). MNC Enter the mobile network code (MNC) for the Target SNPN (Standalone Non-Public Network). NID Enter the network identifier (NID) for the Target SNPN (Standalone Non-Public Network). Customize Format Select this option to enter customized formats for the Target SNPN. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above MCC , MNC and NID fields will be disabled. Requester S-NSSAI List Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Requester S-NSSAI List) S-NSSAI List Enter the Requester S-NSSAI(s) (Single Network Slice Selection Assistance Information) of the service provider. This is included during an access token request for an NF type and not for a specific NF / NF service instance. Customize Format Select this option to enter customized formats for the Requester S-NSSAI list. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above S-NSSAI List field will be disabled. Target S-NSSAI List Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Target S-NSSAI List) Target S-NSSAI list Enter the Target S-NSSAI(s) (Single Network Slice Selection Assistance Information) of the service provider. This list may be included for NF type access token request but not for a specific NF/NF service instance. Customize Format Select this option to enter customized formats for the Target S-NSSAI list. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above Target S-NSSAI List field will be disabled. Requester PLMN List Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Requester PLMN List) PLMN List Enter the Requester PLMN (Public Land Mobile Network) of the service producer. Customize Format Select this option to enter customized formats for the Requester PLMN list. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above Requester PLMN List field will be disabled. Requester SNPN List Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Requester SNPN List) SNPN List Enter the Requester SNPN (Standalone Non-Public Network) of the service producer. Customize Format Select this option to enter customized formats for the Requester SNPN list. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above SNPN List field will be disabled. Target NSI List Open HTTP/2 Client Agent Configuration - NNRF Access Token authentication type (Target NSI List) Target NSI list Enter the target NSI(s) (Network Slice Instances). This list may be included for NF type access token request but not for a specific NF/NF service instance. Customize Format Select this option to enter customized formats for the Target NSI list. The supported format is JSON. For more information, refer to 3GPP TS 29.510 Technical Specification. Note! By selecting this option, the above Target NSI field will be disabled. Note! If an agent is configured on the Authentication Tab to use the NNRF Access Token authentication type, an additional step may be required if the token needs to be obtained via HTTPS protocol. The HTTP/2 Client internally uses the Java built-in HTTP Client to access the NNRF server. If a certificate is required to contact the authentication server, the proper certificate has to be put into the default certificates file for the respective JDK distribution used. Example for OpenJDK 64-Bit Server VM cd $JAVA_HOME/lib/security keytool -import -alias mycert -keystore cacerts -file nnrfHost.cert HTTP/2 Client Proxy If HTTP traffic is required to be routed through a proxy please look at HTTP Proxy Support in order to configure the proxy.

---

# Document 2223: SQL Loader Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740809
**Categories:** chunks_index.json

This section describes the SQL Loader agent. This is a processing agent for batch workflow configurations. The SQL Loader agent is a batch processing agent designed to populate the database with data from existing files, either residing in a local directory or on the server filesystem of the database. The following agents can be used for data collection: Disk FTP SFTP The supported databases are: MySQL Netezza PostgreSQL SAP HANA Sybase IQ Vertica Prerequisites The reader of this information has to be familiar with: Structured Query Language (SQL) UDR structure and contents The section contains the following subsections: SQL Loader Agent Configuration SQL Loader Agent Events SQL Loader Agent Input/Output Data and MIM SQL Loader Agent Transaction Behavior SQL Loader Agent UDRs SQL Statements

---

# Document 2224: Netia FTP Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031933/Netia+FTP+Agent+Events
**Categories:** chunks_index.json



---
**End of Part 94** - Continue to next part for more content.
