# RATANON/MZ93-DOCUMENTATION - Part 80/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 80 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~66.4 KB
---

You open the Categorized Grouping agent configuration dialog from a workflow configuration. To open the Categorized Grouping processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Cat group from the Processing tab of the Agent Selection dialog. Open Categorized Grouping agent - Grouping tab Setting Description Setting Description Browse... Select the Browse... button to open the Configuration Selection dialog. Browse for and select the preferred profile to be added to the agent. Force Single UDR If this is disabled the output files will automatically be divided in multiple UDRs per file. The output files will be divided in suitable block sizes.

---

# Document 1890: Radius Example Ultra Format Definition - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608936/Radius+Example+Ultra+Format+Definition
**Categories:** chunks_index.json

To simplify the example, only one type of request UDRs is accepted, and one of two types of reply UDRs is sent back. Hence, the requestMessage and responseMessage fields of the Radius UDR will be populated with any of these UDR types: UDR Type Description UDR Type Description requestMessage Access_Request UDR - A login request from a supposed user which must be authenticated against a subscriber database. responseMessage Access_Accept UDR - Sent back in case the authentication succeeded. Access_Reject UDR - Sent back in case the authentication failed. The full Ultra Format Definition for the example is not shown, since it is beyond the scope of this manual to handle packet content or UFDL syntax. Shortened Ultra Format Definition The format definition is here stored in the Default directory with the name extendedRadius . external Access_Request_Ext sequential : identified_by(Code == 1), dynamic_size(RecLength) { int Code: static_size(1); int Identifier: static_size(1); int RecLength: static_size(2), encode_value(udr_size); bytearray Authenticator: static_size(16); switched_set( Type ) { int Type: static_size(1); int Length: static_size(1), encode_value(case_size); case(1) { ascii User_Name: dynamic_size( Length - 2 ); }; case(2) { ascii User_Password: dynamic_size( Length - 2 ), terminated_by(0); }; // ... // Further field definitions. // ... }; }; external Access_Accept_Ext sequential : identified_by(Code == 2), dynamic_size(RecLength) { // ... // Further field definitions. // ... }; external Access_Reject_Ext sequential : identified_by(Code == 3), dynamic_size(RecLength) { // ... // Further field definitions. // ... }; internal Vendor_Specific_Int { int Type; int Length; int VendorID; int SubAttrID; int VendorLength; int InfoCode; string Data; }; in_map Access_Request_Map : external(Access_Request_Ext), target_internal(Access_Request_Int) { automatic { Vendor_Specific_Ext: internal( Vendor_Specific_Int ), target_internal( Vendor_Specific_TI1 ); }; }; in_map Access_Accept_InMap: external(Access_Accept_Ext), target_internal( Access_Accept_Int ) { automatic { Vendor_Specific_Ext: internal( Vendor_Specific_Int ); }; }; out_map Access_Accept_Map : external(Access_Accept_Ext), internal(Access_Accept_Int) { automatic; }; in_map Access_Reject_InMap: external(Access_Reject_Ext), target_internal( Access_Reject_Int ) { automatic { Vendor_Specific_Ext: internal( Vendor_Specific_Int ); }; }; out_map Access_Reject_Map : external(Access_Reject_Ext), internal(Access_Reject_Int) { automatic; }; decoder Request_Dec: in_map(Access_Request_Map); encoder Response_Enc: out_map(Access_Accept_Map), out_map(Access_Reject_Map);

---

# Document 1891: Post Configuration for Oracle RAC Connection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204996830/Post+Configuration+for+Oracle+RAC+Connection
**Categories:** chunks_index.json

The following steps are applicable when using Oracle RAC: Set the JDBC URL property mz.jdbc.url . mzsh topo set topo://container:<platform container name>/pico:platform/val:config.properties.mz.jdbc.url  "<jdbc connection string>" Example - JDBC connection string $ mzsh topo set topo://container:main1/pico:platform/val:config.properties.mz.jdbc.url  'jdbc:oracle:thin:@ (description=(address_list= (address=(host=10.0.0.111)(protocol=tcp)(port=1521)) (address=(host=10.0.0.112)(protocol=tcp)(port=1521)) (load_balance=no))(connect_data=(service_name= orcl)))' In order for Fast Connection Failover, FCF, to work, the ONS must be configured in the database as well as in Global variable macro (Development) . This is done by setting the property mz.jdbc.oracle.ons in the platform configuration properties. This property should contain the information found in the ONS configuration file ORACLE_HOME/opmn/conf/ons.config . The ONS string in the property must at least specify the node's ONS configuration attribute, that is, the host:port pairs preceded by the text "node=" and separated by a comma (,). The hosts and ports denote the remote ONS daemons available on the Oracle RAC nodes. mzsh topo set topo://container:<platform container name>/pico:platform/val:config.properties.mz.jdbc.oracle.ons  "<node list>" Example - Setting mz.jdbc.oracle.ons $ mzsh topo set topo://container:main1/pico:platform/val:config.properties.mz.jdbc.oracle.ons  'nodes=10.0.0.85:6300,10.0.0.86:6300' Restart the platform to make the changes take effect.

---

# Document 1892: Header UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654537/Header+UDR
**Categories:** chunks_index.json

The Header UDR is used to create a header. A header is a text that is bigger than normal text. The size of the header can be chosen from 1-5. To create an h2 element, you can use the following code: Header h2 = udrCreate(Header); h2.text = "H2 Header"; h2.headerSize = 2; The following fields are included in the Header UDR : Field Description Field Description attributes (map<string,string>) This field may contain extra attributes to be added. cssClasses (list<string>) This field may contain a list of extra values added to the class attribute. This is typically used to style the component. Please read more on Bootstrap . headerSize (int) This field may contain a size value between 1-5. Default is 1 id (string) This field may contain the id of the component. text (string) This field must contain the header text.

---

# Document 1893: Data Veracity Collection Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032312/Data+Veracity+Collection+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about Data Veracity Collection agent's transaction behavior. For information about the general transaction behavior, see the section Transactions in Workflow Monitor . Emits The agent emits commands that changes the state of the file currently processed. Command Description Begin Batch UDRs - Emitted prior to the routing of the first UDR in the batch created by the UDRs matching the collection definitions. Batches - Emitted prior to the routing of a batch. End Batch UDRs - Emitted when all UDRs have been collected or when a Hint End Batch request is received. The UDRs are then marked as Reprocessed in ECS. Batches - Emitted after each batch has been processed. The batch is then marked as Reprocessed. Retrieves The agent retrieves commands from other agents and, based on them, generates a state change of the file currently processed. Command Description Cancel Batch No Ca ncel Batches are retrieved. Note! If any agent in the workflow emits a cancelBatch , the workflow will abort immediately (regardless of the workflow configuration).

---

# Document 1894: Workflow Bridge Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610146/Workflow+Bridge+Collection+Agent+Configuration
**Categories:** chunks_index.json

You open the Workflow Bridge collection agent configuration dialog from a workflow configuration. To open the Workflow Bridge processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select a workflow type, select Realtime . Click Add agent and select Workflow bridge from the Collection tab of the Agent Selection dialog. Open Workflow Bridge collection agent configuration dialog for a real-time workflow Setting Description Setting Description Profile This is the profile to use for communication between the workflows. For information about how to configure a Workflow Bridge profile, see Workflow Bridge Profile . All workflows in the same workflow configuration can use separate profiles. For this to work, the profile must be set to Default in the Workflow Table tab found in the Workflow Properties dialog. For further information on the Workflow Table tab, refer to Workflow Table . To select a profile, click on the Browse... button, select the profile to use, and then click OK . Port This is the default port that the collecting server will listen to for incoming requests. A valid port value is between 1 and 65535. If you have a collecting workflow with several instances, you have to open the Workflow Properties, and set the WFB Collector - Port field to Default. Then you can enter the different ports you want to use for the different instances in the workflow instance table, as each instance need to listen to a separate port. Note! If both the collection and forwarding workflows are executing on the same execution context, an ephemeral port will be used regardless of the value set in this field. Stream ID Workflow Bridge collection and forwarding agents use Stream IDs to manage connections between multiple workflow instances. Instead of creating separate profiles for each pair of workflows, you can use a single profile and assign a Stream ID to ensure that the correct workflows are linked. This is particularly useful when scaling your system, as it allows new workflow instances to automatically find their corresponding counterparts. Example  Configuring a Stream ID in Workflow Bridge Collection and Forwarding Agents Scenario: You need to scale your system by adding multiple instances of connected workflows. Instead of manually configuring separate Workflow Bridge profiles for each instance, you can use Stream IDs to automatically pair workflows. For example, if three processing workflows (A, B, and C) need to send data to three corresponding collection workflows (X, Y, and Z), you can define stream IDs like "A-X", "B-Y", and "C-Z". This ensures each processing workflow sends data to the correct collection workflow while maintaining a simpler, more scalable configuration.

---

# Document 1895: Aggregation Agent Configuration - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031636/Aggregation+Agent+Configuration+-+Batch
**Categories:** chunks_index.json

The batch Aggregation agent's configuration dialog contains the following tabs: Aggregation - This tab contains the three subsidiary tabs, General , APL Code and Storage. Thread Buffer - For further information about the Thread Buffer tab, see Workflow Template . General Tab The General tab enables you to assign an Aggregation profile to the agent and to define error handling. With the Error Handling settings, you can decide what you want to do if no timeout has been set in the code or if there are unmatched UDRs. Open The Aggregation agent configuration dialog - General tab Setting Description Setting Description Profile In batch workflows, the profile must use file storage, Elasticsearch or SQL. All the workflow instances in the same workflow template can use different Aggregation profiles. For this to work, the profile has to be set to Default in the Field settings tab in the Workflow Properties dialog. After that, each workflow in the Workflow Table can be assigned with the correct profile. Force Read Only Select this check box to only use the Aggregation Storage for reading aggregation session data. Selecting this check box also means that the agent cannot create new sessions when an incoming UDR cannot be matched to an existing session. A UDR for which no matching session is found is handled according to the setting If No UDR Match is Found. If you enable the read only mode, timeout and defragmentation handling is also disabled. Note! When you are using file storage and sharing an Aggregation profile across several workflow configurations, the read and write lock mechanisms that are applied to the stored sessions must be considered: There can only be one write lock at a time in a profile. This means that all but one Aggregation agent must have the Force Read Only setting enabled. If all of the Aggregation agents are configured with Force Read Only , any number of read locks can be granted in the profile. If one write lock or more is set, a read lock cannot be granted. If Timeout is Missing Select the action to take if timeout for sessions is not set in the APL code using sessionTimeout . The setting is evaluated after each consume or timeout function block has been called (assuming the session has not been removed). The available options are: Ignore - Do nothing. This may leave sessions forever in the system if the closing UDR does not arrive. Abort - Abort the agent execution. This option is used if a timeout must be set at all times. Hence, a missing timeout is considered being a configuration error. Use Default Timeout - Allow the session timeout to be set here instead of within the code. If enabled, a field becomes available. In this field, enter the timeout, in seconds. If No UDR Match is Found Select the action that the agent should take when a UDR that arrives does not match any session, and Create Session on Failure is disabled: Ignore - Discard the UDR. Abort - Abort the agent execution. Select this option if all UDRs are associated with a session. This error case indicates a configuration error. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is activated only if Route is selected. APL Code Tab The APL Code tab enables you to manage the detailed behavior of the Aggregation agent. You use the Analysis Programming Language (APL) with some limitations but also with additional functionality. For further information see the APL Reference Guide . The main function block of the code is consume . This block is invoked whenever a UDR has been associated with a session. The timeout block enables you to handle sessions that have not been successfully closed, e g if the final UDR has not arrived. Open Aggregation agent configuration dialog - APL Code tab Item Description Code Area This is where you write your APL code. For further information about the code area and its right-click menu, see Text Editor in Administration and Management in Legacy Desktop . Compilation Test... Use this button to compile the entered code and check for validity. The status of the compilation is displayed in a dialog. Upon failure, the erroneous line is highlighted and a message, including the line number, is displayed. Outline Use this button to display or hide the APL Code Editor Outline navigation panel. The navigation panel provides a view of all the blocks, variables and methods in an APL code configuration and makes it possible to easily navigate between different types in the APL code. For further information on the Outline navigation panel, see Administration and Management in Legacy Desktop . Storage Tab The Storage tab contains settings that are specific for the selected storage in the Aggregation profile. Different settings are available in batch and real-time workflows. File Storage Open The Aggregation agent configuration dialog - Storage tab for File Storage Setting Description Setting Description Defragment Session Storage Files For batch workflows, the Aggregation session storage can optionally be defragmented to minimize disk usage. When checked, configure the defragmentation parameters: Defragment After Every [] Batch(es) Run defragmentation after the specified number of batches. Enter the number of batches to process before each defragmentation. Defragment if Batch(es) Finishes Within [] Second(s) Set a value to limit how long the defragmentation is allowed to run. This time limitation depends on the execution time of the last batch processed. If the last batch is finished within the specified number of seconds, the remaining time will be used for the defragmentation. The limit accuracy is +/- 5 seconds. Defragment Session Files Older Than [] Minute(s) Run defragmentation on session storage files that are older than this value to minimize moving recently created sessions unnecessarily often. Elasticsearch Storage and SQL Storage Open The Aggregation agent configuration agent - Storage tab for Elasticsearch Open The Aggregation agent configuration agent - Storage tab for SQL Setting Description Setting Description If Error Occurs in Storage Select the action that the agent should take when an error occurs in the storage: Ignore - Discard the UDR. Log Event - Discard the UDR and generate a message in the System Log. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is only activated if Route is selected. Disable Timeout Select this check box to disable the timeout handling.

---

# Document 1896: Web Service Profile Security Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609948/Web+Service+Profile+Security+Tab
**Categories:** chunks_index.json

The web services that are part of the profile can be secured using different combinations of security configurations. Open Security Tab The following options are available: Transport Level Security with the option of enabling a Timestamp Transport Level Security with Web Service Security standard with the option of enabling a Timestamp Transport Level Security with Username Token and/or Addressing with the option of enabling a Timestamp Transport Level Security with Web Service Security standard combined with Username Token and/or Addressing with the option of enabling a Timestamp Web Service Security standard with the option of enabling a Timestamp Web Service Security standard with Username Token and/or Addressing with the option of enabling a Timestamp Username Token and/or Addressing with the option of enabling a Timestamp To apply Transport Level Security (TLS v1.2), select the Enable Transport Security checkbox. The Web Service agents provide Web Service security by supporting XML-signature and encryption. A Timestamp records the time of messages. Username Token uses authentication tokens and Addressing provides unique message IDs. Setting Description Setting Description Enable Transport Security Select this checkbox to communicate with the web service using the transfer protocol HTTPS. If you want to use the the transfer protocol HTTP, leave the checkbox empty. Security Profile Click Browse to select a security profile with certificate and configuration to use, if you prefer to use a secure connection. See Security Profile for more information. Note! For Web Service Profiles exported from MZ8 that include embedded Java keystore, the keystore will be exported to a file in the $MZ_HOME/etc directory during import. Additionally, a Security Profile will be automatically created to reference the keystore. Web Service Security Settings Applicable whether you select Enable Transport Security or not. Enable Web Service Security For This Profile When selected, Web Service security is used. The Web Service Security Settings and Username Token and Addressing checkboxes are also enabled for you to configure security settings. If you do not select any other checkboxes on this tab, no Web Service Security is enabled. Enable Encryption When selected, encryption will be enabled. Enable Binary Security Token When selected, messages are signed and the public certificate is sent in the Binary Security Token element in the message header. Use request signing certificate When selected, the public certificate sent in the Binary Security Token element is used to encrypt the message that is sent back to the client. This option is ignored in case you are using a Web Service client agent. Enable Signing When selected, messages will be signed. Security Profile Click Browse to select a Web services security profile. See Security Profile for more information. Enable TimeStamp When selected, messages are recorded with the date and time. Username Token and Addressing Enable Username Token When selected, Username Token authentication is used, and the other text boxes in the dialog are highlighted and must be completed. Note that when selected, this option is applicable to both the Web Service Provider agent and the Web Service Request agent. WS Token Username Enter the WS Token username. WS Token Password Enter the WS Token password. Enable WS Addressing When selected, messages are sent with a unique ID. Generate Keystore for Web Service Security There are multiple ways to set up a server and client keystores. In general, both the client and the server need the public certificate to sign the messages. If the server hosts multiple clients it is not needed to import all clients' certificates into the server keystore but then a Certificate Authority (CA) is needed. So in a multiple client scenario, the server imports the CA certificate and gets its own certificate signed by the CA. All clients get their certificates signed by CA and import the server public certificate into the keystore. Normally this type of certificate is signed by a trusted CA. To generate server and client keystores, you need to follow the steps in the mentioned sequence: Setup a CA as mentioned in Setting Up a Certificate Authority Generate the server keystore and certificate as mentioned in Creating Server Keystore and Certificate Generate the client keystore and certificate as mentioned in Creating Client Keystore and Certificate . You need to select the Binary Security Token checkbox for the Web Service profile client and server. For the server, you also need to select the checkbox Use request signing certificate .

---

# Document 1897: Encryption Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999915/Encryption+Agent+Configuration
**Categories:** chunks_index.json

To open the Encryption Processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch or Realtime . Click Add agent and select Encryption in the Processing tab of the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. The Encryption agent configuration The agent can be set to either decrypt or encrypt data and if you select to unmask data you can select if you want to handle empty data or not. Setting Description Setting Description Profile Select the Encryption profile you want the agent to use. See Encryption Profile for further information. Mode Select whether you want the agent to encrypt or decrypt files by choosing Encryption or Decryption . Handle Empty Data Select this check box if you want the agent to handle files containing no data.

---

# Document 1898: APL Reference Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646109
**Categories:** chunks_index.json

Loading This document describes the Analysis Programming Language, APL, and contains descriptions of all the different APL commands that can be used. For information about Terms and Acryonyms used in this document, see the Terminology document. Loading

---

# Document 1899: Development Toolkit User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645031
**Categories:** chunks_index.json

Search this document: Development Toolkit (DTK) enables you to develop user-defined programs, and plugins, in Java that you then incorporate into the Platform. Note! While this document does not include general information about Java programming, it contains rules and recommendations that you should apply in order to operate your new plugin with the Platform. This document contains an overview of the classes and interfaces used for the different plugins. For a complete list of all the classes, and the full syntax details, see the JavaDoc which can be found in the unzipped devkit_<version>.zip file, under <dekit_dir>/javadoc when DTK has been installed according to Installation and Setup . In the DTK package, examples of full-featured plugins are also included. The reader is encouraged to examine these examples that can be found in unzipped devkit_<version>.zip file, under <dekit_dir>/dtk/examples. Chapters The following chapters and sections are included: Development Toolkit Overview Installation and Setup Creating a DTK Plugin Agent Plugins Data Serialization Exceptions UDR Plugins Event Plugins Notifier Plugins APL Plugins Ultra Field Plugins Workflow Service Plugins Desktop Plugins Environment Interfaces Configuration Contract Agent Plugin Examples

---

# Document 1900: Python Processing Agent Input/Output Data and MIM - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642270/Python+Processing+Agent+Input+Output+Data+and+MIM+-+Real-Time
**Categories:** chunks_index.json

Input/Output Data

---

# Document 1901: Executive Summary - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647440/Executive+Summary
**Categories:** chunks_index.json

Open This page summarizes the content of the Release Notes for the MediationZone 9.3 release. New Functionality In this section, you can see information about the new features and functionality in this release. New Features New Connectors New Kafka Agents Amazon S3 Agents in Real-Time Workflows Amazon SQS Agents APN Processing Agent SAP CC REST Agent MariaDB Supported Agents for MariaDB Additional APL Functions and Event Notifications Support MariaDB Usability New Command Line Interface Desktop Search Log Filter Support for wfexport and wfimport mzsh Commands MZSH Command For Shutting Down Desktop and Legacy Desktop Integration and Security User Security Improvements SAP CTS+ Integration Google Secret Manager Reference Data Management Supports SAP HANA System Statistics Enhancements followRedirects Field for RequestCycle UDR in HTTP/2 Agents HTTP/2 Client Agent Support Additional OAuth Settings Operations REST Interface for Host and Pico SAP RFC Processor Agent Supports Execution Time Threshold Configuration Configurable Maximum Response Size for HTTP/2 Client Data Type Conversion Property for SAP JCo ABAP Type P Database Sizing Rollback See New Features and Enhancements for full details. System Requirements For information about system requirements, see System Requirements . Important Information The following provides important information related to MediationZone 9.3: Support for Insecure Ciphers Removed Replaced and Sunset Functionality Some Functionality only Available in Legacy Desktop Client Auto Edit Mode in UDR File Editor REST is not Supported for Conditional Trace New Delivery Procedure Context-Sensitive Help and Offline Documentation in Browser Not Available Performance Degradation When Using Unsupported Oracle versions and Oracle JDBC drivers Importing Old IPDR SP Agent Workflow Configurations Returned Validation Errors Unavailable mzsh Commands Proxy Support See Important Information for full details. Known Issues Overview In MediationZone 9.3, there are the following known issues: Installation and Upgrade Issues Unable to change platform database after installation Installation Is Not Possible if MZ_HOME Path Contains a Folder Name with Spaces Installation is Reported as Successful Even if the Platform Fails to Start Warning Messages Displayed During Installation Error Occurs When Performing Multiple Installations on a Host with Different UNIX User IDs Latest Oracle JDBC Driver Versions not Supported User Interface issues New Desktop Issues Database Agent Assignment Tab Value Are Not Cleared Database Agent MIM Browser Need To Click Twice To Close It External Reference Profile Datalist Can Be Altered in Read-Only Mode Login Web UI Will Go To Last Page That Last User Accessed Type Assigned Indication not Shown in MIM Browser Idle Timeout Only Warns the User Once Cannot Login to Multiple Platforms Data Veracity Repair Jobs Show in One Line Accessibility Issues Code Editor Issues Known Differences Between Desktops Legacy Desktop Issues Data Veracity issues Data Veracity Search Filtering For Some Fields Are Case-Sensitive Data Veracity Filter's Full Query Is Not Available on Desktop Adding an Empty Query Group With A Parent Condition In Data Veracity Search & Repair Filter Will Result In An Error Importing Old Data Veracity Collection Agent Workflow Configurations Returned Validation Errors Data Veracity Filters Are Not Refreshed Automatically Data Veracity New Restricted Fields That Were Used For An Existing Repair Rule Is Not Disabled Unable To Save A Query With New Group In Data Veracity Search Data Veracity Collection Agent Validation Duplicate UDR issues Duplicate UDR Inspector Stuck Loading When Attempting To Delete Records System Log Shows Unknown User On Deleting A Record In Duplicate UDR Inspector Duplicate UDR Inspector Allows To Delete Records Without Profile Locking Duplicate UDR Inspector Didnt Return Records Properly Duplicate UDR MIMs Shows 0 Incorrect Handling When Duplicate UDR Process Old Data With System Arrival Time Duplicate UDR Inspector Table Header Row Count Is Not Updated When Records Found Is 0 Rename Duplicate UDR Profile Doesnt Auto Reflect In Duplicate UDR Agent Exception Thrown When Truncating Previous Tables After Changing DB Profile In Duplicate UDR Profile File Directory Is Not Empty When Delete Duplicate UDR File Storage Profile Reference Data Management Reference Data Management Table Shows No Data And All Functions Are Locked Import/Export Ongoing Process Are Not Aborted When Navigating Away From Reference Data Management Other issues Logging Issue Error Thrown When SAPCC Agent Not Added With Any Host Inappropriate Validation Handling On Workflow Group Scheduling Database Forwarding Agent Validation Message External References Can Be Removed when a Workflow is Dependent on It Missing <Agent> - 5G <Field> Parameters in Workflow Table and Workflow Instance Table SAP CC Notification Agent TLS Feature Not Working Incorrect HTTP Status Code in Error Response in Operations REST API Workflow Workflow Group And Suspend Execution Is Unable To Remove Workflow Keystore Information Is Gone After Imported Notification Workflow From MZ8.3 See Known Issues for full details. Installation, Upgrade and Downgrade For instructions on how to install, upgrade and downgrade, refer to Installation Instructions .

---

# Document 1902: SMPP Agents Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654034/SMPP+Agents+Events
**Categories:** chunks_index.json

Agent Message Events

---

# Document 1903: APL - PCC Rules Provisioning Plugin - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611673/APL+-+PCC+Rules+Provisioning+Plugin
**Categories:** chunks_index.json

The provisioning functions include: 1 pccGetData 2 pccListData 3 pccCreateData 4 pccUpdateData 5 pccDeleteData 6 pccLastErrorCode 7 pccLastErrorMessage 8 pccSetProvisioningArea 9 pccGetProvisioningArea 10 pccCopyAreaData 11 pccClearArea These functions can be used to interact with the PCC provisioning interface. pccGetData Retrieves a specific UDR from the provisioning interface based on the stated UDR type and key. drudr pccGetData ( string typename, string key) Parameters Parameter Description Parameter Description typename The fully qualified typename of the requested UDR. key The primary key value of the requested UDR, which is unique Returns: The matching UDR or null if no matching UDR was found, or if an error occurred during lookup. Example drudr rule = pccGetData("PCRF.Rules.Provisioning.PCC_Rule",555); will retrieve a UDR of the type PCRF.Rules.Provisioning.PCC_Rule with the key 555 . pccListData Retrieves a list with all the UDRs from the provisioning interface with the stated UDR type. list<drudr> pccListData ( string typename ) Parameters Parameter Description Parameter Description typename The fully qualified typename of the requested UDRs. Returns: A list containing all the UDRs of the requested type. Example list<drudr> pccRulesList = pccListData ("PCRF.Rules.Provisioning.PCC_Rule"); will return a list named pccRulesList with all the UDRs that have the type PCRF.Rules.Provisioning.PCC_Rule . pccCreateData This function inserts a new UDR through the provisioning interface. drudr pccCreateData ( drudr udr ) Parameters Parameter Description Parameter Description udr The new UDR that is going to be inserted. Returns: The created UDR or null if there was an error. Example drudr created = pccCreateData(input); will insert a new UDR named created through the provisioning interface. pccUpdateData This function updates a UDR through the provisioning interface. drudr pccUpdateData ( drudr udr ) Parameters Parameter Description Parameter Description udr The UDR that is going to be updated. Returns: The updated UDR or null if there was an error. Example drudr pccUpdateData(created); will update the UDR named created through the provisioning interface. pccDeleteData This function deletes a UDR in the provisioning interface. drudr pccDeleteData ( drudr udr ) Parameters Parameter Description Parameter Description udr The UDR that is going to be deleted. Returns: The deleted UDR or null if there was an error. Example drudr pccDeleteData(created); will delete the UDR named created through the provisioning interface. pccLastErrorCode If any of the create, delete or update operations should fail, the error code can be retrieved with this method. int pccLastErrorCode() Error code Explanation Error code Explanation 200 Will be returned if the last operation was successful. 400 Will be returned if you are trying to create or update an object that is missing required references. The object will not be created or updated. 401 Will be returned if the delete operation failed because the object contains references from other objects. 402 This error code is only applicable for the pccCreateData function, and will be returned if the object already exists. 403 Will be returned if any of the functions pccGetData, pccUpdateData, or pccDeleteData are referring to a missing object. 500 Will be returned if there was an error trying to read/write to storage. Example If the last error code was 500. int pccLastErrorCode(); will return 500. pccLastErrorMessage If any of the create, delete or update operations should fail, a more detailed description of the error can be retrieved with this method. string pccLastErrorMessage() Parameter Description Parameter Description Returns: A description of the last error. Example If the last error code was 500. string pccLastErrorMessage(); will return a description of error code 500. pccSetProvisioningArea Sets a provisioning working area for the workflow. There are two areas available, TEST and PROD. void pccSetProvisioningArea ( string area ) Parameters Parameter Description Parameter Description area The name of the area to work against. All delete, update, get and create operations will use this area. Must be either TEST or PROD. Example pccSetProvisioningArea("TEST"); will set area TEST as working area for the workflow. pccGetProvisioningArea Retrieves information about the currently active area for the workflow. string pccGetProvisioningArea() Parameters Parameter Description Parameter Description Returns: The name of the currently active provisioning area. Example If the currently active area for the workflow is TEST, string pccGetProvisioningArea(); will return "TEST". pccCopyAreaData Copies the specified configurations in one area to another. Warning! This function will also clear the target area from all previous configurations before the new configurations are inserted. void pccCopyAreaData ( string source, string destination, string product ) Parameters Parameter Description Parameter Description source The name of the source area. destination The name of the destination area. product The name of the product to copy. Example pccCopyAreaData("TEST","PROD","PCRF.Rules"); will copy all the provisioned PCRF Rules data from the TEST area to the PROD area. Note! When working with PCC Rules, the product parameter should be set to PCRF.Rules. pccClearArea Deletes the specified product in the stated area. void pccClearArea ( string area, string product ) Parameters Parameter Description Parameter Description area The name of the area to clear. product The name of the product you want to delete. Example pccClearArea("TEST","PCRF.Rules"); will clear the area TEST from old PCRF Rules configurations. Note! When working with PCC Rules, the product parameter should be set to PCRF.Rules.

---

# Document 1904: Inter Workflow Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605963
**Categories:** chunks_index.json

The Inter Workflow profile enables you to configure the storage server that the Inter Workflow forwarding and collection agents use for communication. It is safe to accumulate a lot of data in the storage server directory. When the initial set of directories has been populated with a predefined number of files, new directories are automatically created to avoid problems with file system performance. You can configure the Inter Workflow profile for the following storage types: Database Storage File Storage The Inter Workflow profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Note! Files collected by the Inter Workflow agent depend on and are connected with the Inter Workflow profile in use. If an Inter Workflow profile is imported to the system, files left in the storage connected to the old profile will be unreachable. Configuration To create a new Inter Workflow profile configuration, click the Build  New Configuration in the upper part of the Desktop window, and then select Inter Workflow Profile from the menu. Open InterWorkflow Profile with Database Storage configuration Open InterWorkflow Profile with File Storage configuration The contents of the menus in the menu bar may change depending on which configuration type is opened. The Inter Workflow profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The External References button in the menu bar is specific to Inter Workflow profile configurations. Button Description Open Select this menu item to enable External References in an agent profile field. For further information, see the section Enabling External References in an Agent Profile Field in External Reference Profile The Inter Workflow profile configuration contains the following settings: Setting Description Available when.. Storage Type From the drop-down list, select either File Storage or Database Storage. Using Database Storage means that a database will be used as storage instead of the EC file system. When choosing this option, several new fields will be displayed. Always available Database Profile Click Browse to select the Database Profile you want to use for the Inter Workflow Profile. Note! Currently, the SQL storage only supports PostgreSQL, SAP HANA, and Oracle databases. Database storage is configured Connection Pool Size Specify the number of simultaneous connections for communication to the database. This value determines the size of the connection pool used by Interworkflow agents when accessing the database. Adjust the size based on the expected load and concurrency needs of your solution. Database storage is configured Show SQL Statement When you select Database Profile , this button will print the SQL statements needed to create the tables and indexes required for the Inter Workflow Profile. You can copy and paste this into your database management software. Note! You will have to copy the SQL script generated in the text box to create the PostgreSQL, SAP HANA, or Oracle tables on their own in the database listed in the Database profile. The Inter Workflow profile will not automatically create the tables for you. Database storage is configured Copy to Clipboard This button will copy the text in the text area below, which was generated by the Show SQL Statement button, to the clipboard on your computer. Database Storage is configured Root Directory The directory's absolute pathname on the storage handler where the temporary files will be placed. If this field is greyed out with a stated directory, it means that the directory path has been hard-coded using the mz.present.interwf.storage.path property. This property is set to false by default. Example - Using the mz.preset.interwf.storage.path property To enable the property and state the directory to be used: mzsh topo set val:common.mz.preset.interwf.storage.path '/mydirectory/iwf' To disable the property: mzsh topo unset val:common.mz.preset.interwf.storage.path For further information about all available system properties, see System Properties . File Storage is configured Storage Host From the drop-down list, select either Automatic or an activated EC group. Using Automatic means that the storage will use the EC Group where the first workflow accessing this profile is started. Following workflows using the same profile will use the same EC Group for storage until the first workflow accessing the profile is stopped. The EC Group where the next workflow accessing this profile is started will then be used for storage. The location of the storage will therefore vary depending on the start order of the workflows. Example - Automatic storage host Below is an example of a scenario where Automatic is used as a storage host with the following setup: Workflow 1 is running on EC Group 1 with the Inter Workflow Forwarding agent Workflow 2 is running on EC Group 2 with the Inter Workflow Collection agent Workflow 2 is started. EC Group 2 is used for storage Workflow 1 is started. EC Group 2 is still used for storage. Workflow 1 is stopped. EC Group 2 is still used for storage. Workflow 2 is stopped. No EC Group is used for storage. Workflow 1 is started. EC Group 1 is used for storage. Note! The workflow must be running on the same EC as its storage resides. If the storage is configured to be Automatic, its corresponding directory must be a shared file system between all the EC Groups. Always available Max Bytes An optional parameter stating the limit of the space consumed by the files stored in the Root Directory or Database. If the limit is reached, any Inter Workflow forwarding agent using this profile will abort. Always available Max Batches An optional parameter stating the maximum number of batches stored in the Root Directory or Database. If the limit is reached, any Inter Workflow forwarding agent using this profile will abort. Always available Compress intermediate data Select this checkbox if you want to compress the data sent between the Inter Workflow agents. The data will be compressed into *.gzip format with compression level 5. Always available Named MIMs A list of user-defined MIM names with no values assigned. They are populated with existing MIM values from the Inter Workflow forwarding agent. This way, MIMs from the forwarding workflow can be passed on to the collecting workflow. Always available

---

# Document 1905: Archiving Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738180/Archiving+Agent
**Categories:** chunks_index.json

You open the Archiving agent configuration dialog from a workflow configuration. To open the Archiving forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Archiving from the forwarding tab in the Agent Selection list. Open Archiving agent configuration dialog - Archiving tab The following options are available in the Archiving agent configuration: Setting Description Setting Description Profile Name of the Archive profile to be used when determining the attributes of the target files. Input Type The agent can act on two input types. The behavior varies depending on the input type that you configure the agent with. The default input type is bytearray. For information about the agent behavior with the MultForwardingUDR input type, see Archiving Agents MultiForwardingUDR Input . Compression Compression type of the target files. Determines if the agent will compress the files before storage or not. No Compression - the agent will not compress the files. Gzip - the agent will compress the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. The configuration of the filenames is managed in the Filename Template tab, only. Agent Directory Name Possibility to select one or more MIM resources to be used when naming a sub-directory in which the archived files will be stored. If more than one MIM resource is selected, the values making up the directory name will automatically be separated with a dot. Note! If at least one Agent Directory Name is selected in Directory Templates in the Archive profile , this directory field is used. Logged MIM Data MIM values to be logged as meta data along with the file. This is used for identification of the files. The meta data is viewed in the Archive Inspector. Note! The names of the created files are determined by the settings in the Filename Template tab. For further information about the Filename Template service, see Workflow Template .

---

# Document 1906: MSMQ Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673923/MSMQ+Agents
**Categories:** chunks_index.json

This section describes the MSMQ agents. This agent is only available for Real-Time workflow configurations. There are two types of MSMQ agents: MSMQ Collection Agent MSMQ Processing Agent The MSMQ (Microsoft Message Queuing) Agent enables MediationZone to act as an MSMQ client communicating with an MSMQ server. MSMQ is a messaging protocol that allows applications running on separate servers/processes to communicate in a failsafe manner. A queue is a temporary storage location from which messages can be sent and received reliably, as and when conditions permit. Supported Features The MSMQ agent supports the following: Setting up connections with domain, username and password Configure the queue name (path name) to send/receive MSMQ message Receiving messages from an MSMQ queue Send messages to an MSMQ queue Prerequisites The reader of this information should be familiar with: MSMQ Microsoft DCOM (For more information, refer to https://docs.microsoft.com/en-us/previous-versions/windows/desktop/legacy/ms703266(v%3Dvs.85) ) The section includes the following subsections: MSMQ Collection Agent MSMQ Agent Preparations MSMQ Processing Agent MSMQ UDR

---

# Document 1907: Closing Batches from APL and Error Handling - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742014/Closing+Batches+from+APL+and+Error+Handling
**Categories:** chunks_index.json

Closing Batches from APL In addition to using the batch closing criteria in the agent's batch closing criteria, you can trigger closing of the current batch by routing a CloseBatchUDR to the agent. This UDR type contains the following field: Field Description Field Description BatchId (string) When the input type is MultiForwardingUDR , this field must contain the fntSpecification UDR. When the input type is bytearray, this field is ignored by the Disk_Deprecated forwarding agent. Error Handling When a worker process fails to synchronize or move data, an ErrorUDR is routed to the workflow. This UDR type contains the following fields: Field Description Field Description BatchId (string) When the input type is bytearray, this field contains the fixed string "SINGLE_BATCH". When the input type is MultiForwardingUDR , this field contains an fntSpecification string. Code (int) This field contains a code that identifies the type of error that occurred. 200 - Incoming data was throttled due to exceeded worker queue size or concurrent batches limit. 300 - Failed to create a stream for synchronized data. 301 - Failed to write data to a temporary file. 302 - Cannot guarantee that data was stored. 400 - Missing fntSpecification in MultiForwardingUDR . Data (list<bytearray>) This field contains the input data (payload) to the agent. The maximum size of the list depends on the settings in the Advanced tab. When data is redirected back into the workflow in an ErrorUDR due to an unexpected error, it is not guaranteed that the data has been written. In the case of throttled data, it is guaranteed that the data has not been written. Note that the redirected data is included by any counters for closing criteria. Therefore, when data has been throttled, for example, the final batch file contains less data than configured using the automatic closing criteria.

---

# Document 1908: Radius Related UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653773/Radius+Related+UDR+Types
**Categories:** chunks_index.json

The UDR types created by default in the Radius agents can be viewed in the UDR Internal Format Browser in the radius folder. To open the browser open an APL Editor , in the editing area right-click and select UDR Assistance... ; the browser opens and you will see the following: The Error Format The Radius Format

---

# Document 1909: Enabling Client Authentication - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647240
**Categories:** chunks_index.json

TLS can be set up to demand authentication from all clients that run outside the local host. The Platform, ECs, SCs will then ask for valid certificates from each connecting pico instance. After you have set up TLS, as described in TLS Standard Setup , follow the steps below to enable client authentication. Set the properties that specifies the keystore path and the passwords in each Execution Container. Use the same values as for the Platform Container. There are two methods that you can use to make the client/server certificates available on all containers. Copy the keystore file that was created in TLS Standard Setup from the Platform Container to each of the Execution Containers. The target path is specified by the property pico.rcp.tls.keystore . Create a keystore and key pair on each Execution Container, then export and import the certficates. The certificate from the Platform Container must be exported to all Execution Containers. The certificates from the Execution Containers must be exported to the Platform Container. Run the following command to export a certificate: $ keytool -keystore <keystore file> -export -rfc -alias <alias_name> -file <certificate filename> Run the following command to import a certificate: $ keytool -import -alias <alias_name> -file <certificate_file_name> -keystore <keystore file> -keypass <password> -storepass <password> Enable client authentication by setting the property pico.rcp.tls.require_clientauth to true . Restart the system Desktops When client authentication is enabled, each desktop installation must authenticate itself to the Platform using a private key. You have to import this key in the Desktop Launcher in order to connect to the Platform. The certificate must also be imported as a trusted certificate in the Platform keystore. Run the following command to create a keystore that contains a private key. $ keytool -genkey -keystore <keystore file> -alias <alias> -keyalg RSA -keysize 2048 Copy the keystore file to the host that will run the Desktop Launcher. Create a certificate file that is associated with the key that you created in the previous step. $ keytool -keystore <keystore file> -exportcert -alias <alias> -file <certificate filename> Import the certificate to the platform $ keytool -keystore <platform keystore> -import -file <certificate filename> -alias <alias> Open the Desktop Launcher. Right-click on a MediationZone instance and then select Instance Settings from the popup menu. Select the Security tab. Right-click on the text field under Client Key and select Import Key From File . Select the key file that you copied in step 2.

---

# Document 1910: Data Hub Query - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610846/Data+Hub+Query
**Categories:** chunks_index.json

You can query the data stored in Data Hub via the Selection Screen in Manage menu. This requires that your user account belongs to an access group with execute permissions on Data Hub. For further information about access groups, see Access Controller . Open Data Hub in Data Management Data Hub Panel To query the Data Hub you must first select a Data Hub profile and one of its associated database tables in the Data Hub panel. Click the Filter button to open the Data Hub Profile panel and click Browse to select the relevant Data Hub profile. Open Data Hub selection panel Query Panel When you have selected a database table, you can configure the following: Display fields Search Query Open Data Hub Query Panel Display Fields This shows the database table columns that you would like to display in the Query Results . Open Display Fields selection dialog box Search Query This panel represents an expression based on the columns in the selected table. When you run a query, rows in the table that matches the expression will be displayed in the Query Results . Open Data Hub search query Item Description Item Description AND Open Click this button to apply AND logic on a set of rules or groups. OR Open Click this button to apply OR logic on a set of rules or groups. Field name This drop-down list contains columns in the selected database table that you can use in your query. The query is carried out based on this selection to populate the result for the Display Field. Open Field names Operator This is a comparison operator that is applied to the values in the specified column and the value. The available operators are listed according to the data type of the selected column. Open Operators drop-down Info! Value This field contains a value that is used for comparison with the selected column. When between and not between is the selected operator, two value fields are displayed. When you click this field and the column is of a date type, or a column with a type hint in the selected profile, a date picker will be displayed. Add Open Click this button to add a new rule to the expression. Add Group Open Click this button to add a group of rules within the expression. The groups can be nested. Delete Open Click this button to delete a rule or a group. When you delete a group, the rules contained within it will be deleted as well. Reset Open Click this button to clear all rules and groups from the Search Query panel. OK Open Click this button to run a new query based on the configured rules. Running a Query To run a query: If your query should contain both AND and OR logic, delete the first rule by clicking on the Delete button, then and add a new group by clicking the Add Group button. Groups are not required, if the query contains only AND or OR. If you have created a new group, click the Add rule button to add a new rule. Select a column that you want to use in a rule. Select an operator. Enter a value for comparison with the selected column. Use the Add group and Add buttons to add more group conditions to the expression. Click the OK button. The result will be displayed in the data grid in the Query Results panel. It may take a few seconds or several minutes for the query to complete. Query Results Panel The Query Results panel will display all available rows including columns. The number of rows is indicated on the top right, next to the Export button. Open Data Hub query results Item Description Item Description Displayed Columns This is a list of columns in the selected table that are displayed in the Query Results panel. Export Open Click this button to export the query result content to a CSV file. For further information, see the section below, Exporting Query Result. Exporting Query Result To export a query result follow the steps below: Click the Export button in the Query Results Panel . In the Export search result dialog, enter the following: Filename - The export filename should contain alphanumeric- or dash (-) characters. Do not add a file extension or a path. Delimeter - You can enter a character or string that should represent the separator token, e.g. a comma or a semi-colon. Open Export search result dialog Select the check box Include Header to add the column names to the first row in the file. Click the OK button. The file will be saved as <filename>.csv in your default download folder. All the columns of the table will be included in the file.

---

# Document 1911: Search - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/247693513/Search
**Categories:** chunks_index.json

You can locate specific configurations, tools, Data Management applications, Ultra, or APL using the Search bar at the Top Menu Bar of the Desktop. You can also apply particular search terms to your query to find content relevant to the terms. The search terms available for you to use in your query are: Search Terms Description Search Terms Description apl: Using apl: allows you to query every APL code in the system. Example Query for finding an event in APL apl: event Open Searching for event in the APL codes. new: Using new: allows you to create new configurations from the search bar. Example Query for Creating a new profile new: profile Open Creating a new profile using the search bar. ultra: Using ultra: allows you to query every Ultra Format in the system. Example Query for finding event in Ultra ultra: event Open Searching for event in Ultra workflow: Using workflow: will query the Analysis Agent, Aggregation Agent or Python Code within every workflow in the system. Example Query for finding session in codes configured in workflows workflow: session Open Searching for session in workflows.

---

# Document 1912: Exceptions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645172/Exceptions
**Categories:** chunks_index.json

DTK encapsulates exceptions in the DRException class. The class offers the possibility to nest exceptions and assigning a severity. Valid severities are defined in the DRSeverity class. Exception related classes are available in the devkit.exception package. Internationalization DRExceptions can be created with a DRTextCode as argument. DRTextCode allows the user to define messages in a property file, from which messages will be extracted using the Java ResourceBundle concept. Currently, DRTextCode only supports English locale. However, DRException defines constructors that take message strings directly. You may use these constructors to define error messages in other languages. For an example on how a property file may be used, see the DTK examples directory.

---

# Document 1913: Ordered Service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/497221733/Ordered+Service
**Categories:** chunks_index.json

Order Service is required by the Ordered Routing Queue Strategy. A function defined in APL code will extract values from any routed UDR or bytearray to decide which partition queue to add it to. Ordered Routing allows you to utilize multicore CPUs, but still have deterministic order. Configuring an Ordered Service Example - Ordered Routing Queue Strategy Scenario: When session updates from a collection point are encoded as a user-defined UDR type, Ordered.Subscriber.Update , the field 'id' contains the session ID. The example below shows how to extract this value and add it to the ordered.SessionIdentifier , which the queue strategy will use to determine the correct partition. import ultra.Ordered.Subscriber; void route(ordered.SessionIdentifier si, any input) { if(instanceOf(input, Update)) { Update u = (Update)input; ordered.addInteger(si, u.id); } } Multiple values can be added to the SessionIdentifier , it will calculate the hash based on them all after the route function returns. If there is more than one type routed into the workflow, there must be a longer if-else chain, with a clause for each type. Function Description Function Description ordered.addInteger(ordered.SessionIdentifier si, int i) Add the value of integer i to the SessionIdentifier ordered.addString(ordered.SessionIdentifier si, string s) Add the value of string s to the SessionIdentifier ordered.addByte(ordered.SessionIdentifier si, byte b) Add the value of byte b to the SessionIdentifier ordered.addBytes(ordered.SessionIdentifier si, bytearray ba) Add the value of bytearray ba to the SessionIdentifier ordered.addByteRange(ordered.SessionIdentifier si, bytearray bs, int offset, int len) Add the values of bytearray ba from offset and the next len bytes to the SessionIdentifier ordered.setPartition(ordered.SessionIdentifier si, int partition) Alternatively, if more control over partition selection is wanted one can set the partition id explicitly, this will override any values added to the SessionIdentifier.

---

# Document 1914: Operating Systems - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657753/Operating+Systems
**Categories:** chunks_index.json

Major Linux distributors such as RedHat, Ubuntu, or SLES, are supported for installation. When using Oracle Java 17, refer to Oracle JDK 17 documentation for details on specific Linux versions supported. When using OpenJDK 17, refer to the vendor documentation for details on specific Linux versions supported.

---

# Document 1915: Event Category - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030622
**Categories:** chunks_index.json

An Event Category is defined in the Event Notification configuration. The Event Category is used to send any kind of information to a Column, as opposed to a direct mapping of a MIM resource. The Event Category name is a string which is used as an argument in the dispatchMessage APL function to route messages to a Column. Open The Event Category dialog, where a user-defined string is specified to be used as the Name for the Category field in an event When the Event Category is defined it is mapped against a Match Value in the Event setup tab. Then the defined Event Category is used as a parameter in the APL code with the dispatchMessage function. Break

---

# Document 1916: HTTPD_Deprecated Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000578/HTTPD_Deprecated+Agent
**Categories:** chunks_index.json

This section describes the HTTPD_Deprecated agent. This is a collection agent for real-time workflow configurations. It also includes APL functions for connecting a client to an external HTTP server. Prerequisites The reader of this information should be familiar with: Hypertext Transfer Protocol version 1.1 (RFC 2616: http://www.ietf.org/rfc/rfc2616.txt ) The Transport Layer Security (TLS) Protocol version 1.2 (RFC 5246: https://www.ietf.org/rfc/rfc5246.txt ) The HTTPD_Deprecated agent (in combination with Analysis or Aggregation agents) can act as a web server, receiving requests and sending responses on a TCP/IP connection. The requests are turned into UDRs, using the standard Hypertext Transfer Protocol, and inserted into a workflow. When a workflow acting as a web server is started, the HTTPD_Deprecated agent opens a port for listening and awaits a request. The workflow remains active until manually stopped. In addition, the agent offers the possibility to use an encrypted communication setup through SSL. Note! To fully support HTTP pipelining, you must add the property ec.httpd.ordered.response with the value true to the configuration file for the EC. If this property is set to true , responses will be guaranteed to be sent in the same order as the pipelined requests were received. For further information on how to add a property to a pico configuration file, see Updating Pico Configurations . To ensure that a request is not blocking responses from being sent for too long, the Server Timeout (sec) should be configured. If a response is not sent for a request within the specified time, the response for the next request will be sent. This property should not be set unless support for pipelining is required! Setting this property to true will also have some effect on the performance since the requests will be cached until the responses have been sent. The section contains the following subsections: HTTPD_Deprecated Agent Configuration APL Functions HTTPD_Deprecated Agent Input/Output Data and MIM HTTPD_Deprecated Agent UDR Type HTTPD_Deprecated Agent Events

---

# Document 1917: REST Client_Deprecated Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642528/REST+Client_Deprecated+Agent+Configuration
**Categories:** chunks_index.json



---
**End of Part 80** - Continue to next part for more content.
