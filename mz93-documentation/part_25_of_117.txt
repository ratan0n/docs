# RATANON/MZ93-DOCUMENTATION - Part 25/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 25 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~65.3 KB
---

To create a Workflow Bridge profile configuration, click on the New Configuration button from Build View , and then select Workflow Bridge Profile from the selection screen. The content s of the buttons i n the bar may change depending on which configuration type has been opened in the currently displayed tab. Workflow bridge uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons .

---

# Document 535: log4j APL Logging Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678516
**Categories:** chunks_index.json

The system includes a log4j extension that enables generation of customized logs from agents configured with APL. This section describes how to configure these logs to meet deployment specific requirements. Configuration Files There is a configuration file called apl-log4j.properties containing default settings located in the $MZ_HOME/etc/logging directory that is used to specify the path of log files, log filtering rules, log level and formatting. If you want to use different settings for different Execution Contexts, this file can be copied and renamed to <ec-name>- apl-log4j.properties where you can configure different settings for the specified EC. To provide some examples and templates, there are a number of pre-existing files with the name apl-log4j-<log level name>.properties , for example apl-log4j-trace.properties , which you can use to either copy properties from, or make a copy of the whole file and rename it to <ec-name>- apl-log4j.properties. Example. Configuration filenames Default configuration file: $MZ_HOME/etc/logging/apl-log4j.properties EC specific configuration file for ec1: $MZ_HOME/etc/logging/ec1-apl-log4j.properties Copying the default configuration file into an EC specific configuration file: $cp apl-log4j.properties ec1-apl-log4j.properties Copying a template configuration file into an EC specific configuration file: $cp apl-log4j-trace.properties ec1-apl-log4j.properties The following template configuration files are included by default: apl-log4j-off.properties apl-log4j-fatal.properties apl-log4j-error.properties apl-log4j-warn.properties apl-log4j-info.properties apl-log4j-debug.properties apl-log4j-trace.properties apl-log4j-all.properties The files listed above have a different log level setting but are otherwise identical. Whenever the APL function log.* is called in the Analysis agent, this function invokes logging with log4j. After the workflow is done executing with a specific EC, a new EC specific configuration file <ec-name>-apl-log4j.properties will be created. This new EC specific configuration file is created by default with the same configurations saved in the file apl-log4j.properties . For instance, if the EC name is ec1 then ec1-apl-log4j.properties is created in $MZ_HOME/etc/logging and will have identical settings as apl-log4j.properties . MZSH does not support dedicated commands to make changes to the log level. Changes on the log level and other properties in the configuration file must be made manually. Whatever changes made to the configuration file takes effect at the next workflow run without the need to restart the EC. In a multi-host installation, the EC specific configuration file is always created on the host where EC is located. Therefore, log4j always read the EC specific configuration according to the EC location. For instance, workflow run by ec1 located on host my-host-name . The log4j always read the EC specific configuration file ec1-apl-log4j.properties located at path $MZ_HOME/etc/logging on host my-host-name . The content of the files defines the logging. Example. Configuration file contents log4j.logger.aplLogger=ALL, a log4j.appender.a=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.a.file=${mz.home}/log/{pico}_{workflow}.log log4j.appender.a.layout=com.digitalroute.apl.log.JsonLayout log4j.appender.a.layout.MdcFieldsToLog=pico, workflow, agent, tag The first line in the example above sets the log level and declares an "appender" named ' a' . The available log levels are listed below in order of severity, from highest to lowest: OFF FATAL ERROR WARN INFO DEBUG TRACE ALL Messages of the same or higher severity than the selected level are logged. For instance, if the configured log level is WARN , messages with the severity ERROR and FATAL will be logged as well. The other settings above mean that messages are logged in files that are started, stopped and stored in JSON formatted files in the $MZ_HOME/log directory in regular intervals. When an active log file has reached its maximum size, it is backed up and stored with a number suffix. A new active log file is then created. The default maximum size is 10 MB, and the default number of backup files is one (1). Appenders There are two different types of appenders; DRRollingFileAppender och DRRollingMultiFileAppender . DRRollingFileAppender Writes to a single defined file based on the log4j.appender.<appender name>.file property DRRollingMultiFileAppender Writes one file for each workflow instance it encounters based on the log4j.appender.<appender name>.file Which workflows are written into which appender is based on the log4j.logger.<class name> property. Examples Appender Configurations # Default log4j.appender.Default=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.Default.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.Default.layout=org.apache.log4j.PatternLayout log4j.appender.Default.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.Default.MaxFileSize=10MB log4j.appender.Default.MaxBackupIndex=20 log4j.logger.aplLogger.Default=TRACE, Default The appender named Default will write a single file for all workflows contained under the Default folder. # PRIMARY log4j.appender.PRIMARY=com.digitalroute.apl.log.DRRollingMultiFileAppender log4j.appender.PRIMARY.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.PRIMARY.layout=org.apache.log4j.PatternLayout log4j.appender.PRIMARY.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.PRIMARY.MaxFileSize=10MB log4j.appender.PRIMARY.MaxBackupIndex=20 log4j.logger.aplLogger.RT_Folder.RT_TEST_WF=TRACE, PRIMARY The appender named Primary will create multiple files; one for each workflow instance based on the RT_Folder.RT_TEST_WF workflow. # SECONDARY log4j.appender.SECONDARY=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.SECONDARY.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.SECONDARY.layout=org.apache.log4j.PatternLayout log4j.appender.SECONDARY.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.SECONDARY.MaxFileSize=10MB log4j.appender.SECONDARY.MaxBackupIndex=20 log4j.logger.aplLogger.RT_Folder.RT_TEST_WF=TRACE, SECONDARY The appender Secondary will create a single file for each workflow instance based on the RT_Folder.RT_TEST_WF workflow. The file will take the name of the first workflow instance it encounters, for example "RT_Folder.RT_TEST_WF.workflow_1" Hint! You can change the maximum file size and the number of backup files by adding the following lines: log4j.appender.a.MaxFileSize=100MB log4j.appender.a.MaxBackupIndex=10 You can add a filtering rule by adding the line log4j.logger.<configuration name>=<log level> . This is useful when you want to set different log levels for specific folders or configurations. If you want to apply the filtering rule to all APL configurations in the default folder, change the last line in the previous example to log4j.logger.Default=DEBUG . Example. Sets the general log level to ERROR and to DEBUG for the agent named agent_1 log4j.logger.aplLogger=ERROR, a log4j.appender.a=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.a.file=${mz.home}/log/{pico}_{workflow}.log log4j.appender.a.layout=com.digitalroute.apl.log.JsonLayout log4j.logger.Default.debug.workflow_1.agent_1=DEBUG Note! For performance reasons it is recommended to use the DRRollingFileAppender and configure individual appenders for each workflow. Only use the DRRollingMultiFileAppender if you need individual files on a workflow instance level. For more information about available settings, see the log4j documentation at https://logging.apache.org/log4j/1.2/manual.html . APL Commands The following functions are used to trigger logging within any of the function blocks in APL: void log.fatal(any, any) void log.error(any, any) void log.warn(any, any) void log.info(any, any) void log.debug(any, any) void log.trace(any, any) For more information about these functions, see Log and Notification Functions . Log Output The output log files are stored in the directory specified in the active logging configuration. Example. Log file in JSON format {"timestamp":"2015-12-20:22:44:10 UTC","level":"DEBUG","thread":"Default.logtestwf.workflow_1: TCP_IP_1_1","category":"Default.logtestwf.workflow_1.Analysis_1","message":"In consume","pico":"EC1","workflow":"Default.logtestwf.workflow_1","agent":"Analysis_1"} The fields in the log output are described below. Field Description Field Description timestamp The time when the message was logged. The UTC timezone and international standard date and time notation is used by default. You can specify the date format by adding the following line in the configuration file: log4j.appender.a.layout.DateFormat=<Java SimpleDateFormat pattern> For information about how to use SimpleDateFormat patterns, see: https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html level The log level i e FATAL , ERROR , WARN , INFO , DEBUG , or TRACE . thread The name of the workflow thread. category The logged configuration. This field contains the category class of the appender that is defined in the configuration file. message The log message is specified in the APL command. pico The name of the Execution Context. Warning! The ECs must be restarted if you manually delete or rename active log files or backup log files. Hint! If the log files are not generated as expected, review the EC logs. Your configuration files may contain errors.

---

# Document 536: Oracle Database Online Backup and Restore - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647270/Oracle+Database+Online+Backup+and+Restore
**Categories:** chunks_index.json

Note! This section is not applicable when the Oracle database is installed in Amazon Web Services. You can take an online backup while the system is still up and running. This backup can be used to restore MediationZone in case of a critical failure. The backup will contain a consistent snapshot of the file system and database at a certain point of time. The snapshot may be used to recover the system to the state it had at the time of the backup. The following sections contain examples of how to perform this procedure, and how to perform a restore, when using an Oracle database. Example of Online Backup of Database and File System The database part of the backup is made as one full backup and a number of incremental changes. For every incremental backup of the database, a full backup has to be taken of the file system. It is vital to keep file system and Oracle-backups in a consistent state with each other. Taking a file system snapshot is exemplified with the fssnap utility. Taking database backup, is exemplified using the Oracle RMAN utility. Note! The instructions presented here are samples used with MediationZone and Oracle running on a single machine. Always consult an Oracle Database Administrator when setting up database backup scripts. Before taking backups, Oracle needs to be set in Archive mode. Make sure the following entries in the initMZ.ora file are present and correct according to the ORACLE installation: log_archive_dest = /path_to_oracle/redologs_directory log_archive_format = "log_%d_%a_%T_%S_%r.arc" Make a secure backup of initMZ.ora . Run the following commands: $ sqlplus "/AS SYSDBA" SQL> shutdown <Note, the shutdown must be graceful> SQL> startup mount SQL> alter database archivelog; SQL> archive log start; SQL> alter database open; SQL> quit; Use the following instructions to find the DBID (Database Identifier) and write it down. It is needed if a recovery is to be performed. $ rman nocatalog RMAN> connect target RMAN-06005: connected to target database: MZ (DBID=749276174) RMAN> exit; Make a full database backup, using a script such as the following: #! /bin/sh # # Full backup of Oracle tablespaces # # Please note that this script does not save the state of # <product> filesystem. To ensure you got everything backedup, # please run the incremental backup script after this script # has finished. # # -------------------------------------------------------------- # Start of Configuration Parameters # -------------------------------------------------------------- # # BACKUP_DIR: Where to store the backup. # BACKUP_DIR=/opt/snap/backup # # CONNECT_STR: Database connection string # CONNECT_STR=backup/backup@MZ # # ------------------------------------------------------------- # End of Configuration Parameters # ------------------------------------------------------------- # Do not change anything after this line. # CURRENT_DATE=`date '+%mm%dd%yy%Hh%Mm%Ss'`; BACKUP_DIR_CURRENT=$BACKUP_DIR/current BACKUP_DIR_SAVED=$BACKUP_DIR/$CURRENT_DATE BACKUP_CONTROL=control.$CURRENT_DATE export BACKUP_DIR_CURRENT CONNECT_STR mv $BACKUP_DIR_CURRENT $BACKUP_DIR_SAVED mkdir $BACKUP_DIR_CURRENT cat <<EOF | $ORACLE_HOME/bin/rman nocatalog connect target $CONNECT_STR; run { # # Allocate backup channel # allocate channel DatabaseBackup type disk format '$BACKUP_DIR_CURRENT/%U'; # # Take a backup of all tablespaces # backup database; # # Tell oracle to switch logfile # sql "alter system switch logfile"; # # We do not really need to backup this controlfile at this # state but it will block until the log switch has occurred. # backup current controlfile; # # Backup archived logfiles # backup archivelog all delete input; } exit; EOF Make an incremental database backup and a file system snapshot. This creates a timestamp backup, to be used for recovery. #! /bin/sh # # Incremental backup of Oracle and full <product> # Filesystem Snap. # # This scripts saves newly created archivelogs from oracle and # the current state of the <product> filesystem. # # -------------------------------------------------------------- # Start of Configuration Parameters # -------------------------------------------------------------- # # BACKUP_DIR: Where to store the backup. # BACKUP_DIR=/opt/snap/backup # # CONNECT_STR: Database connection string # CONNECT_STR=backup/backup@MZ # # ------------------------------------------------------------- # End of Configuration Parameters # ------------------------------------------------------------- # CURRENT_DATE=`date '+%mm%dd%yy%Hh%Mm%Ss'`; BACKUP_DIR_CURRENT=$BACKUP_DIR/current; BACKUP_DIR_CURRENT_MZFS=$BACKUP_DIR_CURRENT/MZFS_$CURRENT_DATE BACKUP_CONTROL=control.$CURRENT_DATE # # ------------------------------------------------------------- # Start of Configuration Parameters # ------------------------------------------------------------- # # SNAP_COMMAND: Command to execute to create the filesystem snap. # SNAP_COMMAND="fssnap -Fufs -omaxsize=500m, bs=/opt/snap,unlink /export/home" # # SNAP_MOUNT: Command to mount the filesystem snapshot. # SNAP_MOUNT="mount -Fufs -o ro /dev/fssnap/0 /opt/snap/mount" # # SNAP_BACKUP: Command to backup the snapshot while its mounted. # SNAP_BACKUP="cp -Rp /opt/snap/mount $BACKUP_DIR_CURRENT_MZFS"; # # SNAP_UNMOUNT: Command to unmount the snapshot. # SNAP_UNMOUNT="umount /opt/snap/mount" # # ------------------------------------------------------------- # End of Configuration Parameters # ------------------------------------------------------------- # Do not change anything after this line. # mkdir $BACKUP_DIR_CURRENT_MZFS; export BACKUP_DIR_CURRENT BACKUP_CONTROL CONNECT_STR export SNAP_COMMAND SNAP_MOUNT SNAP_BACKUP SNAP_UNMOUNT # cat <<EOF | $ORACLE_HOME/bin/rman nocatalog connect target $CONNECT_STR run { allocate channel DatabaseBackup type disk format '$BACKUP_DIR_CURRENT/%U'; # # 1) Backup and remove all old redologs. # sql "alter system switch logfile"; backup current controlfile; backup archivelog all delete input; # # 2) Lock transaction table. # 3) Take a FileSystem Snapshot. # 4) Backup new and active redologs. # 5) Unlock transaction table. # sql "lock table mzadmin.wf_txn in exclusive mode"; sql "alter system switch logfile"; host "$SNAP_COMMAND"; backup current controlfile; backup archivelog all delete input; sql "alter database backup controlfile to ''$BACKUP_DIR_CURRENT/$BACKUP_CONTROL'' reuse"; sql "commit"; host "$SNAP_MOUNT"; host "$SNAP_BACKUP"; host "$SNAP_UNMOUNT"; } exit; EOF Example of Restoring an Online Backup In case of critical failure, the system may be recovered with the backup files produced using a procedure such as the example described in the previous section. Note! TEMP table spaces will not be recovered. Shut down MediationZone. Shut down the Oracle Instance. Make a cold backup of the crashed MediationZone, including the Oracle instance. Locate the backup to restore. That is, a directory containing a full backup and at least one incremental backup. The directory should contain one or several Oracle control files control.<id>, and a snapshot of the file system MZFS_<id> . Remove all table space files, the corresponding archived redo logs, and control files. If necessary, remove the initMZ.ora file as well, and replace it with the backed-up version. Make a backup copy of the control files MZ_control1 and MZ_control2 . Replace the dynamic directories with the corresponding backup directories. An example of a dynamic directory is the storage handler directory, used by the Inter Workflow application. Execute the following commands: $ export BACKUP_DIR=/path_to_backup_to_use $ rman nocatalog RMAN> set dbid = <DBID> RMAN> connect target; RMAN> startup force mount; RMAN> run { RMAN> allocate channel SystemRestore type disk format '$BACKUP_DIR/%U'; RMAN> restore database; RMAN> restore archivelog all; RMAN> } The reply messages are: RMAN-08017: RMAN-08022: RMAN-08510: RMAN-08023: RMAN-08511: RMAN-08024: RMAN-08031: channel SystemRestore: starting archivelog... channel SystemRestore: restoring archivelog archivelog thread=1 sequence=20 channel SystemRestore: restore backup... piece handle=/tmp/backup/0feduv7h_1_1... channel SystemRestore: restore complete released channel: SystemRestore Use the last RMAN-08510 to fill in 'set until logseq NR thread NR' RMAN> run { RMAN> allocate channel SystemRestore type disk format 'BACKUP_DIR/%U'; RMAN> set until logseq 20 thread 1; RMAN> recover database; RMAN> alter database open resetlogs; RMAN>} RMAN> exit; Restart the database instance with the following commands: SQL> shutdown normal SQL> startup Start MediationZone.

---

# Document 537: Python Writer's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001433
**Categories:** chunks_index.json

When writing Python code in the Python agents and the Python Module, the information in this section applies. This section includes the following subsections: Python Function Blocks Functions, Exceptions, Types, and Constants Importing Code Persistent Variables

---

# Document 538: Python Connector Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740056/Python+Connector+Agent+Configuration
**Categories:** chunks_index.json

To open the Python Connector agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select Python Connector in the Collection tab in the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. The Python Connector agent configuration has three tabs: General , API and MIM . General Tab Open The Python Connector agent - General tab Setting Description Setting Description Host The IP address or hostname to which the Python Connector will bind. If left empty, the Python Connector binds to all IP addresses available on the system. Port The port number on which the Python Connector will bind. Ensure that the port is not used by other applications. Allow Multiple Connections If enabled, several connections are allowed simultaneously. If disabled, only one at a time is allowed. Number of Connections Allowed If Allow Multiple Connections is enabled, the maximum number of simultaneous connections is specified as a number between 0 and 99999. Use Connection Control Select this option if you want to control connection requests individually. If you select this option, you may need to make a selection for Route Connection Requests on . See the description below. Route Connection Requests on Select on which route you want to route connection requests when a user requests to connect. Select one of the possible routes listed in the drop-down list. Use per Connection Routing Select this check box if you want to control what each individual connection is routing. Use TLS Select this check box if you want to use TLS. When you select this check box, the Security Profile selection and Require Client Authentication checkbox is enabled. If you want connections to use SSL, see Connecting Using SSL . Security Profile Require Client Authentication Select this check box if you require client authentication from all connecting clients. Accepted Types Select which data types are to be accepted by the Python Connector agent. You can select UDR types or bytearrays. Output Routes Enter which route accepts what data type. API Tab The API tab allows you to define your own agent API exposed to your connecting users. You can use __all__ = ['<name1>', '<name2>', ..] to only expose the names listed as the API instead of adding to the default API. See the example shown in the image below. For further information on writing code in the API tab, see Python Writer's Guide . Open Python Connector agent - API tab MIM Tab In the MIM tab you can set the MIMs that you want the the Python Connector agent to publish. Open Python Connector agent - MIM tab Parameter Description Parameter Description Assigned Select the type of MIM assigned. Only MIMs of the global type are available. Name Enter the MIM resource name. Type Select the data type of MIM resource. Connecting with an Exploration Tool using Python To be able to connect the exploration tool of your choice to the Python Connector agent, you must first import the connector Python module: mzconnector.py . The connector module, mzconnector.py , is extracted to MZ_HOME/python during installation. For easy access, you can copy this module to a directory on your local machine, which you will connect from. All the Python code, which also includes the Python code in the API tab and the code imported from the Python Module profile, is executed by the Python process that connects on your local machine, while the APL code is executed on the EC server. After you have configured a Python Connector agent within a real-time workflow, you must start the workflow before you can start to interact using an exploration tool. You can then start to run your script. In the following example, Jupyter Notebook is used to connect to a simple workflow. Example - Interaction via Python Connector agent The Python Connector agent is configured to accept DRUDR types. Open The Analysis agent is configured to debug the input and send UDRs to route r_2 back to the Python Connector agent. Open As shown below, you can trigger activity in the workflow using an exploration tool, Jupyter Notebook is used in this example. Open

---

# Document 539: Group State Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670783/Group+State+Event
**Categories:** chunks_index.json

This event is triggered when a workflow group changes from one state to another. The following states are available: Idle - This is the state of a workflow group that is valid but where no workflows are being executed. Invalid - A workflow group will change state from Idle to Invalid if the configuration is made invalid. Once the configuration is valid again, the workflow group will change back to the Idle state. Hold - A workflow group will change state from Idle to Hold if configurations are being imported with certain options selected. Once the import is finished, the state will change back to Idle again. If a default import is made, the workflow group will not change into the Hold state. Running - A workflow group will change state from Idle to Running as soon as it is being executed. If the execution is allowed to finish, the state will change back to Idle. Suppressed - If configurations are being imported with certain options selected while a workflow group is in Running state, the state will change to Suppressed. For real-time workflows, the state will change back to Running again once the import is finished. Batch workflows will remain in Suppressed state until all members have finished execution and will then change to Idle state. If the workflow group is manually stopped, the state will change to Stopping. If a default import is made, the workflow group will not change into the Suppressed state. Stopping - If the execution of a workflow group is manually stopped, the state will change from Running or Suppressed to Stopping. Aborted - If one of the members of the workflow group aborts, the workflow group will change state from Running or Suppressed to Aborted once all of its members have finished execution. See Workflow Group States for further information about workflow group states. Filtering In the Event Setup tab, the values for all the event fields are set by default to All in the Match Value(s) column, which will generate event notifications for all state changes for all workflow groups. Double-click on the field to open the Match Values dialog where you can click on the Add button to add which values you want to filter on. If there are specific values available, these will appear in a drop-down list. Alternatively, you can enter a hard coded string or a regular expression. The following fields are available for filtering of Group State events in the Event Setup tab: Group State event specific fields groupName - This field enables you to select which workflow groups you want Group State event notifications to be generated for. Adding Workflow Groups groupState - This field determines for which states you want Group State event notifications to be generated. If the state for one of the matching workflow groups changes into any of the states added for this field, a group state event notification will be generated. Adding workflow states Fields inherited from the Base event The following fields are inherited from the Base event, and can also be used for filtering, described in more detail in Base Event : category - If you have configured any Event Categories, you can select to only generate notifications for Group State events with the selected categories. See Event Category for further information about Event Categories. contents - The contents field contains a hard coded string with event specific information. If you want to use this field for filtering you can enter a part of the contents as a hard coded string, e g the state you are interested in Idle/Running/Stopping/etc. However, for Group State events, almost everything in the content is available for filtering by using the other event fields, e g groupName, groupState, etc. eventName - This field can be used to specify which event types you want to generate notifications for. This may be useful if the selected event type is a parent to other event types. However, since the Group State event is not a parent to any other event, this field will typically not be used for this event. origin - If you only want to generate notifications for events that are issued from certain Execution Contexts, you can specify the IP addresses of these Execution Contexts in this field. However, since the Group State events are only issued from the Platform, this event field should typically not be used for filtering. receiveTimeStamp - This field contains the date and time for when the event was inserted into the Platform database. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06.*" for catching all Group State events from 1st of June, 2012, to 30th of June, 2012. severity - With this field you can determine to only generate notifications for state changes with a certain severity; Information, Warning, Error or Disaster. For example, a state change from Idle to Running will typically be of severity Information, while a state change to Abort state will typically be of severity Error. timeStamp This field contains the date and time for when the Execution Context generated the event. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06-15 09:.*" for catching all Group State events from 9:00 to 9:59 on the 15th of June, 2012. Note! The values of these fields may also be included in the notifications according to your configurations in the Notifier Setup tab.

---

# Document 540: Creating Server Keystore and Certificate - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675233
**Categories:** chunks_index.json

After generating the CA, the next step is to generate a key pair for the server/service. Run the following command: $ keytool -genkey -alias server -keyalg RSA -keystore ./Server.jks -storetype PKCS12 alias = name of the key, for example, server keystore = name of the keystore, for example, server.jks Note! When prompted for first and last name, the hostname where the certificate is valid should be entered, for example, localhost. Other values can be anything. Generate a Certificate Signing Request (CSR) so that we can get server's certificate signed using a CA. $ keytool -certreq -alias server -keystore Server.jks -file Server.csr Get the certificate signed by our the CA, Test CA in this example. See Setting Up a Certificate Authority on how to set up a CA. $ openssl x509 -CA caroot.cer -CAkey cakey.pem -CAserial serial.txt -req -in Server.csr -out Server.cer -days 365 Note! CA , CAkey and CAserial are files generated when setting up the CA. Import the Test CA root self signed certificate in server key store as a trusted certificate. $ keytool -import -alias TestCA -file caroot.cer -keystore Server.jks Import server's certificate signed by Test CA in server key store with the same alias name that was used to generate the key pair during genkey. $ keytool -import -alias server -file Server.cer -keystore Server.jks

---

# Document 541: DB Ref Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638833
**Categories:** chunks_index.json

The DB Ref Event is triggered when a user performs any of the following operations on its tables: Commits changes to a database table Performs import operations If Enable Advanced Logging is selected in a Reference data profile, additional information will be included as follows: SQL statements for commit operations Generated SQL and log files' locations for import operations For more information about Reference Data Management, see the Reference Data Management user's guide. Filtering In the Event Setup tab, the values for all the event fields are set by default to All in the Match Value(s) column, which will generate event notifications every time a DB Ref event is generated. Double-click on the field to open the Match Values dialog where you can click on the Add button to add which values you want to filter on. If there are specific values available, these will appear in a drop-down list. Alternatively, you can enter a hard coded string or a regular expression. The following fields are available for filtering of DB Ref events in the Event Setup tab: DB Ref event specific fields profileIdentity - This fields contains the name of the Data Reference profile. user - This field contains the name of the user that performed the operation. Fields inherited from the Base event The following fields are inherited from the Base event, and can also be used for filtering, and described in more detail in Base Event : category - If you have configured any Event Categories, you can select to only generate notifications for DB Ref events with the selected categories. See Event Category for further information about Event Categories. contents - This field contains a string with event specific information. If you want to use this field for filtering you can enter a part of the contents as a hard coded string. eventName - This field can be used to specify which event types you want to generate notifications for. This may be useful if the selected event type is a parent to other event types. However, since the DB Ref event is not a parent to any other event, this field will typically not be used for this event. origin - This field always contains the IP address of the Platform. severity - With this field you can determine to only generate notifications for events with a certain severity; Information, Warning, Error or Disaster. timeStamp - This field contains the date and time for when the Platform generated the event. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2014-06-15 09:.*" for catching all DB Ref events from 9:00 to 9:59 on the 15th of June, 2014. Note! The values of these fields may also be included in the notifications according to your configurations in the Notifier Setup tab.

---

# Document 542: Format Management Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646665/Format+Management+Overview
**Categories:** chunks_index.json

MediationZone is capable of handling various Usage Detail Record (UDR) formats provided that their structure is known. Vendor UDR formats are often complex and hard to work with in agents that view or manipulate data. Therefore, the system converts the external UDR formats to an internal representation, and vice versa when UDRs are leaving the system. In order to make this conversion, both internal and external format definitions have to be defined. When the definitions exist, there must be a mapping description that maps each external field of interest to an internal field. Ultra does not require a one-to-one relationship between external and internal field names. Normally, only fields of interest or those that are needed when leaving the system are declared in the internal format definition. Often the internal definition holds additional fields that are initially empty and then populated on the way through a workflow. MediationZone does not constrain how these fields are used. Example - Format management In this example, the external field E1C is not needed and is not mapped at all. Internally IFA and IFB are manipulated. IFE is given a value, probably depending on values defined in IFC and IFD during the UDRs way through the workflow. At the exit, the two original fields are mapped into E2A and E2B plus the new (IFE) into E2C. Open MediationZone introduces a language called Ultra Format Definition Language (UFDL) that is used when describing UDR formats and mappings. Apart from describing external formats, internal formats, and mappings, the UFDL requires separate decoder and encoder specifications. A decoder or encoder refers to one or several mappings that are used for the actual translation. Format Storage In order to operate on UDRs, there must be a compiled version (Java class) of the internal format definition available in the Code or Ultra server. Such Java classes are introduced into the servers in two different ways: Format definitions entered in the Ultra Format Editor are compiled and inserted in the Ultra server when saved. Precompiled format definitions (for instance; Radius, NetFlow, and SNMP protocols) are inserted in the Code server during package commit. A format can be recompiled while workflows using it are running. In this case, the changes to the format will take effect the next time the workflow is activated. All Java classes generated by the Ultra server are kept in the system. This is to make UDRs, for instance stored in ECS and using the old format version, processable. For further information, see Ultra Format Converter . Note! Saving a format in the Ultra Format Editor may take some time, since the system may have to re-validate and regenerate many dependent configurations. If an Ultra format is moved to another directory or is renamed, the configurations using the format become invalid. Built-in Formats There are four different built-in external formats that can be used between workflows; MZ Tagged, MZ Tagged (compressed), JSON, and CSV. These can be used by the Decoder and Encoder agents as well as by the real-time collection agents for Inter Workflow, TCP/IP, and GTP'. MZ Tagged Formats The MZ Tagged format is available in all supported agents, while the MZ Tagged (compressed) format is only available in the Encoder agent. Using the compressed format reduces the size of the UDRs significantly. However, since compression requires more CPU, you should consider the trade-off between I/O and CPU when choosing an encoder. No Decoder or Encoder needs to be defined in Ultra to handle these formats. The MZ Tagged formats contain header data that is based on the time zone settings of the host on which it was generated. This is important to consider when you compare output from identical workflows that are running on hosts with different time zone settings. The binary data may differ due to the header content. JSON Format The JSON Format is available in the Decoder, Inter Workflow, TCP/IP, and GTP' agents. With this built-in format, you can also choose to use a DynamicJsonUDR where you can add the payload to the UDR. The format is validated according to the specified JSON schema. CSV Format The CSV Format is available in the Decoder, Inter Workflow, TCP/IP, and GTP' agents. The following predefined CSV formats are supported; Unix, Mac, Windows, and Excel, but you can also create your own custom format. Decoding of a UDR The Ultra engine is designed to decode only the necessary parts of a Usage Detail Record, UDR, to keep workflow performance as high as possible. By default (that is, provided that Full Decode is not selected), the decoder only decodes enough to calculate the size of the UDRs. The UDR content is decoded as the field values are accessed. Hence, if there is a decoding error, this may not be discovered by the decoder but by the agent accessing a specific field. Note! Full decoding of complicated nested UDR structures can be time-consuming. In most cases, the performance cost of this option is however small.

---

# Document 543: Security Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605205/Security+Event
**Categories:** chunks_index.json

The Security event is triggered for each failed login attempt. Filtering In the Event Setup tab, the values for all the event fields are set by default to All in the Match Value(s) column, which will generate event notifications for all state changes for all workflow groups. Double-click-on the field to open the Match Values dialog where you can click on the Add button to add which values you want to filter on. If there are specific values available, these will appear in a drop-down list. Alternatively, you can enter a hard coded string or a regular expression. The following fields are available for filtering of Group State events in the Event Setup tab: Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category - If you have configured any Event Categories, you can select to only generate notifications for System events with the selected categories. See Event Category for further information about Event Categories. contents - The contents field contains a hard coded string with event specific information. If you want to use this field for filtering you can enter a part of the contents as a hard coded string, e g the state you are interested in Idle/Running/Stopping/etc. However, for Security events, the content consists of the text "Login attempt by <username> from host <IP address> failed." eventName - This field can be used to specify which event types you want to generate notifications for. This may be useful if the selected event type is a parent to other event types. However, since the Security event is not a parent to any other event, this field will typically not be used for this event. origin - The Platform IP address. receiveTimeStamp - This field contains the date and time for when the event was inserted into the Platform database. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2018-04.*" for catching all System events from 1st of April, 2018, to 30th of April, 2018. severity - With this field you can determine to only generate notifications for state changes with a certain severity; Information, Warning, Error or Disaster. The severity level for Security events is always Warning. timeStamp This field contains the date and time when the Platform generated the event. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2018-06-15 09:.*" for catching all System events from 9:00 to 9:59 on the 15th of June, 2018. Note! The values of these fields may also be included in the notifications according to your configurations in the Notifier Setup tab. Fields inherited from the Security event systemMessage - This field contains the username and IP address of the Desktop.

---

# Document 544: File System Type - HDFS - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/315228209/File+System+Type+-+HDFS
**Categories:** chunks_index.json

When selecting HDFS as a file system, you will see two tabs  General and Advanced . Open File System profile - HDFS - General tab General tab The following settings are available in the General tab in the File System profile: Setting Description Setting Description General Settings Hadoop Mode Select the Hadoop mode that you want to use, both NON-HA and HA are available. Name Node Settings Host Enter the Hadoop name node host. This option is visible only when the NON-HA Hadoop Mode is selected. Port Enter the Hadoop port number. This option is visible only when the NON-HA Hadoop Mode is selected. Replication Enter the desired number of replications. Advanced tab The Advanced tab allows for advanced properties to be configured in the profile. Open File System profile - HDFS - Advanced tab

---

# Document 545: SAP RFC Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642964
**Categories:** chunks_index.json

You use the SAP RFC profile to dynamically generate UDRs based on selected SAP RFC functions that are part of an SAP system. To open the SAP RFC profile configuration, click Build  New Configuration . Select SAP RFC Profile in the Configurations dialog. The profile contains the standard configuration buttons as described in Common Configuration Buttons and one additional button: Open SAP RFC profile configuration dialog The profile contains the standard configuration buttons as described in Common Configuration Buttons and one additional button: Button Description Button Description Open Click on this button to Enable External References in an agent profile field. Refer to Enabling External References in an Agent Profile Field in External Reference Profile for further information. The following settings are available in the SAP RFC profile: Setting Description Setting Description Connection Details Username Enter the SAP user ID Password Enter the SAP user password Use Secrets Profile Select this check box if you want to use a Secrets profile instead of entering the SAP password in the SAP RFC profile. If you select this option, click on the Browse... button to select the Secrets profiles you want to use. Host Enter the IP address of the SAP System Enable Load Balancing Select this check box to enable load balancing for SAP Jco. System ID Enter the 3 character code for the SAP System Message Server Host Enter the hostname/IP of the SAP Message Server Message Server Port Enter the port of the SAP Message Server. Group Name Enter the group name associated with the SAP System Advanced SAP System Number Enter the SAP System number SAP Client Number Enter the SAP Client number Secure Connection Enable SNC Mode Select this check box to allow the SAP RFC agent to apply a Secure Network Communications (SNC) to the connection between the agent and the SAP RFC system. This option is enabled by default. You must generate a SNC Personal Security Environment (PSE) on the servers running the Platform, EC and also on the machine running the Desktop Client. The PSE's in all of these environment must have the same security certificate and SNC library path. For more information on setting up the SNC, refer to Setting Up the SNC . SNC Library Path Enter the full path of the SAP Cryptographic Library files ( SAPCRYPTOLIB sar files). The files are required for you to use the SAP SNC. You can download the files from the SAP Portal. SNC Name Enter the SNC PSE's distinguished name where the SAP RFC agent is hosted on with the 'p:' prefix. Example: p:CN=CnName, O=MyCompany, C=Country SNC Partner Name Enter the SNC PSE's distinguished name of the target, the SAP RFC system with the 'p:' prefix. Example: p:CN=CnName, O=MyCompany, C=Country SNC Level Protection Select the level of protection to apply to the connection. 1: Authentication only - The SNC verifies the identity of the communication partner. No data protection is applied for this level. 2: Integrity protection - The SNC detects any changes or manipulation of the data, which may have occurred between the two end points of a communication. It will also apply the Authentication only level of protection. 3: Privacy protection - The SNC encrypts the communication as well as apply all the previous levels of protection onto the data. 8: Apply the default level of protection. The default is set to Level 3. 9: Apply the maximum level of protection. Enable SNC SSO Enable this checkbox if the SAP RFC system uses Single Sign-On with its SNC. Test Connection Click this button to test the connection to the SAP RFC system. SAP RFC Functions Click the Add button to add the required RFC functions. Generate RFC UDR Click this button to generate the UDR for the selected SAP RFC function, according to the structure in SAP. When the UDR has has been generated, click Save to save the UDR to the profile.

---

# Document 546: Web Service Request Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741165/Web+Service+Request+Agent
**Categories:** chunks_index.json

The Web Services Request agent can be compared to a client and allows Web Service requests to be sent. The response, if any, can then be routed into the workflow. The section contains the following subsections: Web Service Request Agent Configuration Web Service Request Agent Input/Output Data, MIM and Events

---

# Document 547: Database Storage - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/610664921/Database+Storage
**Categories:** chunks_index.json

This tab is activated only when the Database Storage masking method is selected in the Fields tab. To use the database storage method, you need to create a table to store the mappings between masked and unmasked data. It is important that there are unique indexes for both the masked and unmasked data since the masking method implementation is dependent on the database constraints to ensure consistency in the data table. The following example shows what a definition with a single storage field in an Oracle database may be: CREATE TABLE masking_info ( unmasked NOT NULL, masked NOT NULL, CONSTRAINT masking_info_pk PRIMARY KEY (unmasked) ); CREATE UNIQUE INDEX idx_masking_info_masked ON masking_info (masked); The following settings are available in the Database Storage masking method in the Data Masking profile. Setting Description Setting Description Data Model Database Select the database profile to be used. Table Select the available table in the selected database profile. Unmasked Enter a name for each unmasked storage field. Masked Enter a name for each masked storage field. Key This checkbox is selected for each field by default. If selected, all selected fields will be looked up when unmasking data. Note! If you have a large table or huge amount of lookups, you may consider to only select the necessary fields to be looked up when unmasking data. Advanced Queue Size Sets the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the maximum number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements for retrieving data.

---

# Document 548: FTPS Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033201/FTPS+Collection+Agent
**Categories:** chunks_index.json

The FTPS collection agent collects files from a remote file system and inserts them into a workflow, using the standard FTP (RFC 959) protocol. When activated, the collector establishes an FTPS session towards the remote host. On failure, additional hosts are tried if so configured. On success, the source directory on the remote host is scanned for all files matching the current Filename settings, which are located in the Source tab. In addition, the Filename Sequence service may be used to further control the matching files. All files found will be fed one after the other into the workflow. The agent also offers the possibility to decompress compressed (gzip) files after they have been collected before they are inserted into the workflow. When all the files are successfully processed, the agent stops to await the next activation, scheduled or manually initiated. The FTPS collection agent supports IPv4 and IPv6 environments. The section contains the following subsections: FTPS Collection Agent Configuration FTPS Collection Agent Transaction Behavior FTPS Collection Agent Input/Output Data and MIM FTPS Collection Agent Events

---

# Document 549: System Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881382/System+Properties
**Categories:** chunks_index.json

This section describes the system properties, a type of attribute, that you can set in the STR and in various configuration files. The system properties that you can set in STR are divided into the following categories: Cell properties - applicable on a system level Container properties - applicable on a container level Execution Context properties - applicable to ECs Platform properties - applicable to the Platform instance Service Context properties - applicable to SCs High Availability properties - related to high availability Log properties - related to logging Database properties - related to Oracle, PostgreSQL and SAP HANA connections, i.e. to the MedationZone database and external databases. Desktop properties - default properties for connected Desktops The properties set in your system and in each container, after the initial installation, depend on the container type and the installation settings. Properties that are not defined in the STR are set to their respective default values. You can configure properties for services that should run on Pico instances, e g SCs. For further information about these properties, see Managing Service Configurations . For general information about how to set attributes in the STR, see System Topology Registry and Managing Picos with Topo . This chapter includes the following sections: Cell Properties Container Properties High Availability Properties Execution Context Properties Platform Properties Log Properties Database Properties

---

# Document 550: Workflow Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670835/Workflow+Event
**Categories:** chunks_index.json

This is the parent for all Workflow events. The following fields are included: workflowKey - The name of the internal workflow key. workflowName - A list indicating what workflow(s) information to select. workflowGroupName - The name of the workflow group. The following fields are inherited from the Base event, and described in more detail in Base Event : category contents - Workflow: <Workflow name> eventName origin receiveTimeStamp severity timeStamp

---

# Document 551: SCP Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674705
**Categories:** chunks_index.json

This section includes information about the SCP forwarding agent transaction behavior. For information about the general transaction behavior, see Workflow Monitor . Emits The agent does not emit anything. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is first created in the target directory, if not already created. Then, a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is closed and, finally, the file is moved from the temporary directory to the target directory. If backlog functionality is enabled an additional step is taken where the file is moved from DR_TMP_DIR to DR_POSTPONED_MOVE_DIR and then to the target directory. If the last step failed the file will be left in DR_POSTPONED_MOVE_DIR. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 552: Amazon S3 Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031750/Amazon+S3+Forwarding+Agent
**Categories:** chunks_index.json

The Amazon S3 forwarding agent creates files on the location stated in a referenced File System profile, containing the received data. Files are created when a Begin Batch message is received and closed when an End Batch message is received. In addition, the Filename Template service offers the possibility to compress (gzip) the files, or to further process them, using commands. To ensure that downstream systems will not use the files until they are closed, they are stored in a temporary directory until the End Batch message is received. This behavior also applies to Cancel Batch messages. If a Cancel Batch is received, file creation is canceled. The section contains the following subsections: Amazon S3 Forwarding Agent Configuration Amazon S3 Forwarding Agent Events Amazon S3 Forwarding Agent Input/Output Data and MIM Amazon S3 Forwarding MultiForwardingUDR Input Amazon S3 Forwarding Agent Transaction Behavior

---

# Document 553: JSON Formats - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783675/JSON+Formats
**Categories:** chunks_index.json

MediationZone also includes direct support for encoding and decoding JSON data.

---

# Document 554: Data Masking Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998444/Data+Masking+Profile
**Categories:** chunks_index.json

In the Data Masking profile, you configure the masking method you want to use, which UDR types and fields you want to mask/unmask, and any masking method-specific settings. There are four different masking methods that you can use: Crypto - Uses cryptographic algorithm that can be configured to either derive its key from a passphrase or a Keystore. It uses either AES-128 or AES-256 for data encryption. The data can be unmasked later when required. Database - Enables data model masking to store masked and unmasked data. The data can be unmasked later when required. Hash (one way) - Employs a salt-based encryption scheme for obscuring data only. All masked data using this method cannot be unmasked. Hash/Database - Uses a combination of the database and hash mode. The data can be unmasked later when required. For more information on the supported data types, see Supported Data Types . Configuration To create a new Data Masking profile, click the New Configuration button from Build View , and then select Data Masking Profile from the selection screen. The contents of the buttons bar may change depending on which configuration type has been opened in the currently displayed tab. The Data Masking profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit button content is specific to the Data Masking profile configurations. The Data Masking profile consists of the following tabs: 1 Fields Tab 2 Crypto Tab 3 Database Tab 4 Hash Tab 5 Hash/Database Tab Fields Tab The masking method that is selected in the Fields tab determines which of the other four tabs that will be active as these tabs contain masking method specific configurations. The Fields tab in the Data Masking profile configuration contains the following settings: Open Data Masking Profile - Fields tab Setting Description Setting Description Masking Method Select the masking method to be used from the drop-down list. Storage Fields Add the fields to map the UDR fields to. This section is only applicable to Database Storage and Hash/Database masking methods. UDR Field Mappings Add all the UDR types and fields for the profile to process. Random Algorithm (Only for String type) Specify the algorithm to be used for generating the random character. The supported algorithms are: Default : Default random algorithm. For Crypto, it only supports Base64 format where the Hash or Database are using mixture of alphanumeric and special characters. The supported characters list are: [!, ", #, $, %, &, ', (, ), *, +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, <, =, >, ?, @, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, [, , ], , _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, {, |, }, ~] UUID 4 : Generate UUID string in 8-4-4-4-12 format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Custom : Filtered character based on Default character sets, filter condition can be configured through Regex Pattern For more information on the algorithms for each masking method, see Supported Random Algorithm Type . This section is only applicable to Database Storage , Hash and Hash/Database masking methods. This section is disabled if other masking method is selected. Regex Pattern This is field is enabled when the Custom option is selected. It is a regular expression to extract characters based on the default characters list. String Length This is field is enabled when the Custom option is selected. Specify the length of the output string. Output Format This field is non-editable. It displays the supported character list and sample output preview. Supported Random Algorithm Type The supported random algorithm types for each masking method are as follows: Algorithm Crypto Database Hash Hash/Database Algorithm Crypto Database Hash Hash/Database Default UUID 4 Custom Crypto Tab This tab is enabled only when the Crypto masking method is selected in the Fields tab. Open Data Masking Profile - Crypto tab Setting Description Description Setting Description Description Cipher Mode Cipher Mode to use. The two modes CTR and GCM are non-deterministic in the sense that they will give different outputs for the same input. This means that it will not be possible to correlate data from separate UDRs, but if this is not a requirement then it gives a more complete anonymization. The CBC mode is deterministic and can be used when correlation must be possible on pseudonymized data. It includes a transposition scrambling to protect against prefix matching. The ECB mode is not recommended since it allows for prefix and suffix matching and thus gives weaker security than the CBC mode. It is only included for backwards compatibility. Derive Key from Passphrase Select this option for the cryptographic engine to use a key from the passphrase. The Passphrase and Algorithm fields will be enabled. Passphrase Enter a passphrase manually or click the Random button to generate a random key. The passphrase is then hashed and it is use as the key. If you use a random passphrase and it has been changed, you will not be able to unmask any masked data prior to the change. Algorithm Select the algorithm to be used, either the AES-128 or AES-256 . This can only be used for fields of string and bytearray types. Read Key from Keystore Select this option to use a key from a designated keystore. The keystore must be a JCEKS. The Keystore Path , Keystore Password , Key Name and Key Password fields will be enabled. Example - Creating a symmetric crypto key $ keytool -keystore test.ks -storepass password -storetype jceks -genseckey -keysize 128 -alias testkey -keyalg AES Keystore Path Enter the path to the keystore file. Keystore Password Enter the associated password. Key Name This field is optional. Enter the associated key name. Key Password This field is optional. Enter the associated key password if required, otherwise the Keystore Password is used as the default password. Database Tab This tab is enabled only when the Database Storage masking method is selected in the Fields tab. Open Data Masking Profile - Database tab Setting Description Setting Description Database Model Database Browse and select the Database profile to use. Table Select the database table to view the following information: Field : Shows the field name Unmasked : Shows the unmasked content Masked : Shows the masked content Key : The selected checkbox shows the fields that will be searched when unmasking data. If you have a large table or huge amount of lookups, you may consider to select the necessary fields only for searching when unmasking data Advanced Queue Size Set the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the maximum number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements to retrieve data. Hash Tab This tab is enabled only when the Hash masking method is selected in the Fields tab. Open Data Masking Profile - Hash tab Setting Description Setting Description Salt Enter the entry of the relevant hash or click the Random button to generate a random entry. Hash/Database Tab This tab is enabled only when the Hash/Database masking method is selected in the Fields tab. Open Data Masking Profile - Hash/Database tab Setting Description Setting Description Data Model Database Browse and select the Database profile to use. Table Select the database table to view the following information: Field : Shows the field name Unmasked : Shows the unmasked content Masked : Shows the masked content Key : The selected checkbox shows the fields that will be searched when unmasking data. If you have a large table or huge amount of lookups, you may consider to select the necessary fields only for searching when unmasking data Hash Salt Enter the entry of the relevant hash or click the Random button to generate a random entry. Advanced Queue Size Set the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements to retrieve data. Supported Data Types The supported data types for each masking method are as follows: Data type Crypto Database Hash Hash/Database Data type Crypto Database Hash Hash/Database string integer long short double byte bytearray

---

# Document 555: Diameter Request Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999692/Diameter+Request+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. Debug Events There are no debug events for this agent.

---

# Document 556: APL - PCC BatchData Support - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743235/APL+-+PCC+BatchData+Support
**Categories:** chunks_index.json



---
**End of Part 25** - Continue to next part for more content.
