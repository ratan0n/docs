# RATANON/MZ93-DOCUMENTATION - Part 71/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 71 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~61.2 KB
---

An Event Notification configuration offers the possibility to route information from events generated in the system, to various targets. These targets include: Azure Application Insight Database Log File Send Mail Send SNMP Trap Send SNMP Trap, Alarm SNS Topic System Log There are several different event types that all contain specific data about the particular event. Besides being logged, events may be split up and selected parts may be embedded in user defined strings. For instance, consider an event originating from a user, updating an existing Notifier: userName: mzadmin3 , userAction: Notifier AnumberEvents updated . This is the default event message string for User Events. However, it is also possible to select parts of the information, or other information residing inside the event. Each type of event contains a predefined set of fields. For instance, the event message previously exemplified, contains the userName and userAction fields which may be used to customize event messages to suit the target to which they will be logged: Open Events can be customized to suit any target Note! The Category field in the above picture is left empty intentionally, since it does not have a value for this specific event. A category is user defined and is entered in the Event Categories dialog. It is a string which will route messages sent with the dispatchMessage APL function. The event types form a hierarchy, where each event type adds its own fields and inherits all fields from its ancestors. The event hierarchy is structured as follows: Base Alar Code Manager Group System Security User Workflow Agent Agent Failure Agent Message User Agent Message Agent State ECS Insert Debug Dynamic Update Workflow State External Reference <User Defined> Each event type and its fields are described in Event Types . This chapter includes the following sections: Event Notifications Configuration Event Types Event Category

---

# Document 1682: Encoder Agent Transaction Behavior Batch and Input/Output Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999846/Encoder+Agent+Transaction+Behavior+Batch+and+Input+Output+Data
**Categories:** chunks_index.json

Data Operations - Batch Workflow This section includes information about the Encoder agent transaction behavior. For information about the general transaction behavior, see Workflow Monitor . This agent does not send out anything. The agent acquires commands from other agents and based on them generates a state change of the file currently processed. Command Description Begin Batch Possible headers defined in the Header tab are created and dispatched on all outgoing routes before the first UDR is encoded. End Batch Possible trailers defined in the Trailer tab are created and dispatched on all outgoing routes after the last UDR has been encoded. Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The agent produces bytearray type and consumes the UDR types corresponding to the selected Encoder. If Suppress Encoding is enabled bytearray type is consumed.

---

# Document 1683: The Commands Used - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655836/The+Commands+Used
**Categories:** chunks_index.json

The main benefits with the vcexport / vcimport commands compared to the systemexport / systemimport commands are: The exported configurations are easier to read, which also makes it easier to compare the new export with any previous export. The format is more compact than the traditional export format. vcexport usage: vcexport [options] This command exports configurations in a format that is adapted for version control systems. The vcexport command exports the configurations into a flat structure, i e with file extensions instead of directories. For each exported configuration, a .xsd file will be generated in which the structure of the data is stored, which will produce a more compact format than the other export commands can offer. vcimport usage: vcimport [options] This command imports exports made with the vcexport command. Note! If a key or name conflict occur, the imported data will not overwrite existing configurations.

---

# Document 1684: System Statistics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/312868905/System+Statistics
**Categories:** chunks_index.json

MediationZone constantly collects information from the different sub-systems and hosts within the system. Among other things, this information is used for load balancing. Using System Statistics, you can view, export, and import statistical information. Host Statistics The system collects statistics from the different machines hosting a Platform or Execution Context, such as the load of the CPU or the number of context switches. This is called host statistics. MediationZone uses the vmstat command to collect the information. This binary must be installed to have statistics collected and to perform load-balancing work for workflows. The following list holds all values collected from each host. On newer operating systems, some of these may not be available for collection due to changes in the kernel of the operation system. CPU User Time - This value shows how much time was spent in non-kernel specific code. This value is displayed in percentage. 100% means that all processing power is spent. See CPU System Time as well. CPU System Time - This value shows how much time was spent in kernel-specific code, such as scheduling of different processes or network transfer. This value is displayed in percentage. 100% means that all processing power is spent. Context Switches - The number of context switches per second. A context switch occurs when one process hands over information to another process. The more context switches, the less effective and scalable the system will be. Swapped To Disk - The amount of data that was swapped out. A large value indicates that the system does not have enough RAM to manage the memory requirements of the different processes. Swapped In From Disk - The amount of data that was read from swap. Processes Waiting For Run - Shows how many processes that are waiting to run. A high number indicates that the machine is not fast enough to manage the load. Processes Swapped Out - Processes that have persisted in swap due to insufficient available memory, or due to aggressive management of the memory layer. Processes In Sleep - The number of processes that are presently not doing anything. Pico Instance Every minute, the system collects memory information from the different Java processes defining the Platform and the Execution Context. This information shows how much memory is used and how much memory is available for the running process. Used Memory - Shows the amount of memory currently allocated by the running process. As Java is a language using garbage collection, this number may very well get close to the maximum memory limit without being a problem for the running process. However, if the amount of used memory is close to the maximum limit for a long time, the process needs more memory. This value is displayed in bytes. See the -Xmx and -Xms properties defined in the XML file defining the process. Maximum Memory - Shows the amount of memory that the process can use. This value is displayed in bytes. Process CPU Time - Shows the percentage of CPU time that has been used. Open File Descriptors - This is a Unix measurement that enables you to create a statistical diagram over the number of open files during the last minute, hour, or day. Garbage Collection Count - Shows the number of times the garbage collector has run since the last time statistics was collected. Garbage Collection Time - Shows the amount of time the garbage collector has run since the last time statistics was collected. This value is displayed in milliseconds. Thread Count - Shows the number of allocated threads. Workflow Statistics The system collects statistical data that is sampled every 5 seconds as long as a workflow is being executed. This information includes: Throughput - Displays workflow throughput. As long as a workflow is being executed, the system continuously samples the number of processed UDRs, or raw data, per second. Queue Throughput - Displays queue throughput per second for real-time queues. Statistics for real-time queues are only available when routing UDRs, not raw data. Note! To enable its convenient delegation to external systems, or to generate an alarm if the throughput falls too low, throughput is also defined as a MIM value for the workflow. For further information, see Throughput Calculation in Workflow Properties . Simultaneous - Displays the number of simultaneously running workflows. Queue Size - The size of the queue space that is being used at the time of the sample for each individual queue. Viewing the System Statistics To open the System Statistics, go to Manage and then select System Statistics under the Tools and Monitoring section. Upon first entry, you will see the landing page: Open System Statistics Landing Page You will be provided a choice of searching local statistics by clicking the Filter Local Statistics button or importing statistics with the Import External Statistics button. There are three different types of statistics: host, pico instance, and workflow. To display statistics in System Statistics, you have to use the Filter Local Statistics button, see the section below, Searching the System Statistics, or, for Import statistics, see System Statistics | Importing Statistics . Searching the System Statistics To search, click the Filter Local Statistics button to open the Filter dialog. In the Filter dialog, search criteria may be defined in order to single out the Statistics of interest. Open Filter dialog Setting Description Setting Description Period Select this option to display the statistics from the chosen time interval. Select one of the predefined time intervals; Last Hour , Today, Yesterday , This Week , Previous Week , Last 7 Days , This Month , Previous Month , or select the option User Defined and enter the start and end dates and times of your choice in the From and To fields. Note! If several criteria are enabled, an absolute match is displayed. For instance, if Host and Workflow are specified as well as Period , only the time for which there are both workflow measures and host measures is displayed. Scale Specifies the time scale to be used. There are three different time resolutions on which statistics are collected. The default value is Hour. Host Choose from a list of available hosts by selecting items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended host to filter by. Picos Choose from a list of available picos by selecting items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended pico to filter by. Workflows Choose from a list of available workflows by items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended workflow to filter by. Options in System Statistics Host / Pico / Workflow Views After selecting the hosts, picos and / or workflows, users may click on the Apply button to perform the search. The search cannot be performed when no options are selected from all three types of instances. Note! The default formula that the filter results will be displayed in is Average . Users may select either Minimum or Maximum later after filter result have been displayed. When the new formula is selected, all the values of the charts will be updated accordingly. Another feature within the filter dialog is the ability to select the Picos either by All available picos or by Picos that are running. Open Picos with the [R] label. Picos that are running are denoted with the label [R] before the pico name in the Picos dropdown list. Open System Statistics Simply click on the chart of choice to add to the instance view and deselect to remove it. For each type of statistics you have selected to view, you will see a Statistics type view drop-down displaying the statistical charts selected for the statistics measurement type. To add or remove other measurement types, click the Show / Hide Charts menu button found above each statistics view section of either hosts, pico or workflows statistic. The types of available charts in the drop menu to select from are mentioned above in the System Statistics | Host Statistics section for hosts, System Statistics | Pico Instance section for pico instances, and System Statistics | Workflow Statistics section for workflows. Take for example the show/hide charts list for pico as shown below Open Statistics type charts for pico instances For each statistics type you can see: View Description View Description Host View Each host has its statistics displayed in a separate color. A default measurement type chart is displayed upon first filter application Pico Instance View Each pico instance has its statistics displayed in a separate color. A default measurement type chart is displayed upon first filter application Workflow View Each workflow has its statistics displayed in a separate color, and if you have selected to view queue statistics, each queue has its own color. A default measurement type chart is displayed upon first filter application If you select to view Queue Throughput statistics and you have more than one route with the same name in a workflow, they are displayed separately by default, and are named in the format route name (source agent name), e.g. r_1 (collection1). If you want to group these queues together, e.g. due to legacy settings, select the Ignore Route Owner checkbox, and the source agent is no longer displayed per route. Note! When the Ignore Route Owner checkbox is selected, and there is more than one route with the same name, if the statistics are displayed in hour or day resolution, the average of all routes with the same name is shown. If the statistics are displayed on minute resolution, the statistics for one of the queues with the same name is shown, and there is no rule for which queue is shown. Exporting Statistics Exporting statistics may be useful for several purposes, for example if you want to share the statistical information with someone who does not have access to your system. To export the statistics: Click on the Export button. The Export dialog opens. Open Export Statistics dialog The export dialog resembles the filter dialog. You may choose which hosts / picos / workflows to be part of your export file according your desire time range as per your selection of the time period. After selecting your desired configurations, click on the Export button to save the zip file. It should be in your default download folder as per your browser setting. Hint! The export functionality can also be used to save statistics on a regular basis. Such as every month or every year, to use for comparison with the current statistics. Importing Statistics The Import functionality is used to import statistics that have previously been exported from a MediationZone system. When using the Import functionality you do not have to perform a search in order to display the statistics. Note! An import of statistics does not affect the data in the database, it just displays a snapshot of the statistics at the time it was exported. To import the statistics: In System Statistics, click Import . The Open dialog opens. Browse to the directory where the *.zip file you want to import is located. Select the file and click Open . The statistical information is now displayed in System Statistics. The same search criteria that were set in the Search System Statistics dialog when the statistics were exported is displayed. The date information at the top of the dialog displays the time interval for the imported statistics, and the text "Imported Statistics" also appears in red beside the date information. Disabling Collection of Statistics You can disable the collection of statistics on the Platform by setting the following Platform properties to false in platform.conf : mz.statistics.collect.all - All statistics mz.statistics.collect.pico - Pico statistics mz.statistics.collect.workflow - Workflow Statistics The default value of the properties above is true . When you set the property mz.statistics.collect.all to false , this overrides the settings of mz.statistics.collect.pico and mz.statistics.collect.workflow

---

# Document 1685: Merge Files Collection CollectedFile UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739624/Merge+Files+Collection+CollectedFile+UDR
**Categories:** chunks_index.json

The agent produces and routes CollectedFileUDR types with a structure described in the following example. Example - Using the CollectedFileUDR CollectedFileUDR: internal CollectedFileUDR { string fileName; string baseDirectoryPath; string subDirectory; // relative to base directory int sizeOnDisk; // will differ if file was compressed boolean wasCompressed; // true if file was decompressed on collection date fileModifiedTimestamp int fileIndex // Index number within the current merged batch, starts with 1. bytearray content; boolean isLastPartial; // True if last UDR of the input file int partialNumber; // Sequence number of the UDR within the file. 1 for first, 2 for second so on. };

---

# Document 1686: KPI Management Service Models - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611259/KPI+Management+Service+Models
**Categories:** chunks_index.json

In order to make full use of KPI Management, it is important to be familiar with the structure and objects used in service model definitions. Service models can be defined directly in a KPI Management profile, or provisioned via a REST API. This section includes the following sections: Service Model Definition KPI Output Object Types

---

# Document 1687: SQL Server - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605833/SQL+Server
**Categories:** chunks_index.json

This section contains information that is specific to the database type SQL Server. Supported Functions The SQL Server database can be used with: Audit Profile Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Database Collection/Forwarding Agents Event Notifications Prepared Statements (APL) SQL Collection/Forwarding Agents Task Workflows Agents (SQL) Note! For SQL Server, the column type timestamp is not supported in tables accessed by MediationZone. Use column type datetime instead. See also the System Administration Guide for information about time zone settings. Preparations The drivers that are required to use SQL Server database are bundled with the software and no additional preparations are required. Advanced Connection Configuration for SQL Server If required, you can set the selectMethod when you select the Advanced Connection Setup option. By default, the selectMethod property on all connections towards the SQL Server type of database is set to cursor . If required, you can set the property to direct by selecting the Advanced Connection Setup option, and entering a connection string manually. For example, if you require to use direct mode, you can add a connection string that will create connections using selectMode direct, as shown in the image below. Open Advanced Connection Setup for SQL Server

---

# Document 1688: <User Defined> Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030604/User+Defined+Event
**Categories:** chunks_index.json

A user defined event is a basic event, extended with any variables entered by the user. It is configured in the Ultra Format Editor : Example - User defined event event myEvent { ascii addedField1; int addedField2; }; A user defined event is of Workflow type therefore includes Workflow specific fields. The basic fields are automatically included in myEvent , along with the typed in fields. Population of the fields is done via an APL utilizing agent. Example - Usage of user defined event myEvent x = udrCreate( myEvent ); x.addedField1 = aUDR.anum; x.addedField2 = aUDR.code; dispatchEvent( theEvent ); The fields of a user defined event: agentName - The name of the agent issuing the event. category - A user defined category, as entered in the Event Categories dialog. If utilized, this field is set manually in the APL code. eventName - the name of the Event, as defined in Ultra. origin - the IP address of the Execution Context the Workflow issuing the event is running on. receiveTimeStamp - The date and time for when an event is inserted in the Platform database. This is the time used in for example the System Log. severity - The severity of the event. May be any of: "Information"," Warning", "Error", or "Disaster". The default value is Information. If another severity is required, this field must be set manually in APL to one of the strings: Information, Warning, Error, Disaster. timeStamp - The date and time taken fro the host where the event is issued. workflowKey - The name of the internal workflow key. workflowName - The name of the Workflow issuing the event. <any> - Any information, as stated in the format configuration. The contents field - Workflow name: <Workflow name>, Agent name: <Agent name>

---

# Document 1689: TCP/IP Related UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609644
**Categories:** chunks_index.json

TCPIPUDR The TCPIPUDR is the UDR type created by default in the TCPIP agent. It can be viewed in the UDR Internal Format Browser . To open the browser click Build  New Configuration  APL Code . In the editing area right-click and select UDR Assistance... to open the UDR Internal Format Browser . Open TCPIPUDR Field Description Field Description RemoteIP(ipaddress) The IP address of the client. RemotePort(int) The port through which the agent connects to the client. response(bytearray) The data that the agent sends back to the client. SequenceNumber(long) A per-connection unique number that is generated by the TCPIP agent. This number enables you to follow the order by which the UDRs are collected. The agent counter is reset whenever connection with the agent is established. The UDR fields RemoteIP , RemotePort , and SequenceNumber are accessible from the workflow configuration only if the TCP/IP agent is configured with a decoder that extends the built-in TCP/IP format. For further information see Decoder in the section Decoder Tab in TCP/IP Collection Agent Configuration . The TCPIPUDR cannot be cloned and the socket connection is not initialized if cloning is attempted. It is therefore recommended that you initialize every UDR from the decoder, and then route it into the workflow. TCPIPStateUDR You can configure the collection agent to track the connection state of the client. Whenever the client is connected or disconnected, the agent sends this UDR which contains the status of the connection each time it changes state. To activate this function, go to the TCP/IP tab and select the Send TCPIPStateUDR checkbox. See the TCP/IP Tab in TCP/IP Collection Agent Configuration . Open TCPIPStateUDR Field Description Field Description data (bytearray) This field is only populated when the collection agent tries to send a response to the client but the client is no longer connected. The agent then sends the TCPIPStateUDR back to the workflow with isConnected = false , and also including the response that APL wanted to send. During a normal state change, the value in the data field is null . ipAddress (string) The IP address of the client. isConnected (boolean) This field indicates if the client is connected or disconnected. When there is a new connection, this field is set to true . When a connection is terminated or disconnected, this field is set to false . port (int) The port of the client. response(bytearray) The data that the agent sends back to the client.

---

# Document 1690: Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744299/Overview
**Categories:** chunks_index.json

You manage XML parsing in UFDL by applying the xml_schema construct. xml_schema [: <xml_schema option>] { <XML Schema elements> }; The XML Schema Options The following XML schema options are available: output_encoding - Use this option to specify the type of encoding used. See the official Java SE Documentation from Oracle for information regarding supported encodings. xml_schema : output_encoding("UTF-8") { // ... } schemaLocation - Use this option to specify the location of a schema that contains qualified schema constructs. xml_schema : schemaLocation("http://example.com mydocument.xsd") { // ... } handle_as_string - Use this option to specify any other data type to be handled as a string data type. The example below results in all decimal data type to be handled as string. xml_schema : handle_as_string("decimal") { // ... } Note! Only one XML schema option can be used. The XML Schema Elements MediationZone follows the w3schools website for XML Schema Elements, https://www.w3schools.com/xml/schema_elements_ref.asp , with exceptions listed on page https://github.com/digitalroute/mz-example-workflows/blob/master/ultra_xml/limitations_examples.txt . For examples of what is not supported see https://github.com/digitalroute/mz-example-workflows/blob/master/ultra_xml/dr_xml_schema.bnf . Note! If you want to use union type, you must set the property mz.ultra.xml.restrictions according to your requirements. If you want to use unions and restrictions inside of unions, set this property to union . If you want to use restrictions everywhere, including inside the union type, set this property to on . For further information, see Platform Properties .

---

# Document 1691: Desktop User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638163/Desktop+User+s+Guide
**Categories:** chunks_index.json

Search this document: Chapters The following chapters are included: Desktop Overview Working with Workflows Event Notifications Alarm Detection Inspection Tools & Monitoring Appendix 1 - Profiles Appendix 2 - Batch and Real-Time Workflow Agents Appendix 3 - Task Workflow Agents Appendix 4 - Collection Strategies Legacy Desktop

---

# Document 1692: Decompressor Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032148/Decompressor+Agent
**Categories:** chunks_index.json

Configuration You open the Decompressor processing agent configuration dialog from a workflow configuration. To open the Compressor processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Decompressor from the Processing tab of the Agent Selection dialog. Open The Decompressor agent configuration dialog Setting Description Setting Description Compression Select decompression algorithm: No Compression: The agent will not decompress the files. Gzip: The agent will decompress the files by using gzip. LZO: The agent will decompress the files by using LZO. Error Handling Select how you want to handle errors for files that cannot be decompressed: Cancel Batch: The agent will cancel the batch when a file cannot be decompressed (Default). The default setting for Cancel Batch is to abort the workflow immediately, but you can also configure the workflow to abort after a certain number of consecutive Cancel Batches or to never abort the workflow on Cancel Batch. See Workflow Properties for further information about Workflow Properties. Ignore: The agent will ignore an input batch when a file cannot be decompressed, and a log message will be generated in the System Log, see System Log . Note! If you select the Ignore option, data will continue to be sent until an error occurs in a batch, which means that erroneous data might be routed from the Decompressor agent. Transaction Behavior This section includes information about the Decompressor agent's transaction behavior. For information about the general transaction behavior, see Workflow Monitor . The agent emits commands that change the state of the file currently processed. Command Description Cancel Batch Emitted if a file cannot be decompressed. If you have configured the workflow to abort after a certain number of consecutive Cancel Batches, or never to abort on Cancel Batch, in the Workflow Properties, the collection agent will send the file to ECS along with a message describing the error. See Workflow Properties and Error Correction System for further information. The agent does not retrieve anything. Input/Output Data Input/Output data is the type of data that an agent both recognizes and delivers. The Decompressor agent consumes and delivers bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes The agent does not publish any MIM resources. Accesses MIM Parameter Description Source Pathname This parameter is read from the workflow's collecting agent, in order to create accurate Agent Message Events. Source Filename This parameter is read from the workflow's collecting agent, in order to create accurate Agent Message Events. Agent Message Events The agent generates event messages according to your configuration in the Event Notification editor. Ignored batch OR Ignored batch. The file <filepath/filename> could not be decompressed. Reported when a batch cannot be decompressed and Error Handling is configured with ignore , see the section above, Configuration. Debug Events There are no debug events for this a gen t.

---

# Document 1693: Netia FTP Agent Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738122/Netia+FTP+Agent+Overview
**Categories:** chunks_index.json

The Netia FTP collection agent collects files from a remote file system and inserts them into a workflow using the standard FTP (RFC 959) protocol. The agent supports FTP for Unix, Windows NT and VAX/VMS machines. When activated, the collector establishes an FTP session towards the remote host. On failure, additional hosts are tried if configured. On success, the source directory on the remote host is scanned for all files matching the current filter. All files found are fed one after the other into the workflow. The agent also offers the possibility to decompress compressed (gzip) files after they have been collected before they are inserted into the workflow. When all the files are successfully processed, the agent stops to await the next activation, scheduled or manually initiated.

---

# Document 1694: GIT Support - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639122
**Categories:** chunks_index.json

The following section provides information on how Git is used to handle different system export scenarios when Git is used as a remote target. How Git is working in our product Git is a distributed version control system, more about Git can be read here . MediationZone only supports connecting to a remote repository that is accessible with an HTTP/HTTPS URL and a Token. When the File System Profile of type Git is saved, a clone of the Repository is created in a directory in the Platform. This directory is $MZHOME/gitrepos by default. It can be changed by setting the property mz.git.basePath to some other path accessible from the Platform. Each File System Profile will have its directory with its clone of the corresponding repository. The local copy of the repository is only updated to match the remote repository when these events occur: An import is performed A user clicks the Refresh button in the System Importer. After an export is performed, regardless of if it is successful or failed. When an export is performed to Git the system will: Delete all files in the selected target folder Perform an export to the selected folder Commit the changes to the local repository Push the commit to the remote repository This means that even if just a few configurations have been updated a full export of what was in the target folder from before needs to be made in order not to lose files in the repository version. During step 4, when the export is pushed to the remote repository, Git can detect if the target folder on the remote repository is updated compared to the local target folder. If an update on the remote repository is detected, the system will evaluate if it will cause any conflicting scenarios (described below). So, if the local repository is updated before an export (events described above) i.e. the local repository has the same status as the remote the system will not detect any conflict, and the system will overwrite the content in the target folder. Open Recommendations for the structuring of configuration The structure of the folders in the Git repository is not decided by the system. Several developers working in parallel on the same deployment calls for structured configuration to avoid too many merge conflicts. The recommendation therefore is to have a separate folder in the Git repository for each use case. Export Scenarios When exporting configurations to Git multiple scenarios can happen because the remote repository is updated by another System or person. Different scenarios are described below. Update in a different directory When there have been updates to any other target folder in the same repository the export will be successful. Open Export log when Git Repository is updated in other directory Updated configurations in the same export folder, no conflict When there has been updates in the remote repository to Configurations on other places than current System have modified in the same export folder the System Export will fail. The System Exporter will perform an Import of the Configurations that was changed on remote. They will be imported with a suffix named _ merge. The action needed by you now is: Move the change from the imported configurations to your local version of the same configurations. Then remove the configurations with the suffix. Alternatively you can remove your local version and rename the imported version to the same name as before if there has not been any updates to the local version. The Imported configuration with Suffix will have a different so called Key than the original. So if the Key is important for any other configuration referring to the configuration in question, or for the configuration itself you need to do the first option. This means that it is always recommended that in these cases the local version is retained whenever possible. Example: If the configuration is an Aggregation Profile with File Storage then the Aggregation Storage directory on disk is based on the Key of the Profile. When the actions mention above is performed a new export can be executed. Open Export log when same directory in Git is updated with no conflict Update same configuration in the same Git Directory When there has been updates in the remote repository to the same Configurations as the current System have modified in the same target folder the System Export will fail. The System Exporter will perform an Import of the conflicting Configurations. They will be imported with a suffix named _ conflict. The action needed by you now is: Move the change from the imported configurations to your local version of the same configurations. Then remove the configurations with the suffix. When the actions mention above is performed a new export can be executed. Open Export log when same configuration is updated in Git Configuration added, removed or renamed on Git in same Git Directory The scenarios when configurations is added and removed or renamed will look the same during an export. Configurations that are added in the same export folder by any other system will be imported with the suffix of _added. Actions need by you is: Rename the configurations with suffix _added , by removing the suffix. Configurations that are removed in the same export folder by any other system but exist in current system will be copied to a new configuration with suffix _removed. Actions need by you is: Remove the configurations with suffix _ remove and the local configuration (if it shall be removed) with the same name but without the suffix. When the actions mention above is performed a new export can be executed. Open Export log Configuration is added and removed or renamed

---

# Document 1695: Overview of MediationZone - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848908
**Categories:** chunks_index.json

MediationZone empowers organizations to liberate the value hidden in their usage information via a unique approach to managing data that supports multiple mission-critical aspects of their business. The system is designed in such a way that customers benefit from fewer integration points and flexible data management. MediationZone bridges components residing in any type of network architecture, providing comprehensive functionality that ensures systems can communicate with each other as effectively as possible. Information is created based on raw data distributed across any number of network elements and systems. Data streams are optimized and enriched, translating into reduced cost for hardware, software licenses and maintenance while providing new opportunities to differentiate and personalize services. MediationZone is designed for online and offline processing on one platform. File-based collection and processing is configured with the same graphical workflow technology that is used to create bidirectional real-time communication streams. Design and change in the system is achieved through configuration rather than hard-coding. An intuitive and powerful drag-and-drop management user interface covers all aspects of workflow life-cycle management. Once configured, workflows are automatically deployed on designated servers that are part of an installation and executed in accordance with defined scheduling criteria. High availability capabilities ensure that workflows are executed at all times. MediationZone Optimizes Data Streams Processing functions include analysis, filtering, cloning, splitting, routing, normalization, correlation/aggregation, de-duplication, validation, enrichment, and more. The standard agents can be configured to support operator-specific business logic without the need for customization of the standard product. Integration with external systems for data exchange is achieved with plug-ins, which implement the protocols to be used when communicating with other systems. A large number of protocols are supported off-the-shelf, both for interfacing as well as for secure and consistent operation. Support for additional protocols is added continuously as part of roadmap development as well as on an on-demand basis. MediationZone configuration and control logic is centrally managed while execution can be distributed to any number of nodes in a network. Hardware independence and distributed design ensures scalability with any combination of high-end servers and commodity hardware.

---

# Document 1696: Encrypt Password - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737275/Encrypt+Password
**Categories:** chunks_index.json

Encrypt Password allows you to encrypt any password and prints out the result onto the text field. To open Encrypt Password , go to Manage  Tools & Monitoring and then select Encrypt Password . Open Encrypt password configuration Setting Description Password The password to be encrypted will be entered here. All characters entered here will be masked. Open Click this icon to reveal the password. Click the icon again to mask the password. Generate with Alias Check this box to enable the option for the alias to be used when encrypting a password. Alias An alias to be used when encrypting a password. When this option is not used, the system default key will be used. If you want to use this option, the path and password to the Keystore has to be indicated by setting the Platform properties mz.cryptoservice.keystore.path and mz.cryptoservice.keystore.password . The keystore must also contain keys for all the aliases you want to use. For further information about these properties, see Platform Properties . Note! In order to use this option, an alias must have been generated with the Java key tool. Reset The reset button clears all data. Encrypt Displays the result of the encrypted password. To copy the value within, you must use the Copy button.

---

# Document 1697: Function Declarations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612151/Function+Declarations
**Categories:** chunks_index.json

Functions may be declared and called within the APL code using normal Java syntax. Example - Declaring functions int addNumbers (int a, int b){ return a + b; } consume { debug( "1 + 1 = " + addNumbers( 1, 1 ) ); } To declare functions with no return value, the void keyword should be used. The Synchronized Keyword In order to support multi-threading, functions updating global variables within realtime workflows must be made thread-safe. To achieve this, the function name is preceded with the synchronized keyword: Example - Using the synchronized keyword synchronized int updateSeqNo(){ seqNo = seqNo +1; return seqNo; } It is possible to read global variables from any function block. However, to avoid race conditions with functions updating the global variables, they must only be accessed from within synchronized functions. Note! The synchronized keyword is ignored when used in batch agents. To declare functions with no return value, the void keyword is used.

---

# Document 1698: derbybackup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657047/derbybackup
**Categories:** chunks_index.json

usage: derbybackup <backup directory> Performs a Derby database online backup. For further information about Derby backup, see Derby Database Backup and Restore . Return Codes Listed below are the different return codes for the derbybackup command: Code Description 0 Will be returned if the command was successful or unsuccessful. 1 Will be returned if the argument count is incorrect.

---

# Document 1699: Removing Pico Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657497/Removing+Pico+Configurations
**Categories:** chunks_index.json

Remove existing configurations by manually editing the file <pico name>.conf or use the mzsh topo unset command . Pico Level Attributes Run the following commands to remove a pico instance. $ mzsh topo unset topo://container:<container>/pico:<pico> Example - Removing a pico configuration $ mzsh topo unset topo://container:main1/pico:ec2 You can also remove (unset) individual pico attributes or objects. $ mzsh topo unset topo://container:<container>/pico:<pico>/val:<attribute> Container Level Attributes Run the following commands to remove a container instance. $ mzsh topo unset topo://container:<container> Important! You must remove all attribute levels of a container before you remove the container. Run the following command to remove (unset) an attribute on container level. $ mzsh topo unset topo://container:<container>/val:common.<attribute> Cell Level Attributes Run the following command to remove (unset) an attribute on cell level. $ mzsh topo unset topo://val:common.<attribute>

---

# Document 1700: Performance Tuning with File Storage - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031549
**Categories:** chunks_index.json

This page describes how to tune the Aggregation agent with file storage. Aggregation Cache The Aggregation agent can store sessions on the file system, using a storage server, but also in a cache. The maximum size of the cache will be determined by the Max Cached Sessions parameter in the Aggregation profile and the average size in memory of a session. It is difficult to estimate the exact memory consumption through testing but the following should be considered when implementing an Aggregation workflow: Try to keep the session data small. Specifically, do not use large maps or lists in the sessions. These will take up a lot of memory. If memory issues are encountered, try decreasing the Max Cached Sessions . In order to find out if the cache size is overdimensioned, you can study the memory of the Execution Context that is hosting the workflow in System Statistics. For information about System Statistics, see /wiki/spaces/MD93/pages/204742117 . To avoid a large aggregation cache causing out of memory errors, the aggregation cache detects that the memory limit is reached. Once this is detected, sessions will be moved from the memory cache to the file system. Note! This has a performance impact, since the agent will have to read these sessions from the file system if they are accessed again. The Aggregation agent will log information in the EC's log file if the memory limit has been reached and the size of the cache needs to be adjusted. You can also specify when updated aggregation sessions shall be moved from the cache to the file system by setting the Execution Context property mz.aggregation.storage.maxneedssync property in the relevant <pico> .conf file. This property shall be set to a value lower than Max Cached Sessions. For performance reasons, this property should be given a reasonably high value, but consider the risk of a server restart. If this happens, the cached data might be lost. Hint! To speed up the start of workflows that run locally (on the EC), set the Execution Context property mz.aggregation.storage.profile_session_cache property in the relevant <pico> .conf file to true (default value is false ). By doing so, the aggregation cache will be kept in memory for up to 10 minutes after a workflow has stopped. This in turn enables another workflow, that runs within a 10 minute interval after the first workflow has stopped, and that is configured with the same profile, to use the very same allocated cache. Note that since the cache remains in memory for up to 10 minutes after a workflow stopped executing, other workflows using other profiles might create caches of their own during this time. The memory space of the respective aggregation caches will add up in the heap. If the EC at a certain point runs out of memory, performance deteriorates as cache is cleared and, as a result, sessions have to be read from and written to disk. The profile session cache functionality is only enabled in batch workflows where the Aggregation profile is not set to read-only, and the storage is placed locally to the EC. Memory Handling in Real-Time Warning! In real-time, when memory caching without any file storage, i e Storage Commit Interval is set to zero, make sure that you carefully scale the cache size to avoid losing a session due to cache over-runs. An over-run cache is recorded by the system event in the System Log. For further information, see Aggregation Session Inspector . While the aggregation cache will never cause the EC to run out of memory, it is still recommended that you set the Max Cached Sessions low enough so that there is enough space for the full cache size in memory. This will increase system performance. Multithreading If you have many sessions ending up in timeout, you can improve the performance by enabling multithreading, i e use a thread pool, for the timeout function block in the Aggregation agent. When multithreading is enabled, the workflow can hand over sessions to the pool via the queue without having to wait for the read operations to complete, since the threads in the thread pool will take care of that. With many threads, the throughput of read operations completed per second can be maximized. Multithreading is enabled by adding the property mz.aggregation.timeout.threads , with a value larger than 0, in the relevant <pico> .conf file. Example - Enabling multithreading, setting the property value to 8 mzsh topo set topo://container:<container>/pico:<pico name>/val:config.properties.mz.aggregation.timeout.threads 8

---

# Document 1701: Advanced Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030963/Advanced+Tab
**Categories:** chunks_index.json

You use the Advanced tab to specify the number of consecutive erroneous login attempts permitted by a user, enable logging in the System Log when a user fails to log in to the system, and configure user authentication by selecting the relevant authentication method. Open Access Controller - Advanced tab screen Options Description Options Description Login Number of Consecutive Erroneous Login Attempts To configure the maximum number of consecutive failed login attempts, open the Advanced tab, and set a value in Number Of Consecutive Erroneous Login Attempts . The default is 3. This feature is only enabled when Enhanced User Security is activated. When the maximum number of failed login attempts is reached, the user account is locked. For more information, refer to Enhanced User Security . When a user account is locked, the password settings for the user account must be updated in the Users tab, unless Enable Automatic Unlocking Of Users is selected. Enable Logging for User Login To configure the system to log failed attempts in the System Log, open the Advanced tab , and select the check box Enable Logging For User Login . Successful logins and locked accounts are always logged regardless of this setting. Enable Automatic Unlocking Of Users This checkbox is available when enhanced user security is enabled. For more information, refer to Enhanced User Security Select this check box to automatically unlock accounts disabled due to failed login attempts. Accounts that have been manually disabled from the Users tab are not affected by this setting. Time Before Automatic Unlocking (Minutes) This field is enabled when the checkbox for Enable Automatic Unlocking Of Users is checked. Enter the time that should pass before a locked account is automatically unlocked by the system. The minimum value is 1 minute. Authentication Reauthenticate Users after Inactivity To configure the system to reauthenticate users after a period of inactivity in the Desktop or mzsh shell (interactive mode), open the Advanced tab, and select the check box Reauthenticate Users After Inactivity . Time Before Reauthentication (Minutes) This field is enabled when the checkbox for Reauthenticate Users After Inactivity is checked. Set the maximum inactive time here. On the Desktop, the duration of time that the user does not perform any actions is counted as inactive time, regardless of ongoing processes. However, users are not logged out due to inactivity but must authenticate again to continue the session. In the mzsh shell, the duration of time that the user does not press any key is counted as inactive time, provided that there is no ongoing command execution. Users are logged out due to inactivity and are prompted to enter the password again. Authentication Method There are two selections available in this dropdown list: Default, and LDAP. User authentication is by default performed on the desktop. Alternatively, you can connect the Platform to an external LDAP directory for delegated authentication. This facilitates automation of administrative tasks such as the creation of users and assigning access groups as mentioned in LDAP Authentication By selecting LDAP, more fields for LDAP settings will be displayed. Open Access Controller - Advanced tab - LDAP settings screen

---

# Document 1702: FTP EWSD Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607616/FTP+EWSD+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/output data This section includes information about the data type that the agent expects and delivers. The agent consumes bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop .. Publishes MIM Parameters Description File Retrieval Timestamp This MIM parameter contains a timestamp, that indicates when the file transfer starts. File Retrieval Timestamp is of the date type and is defined as a header MIM context type. Source Filename This MIM parameter contains the name of the file that is currently being processed, as defined at the source. Source Filename is of the string type and is defined as a header MIM context type. Source Host This MIM parameter contains the IP address or hostname of the switch. Source Host is of the string type and is defined as a global MIM context type. Source Username This MIM parameter contains the login name. Source Username is of the string type and is defined as a global MIM context type. CURR-FSIZE This parameter is assigned with the value of the corresponding field of the LIST command output. CURR-FSIZE is of the string type and is defined as a header MIM context type. Accesses The agent itself does not access any MIM resources.

---

# Document 1703: Preparing the Installer File for Platform - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029543/Preparing+the+Installer+File+for+Platform
**Categories:** chunks_index.json

To unpack the software, download the *.tgz file for the version you want to install via the link provided in Release Information and prepare the license file: Note! Contact your Sales Representative to obtain your license file. Create a directory to use when unpacking this release and future releases. For the purpose of these instructions, this designated directory is referred to as the staging directory . Place the *.tgz file from your release delivery into the staging directory . Use a command line tool, go to the staging directory , and unpack the *.tgz file by running the following command: $ tar xvzf <filename>.tgz A directory is then created in the staging directory , containing the software to be installed. For the purpose of these instructions, this directory is referred to as the release content directory . Now copy the MZ license file into the release content directory . $ cp mz.license <release content directory> Enter the release content directory and prepare the install.xml file by running the following command: $ cd <release content directory> $ ./setup.sh prepare The *.mzp packages are now extracted, and the install.xml is extracted into the release content directory . Hint! For more information about the the installation procedure, use the following command: ./setup.sh help to view a description of the different steps in the different installation and upgrade scenarios.

---

# Document 1704: SQL Processing Agent Configuration - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002196/SQL+Processing+Agent+Configuration+-+Real-Time
**Categories:** chunks_index.json

To open the SQL Real-time Processing agent configuration dialog from a workflow configuration, you can do either one of the following : double-click the agent icon select the agent icon and click the Edit button The Agent Configuration contains configuration data that is related to the target database and the UDR Type. Open The SQL forwarding agent configuration dialog Setting Description Setting Description Database Profile defining the database that the agent connects and forwards data to. When selecting the Browse button next to the field it will open a browser where only one database profile can be selected. For further information about database profile setup, see Database Profile . UDR Type The UDR type that the agent accesses as input type. When selecting the Browse button next to the field it will open the UDR Internal Format Browser and one and only one UDR type can be selected. SQL Statement The user enters an SQL statement that the system sends to the database. By right clicking in the pane, selecting MIM Assistance... , the MIM Browser appears. The user can select a MIM value to be used in the SQL query. The value of the MIM is used in the SQL query during execution. The name of the MIM Value for example "Workflow.Workflow Name" is displayed in blue color as "$(Workflow.Workflow Name)" in the text field. By right clicking in the pane, selecting UDR Assistance... , the UDR Internal Format Browser appears. The user can select a field from the UDR specified in the UDR Type selector. The name of the UDR field name for example "UDR.Fieldname" is displayed in green color as "$(UDR.Fieldname)" in the text field. If the input type UDR is changed after writing the SQL syntax, the GUI validation fails (unless the different UDRs have identical field names). The field value is used as an input variable in the SQL Statement in the same way as MIM values are used. There is support for Stored Procedures. When using the forwarding agent, use JDBC to call a stored procedure in the same way as a normal call. The exact supported syntax for stored procedures varies between databases. An example of a procedure with two input arguments could have the following SQL statement: Example - SQL statement for a procedure with two input arguments CALL test_proc($(UDR.field1), $(UDR.field2)) Note! The statement syntax of the statement is not validated in the GUI, but references to MIM values are validated. If an incorrect SQL statement is entered, this generates an exception during runtime. Volume (UDRs) The number of UDRs to be processed between each database commit command. This value may be used to tune performance. If tables are small and contain no Binary Objects, the value may be set to a higher value than the default. Default is 1000. Time (sec) The commit window in seconds. If there is no commit within the time set, when the time set is reached, the agent performs a database commit. The default value is 60. No of threads The number of worker threads. Modify this option for performance tuning. The number of worker threads should be less than the connection pool size. The default value is 2. Queue Max Size The maximum number of batches of UDRs in the queue. The size depends on the configured volume. See the Volume (UDRs) option. The default value is 10000. Queue full retry intervals (s) If the queue is full, the agent waits for the configured interval before trying to insert data into the queue again. The default value is 5. Queue full max retries Set the maximum number of retries if the queue is full. If the number of retries is exceeded, an errorUDRList is sent out containing all the failed UDRs. The default value is 3. UDR TTL time (ms) The UDR Time to Live time in milliseconds. A UDR can only remain in the queue for the time set. If a UDR expires, the MIM value No of discarded expired UDRs , is incremented. The default value is 3000. Route on SQL Exception Select this check box to route error information upon SQL exception. Such exceptions are filtered by the rule that you specify in the Regular Expression Criteria editing pane. Instead of aborting the workflow due to these exceptions, the workflow proceeds to the agent that you can now route the selected exceptions to. Note! Since the error message contains linefeed, the regular expression has to adjusted according to this. Start the regular expression with "(?s)" to ignore linefeed, for example: (?s).*ORA-001.* Retry Commit for every UDR in batch In case of failure during an insert or commit, if you select this option, the agent retries for every single UDR in the batch. You determine the number of UDRs to be processed by setting the Volume (UDRs) option. Regular Expression Criteria Use the Java Regular Expression syntax convention when you enter the expression that selects the SQL error messages. The SQL errors that match this criteria enable the agent to identify the data that should be routed further along the workflow. When the agent identifies erroneous data it generates an Agent Message Event. For further information, see Agent Event . Note! MediationZone specific database tables from the Platform database should never be utilized as targets for output as this might cause severe damage to the system in terms of data corruption that in turn might make the system unusable.

---

# Document 1705: MediationZone Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783709/MediationZone+Desktop
**Categories:** chunks_index.json



---
**End of Part 71** - Continue to next part for more content.
