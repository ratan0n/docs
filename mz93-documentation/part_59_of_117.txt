# RATANON/MZ93-DOCUMENTATION - Part 59/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 59 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.5 KB
---

Usage systemimport [ -s|-skipexisting ] [ -pp|-preservepermissions ] [ -nameconflict re|an|sk ] [ -keyconflict re|an|sk ] [ -namekeyconflict rn|rk|an|sk ] [ -he|-holdexecution [ r|sr|sir|wr ] ] [ -nr|-norollback ] [ -no|-newowner ] [ -cp|-configfrompackage ] [ -iv|-importextrefvalues ] [ -select <xml-selection file> [ -dryrun ] [ -onerror < ABORT | IGNORE > ] ] [ -m|-message ] [ -u|-upgrade ] [ -dryrun ] <export file|directory> [password] ---------------------------------------------------- Import Conflict re = Replace sk = Skip an = Add New rk = Replace Key conflict rn = Replace Name conflict Holdexecution r = Restart sr = Stop and Restart sir = Stop Immediate and Restart wr = Wait for completion and Restart This command imports a ZIP file, an MZP file, or a directory. If an import entry is in conflict with an entry that already exists in the current system, the entry will not be imported. An example of such a conflict might be an identical entry name but a different ID. If you try to import the same MZP file or workflow package twice you get an error. The error is given if the checksums of MZP files are identical. Note! If you export the same configuration twice the checksums will differ. A workflow group that is scheduled to start while an import activity is happening, will not start until the import is complete. Options Option Description Option Description [-s|-skipexisting] If you use the skipexisting option, the imported data will not overwrite existing configurations that have the same key, name, type, and folder. This means that repeatedly importing a configuration, does not overwrite the data. [-pp|-preservepermissions] When user permissions are modified, set the preservepermissions parameter to prevent user permissions in the current system from being updated while importing a configuration. [-nameconflict re|an|sk] Decide what to do when there is a name conflict; replace, add new, or skip. Default is to skip the item when there is a name conflict. Note! If -nameconflict is not specified and there is a name conflict, the configuration is skipped. To avoid this, use -dryrun to verify before importing. [-keyconflict re|an|sk] Decide what to do when there is a key conflict; replace, add new, or skip. Default is to skip the item when there is a key conflict. Note! If -keyconflict is not specified and there is a key conflict, the configuration is skipped. To avoid this, use -dryrun to verify before importing. [-namekeyconflict rn|rk|an|sk] Decide what to do when there is a key or name conflict; replace, add new, or skip. Default is to skip the item when there is a key or a name conflict. Note! If neither of -namekeyconflict, -keyconflict, or -nameconflict is specified and there is a key or name conflict, the configuration is skipped. To avoid this, use -dryrun to verify before importing. [-he|-holdexecution [ r | sr | sir | wr ] ] Use the holdexecution option to prevent scheduled workflow groups from being started while importing configurations. If a workflow or a workflow group does not stop within 5 minutes (300 seconds) when doing a systemimport with either one of the following holdexecution parameters: sr , sir , and wr , a timeout will occur. You can change the timeout value by setting the Platform property mz.import.suppress.timeout . When the import is completed and a workflow group is still running, systemimport -holdexecution [ r | sr | sir | wr ] awaits the current running workflow member to come to a stop, and then restarts the whole group instead of continuing the execution of the member that follows. If you do not specify any of the r , sr , sir or wr options: A batch workflow or a workflow group will remain suppressed until all the workflows finish executing. Then, the workflow or the members of the workflow group, become idle. A real-time workflow group will return to the running state. systemimport -holdexecution generates events. To retrieve the events data, configure an Event Notification to transfer it according to your preferences. holdexecution Parameters Select the action that should resolve held executions: r (restart): When the import activity is done, and a workflow group is partly executed, specifying this option will restart that workflow group. Workflow groups that are fully executed, will not be restarted. A workflow that is started manually, will be restarted if it has been stopped. sr (stop and restart): Stops the workflows or workflow groups that are still running after an import is complete, waits for them to come to a stop or finish processing a batch, and then restarts synchronously all the workflow groups and all the manually started workflows that had not executed completely within the timeout boundaries. Note! A workflow that becomes unreachable after a system import has begun, will be restarted only when and if the contact with the Execution Context that it runs on, is regained while still importing. If a workflow is unreachable when system import is started, the import is aborted and the following error message is generated: Abort Import: At least one wf that is Unreachable . sir (stop immediately and restart): Stops the workflows or workflow groups that are still running after an import is complete, even in the middle of processing a batch, and synchronously restarts all the workflow groups and all the manually started workflows that had not executed completely within the timeout boundaries. Note! An unreachable workflow is restarted once contact with the Execution Context that it runs on, is regained. wr (wait for completion and restart): After an import is complete, synchronously restarts all the workflow groups and all the manually started workflows that had not executed completely within the timeout boundaries. Note! Any workflow that runs past the timeout limit is restarted as soon as it completes execution. Example - Using the holdexecution Option $ systemimport -s -he r export.zip $ systemimport -s -he sr export.zip $ systemimport -s -he sir export.zip $ systemimport -s -he wr export.zip [-nr|-norollback] When you us the systemimport command, a file that contains rollback information will be created. This file contains data about configuration changes during the system import, and is saved in the directory where you apply the command. You use the rollback file to undo a system import and return to the configuration that you had before the system import. Note! Use the importrollback command only to revert the systemimport command and not for the purpose of a system rollback, see mzcli - importrollback . To suppress the creation of the rollback file, provide either a nr or a norollback option. [-no|-newowner] Change ownership of the configuration on import. The user must match an user already defined in the Access Controller . [ -cp|-configfrompackage ] Add this option if you would like to import the Workflow Package (MZP) as configuration instead of a Workflow Package. [ -iv|-importextrefvalues ] Add this option if you would like to import the values from External References. [-select <xml-selection file> [-dryrun ] [-onerror < ABORT | IGNORE > ] ] Note! Do not use -select for Workflow Packages (MZP files). The -select <xml-selection file> parameter enables you to: Provide systemimport with an XML file that specifies your selection of configurations and workflow tables. Use [ -dryrun ] to test data compatibility prior to actually importing Note! Dryrun is only applicable to ZIP file and directory. Use [ -onError < ABORT | IGNORE > ] to manage an occurrence of an error XML Selection File This selection information that you find in the XML selection file corresponds to the selection information that you specify on the System Import tool view, in the Desktop user interface. The XML selection file consists of two main tags: <configurations> : contains your configuration import selections <workflows> : contains your workflow tables import selections <configurations> Select configurations from the <Export file|directory> that systemimport imports. Use the resolveDependencies attribute to either include (true), or ignore (false), dependent configurations. See the following example: Example - The resolveDependencies attribute <import> <configurations> <!-- Ignoring dependencies of the Default.x configuration--> <configuration name="Default.x" resolveDependencies="false"/> <!-- Including dependencies of the Default.y configuration--> <configuration name="Default.y" resolveDependencies="true"/> <!-- Ignoring dependencies of the Default.z configuration--> <!-- Note: Equal to selection of Default.x above --> <configuration name="Admin.C07E02_DEMO_BWF"/> <!-- Ignoring dependencies of configurations within the folder --> <configuration foldername="systemunits"/> <!-- Including dependencies of all the configurations in the folder --> <configuration foldername="billing" resolveDependencies="true"/> </configurations> </import> To import all the configurations that are included in a specific folder, include the following text in the XML file: <configuration foldername="myFolder"/> Note! If you specify configuration selections in the XML file that do not exist in the Export file, a warning is generated. <workflows> This XML tag enables you to use systemimport [-select <xml-selection file>] to import both workflow configurations and their CSV file in one action. Use this tag to associate workflow tables with CSV data files. Note! Set the keepOld attribute to true in order to prevent removal of workflow table data which has no match in the export file. Use false to overwrite the data. This parameter is only used during import, and has no effect during export. The onError attribute can either be set from the XML selection file, or from the systemimport in-line command. If set from both, the XML selection file attribute is the value that applies. For further information about the values that you can choose from, see onError. This parameter is only used during import, and has no effect during export. Set the encryptPassword attribute to the workflow configuration password if the workflow configuration is password protected. Example - The XML selection tags <import> <configurations> <configuration name="myFolder.DB_PROFILE"/> <configuration name="myFolder.APL_PROFILE" resolveDependencies="true"/> <configuration foldername="myFolder"/> </configurations> <workflows > <workflow name="Mobile.FTP_workflow" wfTable="/home/user1/FTP_workflows.csv"/> <workflow name="Mobile.SFTP2_workflow" wfTable="/home/user1/SFTP2_workflows.csv" resolveDependencies="true"/> <workflow name="Mobile.GGSN1_workflow" wfTable="/home/user1/GGSN1_workflows.csv" resolveDependencies="true" onError="ask"/> <workflow name="Mobile.GGSN3_workflow" wfTable="/home/user1/GGSN3_workflows.csv" resolveDependencies="true" onError="ignore"/> <workflow name="Mobile.GGSN4_workflow" wfTable="/home/user1/GGSN4_workflows.csv" resolveDependencies="true" onError="abort"/> <workflow name="Mobile.GGSN5_workflow" wfTable="/home/user1/GGSN5_workflows.csv" resolveDependencies="true" keepOld="false" encryptPassword="password" onError="ask"/> </workflows> </import> [ -m|-message] If you want to add a comment when using the systemimport command, the -m or -message option can be used as in the following example: Example - message $ systemimport -m "My Import" /home/Directory/<file to import>.zip "My Import" will be the commented. The comment will replace the default information saved when making a system import, and will both be included in the System Log message that is generated, as well as visible when selecting to view history in any of the configurations in the imported data. [-u|-upgrade] When exports have been made in a previous version of MediationZone, they may have to be upgraded. In this case you can use the -u or -upgrade option as in the following example: Example - upgrade $ systemimport -u <file to import>.zip The configurations will then be upgraded. [-dryrun] To test data compatibility prior to actually importing. Note! Dry run is only applicable to ZIP file and directory. <export file|directory> Specify the path to the directory or ZIP file that contains the configurations you want to import. [password] To import encrypted configurations, provide a password. Return Codes Listed below are the different return codes for the systemimport command: Code Description Code Description 0 Will be returned if the command is successful. 1 Will be returned if the argument count is incorrect or argument(s) are invalid. 2 Will be returned if the command was unable to find an export (file|directory) at the supplied path. 3 Will be returned if the import could not be started due to locked import. 4 Will be returned if any errors were reported during the import. 5 Will be returned if the XML file did not contain any configurations or workflows to import. 8 Will be returned if the XML file did not contain any workflows to use in dry run. 10 Will be returned if the import results in invalid workflows due to compilation errors. 14 Will be returned if there are configurations not imported 20 Will be returned if the value for a supplied option or option is missing. 100 Will be returned if an error occurs while parsing the selection file 101 Will be returned if an error occurs while parsing a node in the selection XML file. 102 Will be returned if an error occurs while getting the attribute from a configuration tab, or if expected wfTable attribute in tag with the supplied name is missing. 103 Will be returned if the import was unable to parse the value for a booolean XML attribute. 104 Will be returned if an error occurs while parsing dependencies for a configuration. 300 Will be returned if an OutOfMemoryError occurs during import.

---

# Document 1395: Archiving Agents Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032019/Archiving+Agents+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Ready with file: name Note! A message event is reported along with the target filename each time a file is archived. Debug Events The agents do not produce any debug events. Brea

---

# Document 1396: SAP CC REST Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/292618279/SAP+CC+REST+Agent+Configuration
**Categories:** chunks_index.json

You open the SAP CC REST agent configuration dialog from a workflow configuration. To open the SAP CC REST agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select SAP CC REST from the Agent Selection dialog. Client Tab The Client tab contains configuration data that is required to connect to a remote server. Open SAP CC REST agent configuration dialog - Client tab Setting Description Setting Description REST API Choose between Charging Service or Loading Service API. This will allow the SAP CC REST agent to switch between the two services. REST API Version The version of the Charging Service or Loading Service API. Each of the API versions will have its specific services associated with it that can be viewed under View Supported Services . View Supported Services Shows the supported services within the API that is selected in the REST API and REST API Version fields. Host Add IP address or hostname of the SAP Convergent Charging instance on the SAP Cloud. ONLY accepts alphanumerical, dots and hyphens as valid values Port Add the Charging or Loading port of the SAP Convergent Charging instance on the SAP Cloud. API Path The path for the API to access the content from the SAP Convergent Charging instance on the SAP Cloud. Route Error UDR When this option is selected, and there is any unexpected exception, an Error UDR will be sent as an output from the agent. For example, Connection Failure or timeout due to wrong host, port, or authorization credentials. If this option is not selected, the error will not be routed. For more information, refer https://infozone.atlassian.net/wiki/spaces/MD93/pages/319717377/SAP+CC+REST+UDRs#errorUDR Use TLS Select to use TLS encrypted communication with Host . You will be able to browse and select the Security Profile with the necessary credentials. For more information about Security Profile, refer to Security Profile . Info! Refer to the Securing Communications with the Core Server System section in the SAP Convergent Charging Installation and Maintenance Guide for more information on how to apply TLS on the SAP CC server. Authentication tab The Authentication tab contains configurations related to the authentication required to establish a connection between the SAP CC REST agent and the SAP CC server. Open SAP CC REST agent configuration dialog for OAuth2 authentication type using Client Credentials - Authentication tab Setting Description Setting Description Authentication Type Choose between the following authentication method for the SAP CC REST agent: Basic - Standard username and password authentication method used to connect to the SAP CC server. OAuth 2.0 - Use of tokens created using an OAuth2 authorization server based on two grant types supported by the agent. Client Credentials and Resource Owner Password Credentials . Basic Authentication Username The username for accessing the SAP CC server. Password The password associated with the username. OAuth 2.0 Authentication - Client Credentials Grant Type Select the grant type: Client Credentials The agent fetches the access token from the Access Token URI during initialization, using client id and client secret for basic authentication. The credentials are base64 encoded and sent in the header of the request. The response contains an access token, which is then used in subsequent requests. Resource Owner Password Credentials The agent fetches the access token from the Access Token URI during initialization, using the following credentials for authentication: Client ID Client Secret Username Password The credentials are sent in the body of the request. The response contains an access token, which is then used in subsequent requests. Client ID Enter the unique client identifier issued by the authorization server. Client Secret Enter the client's secret. Username Enter the resource owner username, this can be the end-user granting access to a protected resource. This field is required when you have selected Resource Owner Password Credentials from the drop-down list Grant Type . Password Enter the password associated with the username. This field is required when you have selected Resource Owner Password Credentials from the drop-down list Grant Type . Access Token URI Enter the URI where the access token can be obtained. Token Expiration Override (s) Enter a time in seconds when you would like to refresh the access token prior to the expiration. This allows the application to obtain a new access token without the user's interaction. Additional Parameters Some authentication servers may require additional parameters in the body of the token requests. To add a parameter, click the Add button and then enter the name of the parameter in the Key field and the value of the parameter in the Value field. Do not use escape characters in the value field, these will be added automatically by the agent. For instance, "https://example.com/ " will be sent as "https%3A%2F%2example.com%2F". Note! If an agent is configured on the Authentication Tab to use OAuth 2.0, an additional step may be required if the token needs to be obtained via HTTPS protocol. The SAP CC REST agent internally uses the Java built-in HTTP Client to obtain a token. If a certificate is required to contact the authentication server, the proper certificate has to be put into the default certificates file for the respective JDK distribution used. Example for OpenJDK 64-Bit Server VM Zulu17.40+19-CA cd $JAVA_HOME/lib/security keytool -import -alias mycert -keystore cacerts -file oauth2Host.cert Traffic Control Tab The Traffic Control tab contains configurations required to control the request flow with the SAP CC Server. Open SAP CC REST agent configuration dialog - Traffic Control tab Setting Description Setting Description Http Version Allows the selection of the HTTP version for use in the Traffic Control. The fields under Connection Control as well as the Server Monitor will only be enabled when HTTP/2.0 is selected. Connection Control (Enabled only with HTTP/2 as Http Version) Min. Connection Per Destination Specify the minimum number of connections towards an endpoint (identified by host:port) that the agent will try to maintain. A value of 0 indicates "no minimum". This is not a hard limit as there could be more connections created during the spike in traffic. Min. Connection Per Destination Check Interval Specify how often you wish to check the established connections. A value of 0 indicates "no check will be performed". After each interval, the agent will try to create additional connections to achieve number set in the Min Connections Per Destination parameter. No connections will be shut down if the current number of connections is greater than the number set in Min Connections Per Destination . Default Concurrent Connection Streams The initial value of concurrent streams that the client would like to handle per connection. The default value is 1000. Note! This value may be overridden by any value sent by server. Timeout Request Timeout (s) Enter the timeout period in seconds for the request to the SAP CC REST agent to wait for a response before timing out. Queue Settings Max Requests Queued Per Destination The size of the message queue in the Jetty server. The default value is 20000. Note! Use this property to manage the memory usage. It is recommended that the EC running the workflow have xmx that is at the minimum (message size X queue size), otherwise there is a risk for out of memory errors. Retry Interval For Full Queues (ms) A millisecond value to indicate the time it will take for the request to try again when the queue is full. Max Retries For Full Queue This field will indicate how many times the request will attempt to retry before routing it to Error UDR. Server Monitor (Enabled only with HTTP/2 as Http Version) Use HTTP/2.0 Server Monitor Select this option to monitor the connection status of all servers that the agent has sent requests to. The monitoring is done by sending regular ping messages to the servers. If the servers are not responding, or there are other communication errors, they will be indicated as Unavailable. A list of the Available and Unavailable servers are available in two MIM values: Available Servers and Unreachable Servers . Ping Interval (s) Define the ping message interval for the Server Monitor. The time unit is seconds and 10 seconds is default.

---

# Document 1397: log4j APL Logging Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678516/log4j+APL+Logging+Configurations
**Categories:** chunks_index.json

The system includes a log4j extension that enables generation of customized logs from agents configured with APL. This section describes how to configure these logs to meet deployment specific requirements. Configuration Files There is a configuration file called apl-log4j.properties containing default settings located in the $MZ_HOME/etc/logging directory that is used to specify the path of log files, log filtering rules, log level and formatting. If you want to use different settings for different Execution Contexts, this file can be copied and renamed to <ec-name>- apl-log4j.properties where you can configure different settings for the specified EC. To provide some examples and templates, there are a number of pre-existing files with the name apl-log4j-<log level name>.properties , for example apl-log4j-trace.properties , which you can use to either copy properties from, or make a copy of the whole file and rename it to <ec-name>- apl-log4j.properties. Example. Configuration filenames Default configuration file: $MZ_HOME/etc/logging/apl-log4j.properties EC specific configuration file for ec1: $MZ_HOME/etc/logging/ec1-apl-log4j.properties Copying the default configuration file into an EC specific configuration file: $cp apl-log4j.properties ec1-apl-log4j.properties Copying a template configuration file into an EC specific configuration file: $cp apl-log4j-trace.properties ec1-apl-log4j.properties The following template configuration files are included by default: apl-log4j-off.properties apl-log4j-fatal.properties apl-log4j-error.properties apl-log4j-warn.properties apl-log4j-info.properties apl-log4j-debug.properties apl-log4j-trace.properties apl-log4j-all.properties The files listed above have a different log level setting but are otherwise identical. Whenever the APL function log.* is called in the Analysis agent, this function invokes logging with log4j. After the workflow is done executing with a specific EC, a new EC specific configuration file <ec-name>-apl-log4j.properties will be created. This new EC specific configuration file is created by default with the same configurations saved in the file apl-log4j.properties . For instance, if the EC name is ec1 then ec1-apl-log4j.properties is created in $MZ_HOME/etc/logging and will have identical settings as apl-log4j.properties . MZSH does not support dedicated commands to make changes to the log level. Changes on the log level and other properties in the configuration file must be made manually. Whatever changes made to the configuration file takes effect at the next workflow run without the need to restart the EC. In a multi-host installation, the EC specific configuration file is always created on the host where EC is located. Therefore, log4j always read the EC specific configuration according to the EC location. For instance, workflow run by ec1 located on host my-host-name . The log4j always read the EC specific configuration file ec1-apl-log4j.properties located at path $MZ_HOME/etc/logging on host my-host-name . The content of the files defines the logging. Example. Configuration file contents log4j.logger.aplLogger=ALL, a log4j.appender.a=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.a.file=${mz.home}/log/{pico}_{workflow}.log log4j.appender.a.layout=com.digitalroute.apl.log.JsonLayout log4j.appender.a.layout.MdcFieldsToLog=pico, workflow, agent, tag The first line in the example above sets the log level and declares an "appender" named ' a' . The available log levels are listed below in order of severity, from highest to lowest: OFF FATAL ERROR WARN INFO DEBUG TRACE ALL Messages of the same or higher severity than the selected level are logged. For instance, if the configured log level is WARN , messages with the severity ERROR and FATAL will be logged as well. The other settings above mean that messages are logged in files that are started, stopped and stored in JSON formatted files in the $MZ_HOME/log directory in regular intervals. When an active log file has reached its maximum size, it is backed up and stored with a number suffix. A new active log file is then created. The default maximum size is 10 MB, and the default number of backup files is one (1). Appenders There are two different types of appenders; DRRollingFileAppender och DRRollingMultiFileAppender . DRRollingFileAppender Writes to a single defined file based on the log4j.appender.<appender name>.file property DRRollingMultiFileAppender Writes one file for each workflow instance it encounters based on the log4j.appender.<appender name>.file Which workflows are written into which appender is based on the log4j.logger.<class name> property. Examples Appender Configurations # Default log4j.appender.Default=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.Default.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.Default.layout=org.apache.log4j.PatternLayout log4j.appender.Default.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.Default.MaxFileSize=10MB log4j.appender.Default.MaxBackupIndex=20 log4j.logger.aplLogger.Default=TRACE, Default The appender named Default will write a single file for all workflows contained under the Default folder. # PRIMARY log4j.appender.PRIMARY=com.digitalroute.apl.log.DRRollingMultiFileAppender log4j.appender.PRIMARY.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.PRIMARY.layout=org.apache.log4j.PatternLayout log4j.appender.PRIMARY.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.PRIMARY.MaxFileSize=10MB log4j.appender.PRIMARY.MaxBackupIndex=20 log4j.logger.aplLogger.RT_Folder.RT_TEST_WF=TRACE, PRIMARY The appender named Primary will create multiple files; one for each workflow instance based on the RT_Folder.RT_TEST_WF workflow. # SECONDARY log4j.appender.SECONDARY=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.SECONDARY.file=${mz.home}/log/log4j/{workflow}.log log4j.appender.SECONDARY.layout=org.apache.log4j.PatternLayout log4j.appender.SECONDARY.layout.ConversionPattern=[%d{dd MMM yyyy HH:mm:ss,SSS}];[%-5p];[pico=%X{pico}];[%t];[tag=%X{tag}];[%c]:%m%n log4j.appender.SECONDARY.MaxFileSize=10MB log4j.appender.SECONDARY.MaxBackupIndex=20 log4j.logger.aplLogger.RT_Folder.RT_TEST_WF=TRACE, SECONDARY The appender Secondary will create a single file for each workflow instance based on the RT_Folder.RT_TEST_WF workflow. The file will take the name of the first workflow instance it encounters, for example "RT_Folder.RT_TEST_WF.workflow_1" Hint! You can change the maximum file size and the number of backup files by adding the following lines: log4j.appender.a.MaxFileSize=100MB log4j.appender.a.MaxBackupIndex=10 You can add a filtering rule by adding the line log4j.logger.<configuration name>=<log level> . This is useful when you want to set different log levels for specific folders or configurations. If you want to apply the filtering rule to all APL configurations in the default folder, change the last line in the previous example to log4j.logger.Default=DEBUG . Example. Sets the general log level to ERROR and to DEBUG for the agent named agent_1 log4j.logger.aplLogger=ERROR, a log4j.appender.a=com.digitalroute.apl.log.DRRollingFileAppender log4j.appender.a.file=${mz.home}/log/{pico}_{workflow}.log log4j.appender.a.layout=com.digitalroute.apl.log.JsonLayout log4j.logger.Default.debug.workflow_1.agent_1=DEBUG Note! For performance reasons it is recommended to use the DRRollingFileAppender and configure individual appenders for each workflow. Only use the DRRollingMultiFileAppender if you need individual files on a workflow instance level. For more information about available settings, see the log4j documentation at https://logging.apache.org/log4j/1.2/manual.html . APL Commands The following functions are used to trigger logging within any of the function blocks in APL: void log.fatal(any, any) void log.error(any, any) void log.warn(any, any) void log.info(any, any) void log.debug(any, any) void log.trace(any, any) For more information about these functions, see Log and Notification Functions . Log Output The output log files are stored in the directory specified in the active logging configuration. Example. Log file in JSON format {"timestamp":"2015-12-20:22:44:10 UTC","level":"DEBUG","thread":"Default.logtestwf.workflow_1: TCP_IP_1_1","category":"Default.logtestwf.workflow_1.Analysis_1","message":"In consume","pico":"EC1","workflow":"Default.logtestwf.workflow_1","agent":"Analysis_1"} The fields in the log output are described below. Field Description Field Description timestamp The time when the message was logged. The UTC timezone and international standard date and time notation is used by default. You can specify the date format by adding the following line in the configuration file: log4j.appender.a.layout.DateFormat=<Java SimpleDateFormat pattern> For information about how to use SimpleDateFormat patterns, see: https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html level The log level i e FATAL , ERROR , WARN , INFO , DEBUG , or TRACE . thread The name of the workflow thread. category The logged configuration. This field contains the category class of the appender that is defined in the configuration file. message The log message is specified in the APL command. pico The name of the Execution Context. Warning! The ECs must be restarted if you manually delete or rename active log files or backup log files. Hint! If the log files are not generated as expected, review the EC logs. Your configuration files may contain errors.

---

# Document 1398: Appendix 4 - Collection Strategies - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205035499/Appendix+4+-+Collection+Strategies
**Categories:** chunks_index.json

Collection Strategies are used to setup rules for handling collection of files from the Disk, FTP, SFTP, and SCP collection agents. This appendix describes the various collection strategies available. The available collection strategies are: APL Collection Strategy Control File Collection Strategy Duplicate Filter Collection Strategy Multi Directory Collection Strategy

---

# Document 1399: ErrorCycleUDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002857/ErrorCycleUDR
**Categories:** chunks_index.json

When a Workflow Bridge Agent (Forwarder or Collector) attempts to send a UDR over the bridge but fails, an ErrorCycleUDR is automatically generated. Instead of being sent across the bridge, the ErrorCycle UDR is routed back within the originating workflow for error handling. You can design your workflow to attempt to send the ErrorCycle UDR when an error occurs in the collection workflow. In a forwarding workflow: The Workflow Bridge Forwarding Agent creates the ErrorCycleUDR when it cannot deliver a UDR. The ErrorCycleUDR is not sent to the collection agent but is instead received by the downstream agent in the same producer workflow (after the Forwarding Agent). The Forwarding Agent can receive an ErrorCycle UDR from the collection workflow, but you must design the collector workflow to do so. In a collection workflow: The Workflow Bridge Collection Agent creates the ErrorCycleUDR when it cannot deliver a UDR. The ErrorCyclenUDR is not automatically sent to the Forwarding Agent but is instead received by the downstream agent in the same collection workflow (after the collection agent). The collection workflow can be designed to route an ErrorCycle UDR back to the Workflow Bridge Collection Agent, attempting to resend it over the bridge to the Forwarding Agent The ErrorCycleUDR contains: Field Description Field Description OriginalUDR (WorkflowBridgeUDR) Contains the original UDR that failed to be sent. AgentId Identifies the agent involved in the failure. To process the ErrorCycle UDR, you need to configure an Analysis Agent after the Workflow Bridge Forwarding Agent. There are two common ways to handle it: Retry Logic : Extract the OriginalUDR and attempt to resend it. Error Logging & Alerts : Log the error or trigger an event. Example - Workflow Structure with Error Cycle UDR Producer Workflow (Forwarding Workflow) Collector  Decoder  Analysis1  WorkflowBridge Forwarder  Analysis2 Analysis1 : Prepares data before sending it via Workflow Bridge. WFB (Forwarding Agent) : Attempts to send UDRs to the Collection Agent. Analysis2 : Receives ErrorCycleUDRs and handles failures. Consumer Workflow (Collection Workflow) WorkflowBridge Collector <---> Analysis

---

# Document 1400: Categorized Grouping Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640260/Categorized+Grouping+Agent+Example
**Categories:** chunks_index.json

In a workflow the Categorized Grouping Agent is usually connected with one Analysis agent before and after the agent. Open Typical workflow containing a Categorized Grouping agent Analysis_1 In the first Analysis agent a UDR of the type CGAgentInUDR is created and populated. The example below shows a possible Analysis_1 agent configuration: Example - Analysis_1 agent configuration consume{ //Create CGAgentInUDR. CatGroup.CGAgentInUDR udr = udrCreate(CatGroup.CGAgentInUDR); //Set categoryId, data and filename UDR values. debug(input.categoryID); udr.categoryID = (string)input.categoryID; udr.data = input.OriginalData; udr.fileName= "IncomingFile"; //When "Activate use of //grouping" is enabled in the Cat_Group //profile this file name will be used for //the grouped data in the tar-file. //When closeGroup is set to "true" the category can be closed //from APL, else settings in the Cat_Group profile will be used. udr.closeGroup = false; //Route UDR udrRoute(udr); } When the CGAgentInUDR is created, the field structure can be viewed from the UDR Internal Format Browser . For further information, see Categorized Grouping Related UDR Types in Categorized Grouping Agent Overview . Cat_Group_1 The Categorized Grouping agent is configured via the GUI. For configuration instructions see Categorized Grouping Profile . Open Example configuration of a Categorized Grouping profile When configured correctly incoming UDRs with different categoryIDs are collected and handled by the agent to return UDRs consisting of identical categoryID types. The outgoing data is collected in a CGAgentOutUDR with the following structure. Example - Outgoing data collected in a CGAgentOutUDR internal CGAgentOutUDR { string categoryID; int closingCondition; //Indicates closing condition that emits the file. bytearray data; boolean isLastPartial; //True if last UDR of the input file. int partialNumber; //Sequence number of the UDR in the //file. 1 for the first, 2 for the //second so on. }; Analysis_2 In the Analysis_2 agent the CGAgentOutUDR will be processed in the way the agent is configured. In this example, one directory, one directory delimiter, and one file name are created. The CGAgentOutUDR information is thereafter put in amultiForwardingUDR and last routed to the Disk_2 agent. Example - Analysis agent processing the CGAgentOutUDR persistent int counter = 1; consume { //Create a fntUDR FNT.FNTUDR fntudr = udrCreate(FNT.FNTUDR); //Create a directory name fntAddString(fntudr, "CG_Directory"); //Add a directory delimiter. fntAddDirDelimiter(fntudr); //Create a filename. fntAddString(fntudr, "File_" + (string)counter); //Create a multiForwardingUDR. FNT.MultiForwardingUDR multiUDR = udrCreate(FNT.MultiForwardingUDR); //Add the fntUDR created above containing the directory, //directory delimiter and the file name. multiUDR.fntSpecification = fntudr; //Add the data from the CGAgentOutUDR to the multiForwardingUDR. multiUDR.content = input.data; // print closingCondition, // 0 = timeout, // 1 = close on deactivation, // 2 = APL requested closure, // 3 = input file count limit is reached, // 4 = the input file size limit is reached debug("Closing condition= " + input.closingCondition); udrRoute(multiUDR); counter = counter + 1; }

---

# Document 1401: FNTUDR Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656844
**Categories:** chunks_index.json

The functions described below operate on values in UDRs of type FNTUDR. The value in an FNTUDR is a delimited string, representing file system paths. The FNTUDR types are usually used in two different cases: Creation of a dynamic path for the output file from a disk oriented forwarding agent. Using the filename template in the forwarding agent the FNTUDR value is, via a MIM resource, used to publish the respective MIM value. For further information about using the FNTUDR in filename templates, see the relevant agent documentation in the Desktop User's Guide . Grouping of output data using a disk oriented forwarding agent set to expect input of MultiForwardingUDR type. The MultiForwardingUDR has two fields, the first contains the data that will be written in the output file, the second contains filename and path specified by the FNTUDR. Both fields are mandatory unless the Produce Empty Files check box is selected in the agent, in this case the data field is not required. For further information about using the MultiForwardingUDR , please refer to respective file based forwarding agents documentation. Example - How an FNTUDR is constructed The following APL code shows how a FNTUDR is constructed with the help of the functions that are described in this section. import ultra.FNT; consume { FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, "ready_folder"); fntAddDirDelimiter(fntudr); date now = dateCreateNow(); fntAddString(fntudr, ""+dateGetYear(now) + "-"); fntAddString(fntudr, ""+dateGetMonth(now), 2, "0", false); fntAddString(fntudr, "-"); fntAddString(fntudr, ""+dateGetDay(now), 2, "0", false); } The fntudr variable will have a value corresponding to a path like r eady_folder/2018-10-31 , where the character '/' represent a directory delimiter that can be any character, depending on the target system. The following functions for FNTUDR described here are: fntAddString The fntAddString function appends a text string to the specified FNTUDR . void fntAddString( FNTUDR fntudr, string str, int size, (optional) string padding, (optional) boolean leftAlignment (optional) ) Parameter Description fntudr The FNTUDR that the text string is going to be added to. A runtime error will occur, if the parameter is null. str The string that will be added. If the string is null a runtime error will occur. The string can be extended or truncated if the size parameter is specified and the string does not match. Note! Do not include directory delimiters, e g "/", in the string. To add delimiters, use the fntAddDirDelimiter function instead. size The optional parameter size defines a fixed size for the appended string. A runtime error will occur, if the size isn't greater than zero. padding The optional parameter padding defines a padding string that will be repeated to fill the string to the size specified in the size parameter. A default padding will be used if the argument isn't specified or if the padding string is null or an empty string. leftAlignment The optional parameter leftAlignment specifies if the padding shall be kept right or left of the string. If the parameters value is true the string will be left aligned and the padding will added to the right. If the parameter isn't specified, the default setting is left aligned. Returns Nothing fntAddDirDelimiter The fntAddDirDelimiter function adds a directory delimiter to the end of the FNTUDR . void fntAddDirDelimiter(FNTUDR fntudr) Parameter Description fntudr The FNTUDR that the text delimiter is going to be added to. A runtime error will occur, if the parameter is null. Returns Nothing Loading

---

# Document 1402: Workflow Bridge Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610103
**Categories:** chunks_index.json

The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. The profile ties the workflows together and is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. The section contains the following subsections: Workflow Bridge Profile Configuration

---

# Document 1403: Web Service Request Agent Input/Output Data, MIM and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643934/Web+Service+Request+Agent+Input+Output+Data+MIM+and+Events
**Categories:** chunks_index.json

Input/Output Data

---

# Document 1404: desktopadmin - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678013/desktopadmin
**Categories:** chunks_index.json

usage: desktopadmin [options] This command can be used for shutting down connected Desktops, Legacy Desktops or for displaying an administrator message to all users. Upon shutting down connected Desktops or Legacy Desktops, locked configurations would be released as well. Any mention of desktops in the description below will represent both the Desktop and Legacy Desktop UI unless specified. Info! Only users belonging to the Administrator group are authorized to execute this command. Options The command accepts the following options: Option Description Option Description [-l, --list] List all connected desktop sessions include SSO login. Returns a table with session details. Format: <Name>, <Session ID>, <Username> Example, +----------------+---------------------+----------+ | Name | Session ID | Username | +----------------+---------------------+----------+ | legacy-desktop | 192.168.0.224:53631 | john | | desktop | 154379321 | sarah | +----------------+---------------------+----------+ For Legacy Desktop, Session ID column will display <IP Address:Port>. For Desktop, Session ID column will display Session ID. [-k, --locks] Retrieves a list of locked configurations for each currently connected desktop. Must be used with the -l option. Example, +----------+------------+----------+-------------------------+ | Name | Session ID | Username | Locked Configuration(s) | +----------+------------+----------+-------------------------+ | desktop | 1363198261 | john | | | desktop | 1545132119 | sarah | autotest.asserts | +----------+------------+----------+-------------------------+ [-m, --message] Broadcasts a message to all currently logged-in desktops and those that log in later. Use with -n to send the message to a specific desktop. This message can be removed using the -r option. [-r, --reset] Cancel and clear scheduled shutdown and message, if any. Use it with -n to cancel scheduled shutdown for a specific desktop. [-n, --session] Apply the command to a specific desktop session obtained using the -l option. Use with one of the following options: -x , -m , -r , or -s . [-x, --shutdown] Schedules a forced shutdown for all desktops after the specified number of minutes (0 for immediate shutdown). Use it with -n to shutdown a specific desktop. An optional message can be included using -m . Example, desktopadmin -n 15300940 -x 5 "Shutting down in 5minutes" This message will pop up on the desktop and count down of 5 minutes to shutdown. Scheduled shutdown can be canceled using the -r option. [-s, --status] Use this option to display the current shutdown timeout and message, if any. Use it with -n to display the status of a specific desktop. Return Codes Listed below are the different return codes for the desktopadmin command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if there was a problem parsing command parameters. 2 Will be returned if there was a failure scheduling shutdown of Desktops

---

# Document 1405: Aggregation Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031494/Aggregation+Agent
**Categories:** chunks_index.json

This section describes the Aggregation profile and the Aggregation agent. The agent is a processing agent for batch and real-time workflow configurations. The Aggregation consolidates related UDRs that originate from either a single source or from several sources, into a single UDR. Related UDRs are grouped into "sessions" according to the value of their respective fields, and a set of configurable conditions. The first UDR to match a set of conditions triggers the generation of a new session. Consecutive UDRs that match the same conditions can then be handled in the context of the same session. The agent stores the session data in a file system, Couchbase, Redis, or Elasticsearch. It is also possible to use the settings for file storage to keep the session data in memory only. When selecting which storage type you require, note that File Storage can be used in batch and real-time workflows; Couchbase and Redis can only be used in real-time workflows; Elasticsearch can only be used in batch workflows. To ensure the integrity of the session's data in the storage, the Aggregation agent may use read- and write locks. When you have selected file storage and an active agent has write access, no other agent can read or write to the same storage. It is possible to grant read-only access for multiple agents, provided that the storage is not locked by an agent with write access. When you have selected Couchbase or Redis storage, multiple Aggregation agents can be configured to read and write to the same storage. In this case, write locks are only enforced for sessions that are currently updated and not the entire storage. For further information about how to configure read-only access, see Aggregation Profile . In a batch workflow, the aggregation agent receives collected and decoded UDRs one by one. Open Aggregation in a batch workflow In a real-time workflow, the aggregation agent may receive UDRs from several different agents simultaneously. Open Aggregation in a real-time workflow The aggregation flow chart below illustrates how an incoming UDR is treated when it is handled by the Aggregation agent. If the UDR leaves the workflow without having called any APL code, it is handed over to error handling. For detailed information about handling unmatched UDRs see the section General Tab in Agent Configuration - Batch, Aggregation Agent Configuration - Batch , or Agent Configuration - Real-Time in Aggregation Agent Configuration - Real-Time . Open The aggregation flow chart When several matching sessions are found, the first one is updated. If this occurs, redesign the workflow. There must always be zero or one matching session for each UDR. High-level steps for configuration of Aggregation Follow the steps below to configure Aggregation: Create a session UDR in Ultra format. Create a storage profile, i.e. Couchbase or Redis profile (not required for file storage). Create an Aggregation profile. Create a workflow containing an Aggregation agent. Prerequisites The reader of this information should be familiar with: Couchbase Redis SQL The section contains the following subsections: Aggregation Agent Overview Aggregation Example - Association of IP Data Aggregation Performance Tuning Storage Profiles Aggregation Profile Aggregation Agents Configuration Session UDR Type Aggregation Session Inspector

---

# Document 1406: Example - ECS handling of Batches - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673037/Example+-+ECS+handling+of+Batches
**Categories:** chunks_index.json

The ECS forwarding agents are not used for batches. Instead, a batch is sent to the ECS directly from a collection agent when receiving a cancelBatch . It is also possible to associate an error UDR with a batch. This UDR can in turn be assigned error information with udrAddError . This section contains the following subsections: ECS Forwarding Workflow (batch) ECS Collection Workflow (batch)

---

# Document 1407: Dynamic Update Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605273/Dynamic+Update+Event
**Categories:** chunks_index.json

Occurs whenever you dynamically update a Workflow configuration. The following fields are included: systemMessage - The message string. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents eventName origin receiveTimeStamp severity timeStamp Fields inherited from the Workflow event The following fields are inherited from the Workflow event, and described in more detail in Workflow Event : workflowKey workflowName workflowGroupName

---

# Document 1408: Workflow Monitor - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604893
**Categories:** chunks_index.json

The Workflow Monitor controls workflow execution and presents a detailed view of the workflow execution status. Agent states and events can be monitored during workflow execution. The monitor also allows for dynamic updates of the configuration for certain agents by sending commands. A command can, for example, tell an agent to flush or reset data in memory. For further information about applicable commands, see the section for the relevant agent. See also Dynamic Update below. Note! The Workflow Monitor can apply commands to only one workflow at a time, in one monitor window per workflow. The monitor functionality is not available for groups or the whole workflow configuration. The Workflow Monitor displays the active version of the workflow. Monitoring a workflow does not imply exclusive rights to start or stop it, the workflow can be activated and deactivated by another user while monitored, or by scheduling. Opening the Workflow Monitor You can open the workflow monitor from either a workflow configuration or the Execution Manager . To open the monitor from a workflow configuration: Click the ( Run ) button. From the Run Workflow dialog, Select the workflow you want to view in the workflow monitor. Click the Run button to open the selected workflow in the workflow monitor. The workflow will only run once you click on the Start button in the workflow monitor. Open Workflow Monitor To open the monitor from the Execution Manager, see the section The Right-Click Menu, in the Execution Manager . Viewing Agent Events While a workflow is running, agents report progress in terms of events. These events can be monitored in the two Agent debug panels at the bottom. You can also Enable Debug to view more events or messages from the debug command in the Analysis or Aggregation agents. Note! When you Enable Debug , it may slow down the workflow due to logging, especially if you set it to log debug into files in the workflow properties. For more regarding debug in workflow properties, refer to Debug Type in the Execution Tab . The following steps must be taken in order to view these events. To view events for all agents in the workflow, click Select All from either of the Agent debug panels. To only view events for some selected agents, click Select agent(s) to monitor event from either of the Agent debug panels and select only the agents you want to view events for. Workflow Execution States All running workflows are monitored by the workflow server on the Platform. A workflow can be in any one of the following states: Open The workflow state diagram State Description Building When a workflow, or any of a workflow's referenced configurations, are being re-built, for example when saving or recompiling, the workflow will be in the Building state. When a workflow is in the Building state, in the worklow monitor, the text "Workflow is building" is also displayed. Workflows started by scheduling configurations will wait until the workflow leaves the Building state before they start. Executed A Workflow becomes Executed after one of the following: Note! This is a transient state, after the workflow has run, where cleanup is handled. This state is not reached if the workflow is aborted. In the GUI, the workflow is shown as Stopped even after it has reached the Idle state. Hold A workflow that is in the Idle state and is being imported - either by the mzsh systemimport r | sr | sir | wr or by the System Importer configured to Hold Execution - enters the Hold state until the import activity is finished. The Workflow group then resumes its Idle state. Idle A workflow is in Idle state if there is no activity going on in the workflow. It means that the workflow is not running, not in process of starting or stopping, or not in any other state (Invalid or Hold). After execution, although the workflow is indeed Idle, the state space on the display may remain as one of the following states: Invalid The workflow configuration is erroneous. Once you correct the error, the workflow assumes the Idle state. Note! The workflow cannot be executed if it is in invalid state. Loading The platform is uploading the workflow to the Execution Context. When the transfer is complete, the Execution Context initializes the agents. When the workflow starts running the state changes to Running. Running The workflow is currently executing. Unreachable If the Platform fails to establish connection with the EC where a workflow is executing, the workflow enters the Unreachable state. When the workflow server successfully reestablishes the connection, the workflow is marked as Running, Aborted, or Executed, depending on the state that the workflow is in. An Unreachable workflow may require manual intervention if the workflow is not running any more. For further information see Execution Context in Desktop User's Guide . Waiting The Waiting state applies only to workflows that are included as members in a workflow group. In the Waiting state, the workflow cannot start execution due to two parameters in the workflow group configuration: The Startup Delay parameter, and the Max Simultaneous Running quota. A workflow in the Waiting state changes to Running when triggered either by a user, the scheduling criteria of its parent workflow group, or by a more distant ancestor's scheduling criteria. For further information see the Scheduling section in Managing a Workflow Group . Locked Locked state is a special case that is only relevant when a batch workflow is present in multiple versions. That is, there are multiple workflows with the same name and instance id from the same workflow configuration. These can be in different versions of a package or be the same as a workflow outside any package. This means that in case of same workflows, the packages that these workflows are part of are ignored when considering whether they are different versions of the same workflow. If this is the case (and again, only for batch workflows), then only one version is allowed to run at a time. When one version of the workflow starts, the other ones enter into the locked state until the started one completes its execution. Only then is the next version allowed to start. This allows for collection using the file sequence number service to stay consistent even if the workflow version is updated. Workflow Execution State The workflow execution state provides granular information about a workflow's current execution status. In APL, there are function blocks that are called for each workflow execution state. For more information about which APL functions that can be used in each state, see Analysis Agent . Real-time agents only have three different execution states, that occur at every execution of the workflow. Open Execution flow for real-time agents The batch agents are more complex and contain additional states in order to guarantee transaction safety. Open Execution flow for batch agents State Description State Description initialize The initialize state is entered once for each invocation of the workflow. During this phase the workflow is being instantiated and all agents are set up according to their configuration. beginBatch This state is only applicable for batch workflows. At every start of a new batch, the batch collection agent emits a beginBatch call. All agents then prepare for a new batch. This is normally done every time a new file is collected, but differs depending on the collection agent. consume The agents handle all incoming UDRs or bytearrays during the consume state. drain This state is only applicable for batch workflows. When all UDRs within a batch have been executed, the agents enter the drain state. This state can be seen as a final consume state with the difference that there is no incoming UDR or bytearray. The agent may however send additional information before the endBatch state. endBatch This state is only applicable for batch workflows. The collection agent calls for the endBatch state when all UDRs or the byte arrays are transferred into the workflow. This is normally done at the file end, but is dependent on the collection agent or when a hintEndBatch call is received from any agent capable of utilizing APL code. commit This state is only applicable for batch workflows. Once the batch is successfully processed or sent to the ECS, the commit state is entered. During this phase, all actions that concern transaction safety are executed. deinitialize This is the last execution state for each workflow invocation. The agents clean and release resources, such as memory and ports, and stop execution. cancelBatch This state is only applicable for batch workflows. If an agent fails the processing of a batch it may emit a cancelBatch call and the setting in Workflow Properties defines how the workflow should act. For more information regarding the Workflow Properties , see Error Tab in Workflow Properties . Note! The execution states cancelBatch and endBatch are mutually exclusive - only one per batch can be executed. rollback This state is only applicable for batch workflows. If the last execution of the workflow aborted, the agents enter the rollback execution state right after the initialize state. The agents recover the state prior to the failing transaction and then enter beginBatch or deinitialize depending on if there are additional batches to process. Viewing Abort Reasons In most cases if a workflow has aborted, one of its agents has a red square outline surrounding it. Double-clicking such an agent displays the Agent Status dialog with the abort reason. The System Log also holds valid information for these cases. Open A workflow with an aborted agent, indicated by the red square outline. In a batch workflow, a detected error causes the workflow to abort and the detected error is shown as part of the abort reason and inserted into the System Log. A real-time workflow handles errors by only sending them to the System Log. The only time a real-time workflow aborts is when an internal error has occurred. It is therefore important to pay attention to the System Log or subscribe to workflow error events to fully understand the state of a real-time workflow. Note! Although a workflow has aborted, its scheduling is still valid. Thus, if it is scheduled to execute periodically, it is automatically started again the next time it is due to commence. This is because the abortion might be a lost connection to a network element, which may be available again later. Therefore, a periodically scheduled workflow, which has aborted, is treated as Active until it is manually deactivated. Open Status details for an aborted agent In the Agent Status dialog, there is a Stack Trace section provided in the dialog. Use the information that it provides when consulting Support. Agent States An agent can have one of the following states: State Description State Description Created The agent is starting up. No data may be received during this phase. This state only exists for a short while during workflow startup. Idle The agent is started, awaiting data to process. This state is not available for a real-time workflows. Running The agent has received data and is executing. For agents that are running, there is a green square outline surrounding them. Stopped The agent has successfully finished processing data. Aborted The agent has terminated erroneously. For an agent that is aborted, there is a red square outline surrounding the aborted agent. The error reason can be tracked either by double-clicking the agent or by examining the System Log. Agent Configuration Some agents allow parts of them to be reconfigured while active. When double-clicking the agent in monitor mode, the agent's configuration is shown if the agent can be reconfigured. When the configuration has been updated it can be committed to the active workflow using the Dynamic Update button in the toolbar. Dynamic Update While a real-time workflow is being executed, you can change the value of the following parameters: The Host and Port parameters of the TCP/IP agent The NAS list of the Radius agent The Interval of the Pulse agent Open Example of the TCP/IP configuration in a running Realtime workflow To be able to dynamically update TCP/IP Host and Port parameters, you need to set them to either Default or Per Workflow in the Workflow Properties dialog. See the figure The Workflow Table Tab in Workflow Table Tab . To update, click Dynamic Update . On the bottom left of the workflow monitor, the text Dynamic Update followed by a number appears. It represents the number of times that you have updated the workflow configuration while running it, that is, since the last time you started it. Open Example of a dynamic update status after clicking the Dynamic Update button Transactions A workflow operates on a data stream and MediationZone supplies a transaction model where persistent synchronization is made from the workflow data and agent-specific counters. In theory the synchronization could be performed continuously on a byte level, but in practice this would drastically decrease the performance of the system. The transaction model is based on the premises that collection agents are free to initiate a transaction to the transaction server. At the moment the complete workflow is frozen and the transaction server saves the state of the workflow data that is queued for each agent. In practice, agents indirectly emit a transaction when an End Batch is propagated. When all data is secured, the workflow continues the execution. Debug A workflow can be configured to send debug logs as an event or to a file. When an event is selected, the debug logs will be present in the text areas in Workflow Monitor View. Open Workflow Properties to select Debug type When the Debug Type is File, the debug will be written to a file in MZHOME of the Execution context. The default file location is in a debug directory inside the temporary directory decided by this property pico.tmpdir . It can also be set directly using this property, mz.wf.debugdir . When the debug is sent to a file, an additional button will be visible in the Workflow Monitor, Download debug files . Open Monitor buttons when debug is sent to file After a run the Download debug files button is enabled and once you click on the button, the following dialog appears. Open Download debug files dialog

---

# Document 1409: Policy Control (PCF) 5G - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205882055/Policy+Control+PCF+5G
**Categories:** chunks_index.json

Policy Control Function (PCF) is defined using the policy and charging protocol architecture as outlined by 3GPP TS23.503 as the baseline. The purpose of the function is to act as Quality of Service (QoS) coordinator between the external PDN (Public Data Network) and the 5GC (5G Core), managing: Verification and enforcement of QoS parameters. Facilitate usage charging in accordance with QoS parameters. This Right to Use (RTU) grants the licensee the right to use DigitalRoutes MediationZone software in accordance with this definition.

---

# Document 1410: Encoder Agent Services Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641019/Encoder+Agent+Services+Batch
**Categories:** chunks_index.json

The agent utilizes two specialized services allowing the user to add header and trailer information into each data batch. They both offer the possibility of using MIM values, constants, and user-defined values in the header or trailer. When selecting MIM resources, note that MIM values used in the data batch header are gathered when a new batch begins, while MIM values used in the data batch trailer are gathered when a batch ends. Thus, the numbers of outbound bytes, or UDRs, for any agent will always be zero if they are referred to in data batch headers. The windows for both header and trailer configuration are identical. Open Encoder configuration dialog - Header tab Setting Description Setting Description Suppress On No Data Indicates if the header/trailer will be added to the batch even if the batch does not contain any data (UDRs or byte arrays). Value Click on the Add button to populate the columns with items to the header or trailer of the file. They will be added in the order they are specified. Open Add Header/Trailer Content dialog Setting Description Setting Description MIM Defined If enabled, a MIM value will be part of the header. Size and Padding must be entered as well. Note! For data batch headers, the MIM values are gathered at beginBatch . User Defined If enabled, a user defined constant must be entered. If Size is empty or less than the number of characters in the constant, Size is set to the number of characters in the constant. If Size is greater than the length of the constant, Padding must be entered as well. Pad Only If enabled, a string is added according to the value entered for Size , filled with Padding characters. Size Size must always be entered to give the item a fixed length. It can only be omitted if User Defined is selected, in which case it will be calculated automatically. Padding Character to pad any remaining space with. Either a user defined character can be entered, or one of the four predefined/special characters can be selected (Carriage return, Line feed, Space, Tabulator). Alignment Left or right alignment within the allocated field size. Date Format Enabled when a MIM of type date is selected. A Date Format Chooser dialog is opened, where a date format may be entered.

---

# Document 1411: System External Reference Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737042/System+External+Reference+Event
**Categories:** chunks_index.json



---
**End of Part 59** - Continue to next part for more content.
