# RATANON/MZ93-DOCUMENTATION - Part 49/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 49 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~70.0 KB
---

Reference Data Management allows you to bulk load data to and from the database tables using the Import and Export function. Note! Both importing and exporting data apply to the Standard Search Type . If you have selected Join Table Search Type , only exporting is allowed. Refer to Querying A Table In Reference Data Management for more information. You can refer to the following for more information on its respective functions: 1 Importing Data 2 Exporting Data Importing Data You can import data into your table from a CSV file using Reference Data Management. Open Import dialog Option Description Option Description Select Files This button opens a file selection dialog. You can only select one file at a time but it is possible to upload multiple files for import. Each added file is listed below the button. The play button starts the import of the selected file. Ignore Error during Import When this check box is selected, errors during import are logged but does not interrupt the process. Settings Append When this radio button is selected, all rows to be imported into the table will be appended into the table as new rows. Truncate When this radio button is selected, all rows in the existing table will be truncated and the rows will be imported into an empty table. Text Qualifier Character This field specifies the special character used to indicate the start and end point of the value within a text based field. The character for text qualifier is configurable between single quote (') or double quote (). The default character is a double quote () Separator Character This field specifies the character that should be interpreted as separator between values in the imported file. The default separator is a comma. Exporting Data You can export data from the entire table or just the query result from the Reference Data Management. Note! Only supported data types(Varchar/String, Numeric, Data time, Boolean) will be added for exporting, non supported type will be excluded. Open Export dialog Options Description Options Description Column Selector Allows you to choose from a drop down list to include or exclude a particular column from the export. Clicking the All button will include all columns in the table. Text Qualifier Character This field specifies the special character used to indicate the start and end point of the value within a text based field. The character for text qualifier is configurable between single quote (') or double quote (). The default character is a double quote () Separator Character This field specifies the character that will be used for separating values in the export. The default separator is a comma (,) Export All Rows When this radio button is selected, all rows in the table will be exported. Example - Exported row values If the prefetchSize =0, maxRowsPerQuery =2500, and table size = 100,000: Export All Rows = 100,000 rows Export Retrieved Rows = 2,500 rows If the prefetchSize =200, maxRowsPerQuery =2500, and table size = 100,000. Export All Rows = 100,000 rows Export Retrieved Rows = 2,500 rows If the prefetchSize =1000, maxRowsPerQuery =200,000, and table size = 100,000. Export All Rows = 100,000 rows Export Retrieved Rows = 10,000 rows Export Retrieved Rows When this radio button is selected, the rows in the last query will be exported. The maxRowsPerQuery setting in the Reference Data Profiles Advanced tab controls the number of exported records, with a default limit of 50,000. Open Example of CSV exported data

---

# Document 1135: startup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646464/startup
**Categories:** chunks_index.json

usage: startup [ -e <property=value> ] [ -f ] [ -q ] <server process>... This command is used to start pico instances. Note! This command is valid only for the MZ_HOME owner. The Platform must be started before any other pico instance. If there is a problem during startup an error message will be shown and more information will be visible in the Platform log. This command will not terminate until a pico instance has reported back that it is up and running. If you have specified more than one pico instance in the arguments, it will not continue with the next one until the previous instance has reported that it is up and running. Options The command accepts the following option: Option Description Option Description [-e <property=value> ] Use this option to set or override a system property. This is useful for test purposes but not recommended for production environments. Example. Setting properties in the mzsh startup command. $ mzsh startup platform ec1 -e pico.log.level="FINEST" [-f] Use this option to force start of an existing pico instance that is unreachable. Note! When you want to start a pico instance that is already started but unreachable, you must first delete it or use the -f option. For further information, see 6.10 Pico Viewer in the Desktop User's Guide . [-q] Use this option to generates less or no messages in the output. Return Codes Listed below are the different return codes for the startup command: Code Description Code Description -1 Will be returned if there is an old pico instance running or if the remote (../temp/.remote) file cannot be deleted. 0 Will be returned if the command was successful, or if the pico instances are already started. 1 Will be returned if the JVM failed to start. (The JVM has logged too much on stderr.) 102 Will be returned if the JVM failed to start. (The timeout on the callback from the JVM was exceeded.) 103 Will be returned if the command has been interrupted with CTRL-C. 104 Will be returned if the JVM failed to start. (The JVM started with (a) critical error(s).)

---

# Document 1136: Prometheus Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001299/Prometheus+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent consumes data from Prometheus UDR Type and /wiki/spaces/temp/pages/23214920 . MIM For information about the MIM and a list of the general MIM parameters, s ee Administration and Management in Legacy Desktop . Publishes The agent does not publish any MIM resources. Accesses The agent does not access any MIM resources.

---

# Document 1137: Starting and Stopping the Spark Cluster - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611546/Starting+and+Stopping+the+Spark+Cluster
**Categories:** chunks_index.json

Note! As a prerequisite, the scripts must be prepared according to Preparing and Creating Scripts for KPI Management . Starting the Spark Cluster The Spark cluster is started automatically when you start the Spark service. For information about how to configure and start this service, see KPI Management - External Software plus the subsections thereof. $ start_master_workers.sh For further information about the Spark command, see spark . Stopping the Spark Cluster $ stop.sh For further information about the Spark command, see spark Note! The cluster stop command does not automatically trigger the output of the currently processed KPIs.To flush all data, you must start a new period for your KPIs. For further information, see KDR . Alternatively, you can manually flush all the pending KPIs to the output topic, by using the spark by using the flush scripts, which triggers an output of previously unclosed periods. Example: flush.sh kpiapp

---

# Document 1138: GCP Storage Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739061/GCP+Storage+Forwarding+Agent
**Categories:** chunks_index.json

The GCP Storage forwarding agent creates files on the location stated in a referenced File System profile, containing the received data. Files are created when a Begin Batch message is received and closed when an End Batch message is received. In addition, the Filename Template service offers the possibility to compress (gzip) the files or to further process them, using commands. To ensure that downstream systems will not use the files until they are closed, they are stored in a temporary directory until the End Batch message is received. This behavior also applies to Cancel Batch messages. If a Cancel Batch is received, file creation is cancelled. Open Example workflow with a GCP Storage Forwarding Agent The section contains the following subsections: GCP Storage Forwarding Agent Configuration GCP Storage Forwarding Agent Events GCP Storage Forwarding Agent Input/Output Data and MIM GCP Storage Forwarding MultiForwardingUDR Example GCP Storage Forwarding Agent Transaction Behavior

---

# Document 1139: dumpsyslog - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657071/dumpsyslog
**Categories:** chunks_index.json

usage: dumpsyslog [ -n <N>] [ -b ] [-m] [ -s <I | W | E | D> ] [ -d <date> ] [ -h <hour> ] [ -f <filename> ] [-q <C | S | D>] [ -t ] This command will allow displaying and saving entries from the System Log. The command accepts the following filter options: [-n <N>] Maximum number (N) of entries to dump. By default the most recent entries in the System Log are shown. [-b] Will select entries from the beginning of the System Log, that is, the oldest entries instead of the most recent entries. This option is preferably used in conjunction with the -n option. [[ -s ]] Displays the severity type for each entry. The possible severity types are: I Information W Warning E Error D Disaster [-d <DATE>] Filter on a certain date or time interval. The date must be given in the format specified in the config.xml file. If only one date is given, the display will hold entries for that date only (time 00:00:00 to 23:59:59), unless the -h option is used. MZ>> dumpsyslog -d 1982-01-01 Will give all log entries for the specified date from 00:00:00 to 23:59:59. If an interval is given, the entries must be enclosed within quotation marks "d1 d2": MZ>> dumpsyslog -d "1982-01-01 2007-12-07" Will give all log entries between the two dates, from date1 (time 00:00:00) to date2 (time 23:59:59). [-h <HOUR>] Filter on a certain hour or time interval. Time is given in the format hh:mm:ss and if an interval is given the times have to be enclosed within quotation marks; "h1 h2" . If only one time is given, the display will hold entries for that hour only (time 00:00:00 to 00:59:59). If giving two times, all entries in the interval will be displayed. Note! If both date and time intervals are entered; -d "d1 d2" -h "h1 h2" , the displayed entries will be from date d1 at time h1 to date d2 at time h2 . [-f <FILENAME>] Direct output to a file (including directory path). [-q <C|S|D>] Displays general information about the System Log entries. C Number of entries. S Number of entries of each severity. D Earliest and latest date of entries. [-t] To include stack trace information in the System Log printout. [-m] Sometimes log messages can get truncated, so use this flag to display full log messages. Return Codes Listed below are the different return codes for the dumpsyslog command: Code Description 0 Will be returned if the command was successful. 1 Will be returned if there was a problem parsing command parameters. 2 Will be returned if there was an error when trying to access the system log database. 3 Will be returned if the output file already exists, or in the event of file write errors.

---

# Document 1140: Data Hub Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676378/Data+Hub+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Data Hub agent expects UDRs of the type selected in the Tables Mapping tab of the selected Data Hub profile. MIM This agent do not publish or access any additional MIM parameters. For information about the MIM and a list of the general MIM parameters, see the Desktop User's Guide .

---

# Document 1141: Aggregation Example - Association of IP Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606151/Aggregation+Example+-+Association+of+IP+Data
**Categories:** chunks_index.json

To illustrate the Aggregation agent's features, an association example according to the following workflow setup is presented below. The workflow is handling IP traffic data, and will group information from routers and the corresponding network access servers. Open An example where an Aggregation agent is used to associate IP data The Netflow agent collects router data and logs the interacting network elements' addresses and amount of bytes handled, while the Radius agent keeps track of who has initiated the connection, and for how long the connection was up. Thus, each user login session consists of two Radius UDRs (start and stop), and one or several Netflow UDRs. The Aggregation agent is used to associate this data from each login session. These additional rules apply: A Radius UDR belonging to a specific login session must always arrive before its corresponding Netflow UDRs. If a Netflow UDR arrives without a preceding Radius UDR, it must be deleted. Within a Netflow UDR, the user initiating the session may act as a source or destination, depending on the direction of data transfer. Thus, it is important to match the IP address from the Radius UDRs with source or destination IP from the Netflow UDRs. Note! The Radius specific response handling will not be discussed in this example. For further information, see Radius Agents . Session Definition For each session, all the necessary data must be saved. A suggestion of useful variables for this scenario is described below. Note! The input UDRs are not stored. Information from the UDRs is extracted and saved in the session variables. The Ultra definition for the session type: session ExampleSession { string user; string IPAddress; long sessionID; long downloadedBytes; long uploadedBytes; }; Variable Description Variable Description user The user initiating the network connection. This value is fetched from the start of Radius UDR. IPAddress The IP address of the user initiating the network connection. This value is fetched from the start Radius UDR. sessionID A unique ID grouping a specific network connection session for the specific user. This value is fetched from the start Radius UDR. downloadedBytes The amount of downloaded bytes according to information extracted from Netflow UDRs. uploadedBytes The amount of uploaded bytes according to information extracted from Netflow UDRs. Association - Radius UDRs The Radius UDRs are the Aggregation session-initiating units. They may be of two types in this example; start or stop. Open The Aggregation profile - Association tab - Radius UDRs This is how arriving Radius UDRs are evaluated when configured according to the figure The Aggregation Profile - Association Tab - Radius UDRs: Initially, the UDR is evaluated against the Primary Expression. If it evaluates to false , all further validation is interrupted and the UDR will be deleted without logging (since no more rules exist). Usually, invalid UDRs are set to be deleted. In this case, only the UDRs of type start ( acctStatusType=1 ) or stop ( acctStatusType=2 ) are of interest. If the Primary Expression evaluation was successful, the field Framed_IP_Address entered in the ID Fields area, together with the Additional Expression (if any) are used as secondary verification. If it evaluates to true , the UDR will be added to the session, if not - refer to the subsequent step. Create Session on Failure is the final setting. It indicates if a new session will be created if no matching session has been found in Step 2. Association - Netflow UDRs As previously mentioned, the IP address to match against in the Netflow UDRs depends on if data is being uploaded or downloaded. This results in the session initiator being either the source or destination. Hence, both these fields need to be evaluated in the Aggregation agent: Open The Aggregation profile - Association tab - Netflow UDRs This is how arriving Netflow UDRs are evaluated when configured according to the figure The Aggregation Profile Editor - Association Tab - Netflow UDRs: If the DestinationIP, situated in the ID Fields area in the first Rules tab, does not match any existing session, and no new session is created. If a match is found, the UDR is associated with this session. Regardless of the outcome of the first rule, all rules are always evaluated. Hence the second rule is evaluated. If the SourceIP situated in the ID Fields area in the second Rules tab does not match any existing session, no new session is created. If a match is found, the UDR is associated with this session. The APL Code From the APL code (the agent configuration dialog), all actions related to both initiating and matching a session are defined. When a session is considered associated, the session variables are saved in a new UDR Type ( outputUDR ( out )) containing fields with the same name as the variables. Note! The timeout of a session is set to five days from the current date. Outdated sessions are removed and their data is transferred to a UDR of type outputUDR , which is sent to ECS. import ultra.Example.Out; sessionInit { Accounting_Request_Int radUDR = (Accounting_Request_Int) input; session.user = radUDR.User_Name; session.IPAddress = radUDR.framedIPAddress; session.sessionID = radUDR.acctSessionId; } consume { /* Radius UDRs. If a matching session is found, then there are two Radius UDRs and the session is considered completed. Remove session and route the new UDR. */ if (instanceOf(input, Accounting_Request_Int)) { Accounting_Request_Int radUDR = (Accounting_Request_Int)input; if (radUDR.acctStatusType == 2 ) { OutputUDR finalUDR = udrCreate( OutputUDR ); finalUDR.user = session.user; finalUDR.IPAddress = (string)session.IPAddress; finalUDR.downloadedBytes = session.downloadedBytes; finalUDR.uploadedBytes = session.uploadedBytes; udrRoute( finalUDR ); sessionRemove(session); return; } } /* Netflow UDRs. Depending on if the user downloaded or uploaded bytes, the corresponding field data is used to update session variables. */ if (instanceOf(input, V5UDR)) { V5UDR nfUDR = (V5UDR)input; if ( session.IPAddress == nfUDR.SourceIP ) { session.downloadedBytes = session.downloadedBytes + nfUDR.BytesInFlow; } else { session.uploadedBytes = session.uploadedBytes + nfUDR.BytesInFlow; } } // A session will be considered outdated in 5 days. date timer=dateCreateNow(); dateAddDays( timer, 5 ); sessionTimeout( session, timer ); } timeout { // Outdated sessions are removed, and a resulting UDR is sent on. OutputUDR finalUDR = udrCreate( OutputUDR ); finalUDR.user = session.user; finalUDR.IPAddress = (string)session.IPAddress; finalUDR.downloadedBytes = session.downloadedBytes; finalUDR.uploadedBytes = session.uploadedBytes; udrRoute( finalUDR ); sessionRemove(session); }

---

# Document 1142: Duplicate UDR Inspector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607196/Duplicate+UDR+Inspector
**Categories:** chunks_index.json

To open the Duplicate UDR Inspector, click Manage  Duplicate UDR Inspector . Note! Duplicate UDR Inspector currently does not support Duplicate UDR profiles with Enable Separate Storage Per Workflow option selected. Note! Applicable only to Legacy Desktop, ensure that the Read Only check box is selected unless you need to delete batches from the cache. If not selected, the profile will be locked and workflows using the profile will not be able to write to the cache. Inspect Duplicate UDR Batches Once the search criteria have been specified in the Filter dialog, the Duplicate UDR Inspector table is populated with matching batches. Open The Duplicate UDR Inspector Table The Duplicate UDR Inspector displays a table of duplicate UDR batches that were detected in workflows that contained a Duplicate UDR Agent. The table has the following columns: Item Description Item Description ID The index of the batch in the search results. Processed Date The date when the batch was processed. Content Start / End The Duplicate UDR Detection agent stores batches in date segments. These columns show the date range of the Indexing field value of the UDR that was used for duplication checks during transaction. If the transaction contains dates older than the Max Cache Age, configured in the Duplicate UDR profile, Outside range is displayed. If both Start and End show Outside range, all dates in the transaction were older than Max Cache Age. These columns are only visible if Date Field is enabled in the Duplicate UDR profile. Records The number of records (UDRs) processed for a given batch. Duplicates The number of duplicates found in the batch. Action MIM Values The MIM data stored for the batch. Click this button to view all MIM values. You can use the Display page selector at the bottom of the page to navigate between pages. The table displays up to 500 Duplicate UDR transactions per page. You can also select individual batches by selecting the checkbox and choose to either Delete or Clear selection(s) . The Duplicate UDR Inspector table also has the following options: Filter Refresh Delete Filter Clicking this button opens the Filter window where you can specify the UDR profile and a date range for the required results. Open Duplicate UDR Inspector Filter Dialog Menu option Description Duplicate UDR profile Click Browse button to select the appropriate Duplicate UDR Profile to filter the results for. Processed start date and time Processed end date and time The Duplicate UDR Detection agent stores batches in date segments. Populate the date-time picker to filter the results for duplicates detected between a certain time period. Close Click to close the Filter window. Reset Click to reset all selections in the Filter window. OK Click to apply the filters and close the Filter window. Refresh and Delete Click Refresh button to refresh the data in the Duplicate UDR results table. Click Delete to delete all the rows in the table.

---

# Document 1143: ADLS2 File Forwarding MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737945/ADLS2+File+Forwarding+MultiForwardingUDR+Input
**Categories:** chunks_index.json

Warning! MultiForwarding is not fully supported for ADLS2 File Forwarding as of version 8.1.5.0 When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the package FNT. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is checked. If not checked, a runtime error will occur if a previously unknown directory exists in the FNTUDR of an incoming MultiForwardingUDR . Every configuration option referring to bytearray input is ignored when MultiForwardingUDR s are expected. Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDR s. import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previous in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the root directory. The Create Non-Existing Directories check box under the Filename Template tab in the configuration of the forwarding agent must be checked if the directories do not previously exist.

---

# Document 1144: Diameter Syntax Description - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685098/Diameter+Syntax+Description
**Categories:** chunks_index.json

In this section, you will find a detailed description of the import and export syntax formats sup po rted. The section contains the following subsections: Diameter ABNF Specification Syntax Diameter XML Specification Syntax

---

# Document 1145: Execution Elasticity - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881745
**Categories:** chunks_index.json

MediationZone can scale out both vertically and horizontally. Vertically by adding CPU and/or memory to the host, and horizontally by adding Execution Contexts (ECs) and/or Workflow instances to the execution cluster. A typical scale out scenario, is an online use case where the load is expected to grow over time. The solution is adding several workflow instances executing in parallel on additional ECs. This can be achieved automatically by linking Workflow Templates to EC Templates/Groups. Then, each EC member started, will automatically start up a Workflow Instance. In the example below, the ECs are all named "A-*", and they will start the next Workflow Instance of the Template 'Base'. It is good practice to set up surveillance of the throughput and performance of the deployment, then trigger startup of a new EC when a threshold has been reached. The reverse works as well; that is, you can scale down by removing EC:s using the same mechanism. A summary of the different Desktop dialogues used for enabling automatic scaling: Pico Manager ; defines the EC Group/Template: Workflow Template ; defines which EC Group/Template the workflows shall execute on. Execution Manager  Autostart tab ; this is where the status can be viewed. In the example below, the EC Group/Template is "ecbackend", and the WF Template is "realtime_realtime_loadbalance". Open You can link several Workflow Templates to the same EC Group/Template. This way, you can start several workflows on one EC automatically.

---

# Document 1146: Data Veracity Task Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032375
**Categories:** chunks_index.json

The Data Veracity Task agent configures and schedules automated repair tasks for UDRs located in Data Veracity tables. When triggered by a workflow group scheduler, the agent will execute all repair rules that were added into the task agent itself. To open the Data Veracity task agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button Open Data Veracity Task agent configuration Data Veracity Tab Setting Description Setting Description Profile Click Browse to select a predefined Data Veracity Profile. The profile contains the configuration of the Data Veracity connection details and the Data Veracity table schema sql generation. For further information, see Data Veracity Profile . Repair Configurations Adding a repair configuration into the table will allow the agent to determine which UDR to repair, which saved filter to use to source out the UDRs that needed to be repaired as well as which repair rule to execute on the chosen UDRs. Error Behaviour On Error This combo box displays two options that determine how the workflow will behave when the repair job encounters an issue during execution. When set to Ignore, the agent will ignore any errors by a repair job and continue executing the next repair job in the Repair Configurations table. When it is set to Abort, the agent will trigger the workflow to abort when a repair job hits a single error. The default value is Ignore.

---

# Document 1147: Avro Decoding/Encoding Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656774/Avro+Decoding+Encoding+Functions
**Categories:** chunks_index.json

The functions described in this section are used to decode binary encoded Avro messages. The following functions for AVRO Decoding/Encoding described here are: 1 avroInitialize 2 avroDeserialize 3 avroSerialize avroInitialize This function initialises a schema registry provider that is used to obtain schemas used in avroDeserialize and avroSerialize functions void avroInitialize(string schemaRegistry, string schemaField) Parameter Description Parameter Description schemaRegistry The binary encoded Avro message to decode schemaField A DynamicAvro UDR Returns avroDeserialize This function decodes a binary encoded Avro message to a DynamicAvro UDR DynamicAvro avroDeserialize ( AvroDecoderUDR avroDecoderUDR) Parameter Description Parameter Description avroDecoderUDR AvroDecoderUDR containing a bytearray with a binary encoded Avro message to decode Returns A DynamicAvro UDR containing decoded message Note avroDeserialize function has to be used inside try-catch block if the exception handling is required Example - Example - Decoding AVRO binary encoded message import apl.Avro; initialize { string schemaRegistry = "http://localhost:8081/schemas/ids"; string schemaField = "schema"; avroInitialize(schemaRegistry, schemaField); } consume { //This example assumes that the input of an analysis agent is a bytearray //representing complete Avro message debug(input); Avro.AvroDecoderUDR decoder = udrCreate(Avro.AvroDecoderUDR); decoder.readerSchemaID = "5"; decoder.writerSchemaID = "5"; decoder.data = input; Avro.DynamicAvro decodedAvro = (Avro.DynamicAvro)Avro.avroDeserialize(decoder); debug(decodedAvro); } avroSerialize This function encodes a schema defined structure using binary Avro encoder bytearray avroSerialize ( AvroEncoderUDR avroEncoderUDR ) Note avroSerialize function has to be used inside try-catch block if the exception handling is required Parameter Description Parameter Description avroEncoderUDR AvroEncoderUDR containing data to encode and specifying selected schemaID (data must match selected schema) Returns binary encoded Avro message Example - Encoding UDR to AVRO APL: import apl.Avro; string writerSchemaID = "5"; // assumes the correct registered schema is "5" initialize { string schemaRegistry = "http://localhost:8081/schemas/ids"; string schemaField = "schema"; avroInitialize(schemaRegistry, schemaField); } consume { Avro.AvroRecordUDR record = create_output(); Avro.AvroEncoderUDR encode_UDR = udrCreate(Avro.AvroEncoderUDR); encode_UDR.data = record; encode_UDR.writerSchemaID = writerSchemaID; bytearray serializedPayload = Avro.avroSerialize(encode_UDR); } Avro.AvroRecordUDR create_output(){ Avro.AvroRecordUDR record = udrCreate(Avro.AvroRecordUDR); record.fullname = "example.avro.User3"; map<string,any> fieldz = mapCreate(string,any); // name Avro.AvroRecordUDR name = udrCreate(Avro.AvroRecordUDR); map<string,any> namefields = mapCreate(string,any); mapSet(namefields, "firstName", ""); mapSet(namefields, "lastName", "Kula"); name.fullname = "example.avro.FullName2"; name.fields = namefields; mapSet(fieldz, "name", name); // favorite number mapSet(fieldz, "favorite_number", (int)1632); // colour mapSet(fieldz, "favorite_color", "Green"); // football team (spelled with one "o" in schema) Avro.AvroEnumUDR favorite_fotball_team = udrCreate(Avro.AvroEnumUDR); favorite_fotball_team.fullname = "example.avro.teams.teams"; favorite_fotball_team.symbol = "Djurgarden"; mapSet(fieldz, "favorite_fotball_team", favorite_fotball_team); // IP address list<any> ipAddresslist = listCreate(any); Avro.AvroFixedUDR ipv4Address = udrCreate(Avro.AvroFixedUDR); ipv4Address.fullname = "example.avro.ipv4Address"; ipv4Address.bytes = baCreateFromHexString("00ABCDEF"); listAdd(ipAddresslist, ipv4Address); Avro.AvroFixedUDR ipv6Address = udrCreate(Avro.AvroFixedUDR); ipv6Address.fullname = "example.avro.ipv6Address"; ipv6Address.bytes = baCreateFromHexString("00ABCDEFEFEFEFEFEFEFEFEFEFEFEFEF"); listAdd(ipAddresslist, ipv6Address); mapSet(fieldz, "ipAddresses", ipAddresslist); // favoriteFoodList Avro.AvroRecordUDR favoriteFoodList = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish1 = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish2 = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish3 = udrCreate(Avro.AvroRecordUDR); map<string,any> dish3Map = mapCreate(string,any); mapSet(dish3Map, "dish", "Brown beans"); mapSet(dish3Map, "next", null); dish3.fields = dish3Map; dish3.fullname = "example.avro.favoriteFood"; map<string,any> dish2Map = mapCreate(string,any); mapSet(dish2Map, "dish", "Pannkakor"); mapSet(dish2Map, "next", dish3); dish2.fields = dish2Map; dish2.fullname = "example.avro.favoriteFood"; map<string,any> dish1Map = mapCreate(string,any); mapSet(dish1Map, "dish", "Ramen"); mapSet(dish1Map, "next", dish2); dish1.fields = dish1Map; dish1.fullname = "example.avro.favoriteFood"; favoriteFoodList.fullname = "example.avro.favoriteFood"; favoriteFoodList.fields = dish1Map; mapSet(fieldz, "favoriteFoodList", favoriteFoodList); // salary mapSet(fieldz, "salary", 347857486343); // myFixed Avro.AvroFixedUDR myFixed = udrCreate(Avro.AvroFixedUDR); bytearray myFixed_bytes = baCreateFromHexString("DEADBEEF"); myFixed.bytes = myFixed_bytes; myFixed.fullname = "example.avro.myfixed"; mapSet(fieldz, "myFixed", myFixed); // myFloat mapSet(fieldz, "myFloat", (float)0.4); // myDouble mapSet(fieldz, "myDouble", (double)0.5); // RootUsers map<string,any> rootUsers = mapCreate(string,any); Avro.AvroRecordUDR olleBack = udrCreate(Avro.AvroRecordUDR); olleBack.fullname = "example.avro.RootUsers"; map<string,any> fields1 = mapCreate(string,any); mapSet(fields1, "rootUser", "Allan"); mapSet(fields1, "privileges", (int)3); olleBack.fields = fields1; Avro.AvroRecordUDR nisseHult = udrCreate(Avro.AvroRecordUDR); nisseHult.fullname = "example.avro.RootUsers"; map<string,any> fields2 = mapCreate(string,any); mapSet(fields2, "rootUser", "Guran"); mapSet(fields2, "privileges", (int)2); nisseHult.fields = fields2; mapSet(rootUsers, "olleBack", olleBack); mapSet(rootUsers, "nisseHult", nisseHult); mapSet(fieldz, "rootUsers", rootUsers); // array_of_maps list<any> array_of_maps = listCreate(any); map<string,any> map1 = mapCreate(string,any); Avro.AvroRecordUDR Nyckel = udrCreate(Avro.AvroRecordUDR); map<string,any> nyckelFields = mapCreate(string,any); mapSet(nyckelFields,"favoriteUser", "Sture D.Y."); mapSet(nyckelFields,"favoriteNumber", (int)10); Nyckel.fullname = "example.avro.some_record"; Nyckel.fields = nyckelFields; mapSet(map1, "Nyckel", Nyckel); listAdd(array_of_maps, map1); map<string,any> map2 = mapCreate(string,any); Avro.AvroRecordUDR Nyckel2 = udrCreate(Avro.AvroRecordUDR); map<string,any> nyckel2Fields = mapCreate(string,any); mapSet(nyckel2Fields,"favoriteUser", "Sture D.A."); mapSet(nyckel2Fields,"favoriteNumber", (int)12); Nyckel2.fullname = "example.avro.some_record"; Nyckel2.fields = nyckel2Fields; mapSet(map2, "Nyckel2", Nyckel2); listAdd(array_of_maps, map2); mapSet(fieldz, "array_of_maps", array_of_maps); // debug("--------------- fields ------------------"); // debug(fieldz); // debug("--------------- fields ------------------"); record.fields = fieldz; debug("-------------- record -------------------"); debug(record); debug("-------------- record -------------------"); return record; }

---

# Document 1148: Appendix C - PostgreSQL HBA Settings - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029924/Appendix+C+-+PostgreSQL+HBA+Settings
**Categories:** chunks_index.json

Below is an example of a PostgreSQL host-based authentication configuration in $PGDATA/pg_hba.conf . Example - PostgreSQL HBA Configuration File # TYPE DATABASE USER ADDRESS METHOD # "local" is for Unix domain socket connections only local all all peer # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 ident

---

# Document 1149: PCC System Administration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816223/PCC+System+Administration
**Categories:** chunks_index.json

Search this document: This document describes the different tasks and areas you need to know as a system administrator of a PCC System. Terms and Acronyms This section contains glossaries for all terms and acronyms used throughout the PCC and MediationZone documentation. PCC Terms and Acronyms Term/Acronym Definition Term/Acronym Definition PCC Policy and Charging Control PCRF Policy and Charging Rules Function 3GPP 3rd Generation Partnership Project General Terms and Acronyms For information about general terms and acronyms used in this document, see the Terminology document. Chapters The following chapters and sections are included: System Architecture and Requirements PCC Basic Administration PCC High Availability for PCC Configuration of Data Repository for PCC Backup and Maintenance for PCC

---

# Document 1150: Managing Picos - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744473/Managing+Picos
**Categories:** chunks_index.json

Picos can be managed using either topo or in the Pico Management screen on the Desktop. This chapter includes the following sections: Managing Picos with Topo Managing Picos in Desktop

---

# Document 1151: Alarm Detection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670892
**Categories:** chunks_index.json

Alarm Detection helps you define the criteria for the generation of alarm messages. You select a condition, or combine a set of conditions, that within specific limits, generate an alarm message. To monitor the system alarms, you use the Desktop. Note that you can deliver alarm messages to SNMP monitoring systems, as well. To create a new Alarm Detection configuration, click Build  New Configuration in the Desktop, and then select Alarm Detection from the Configurations window. To open an existing Alarm Detection configuration, double-click on the configuration in the Configuration browser, or right-click a configuration and then select View Configuration . The Alarm Detection configuration contains the standard configuration buttons as described in Common Configuration Buttons and one additional button: Button Description Button Description Open To define a variable to use in the APL code, see the APL Reference Guide , and the section below, Workflow Alarm Value for further information. Defining an Alarm Detection To define Alarm Detection, you must Configure the Alarm Detection and Add an Alarm Condition. An Alarm Detection definition is made up of: A condition, or a set of conditions, see the section below, Alarm Conditions An object such as host, pico instance, or workflow, that the alarm should supervise The parameter that you want the alarm to supervise, for example, the Statistics value Time and value limits of supervision To create a valid alarm detection configuration make sure that: The Alarm Detection includes at least one condition. Two conditions within an alarm guard the same object: host, pico instance, or workflow. Two conditions are set to the same time interval criteria. This section contains the following subsection: Alarm Detection Configuration

---

# Document 1152: Web Service Profile Saving - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002594/Web+Service+Profile+Saving
**Categories:** chunks_index.json

When saving a WS profile, the profile will be saved in the folder selected in the Save As dialog with the name entered.

---

# Document 1153: Installation and Configuration of Legacy Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741690/Installation+and+Configuration+of+Legacy+Desktop
**Categories:** chunks_index.json

You can choose how you want to display the Legacy Desktop, and you can start the Legacy Desktop for multiple instances of the Platform via the Desktop Launcher application, or you can start it in your browser. Refer to the relevant section for how to install and configure the Legacy Desktop for each of these options. This chapter includes the following sections: Desktop Launcher

---

# Document 1154: Storage Profiles - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031564/Storage+Profiles
**Categories:** chunks_index.json

To store sessions in Couchbase, Redis, Elasticsearch or SQL, you must create a profile for the respective storage type. Couchbase For information about how to configure a Couchbase profile, see Couchbase Profile . Elasticsearch For information about how to configure an Elasticsearch profile, see Elasticsearch Profile Redis For information about how to configure a Redis profile, see Redis Profile . SQL You will use the Database profile to configure the connection string for the SQL based databases that the Aggregation session storage supports. For information about how to configure a Database profile, see Database Profile .

---

# Document 1155: Enabling Client Authentication - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647240/Enabling+Client+Authentication
**Categories:** chunks_index.json

TLS can be set up to demand authentication from all clients that run outside the local host. The Platform, ECs, SCs will then ask for valid certificates from each connecting pico instance. After you have set up TLS, as described in TLS Standard Setup , follow the steps below to enable client authentication. Set the properties that specifies the keystore path and the passwords in each Execution Container. Use the same values as for the Platform Container. There are two methods that you can use to make the client/server certificates available on all containers. Copy the keystore file that was created in TLS Standard Setup from the Platform Container to each of the Execution Containers. The target path is specified by the property pico.rcp.tls.keystore . Create a keystore and key pair on each Execution Container, then export and import the certficates. The certificate from the Platform Container must be exported to all Execution Containers. The certificates from the Execution Containers must be exported to the Platform Container. Run the following command to export a certificate: $ keytool -keystore <keystore file> -export -rfc -alias <alias_name> -file <certificate filename> Run the following command to import a certificate: $ keytool -import -alias <alias_name> -file <certificate_file_name> -keystore <keystore file> -keypass <password> -storepass <password> Enable client authentication by setting the property pico.rcp.tls.require_clientauth to true . Restart the system Desktops When client authentication is enabled, each desktop installation must authenticate itself to the Platform using a private key. You have to import this key in the Desktop Launcher in order to connect to the Platform. The certificate must also be imported as a trusted certificate in the Platform keystore. Run the following command to create a keystore that contains a private key. $ keytool -genkey -keystore <keystore file> -alias <alias> -keyalg RSA -keysize 2048 Copy the keystore file to the host that will run the Desktop Launcher. Create a certificate file that is associated with the key that you created in the previous step. $ keytool -keystore <keystore file> -exportcert -alias <alias> -file <certificate filename> Import the certificate to the platform $ keytool -keystore <platform keystore> -import -file <certificate filename> -alias <alias> Open the Desktop Launcher. Right-click on a MediationZone instance and then select Instance Settings from the popup menu. Select the Security tab. Right-click on the text field under Client Key and select Import Key From File . Select the key file that you copied in step 2.

---

# Document 1156: Suspend Execution - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638678
**Categories:** chunks_index.json

This section includes information about the configuration option Suspend Execution. The Suspend Execution configuration enables you to apply a restriction that prevents specific workflows and/or workflow groups from running in specific periods of time. Note! Grouping workflows is possible in the Suspend Execution for the sole purpose of suspending them during a defined period of time. These groups are not workflow group configurations. Suspend Execution To open the Suspend Execution configuration, click Build  New Configuration , and then select Suspend Execution from the menu. Suspend Execution Buttons The toolbar changes depending on which configuration type that is currently open in the active tab. There is a set of standard buttons that are visible for all configurations and these buttons are described in the Common Configuration Buttons . There is one additional button for Suspend Execution. Button Description Button Description Open Enables you to include or exclude the following from the Available To Add list: Workflow Groups Workflows Batch and Task workflows Real-time workflows Suspend Execution Tabs Suspend Execution includes two tabs: Members Tab Scheduling Tab Members Tab In the Members tab you select workflows that you want to suspend temporarily, i.e. workflows that should not be executed during specific periods of time. Open The Suspend Execution configuration - Members tab Item Description Item Description Available to Add Upper pane: Displays a tree view of the workflows and workflow groups that are saved within their respective configurations that you can apply execution suspension for. Lower pane: Shows a list of workflows that are included in the workflow configuration that you select from in the upper pane. Note! A workflow group can be a member of another workflow group. Members A list of the current workflow group members. Button Description Button Description Open Click to add a member to the list. Scheduling Tab From the Scheduling tab you suspend and enable the activation of workflows that you select on the Members tab, if they are executed during the suspension interval. Open The Suspend Execution configuration - Scheduling tab The Scheduling tab contains the following table columns: Column Description Column Description Time When you click the Add Row button that is located at the bottom of the Scheduling tab, a new row appears in the Scheduling tab table. This row includes the current time stamp. You change the time stamp to a future date by first double-clicking the row and then clicking the button that appears in the selected row. Then, from the Time Chooser dialog, you select a time and a date. Note! As soon as a specified date has passed, according to the Desktop (client) clock, the text in the affected row becomes italicized. Enable Double-click the table cell to select it, and then check to enable the activation of the workflow at the specified time stamp. Disable Double-click the table cell to select it, and then check to suspend the workflow at the specified time stamp. To Suspend a Workflow On the Members tab, from the Available To Add list, select the workflow or the workflow group, that you want to suspend during a certain period of time. Click to select the row; the button appears. Click the button to move each selection into the Members list on the right-hand side of the tab. On the Scheduling tab, click the button; the current timestamp is added to the table. At this point, you can either suspend the workflow immediately by checking Disable , or you can edit the time and date to have the suspension start later. To do this, select the relevant row and click the ( Edit ) button; the Date Chooser dialog opens. Select the year, month, day, and input the hour and minutes. Click OK . The row is updated with a later time stamp. Check Enable to remove the execution suspension, or Disable to suspend a workflow at the specified time. Save the Suspend Execution configuration. The Platform should be running when both the suspend and the enable activation dates occur, for these actions to be effective.

---

# Document 1157: Local Archiving Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640185/Local+Archiving+Agent
**Categories:** chunks_index.json

You open the Archiving Local agent configuration dialog from a workflow configuration. To open the Archiving forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Archiving Local from the Forwarding tab of the Agent Selection dialog. Open The Archiving Local agent configuration dialog - Archiving Local tab The following options are available in the Archiving Local agent configuration: Setting Option Setting Option Profile Name of the Archive profile to be used when determining the attributes of the target files. All workflows in the same workflow configuration using the Archiving Local agent can use separate archiving profiles, if that is preferred. In order to do that the profile must be set to Default in the Workflow Table tab found in the Workflow Properties dialog. After that each workflow in the table can be appointed the correct profile. Input Type The agent can act on two input types. The behavior varies depending on the input type that you configure the agent with. The default input type is bytearray. For information about the agent behavior with the MultForwardingUDR input type, see Archiving Agents MultiForwardingUDR Input . Compression Compression type of the target files. Determines if the agent will compress the files before storage or not. No Compression - the agent will not compress the files. Gzip - the agent will compress the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. The configuration of the filenames is managed in the Filename Template tab, only. Agent Directory Name Possibility to select one or more MIM resources to be used when naming a sub-directory in which the archived files will be stored. If more than one MIM resource is selected, the values making up the directory name will automatically be separated with a dot. Note! If at least one Agent Directory Name is selected in Directory Templates in the Archive profile , this directory field is used. Produce Empty Files If enabled, files are produced although containing no data. Note! The names of the created files are determined by the settings in the Filename Template tab. For further information about the Filename Template service, see 3.1.6 Workflow Template .

---

# Document 1158: Alarm Detection Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638956/Alarm+Detection+Configuration
**Categories:** chunks_index.json

Configuring an Alarm Detection Create an Alarm Detection configuration by clicking the New Configuration button in the upper left part of the Desktop window, and then selecting Alarm Detection from the menu. Open Alarm Detection configuration dialog Select the Workflow Alarm Value Names option to define a variable you can use in the APL code, see the APL Reference Guide . Field/Option Description Field/Option Description Alarm Detection Enabled Select this checkbox to turn on alarm detection. Description Enter a statement that describes the Alarm Detection that you are defining. Severity Select the importance priority of the alarm from the drop-down list. The available severities are: Indeterminate Warning Minor Major Critical Adding an Alarm Condition At the bottom of the Alarm Detection configuration, click the Add button. The Add Alarm Condition dialog box opens. Open Alarm Condition configuration dialog Select a condition in the Alarm Condition drop-down list. Alarm Conditions The Alarm conditions enable you to define specific situations or events for which you want the system to produce an alarm. You configure a condition to produce an alarm whenever a certain behavior occurs, within specific limits. Note! An alarm is generated only if all conditions in the Alarm Detection are met. The Alarm condition limits are reset: Every time you restart the platform Every time you save the alarm configuration When you resolve the alarm The Alarm Conditions that you can choose from are: 1 Host Statistic Value 2 Pico Instance Statistic Value 3 System Event 4 Workflow Alarm Value 5 Workflow Execution Time 6 Workflow Group Execution Time 7 Workflow Throughput Host Statistic Value The Host Statistic Value condition enables you to set up an Alarm Detection for the various statistic values by the host. Open Setting Host Statistic Value conditions Item Description Item Description Event Properties Host Select a host for applying the alarm condition. Statistic Value The statistical criteria on the host include: Processes Waiting For Run (#) Processes in Sleep (#) Processes Swapped Out (#) Swapped In From Disk (blocks/s) Swapped To Disk (blocks/s) Context Switches (/s) CPU User Time (%) CPU System Time (%) Limits Specify the condition for the alarm to be triggered. The options are Exceeds or Falls Below the selected statistic value. Enable the During Last checkbox to further focus on the X duration of Minutes/ Hour. Pico Instance Statistic Value The Pico Instance Statistic Value condition enables you to set up an Alarm Detection for the various statistic values by Pico instances. Open Adding Pico Instance Statistic Value Item Description Item Description Event Properties Pico Instance Select a Pico instance for configuring the alarm, it can be Platform, EC Statistic Value The statistical criteria on the Pico instance include: Used Memory (Kb) Garbage Collection Count (#) Garbage Collection Time (ms) Thread Count (#) Open files count (#) CPU usage time (%) Limits Specify the condition for the alarm to be triggered. The options are Exceeds or Falls Below the selected statistic value. Enable the "During Last" checkbox to further focus on the X duration of Minutes/ Hour. System Event The System Event condition enables you to setup an Alarm Detection for the various Event types. Open Setting up System Event condition Item Description Item Description Event Properties Type Select an event-related reason for an alarm to be invoked. For a detailed description of every event type, see Event Types . Filter Use this table to define a filter of criteria for the alarm messages that you are interested in. To define an entry, double-click on the row. The Edit Match Value dialog box opens. Click the Add button to add a value. Limits Specify the condition for the alarm to be triggered. The options are based on the number and frequency of occurrence of the event: Occurred Once , Occurred More Than , Occurred Less Than . In During Last , specify the time frame during which the Limits value should be compared. If a match is detected, an alarm is invoked. Caution! The parameters in the following example do not apply to any specific system and are only presented here to enhance understanding of the alarm condition. Example - Configuring a System Event condition Configure an Alarm Detection that applies the System Event condition. Open Configuring an Alarm Detection On the Edit Alarm Condition dialog box, from the Event Type drop-down list, select Workflow State Event . On the Filter table double-click workflowName ; the Edit Match Value dialog box opens. Click Add to browse and look for the specific workflow. Enter a limit of occurred more than 3 times during the last 24 hours. Open Editing an Alarm Condition The alarm will be triggered by every 4th occurrence of a "Workflow State Event" during the last 24 hours. Workflow Alarm Value The Workflow Alarm Value condition is a customizable alarm condition. It enables you to have the Alarm Detection watch over a variable that you create and assign through the APL code. To apply the Workflow Alarm Condition use the following guidelines: Create a variable. Assign the variable with a value. Setup the Workflow Alarm Value condition. To create a variable name: From the Edit menu in the Alarm Detection configuration menu, select Workflow Alarm Value Names . The Workflow Alarm Value dialog box opens. Click the Add button and enter a variable name, e g CountBillingFiles. Click OK and then close the Workflow Alarm Value dialog box. To define the Value field: In the APL code, include the command d ispatchAlarmValue . For example: consume { dispatchAlarmValue ("CountBillingFiles",1); udrRoute(input); } To configure the Workflow Alarm Value Condition: At the bottom of the Alarm Detection configuration, click Add ; the Add Alarm Condition dialog box opens. From the Alarm Condition drop-down list select Workflow Alarm Value . From the Value drop-down list, select the name of the variable that you created. Click Browse... to select the Workflow that the Alarm Detection should guard. Configure the Limits according to the description of The Workflow Alarm Value and click OK . Open The Workflow Alarm Value configuration Item Description Item Description Workflow Properties Value Select an alarm value from the drop-down list. Workflow Click Browse... to enter the workflow instance(s) that you want to apply the alarm to. Limits Summation : Select this check box to add up the dispatchAlarmValue variable (countBillingFiles in the figure above, The Workflow Alarm Value configuration) whenever it is invoked. Alarm Detector compares this total value with the alarm limit (exceeds or falls below), and generates an alarm message accordingly. Note: Selecting Summation means that the During Last entry refers to the time period during which a sum is added up. Once the set period has ended, that sum is compared with the limit value. For All Workflows : Select this check box to add up the values (see Summation above) of all the workflows that the alarm supervises. Alarm Detector compares this total value with the alarm limit (exceeds or falls below), and generates an alarm message accordingly. Note: This check box can only be selected when Workflow is set to Any. Workflow Execution Time The Workflow Execution Time condition enables you to generate an alarm whenever the execution time of a particular, or all workflows, exceed or fall below the time limit that you specify. Open Workflow Execution Time configuration Item Description Item Description Workflow The default workflow value is Any . Use this value when you want to apply the condition to all the Workflows. Otherwise, click Browse to select a Workflow that you apply the condition to. Caution! The parameters in the following example do not apply to any specific system and are only presented here to enhance understanding of the alarm condition. Example - Configuring a Workflow Execution Time condition A telecom provider wants the system to identify a workflow that has recently run out of input, and to generate an alarm that warns about a processing time that is too short. Configure an Alarm Detection to use the Workflow Execution Time condition. Open Using Workflow Execution Time condition in the Alarm Detection Click Browse ...; the Workflow Instance Selection dialog box opens. At the bottom of the dialog box click Any or select the workflow specific for this alarm. Set a limit to generate an alarm if the workflow execution time Falls below 2 seconds. Open Setting a limit to an Alarm Condition An alarm is generated whenever an active workflow seems to process data too fast (in less than 2 seconds). Workflow Group Execution Time The Workflow Group Execution Time alarm condition enables you to generate an alarm whenever the execution time of a workflow group exceeds or falls below the time limit that you specify. Open Workflow Group Execution Time configuration dialog Item Description Item Description Workflow Group Click Browse... to enter the address of the workflow group to which you want to apply the alarm. Caution! The parameters in the following example do not apply to any specific system and are only presented here to enhance understanding of the alarm condition. Example - Configuring a Workflow Group Execution Time condition You want the system to generate an alarm if a billing workflow group has been active longer than 3 hours. Configure an Alarm Detection that uses the Workflow Group Execution Time condition. Open Using Workflow Group Execution Time condition in an Alarm Detection On the Edit Alarm Condition dialog box click Browse... to enter the workflow group you want the alarm detection to supervise. Open Editing the Alarm Condition Enter a limit of Exceeds 3 hours. The alarm will be triggered if the workflow group has been active longer than 3 hours. Workflow Throughput The Workflow Throughput alarm condition enables you to create an alarm if the volume-per-time processing rate of a particular workflow exceeds, or falls below, the throughput limit that you specify. Open Workflow Throughput configuration dialog Item Description Item Description Workflow Select a workflow with the throughput value, and the processing speed, that you want to supervise. For further information about the throughput value calculation, see Throughput Calculation in 3.1.8 Workflow Properties . An alarm is generated if the throughput value is not within the condition limits. Limits Specify the condition for the alarm to be triggered. The options are Exceeds or Falls Below the selected statistic value. Enable "During Last" checkbox to further focus on the X duration of Minutes/ Hour. Caution! The parameters in the following example do not apply to any specific system and are only presented here to enhance understanding of the alarm condition. Example - Configuring a Workflow Throughput condition You want the system to warn you on detection of decreased processing rate. Configure an Alarm Detection to use the workflow throughput condition. Open Using Workflow Throughput conditions On the Edit Alarm Condition dialog box click Browse... to select the workflow with the processing rate that you want to supervise. Enter a limit of Falls Below 50000 (batches, UDRs, Bytearray). Open Configuring Limits when adding an Alarm Condition The alarm will be triggered by every occurrence of a workflow slowing down its processing rate to a throughput that is lower than 50000 units per second.

---

# Document 1159: Batch-Based Real-Time Agents - UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741505
**Categories:** chunks_index.json

This section describes the UDR types that are used with the agents. The UDR types used by the agents can be viewed in the UDR Internal Format Browser . To open the browser open an APL Editor, in the editing area. Right-click and select UDR Assistance... and the browser opens. BeginBatch The BeginBatch UDR type is used to indicate that collection of a file has begun. This UDR type does not contain any data from the collected file. Field Description Field Description batchCount (long) This field contains the sequential numbering of the file being collected. fileName (string) This field contains the name of the file being collected. CancelBatch The CancelBatch UDR type is used to indicate that collection of a file has been cancelled. This UDR type does not contain any data from the collected file. Field Description Field Description batchCount (long) This field contains the sequential numbering of the file being collected. fileName (string) This field contains the name of the file being collected. EndBatch The EndBatch UDR type is used to indicate that the collection agent has routed the last file record to the workflow. This UDR type does not contain any data from the collected file. Field Description Field Description batchCount (long) This field contains the sequential numbering of the file being collected. fileName (string) This field contains the name of the file being collected. UDR types consumed by the Collecting or Forwarding Agents ForceEndBatch The Force EndBatch UDR type is used to force an end to the batch. The UDR has no specific fields.

---

# Document 1160: SAP HANA Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604229/SAP+HANA+Preparations
**Categories:** chunks_index.json

This section describes the preparations necessary when installing SAP HANA as a database and includes the following subsections: Extract Database Definition Files to SAP HANA SAP HANA Database Creation

---

# Document 1161: Python Agents in Real-Time Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739985/Python+Agents+in+Real-Time+Workflows
**Categories:** chunks_index.json

This section describes how to configure the Python agents in real-time workflows. The following general information applies: Threading Model In real-time workflows you may have several threads running in parallel. When routing a UDR to the Python collection or processing agent, the threading context is kept, which means that when using synchronous routes, the same thread that routed the UDR into the agent, will also route it out to the next agent. It also means that many threads may potentially be created in the Python Interpreter, one per defined workflow thread. You are recommended to use locks and mutexes available in the Python standard threading library to avoid threading issues. Garbage Collection A UDR that is routed to a Python agent is kept in both the Python and Java heaps as long as it is referenced. When the UDR is no longer referenced in Python, an automatic message is sent to Java saying that it may be released from the Java heap. If you need to route many large UDRs to Python and keep them alive for a longer period of time, a good solution could be to copy the relevant information into a suitable Python structure in order to allow the UDR to be released. If this is not the case, you should not have to take any specific measures in order for garbage collection to work as it should. Timeouts You can use timeouts to implement session timeouts or delayed actions. See Function Blocks for Agents in Real-Time Workflows for an example of how to write a timeout function block. Loading

---

# Document 1162: Legacy Kafka Producer - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138637/Legacy+Kafka+Producer
**Categories:** chunks_index.json

The Kafka Producer is a real-time agent that is responsible for sending data to Kafka. Configuration The Kafka Producer configuration contains the following settings: Setting Description Setting Description Profile Browse and select the profile the agent should use, as defined in the Legacy Kafka Profile . Standard Select this option if you do not require acknowledgments to be sent to an external system. Acknowledged Select this option to enable an acknowledgment to be sent back to an external system. Acknowledged execution mode consumes more resources since every outstanding request contains data from a response UDR. Route On Error Select this check box if you want a KafkaExceptionUDR , containing the error message, to be routed from the forwarding agent when an error occurs. You can only select this check box if you have selected Standard execution mode. Note! The emission of error UDRs is under flood protection, which means only one unique error message UDR is issued per second to prevent flooding of identical errors. In addition, if you set the number of retries to a value greater than 0 in the Advanced tab of the Producer tab in the Kafka Profile configuration dialog, the error message UDR is not emitted until the final retry has failed. Suppress Non-Fatal Errors Select this check box if you do not want to log non-fatal errors produced by the Kafka forwarding agent in the System Log. Input/Output Data Input Data Legacy KafkaUDR Output Data Legacy KafkaExceptionUDR

---

# Document 1163: Workflow Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation


---
**End of Part 49** - Continue to next part for more content.
