# RATANON/MZ93-DOCUMENTATION - Part 5/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 5 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~68.7 KB
---

The batch Aggregation agent's configuration dialog contains the following tabs: Aggregation - This tab contains the three subsidiary tabs, General , APL Code and Storage. Thread Buffer - For further information about the Thread Buffer tab, see Workflow Template . General Tab The General tab enables you to assign an Aggregation profile to the agent and to define error handling. With the Error Handling settings, you can decide what you want to do if no timeout has been set in the code or if there are unmatched UDRs. Open The Aggregation agent configuration dialog - General tab Setting Description Setting Description Profile In batch workflows, the profile must use file storage, Elasticsearch or SQL. All the workflow instances in the same workflow template can use different Aggregation profiles. For this to work, the profile has to be set to Default in the Field settings tab in the Workflow Properties dialog. After that, each workflow in the Workflow Table can be assigned with the correct profile. Force Read Only Select this check box to only use the Aggregation Storage for reading aggregation session data. Selecting this check box also means that the agent cannot create new sessions when an incoming UDR cannot be matched to an existing session. A UDR for which no matching session is found is handled according to the setting If No UDR Match is Found. If you enable the read only mode, timeout and defragmentation handling is also disabled. Note! When you are using file storage and sharing an Aggregation profile across several workflow configurations, the read and write lock mechanisms that are applied to the stored sessions must be considered: There can only be one write lock at a time in a profile. This means that all but one Aggregation agent must have the Force Read Only setting enabled. If all of the Aggregation agents are configured with Force Read Only , any number of read locks can be granted in the profile. If one write lock or more is set, a read lock cannot be granted. If Timeout is Missing Select the action to take if timeout for sessions is not set in the APL code using sessionTimeout . The setting is evaluated after each consume or timeout function block has been called (assuming the session has not been removed). The available options are: Ignore - Do nothing. This may leave sessions forever in the system if the closing UDR does not arrive. Abort - Abort the agent execution. This option is used if a timeout must be set at all times. Hence, a missing timeout is considered being a configuration error. Use Default Timeout - Allow the session timeout to be set here instead of within the code. If enabled, a field becomes available. In this field, enter the timeout, in seconds. If No UDR Match is Found Select the action that the agent should take when a UDR that arrives does not match any session, and Create Session on Failure is disabled: Ignore - Discard the UDR. Abort - Abort the agent execution. Select this option if all UDRs are associated with a session. This error case indicates a configuration error. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is activated only if Route is selected. APL Code Tab The APL Code tab enables you to manage the detailed behavior of the Aggregation agent. You use the Analysis Programming Language (APL) with some limitations but also with additional functionality. For further information see the APL Reference Guide . The main function block of the code is consume . This block is invoked whenever a UDR has been associated with a session. The timeout block enables you to handle sessions that have not been successfully closed, e g if the final UDR has not arrived. Open Aggregation agent configuration dialog - APL Code tab Item Description Code Area This is where you write your APL code. For further information about the code area and its right-click menu, see Text Editor in Administration and Management in Legacy Desktop . Compilation Test... Use this button to compile the entered code and check for validity. The status of the compilation is displayed in a dialog. Upon failure, the erroneous line is highlighted and a message, including the line number, is displayed. Outline Use this button to display or hide the APL Code Editor Outline navigation panel. The navigation panel provides a view of all the blocks, variables and methods in an APL code configuration and makes it possible to easily navigate between different types in the APL code. For further information on the Outline navigation panel, see Administration and Management in Legacy Desktop . Storage Tab The Storage tab contains settings that are specific for the selected storage in the Aggregation profile. Different settings are available in batch and real-time workflows. File Storage Open The Aggregation agent configuration dialog - Storage tab for File Storage Setting Description Setting Description Defragment Session Storage Files For batch workflows, the Aggregation session storage can optionally be defragmented to minimize disk usage. When checked, configure the defragmentation parameters: Defragment After Every [] Batch(es) Run defragmentation after the specified number of batches. Enter the number of batches to process before each defragmentation. Defragment if Batch(es) Finishes Within [] Second(s) Set a value to limit how long the defragmentation is allowed to run. This time limitation depends on the execution time of the last batch processed. If the last batch is finished within the specified number of seconds, the remaining time will be used for the defragmentation. The limit accuracy is +/- 5 seconds. Defragment Session Files Older Than [] Minute(s) Run defragmentation on session storage files that are older than this value to minimize moving recently created sessions unnecessarily often. Elasticsearch Storage and SQL Storage Open The Aggregation agent configuration agent - Storage tab for Elasticsearch Open The Aggregation agent configuration agent - Storage tab for SQL Setting Description Setting Description If Error Occurs in Storage Select the action that the agent should take when an error occurs in the storage: Ignore - Discard the UDR. Log Event - Discard the UDR and generate a message in the System Log. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is only activated if Route is selected. Disable Timeout Select this check box to disable the timeout handling.

---

# Document 85: The Error Format - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740273/The+Error+Format
**Categories:** chunks_index.json

Error UDRs can be helpful in case of troubleshooting, testing or monitoring. Each Error UDR contains information about the errors in a UDR and in case you have configured Error UDR you can see the following information regarding the error: reason source destination the raw request response There can be different reasons of an erroneous message, like: UDRs are too old (timed out) Unable to decode the UDR, or, The shared secret is not the same Field Description Field Description errorCode (int) The error code. The reason is specified in the errorCode and errorMessge . errorMessage (string) Detailed description of the error. remoteIP (ipaddress) Source or destination IP address of the NAS depending on type of the Radius agent and the message. remotePort (int) Source or destination port used for communication with the NAS depending on the type of Radius agent and the message. requestMessage (bytearray) Byteary with raw request or null. This field contains a request UDR. Depending on the settings on the NAS, it can be any of the following packet types : ACCESS_REQUEST ACCOUNTING_REQUEST DISCONNECT_REQUEST CHANGE_OF_AUTHORIZATION_REQUEST In terms of authentication, CHAP is supported. PAP, MSCHAP and EAP are not fully supported. To use these protocols, you must configure the APL code as required. See the example below, APL code to configure authentication using CHAP. For details on these standards, see RFC 2865 ( http://www.ietf.org/rfc/rfc2865.txt ), RFC 2866 ( http://www.ietf.org/rfc/rfc2866.txt ) and RFC 5176 ( http://www.ietf.org/rfc/rfc5176.txt ). An Ultra Format Definition must be designed to handle decoding of this field. responseMessage (bytearray) Byteary with raw response or null. The field containing a response UDR. Depending on the settings on the NAS, it can be any of the following packet types : ACCESS_ACCEPT ACCESS_REJECT ACCOUNTING_RESPONSE DISCONNECT_ACK DISCONNECT_NAK CHANGE_OF_AUTHORIZATION_ACK CHANGE_OF_AUTHORIZATION_NAK In terms of authentication, CHAP is supported. PAP, MSCHAP and EAP are not fully supported. To use these protocols, you must configure the APL code as required. See the example below, APL code to configure authentication using CHAP. For details on these standards, see RFC 2865 ( http://www.ietf.org/rfc/rfc2865.txt ), RFC 2866 ( http://www.ietf.org/rfc/rfc2866.txt ) and RFC 5176 ( http://www.ietf.org/rfc/rfc5176.txt ). An Ultra Format Definition must be designed to handle decoding of this field.

---

# Document 86: Handling Erroneous UDRs - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643269/Handling+Erroneous+UDRs+-+Batch
**Categories:** chunks_index.json

The SQL forwarding agent encapsulates en erroneous UDR along with the error message that describes the error, in a new UDR. This in turn, enables you to process the faulty UDR, adjust the processing according to the error type, and prevent the workflow from aborting due to selected errors types. Example - Handling erroneous UDRs in a batch workflow Consider the following workflow: Open An SQL forwarding workflow The SQL forwarding agent identifies the asciiSEQ_TI UDR as erroneous, creates an errorUDR that wraps together the original UDR with the error message that was generated: The erroneous UDR before and after SQL forwarding If you have selected the option Upon Exception, route entire executed batch instead of single UDR , an errorUDRList UDR is routed instead: Open The errorUDRList UDR

---

# Document 87: Batch-Based Real-Time Agents - Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741523/Batch-Based+Real-Time+Agents+-+Transaction+Behavior
**Categories:** chunks_index.json

This section describes the transaction behavior for the agents. For more information about general transaction behavior, refer to the section Transactions in Workflow Monitor . Transaction behavior for the agents differs from when the batch agents are added to batch workflows. Cancel Batch does not function in the same way in real-time workflows as it does in batch workflows. You configure in the Execution tab of agent configuration if you want Cancel Batch messages to be sent if a decoding error occurs, as described in Batch-Based Real-Time Agents - Agent Configuration . Cancel Batch messages are sent: If a decoding error occurs, and you have selected the Cancel Batch option in the agent configuration If you have selected for files to be decompressed in the Decompression tab of agent configuration and it fails, e g because a file is corrupt or is not compressed. When Cancel Batch is called, the file is handled according to how After Collection behavior has been specified in the agent configuration. Nothing is sent to ECS, and processing skips to the next batch. Publication The agent publishes commands that change the state of the file currently processed. Command Description Command Description Begin Batch Emitted before the first part of each collected file is fed into a workflow. End Batch Emitted after the last part of each collected file has been fed into the system. Acquisition The agent acquires commands from other agents and based on them generates a state change of the file currently processed. Command Description Command Description Cancel Batch If a Cancel Batch message is received, the file is handled according to how After Collection behavior has been specified in the agent configuration. Note! If the Cancel Batch behavior defined in the collection agent dialog is configured to abort the workflow, the agent will never receive the last Cancel Batch message.

---

# Document 88: Avro Types and UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205684981/Avro+Types+and+UDRs
**Categories:** chunks_index.json

Types mapping Types described in the Avro specification have to be mapped to types present in the MediationZone platform. You can find the mapping in the table below: Avro type Corresponding MZ type null null boolean boolean int int* long long* float float* double double* string string record AvroRecordUDR enum AvroEnumUDR fixed AvroFixedUDR *Numeric types need casting while preparing data for encoding. See Avro Decoder Example example to see the usage. UDRs The following UDRs should be used while working with Avro Decoder/Encoder AvroDecoderUDR AvroDecoderUDR is used as an input for Avro Decoder. The following fields are included in the AvroDecoderUDR : Field Description data (bytearray) This field contains a binary encoded avro message payload. This should be just a message payload without any metadata. readerSchemaID (string) Avro Reader SchemaID - ID of the compatible schema used for reading data writerSchemaID (string) Avro Writer SchemaID - schema used for encoding the message DynamicAvroUDR DynamicAvroUDR is an output of Avro Decoder. It consists of only one field called data . The type of data field depends on a schema used for decoding operation. It can be both primitive or complex type. See https://infozone.atlassian.net/wiki/spaces/MD92/pages/182648833/Avro+Types+and+UDRs#Types-mapping for mapping Avro types to MediationZone type. Example If type of the top element in a schema is record then the field type of data will be AvroRecordUDR and if type is string then the field type of data will be string. See Avro Decoder Example to see an example usage of this UDR. The following fields are included in the DynamicAvroUDR : Field Description data (any) Contains content decoded using a decoder AvroEncoderUDR AvroEncoderUDR is used as an input for Avro Encoder. The following fields are included in the AvroEncoderUDR : Field Description data (any) This field contains a UDR/type to be encoded using a selected schema writerSchemaID (string) Avro Writer SchemaID - schema used for encoding the message AvroEnumUDR AvroEnumUDR is used to represent Enum avro type. The following fields are included in the AvroEnumUDR : Field Description fullname (string) Name of the enum field (example example.avro.myFixed) symbol (string) Selected value of the specific enum type AvroFixedUDR AvroFixedUDR is used to representive Fixed avro type. The following fields are included in the AvroFixedUDR : Field Description bytes (bytearray) Byte value of the field fullname (string) Name of the fixed field (example example.avro.myFixed) AvroRecordUDR AvroRecordUDR is used to represent Avro Record structure. The following fields are included in the AvroRecordUDR : Field Description fields (map<string,any>) Map containing fields of the record. fullname (string) Name of the record including the namespace (example example.avro.User3)

---

# Document 89: FTP NMSC Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673191/FTP+NMSC+Agent+Configuration
**Categories:** chunks_index.json

You open the FTP NMSC collection agent configuration dialog from a workflow configuration. To open the configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select FTP NMSC from the Collection tab of the Agent Selection dialog. Switch Tab The Switch tab consists of configuration settings that are related to the remote host and directory, where the control and data files are located, and timezone location specification. Open FTP NMSC collection agent configuration - Switch tab Setting Description Setting Description Host Name Enter the name of the Host or the IP-address of the switch that is to be connected. User Name Enter the name of the user whose account on the remote Switch will enable the FTP session to be created. Password Enter the user password. File Information Detailed specification about control files that are going to be collected by the agent. Data Type Select from the drop down list the data file format, either SMS or MMS, that the agent should collect. File Directory Enter the physical path to the source directory on the remote Host, where the control and data files are saved. Switch Time Zone Select the timezone location. Timezone is used when updating the transaction control file. Advanced Tab The Advanced tab includes configuration settings that are related to more specific use of the FTP service. Open FTP NMSC collection agent configuration - Advanced tab Item Description Item Description File Name Detailed specification about the data file that is to be collected by the agent Prefix Enter the data files name prefix. Number Positions Select the length of the number-part in the data file name as follows: 01 for a two digit number 001 for a three digit number 0001 for a four digit number For example: If you select 0001, data file number 99 will include the following four digits: 0099 . Settings Detailed specification of specific use of the FTP service Command Port Enter the port number for the FTP server to connect to, on the remote Switch. Local Data Port Enter the local port number that the agent will listen on for incoming data connections. This port will be used when communication is established in Active Mode. If the default value, zero, is not changed, the FTP server will negotiate about which port the data communication will be established. Number Of Retries Enter the number of attempts to reconnect after temporary communication errors. Retry Interval Enter the time interval, in milliseconds, between connection attempts. Active Mode (PORT) Select this check box to set the FTP connection mode to ACTIVE. Otherwise, the mode is PASSIVE. Transfer Type Select either Binary or ASCII transfer of the data files. Note! Setting Transfer Type to the wrong type might corrupt the transferred data files. FTP Command Trace Select this check box to generate a printout of the FTP commands and responses. This printout is logged in the Event Area of the Workflow Monitor. Use this option only to trace communication problems, as workflow performance might deteriorate. TTS Header Size This is the size of the header record in the TTS file. If the size is not specified, the default value 9 will be used for MMS, and 8 for SMS. TTT Header Size This is the size of the header record in the TTT file. If the size is not specified, the default value 8 will be used for MMS, and 7 for SMS.

---

# Document 90: Workflow Packages - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030320
**Categories:** chunks_index.json

Concept A Workflow Package is a configuration of workflows containing a set of configurations that can be bundled together and imported to different staging or integration environments. Once a certain configuration is exported from a current set, it can be assigned a version number thereby creating a version control system. Workflow Packages ar e used to facilitate the transportation of data between different environments, such as development, test, and production environments. The key features of Workflow Packages are the following: Version control : Development, test, and production environments are integrated into the same artifact repository, where they are version-controlled. Rollback : Changes are easy to roll back. Predictable : The same content is guaranteed in all environments. Dependent : Simple dependency handling. Workflow Packages are accessed and managed from the System Exporter ( Manage  System Exporter ). Workflow Packages Management In the System Exporter , set the Export type to Workflow Package . There are two main sections  the Available entries list on the left and the Logs screen on the right. Open Workflow Packages Selection Screen At the top to the right, there are buttons used to manage Workflow Packages. Open Workflow Package management buttons Button Description Button Description Export Opens the Workflow Package export pop-up dialog. Refresh Refreshes the Available entries list. Option Not available when the Workflow package is selected. Expand all Expands all available workflows in the entries list. Collapse all Collapses all available workflows in the entries list. To create a Workflow Package select the list of workflows to include and click the Export button. This opens a pop-dialog. Open Workflow package export dialog Setting Description Setting Description Package name Specify the name of the Workflow Package. Package version Designate the Workflow Package version. Only numbers can be entered. Output option You can specify either a Download or a Commit Workflow Package export format. You can cancel the Workflow Package creation by clicking Close at any time or create the Workflow Package by filling in all the necessary information and clicking the Export button. When a successful export has been created a pop-up dialog is shown with the title and download link of the Workflow Package. Follow the download link to retrieve a copy of the exported workflow, and Close it by clicking the button.

---

# Document 91: mzcli Commands - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979379/mzcli+Commands
**Categories:** chunks_index.json

mzcli supports the following commands: mzcli - configuration mzcli - derbybackup mzcli - desktopadmin mzcli - disconnect mzcli - dumpsyslog mzcli Exit Codes mzcli - generate_pcc_classes mzcli - help mzcli - importrollback mzcli - logger mzcli - packageexport mzcli - pico mzcli - plist mzcli - premove mzcli - refreshdbmeta mzcli - regenerateconfigs mzcli - resumeexecution mzcli - service mzcli - sldreg mzcli - slowmethods mzcli - system mzcli - systemexport mzcli - systemimport mzcli Textual Pattern Matches mzcli - topo mzcli - ultra mzcli - unregister mzcli - user mzcli - vcexport mzcli - vcimport Workflow and Workflow Group Commands

---

# Document 92: ECS Changing State - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738720
**Categories:** chunks_index.json

You can change the state of a selected number of entries, or if no entries are selected, all entries. Possible states are New or Reprocessed . Reprocessed means that the entry has been collected by an ECS agent and been reprocessed with errors. Already processed data can be reset to New to enable recollection. Note! If the number of matches is larger than the maximum number of UDRs that can be displayed (see Maximum Number of Displayed UDR Entries in the ECS Inspector ), the state change is still applied to all matching entries. To change state of selected or all entries: Select the entries you want to change in the table, or click the Select All button to apply changes to all matching entries. Then right-click and select Set State... The Set State dialog opens where you can see the total number of entries that will be affected. Note! If the number of matching entries exceeds the maximum number of entries that can be displayed in the ECS Inspector, the dialog only tells you that all matching entries will be affected. If you proceed, another dialog opens up with information about the total number of entries that will be affected, asking you if you want to continue. Select state New or Reprocessed and click OK . If the number of matching entries exceeds the maximum number of entries that can be displayed you will get a question if you want to continue. Click Yes if you want to continue. If the number of matching entries exceeds the maximum number of entries that can be displayed, a progress bar will show the progress of the state change. This may be useful if you are changing the state for a large number of entries. Otherwise, the state simply changes in the table and the timestamp in the Last RP State Change column is updated.

---

# Document 93: Data Hub Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644935/Data+Hub+Forwarding+Agent
**Categories:** chunks_index.json

The Data Hub forwarding agent is a batch agent that bulk loads data to an Impala database specified by a Data Hub Profile . Loading

---

# Document 94: Error Classification and Grouping - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658188/Error+Classification+and+Grouping
**Categories:** chunks_index.json

Any UDRs can be associated with error codes. Error Codes are in an APL function that attaches the Error Code and optional message to the UDR to build an error case such as No_Match_cust_data: A_Number = 123456789. This allows grouping of UDRs with the same error code and/or error case into reprocessing groups to facilitate corrective action such as collection from ECS into a workflow including the use of the bulk-edit functionality. It is possible to associate a Reprocessing Group to an error code automatically when the UDR with the error code is sent to ECS. If not done automatically as described above, this assignment is done manually usually after the corrective action is taken so that the record can be processed to completion in a workflow.

---

# Document 95: Avro Support - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646682/Avro+Support
**Categories:** chunks_index.json

This page describes the Avro support for Ultra Format Definition Language (UFDL). This functionality enables you to compile Avro definitions, to encode data into, and from the Avro format. A schema in Avro is represented in JSON. You can implement non-protocol data issues in an Analysis agent. Note! It is also possible to encode/decode Avro messages using schema retrieved from an external schema registry. See Decoder Agent Configuration or Encoder Agent for more information. Overview Avro parsing in UFDL is managed by applying the avro_block construct. avro_block { <avro json schema> }; The output from an Avro encoder represents a single Avro record, not a complete Avro data file. To de-serialize these records using a third-party tool, add the correct Avro header to the records based on the description of Avro provided in the link below. The full description of the Avro language can be found at: https://avro.apache.org/docs/current/spec.html . The Avro Types The Avro JSON schema can be defined with any of the following types: Note! There are limitations to the implementation of the following elements. Refer to the the section below, Limitations, for further information. Primitive Type Notes null Implemented as type byte. boolean Value must be true or false . int 32-bit signed. long 64-bit signed. float Single precision (32-bit) IEEE 754 floating-point number. double Double precision (64-bit) IEEE 754 floating-point number. bytes Sequence of 8-bit unsigned bytes. string Unicode character sequence. Complex Type Notes record Supports the attributes: name , namespace, doc (optional), aliases (optional), fields . The fields attribute is an array of listing fields and supports the attributes: name , doc (optional), type , default , order (optional), aliases (optional). enum Supports the attributes: name , namespace, doc (optional), aliases (optional), symbols . array Supports the attribute: items map Supports the attribute: values union Implemented as a record with all types set to optional. If two values are set, both are encoded. If no values are set, none are encoded. To specify a union value, you must use udrCreate . fixed Supports the attributes: name , namespace, aliases (optional), size Note! Use the doc attribute to add a comment. Limitations The following limitations apply for the Avro support: Line and Column are not fully implemented. No string values are permitted in APL for the enum type. Encoding the Avro object container file is not supported. aliases are not fully supported. default is not fully supported. Cross references to other Ultra definitions are not supported. array of arrays is not correctly implemented. For example, array of array of x is implemented with array of y , where y is a record with one value field. This value field is an array of x . map of x is implemented via an array of y , where y is a record with two fields: key and value , where value is x . Avro Format Example To encode an Arvo data file, a format definition is included in the Ultra avro_block in the Ultra format. Example - The xml_schema Ultra Code Block avro_block { { "namespace": "example.avro", "type": "record", "name": "User3", "fields": [ {"name": "name", "type": { "name": "FullName2", "type": "record", "fields": [ {"name": "firstName", "type": "string"}, {"name": "lastName", "type": "string"} ] } }, {"name": "favorite_number", "type": ["int", "null"]}, {"name": "favorite_color", "type": ["string", "null"]}, {"name": "favorite_fotball_team", "type": { "name": "teams", "type": "enum", "namespace": "example.avro.teams", "symbols": ["Djurgården", "Hammarby", "Malmö", "Göteborg"] } }, {"name": "ipAddresses", "type": { "type": "array", "items": [ { "name": "ipv4Address", "type": "fixed", "size": 4 }, { "name": "ipv6Address", "type": "fixed", "size": 16 } ] } }, {"name": "favoriteFoodList", "type": { "name": "favoriteFood", "type": "record", "fields": [ {"name": "dish", "type": "string"}, {"name": "next", "type": ["null", "favoriteFood"]} ] } }, {"name": "salary", "type": "long"}, {"name": "myFixed", "type": { "name": "myfixed", "type":"fixed", "size": 4 }}, {"name": "myFloat", "type": "float"}, {"name": "myDouble", "type": "double"}, {"name": "rootUsers", "type": { "type": "map", "values": { "name": "RootUsers", "type": "record", "fields": [ {"name": "rootUser", "type": "string"}, {"name": "privileges", "type": "int"} ] } } }, {"name": "maps", "type": { "type": "array", "items": { "type": "map", "values": { "name": "Map2", "type": "record", "fields": [ {"name": "favoriteUser", "type": "string"}, {"name": "favoriteNumber", "type": "int"} ] } } } } ] } };

---

# Document 96: A Radius Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686787/A+Radius+Example
**Categories:** chunks_index.json

A Radius agent can act as an extension to a NAS and to illustrate such a scenario an example is introduced. In the example an Analysis agent is used to validate the content of the received UDP packet, and depending on the outcome a reply is sent back (also in the form of a UDP packet). Valid UDRs are routed to the subsequent agent, while invalid UDRs are deleted. Schematically, the workflow will perform the following: Decode the data into a UDR. Discard and continue with the next packet upon failure. Validate the UDR. If it is a Access_Request_Int , a comparison with a subscriber table must be performed to make sure the user is authorized (that is, exists in the table). All other UDR types must be deleted. If the user was found in the table, send the UDR to the next agent and a reply UDR of type Access_Accept_Int back to the Radius agent. If the user was not found, delete the UDR and send a reply UDR of type Access_Reject_Int to the Radius agent. Both reply UDRs must have the Identifier field updated first. Note! To keep the example as simple as possible, valid records are not processed. Usually, no reply is sent back until the UDRs are fully validated and manipulated. The example focuses on MediationZone specific issues, such as decoding, validation and reply handling. This section includes the following subsections: Radius Example Workflow Setup Radius Example Ultra Format Definition Radius Example Analysis Agent

---

# Document 97: Google Protocol Buffer Support - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783082/Google+Protocol+Buffer+Support
**Categories:** chunks_index.json

This chapter describes the GPB (Google Protocol Buffers) addition to the Ultra Format Definition Language (UFDL). This addition enables you to compile GPB definitions, and to decode the GPB input data as well as encode data into the GPB format. Both the proto2 and proto3 versions of the google protocol buffers language are supported. Overview You manage GPB parsing in UFDL by applying the gpb_block construct. The syntax differs whether you are using proto2 or proto3. Syntax for proto2 gpb_block { <GPB message elements> }; Syntax for proto3 gpb_block { syntax = "proto3"; <GPB message elements> }; The full description of the GPB language for proto2 and proto3 can be found at: https://developers.google.com/protocol-buffers/docs/proto or https://developers.google.com/protocol-buffers/docs/proto3 . The GPB Field Rules You specify that message elements are formatted according to one of the following rules: For proto2 only: required optional For proto2 and proto3: repeated The GPB Scalar Value Types The GPB message elements can be defined with any of the following types: Type Notes Type Notes double 8 bytes signed. float 4 bytes signed. int32 Uses variable-length encoding. Inefficient for encoding negative numbers  if your field is likely to have negative values, use sint32 instead. int64 Uses variable-length encoding. Inefficient for encoding negative numbers  if your field is likely to have negative values, use sint64 instead. uint32 Uses variable-length encoding. uint64 Uses variable-length encoding. sint32 Uses variable-length encoding. Signed int value. This more efficiently encode negative numbers than regular int32s. sint64 Uses variable-length encoding. Signed int value. This more efficiently encode negative numbers than regular int64s. In MediationZone sint64 will be more efficient than uint64. fixed32 Always four bytes. More efficient encoded than uint32 if values are often greater than 228. fixed64 Always eight bytes. More efficient encoded than uint64 if values are often greater than 256. sfixed32 Always four bytes. sfixed64 Always eight bytes. bool Use the bool type to define the GPB message elements. string Use the string type to define the GPB message elements. bytes May contain any arbitrary sequence of bytes. Limitations The following limitations apply for the GPB support for proto2: Default specifiers are not supported. Groups are not supported. The packed option is not supported. Import statements with the gpb_block will have no effect. Nested types are not fully supported, since their names become a part of the global scope. However, you can avoid this problem by changing names on one of the sub types. The extensions specifier is not supported. The packed specifier is not supported. Options are not supported. Packages are not supported. Import public specifiers is not supported. Definitions of services are not supported, only messages. The following limitations apply for the GPB support for proto3: The options that are supported are allow_alias in enums and packed for fields. Importing definitions with the gpb_block is not supported. Import public specifiers is not supported. The parent message type is not supported. The any type is not supported. Packages are not supported. Definitions of services are not supported, only messages. Note! The GPB message format is not self delimiting, which should be considered when decoding a stream of messages, or a file containing several messages. Nested Types in Proto3 Nested types are supported in proto3. For further information on the specification for nested types, see https://developers.google.com/protocol-buffers/docs/proto3 . When referring to a nested type by using its qualified name, a point is used as delimiter, for example M1.M2 or .M1.M2 . However, note that while nested types are indicated with a point in the GPB specification, when mapping to Ultra and APL, you must use an underscore instead. See the example provided below. Example - Nested types in proto3 In this example M2 is nested inside M1 . Depending on where you are inside the gbp_block you can refer to a message by its relative name M2 or its qualified name .M1.M2. message M1 { message M2 { string f1 = 1; } M2 f2 = 2; .M1.M2 f3 = 3; } The following shows how to map M1 to an internal configuration: in_map M1_in: external(M1), target_internal(M1) { automatic: use_external_names; } In APL you can refer to M1 and M2 by their qualified names M1 and M1_M2, where an underscore is used instead of a point. udrCreate(M1); udrCreate(M1_M2); A GPB Format Example To decode a GPB data file, a format definition is included in the Ultra gpb_block in the Ultra format. Example - GPB format using proto2 gpb_block { message MyData{ required string myName =1; required string myText =2; required string extraName =3; required int32 myPriority =4; required uint32 myId =5; required uint32 equipmentId =6; required MyParam myParams = 7; } message MyParam { repeated string someField = 1; } message MyAdditional { required uint32 action = 1; required string alias = 2; required int64 content = 3; optional int32 newId = 4; optional int32 newType = 5; optional uint64 myKey = 6; } message FlashEx { required string someField = 1; } message MyExtras { repeated FlashEx ex = 1; } message MyList { repeated string list = 1; } message SysmanData { required string someField = 1; } }; external Wrapper { int dataSize: static_size(4), encode_value(udr_size-4); SysmanData data : dynamic_size(dataSize); }; in_map Wrapper_inMap: external(Wrapper), target_internal(Wrapper_int) { automatic; }; out_map Wrapper_outMap: external(Wrapper), internal(Wrapper_int) { automatic; }; decoder Wrapper_decoder: in_map(Wrapper_inMap); encoder Wrapper_encoder: out_map(Wrapper_outMap); Example - GPB format using proto3 gpb_block { syntax = "proto3"; message MyData{ string myName =1; string myText =2; string extraName =3; int32 myPriority =4; uint32 myId =5; uint32 equipmentId =6; MyParam myParams = 7; map<string, MyParam> myMap = 8; AnEnum anEnum = 9; AnotherEnum anotherEnum = 10; } enum AnEnum { V0 = 0; V1 = 1; } enum AnotherEnum { option allow_alias = true; V0 = 0; V1 = 0; } message MyParam { repeated string someField = 1; } message MyAdditional { uint32 action = 1; string alias = 2; int64 content = 3; int32 newId = 4; int32 newType = 5; uint64 myKey = 6; } message FlashEx { string someField = 1; } message MyExtras { repeated FlashEx ex = 1; } message MyList { repeated string list = 1; } message SysmanData { string someField = 1; } }; external Wrapper { int dataSize: static_size(4), encode_value(udr_size-4); SysmanData data : dynamic_size(dataSize); }; in_map Wrapper_inMap: external(Wrapper), target_internal(Wrapper_int) { automatic; }; out_map Wrapper_outMap: external(Wrapper), internal(Wrapper_int) { automatic; }; decoder Wrapper_decoder: in_map(Wrapper_inMap); encoder Wrapper_encoder: out_map(Wrapper_outMap); Note! Since GPB does not specify the size, this has to be done externally, which is why int dataSize has to be included in the external unless the size is previously known.

---

# Document 98: Script UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609797/Script+UDR
**Categories:** chunks_index.json

The Script UDR is used to set extra Scripts on the UI page. It can be referring to a URI where the script can be found or created in this UDR. The following fields are included in the Script UDR : Field Description Field Description attributes (map<string,string>) This field may contain extra attributes to be added to the script tag. crossOrigin (string) This field may contain the value for CORS settings of a script. integrity (string) This field may contain the value for integrity attribute. placement (int) This field may contain placement of the script tag on the page. Default is LAST_BODY. Possible values are: HEADER (1) or LAST_BODY (0) srcUrl (string) This field may contain a url to a script. text (string) This field may contain a script text. type (string) This field may contain MIME type of a script. Typically text/javascript . Two constants are added to help, JAVASCRIPT or ECMASCRIPT .

---

# Document 99: SAP RFC Processor Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034548/SAP+RFC+Processor+Agent+Example
**Categories:** chunks_index.json

This section provides an example of a workflow to illustrate how the SAP RFC Processor agent can be used. Open Example workflow with SAP RFC Processor agent An example of an SAP RFC profile configuration: Open Example SAP RFC profile Analysis Agent to Create UDRs In this example, an Analysis agent, CreateUDR , has been used in order to populate the UDR that is generated by the SAP RFC profile and sent to the SAP RFC Processor agent. import ultra.SAP_RFC.Default.ZCI_PLAN_DISPLAY_UPDATE; import ultra.SAP_RFC.Default.ZCI_PLAN_DISPLAY_UPDATE.subUdr; int seqNo = 0; synchronized int increaseSeq() { return seqNo++; } consume { ZCI_PLAN_DISPLAY_UPDATE_UDR rfcUdr = udrCreate(ZCI_PLAN_DISPLAY_UPDATE_UDR); rfcUdr.tableParams = udrCreate(tableParams_ZCI_PLAN_DISPLAY_UPDATE); rfcUdr.tableParams.IT_PLAN_UPDATE = listCreate(ZCI_PLAN_DISPLAY_S); ZCI_PLAN_DISPLAY_S item = udrCreate(ZCI_PLAN_DISPLAY_S); item.PLAN_DISPLAY_ID = (string) increaseSeq(); item.PLAN_DISPLAY_NAME = baToStr(input.Data) + item.PLAN_DISPLAY_ID; listAdd(rfcUdr.tableParams.IT_PLAN_UPDATE, item); udrRoute(rfcUdr); }

---

# Document 100: Prometheus Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642057/Prometheus+Forwarding+Agent
**Categories:** chunks_index.json

Use the Prometheus forwarding agent to publish metrics to a Prometheus scrape endpoint. In this way you can send any data that is processed in a workflow as metrics. This section includes the following subsections: Prometheus Forwarding Agent Events Prometheus Forwarding Agent Configuration Prometheus Forwarding Agent Input/Output Data and MIM Prometheus Forwarding Agent Example

---

# Document 101: Kafka Real-Time Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138396/Kafka+Real-Time+Collection+Agent+Configuration
**Categories:** chunks_index.json

You open the Kafka collection agent configuration dialog from a workflow configuration. Click Build  New Configuration. Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime. Click Add agent and select Kafka from the Collection tab in the Agent Selection dialog Open Kafka real-time collection agent configuration. Setting Description Setting Description Profile Select the Kafka profile you want the agent to use in this drop-down list. The Kafka profile defines from which Kafka broker the agent collects data. Consumer Group The consumer group to collect data from. Maximum poll size The number of messages to request at a time. A low value will affect performance negatively. Offset management At Least Once When you select this option, a message is guaranteed to be collected at least once. This setting may sometimes result in duplicates. At Most Once When you select this option, a message is guaranteed to be collected at most once. This setting may sometimes result in data being lost. Start At Requested When you select this option you must determine from which offset you want to start collecting data and add an incoming route for the UDRs. When a UDR arrives on that route, the collection of data will start from the given offset. You set the offset using the KafkaOffset UDR in an Analysis agent, see KafkaOffset . This setting reduces the risk of data loss, and prevents messages from being processed multiple times after a restart. See the example in Legacy KafkaOffsetUDR . Start At Beginning When you select this option you must determine from which offset you want to start collecting data. Messages are then collected from the first offset. With this setting there is a risk that messages will be processed multiple times after a restart. Start At End When you select this option you must determine from which offset you want to start collecting data. Messages are then selected from the last offset from when the workflow was started. With this setting there is a risk that data can be lost after a restart. Assignment When you select this option, messages can be collected from one or several topics. The topics can be identified in two different ways: Topic Pattern Enter a regular expression that the names of the topics you want to collect from must match, see Java Platform, Standard Edition Java API Reference . Topic Names This option displays a list and an Add button. Add one or several topic names to collect from. The exact names must be entered. Regular expressions cannot be used. Topic Partitions This option will display a list and an Add button. Add one or several topic names and partitions to collect from. The exact names must be entered. Regular expressions cannot be used. Below are a few examples of valid partition declarations: Example of collection from partition 0: Partitions: 0 Example of collection from the three partitions 0, 8 and 12: Partitions: 0,8,12 Example of collection from the six partitions 0, 3, 4, 5, 6, and 7: Partitions: 0,3-7 Note! If you select Topic Partitions , automatic rebalancing will not take place, and you will have to handle potential rebalancing manually if needed.

---

# Document 102: Resetting and Activating Pico Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657515/Resetting+and+Activating+Pico+Configurations
**Categories:** chunks_index.json

When there are staged changes the STR, you can attempt to move these changes to the active registry or reset the master registry. Resetting Configurations Run the following command to rollback changes that cannot be validated. $ mzsh topo reset Note! When you run this command, the directory structure of the STR will be recreated and any existing directory- or file handles will be stale. For instance, if you run the command from a shell and the current directory is in the STR, i e under $MZ_HOME/common/config/cell/default , it will appear empty. Activating Configurations Run the following command to validate configurations in the master registry and propagate changes to the active registry. $ mzsh topo activate Note! This command is only required when you use the mzsh topo command with the flag --no-activation, or after manual edits in the STR. When you run this command, the directory structure of the STR will be recreated and any existing directory- or file handles will be stale. For instance, if you run the command from a shell and the current directory is in the STR, i e under $MZ_HOME/common/config/cell/default , it will appear empty. Hint! When you run mzsh topo activate with the -v flag after manual edits, the printout on stdout contains mzsh topo set commands that correspond to the staged changes in the file pico.conf . You can use this printout for scripting.

---

# Document 103: Appendix D - Java Version Change - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/281903108/Appendix+D+-+Java+Version+Change
**Categories:** chunks_index.json

This section describes how to replace the JDK and JRE products on an existing installation. For information about required third-party products Refer to System Requirements . Platform Container To change the Java version on the Platform Container: Ensure that the memory settings for Java are set correctly in STR. Container Default Settings Container Default Settings Platform Default settings when using an Oracle database: $ mzsh topo set topo://container:<container>/pico:platform/obj:config.jvmargs  'xmx:["-Xmx1024M"] xms:["-Xms128M"] maxMetaspace:["-XX:MaxMetaspaceSize=256M"] Default settings when using a Derby database: $ mzsh topo set topo://container:<container>/pico:platform/obj:config.jvmargs  'xmx:["-Xmx1024M"] xms:["-Xms192"] maxMetaspace:["-XX:MaxMetaspaceSize=256M"] EC Default settings: $ mzsh topo set topo://container:<container>/pico:<pico-name>/obj:config.jvmargs  'xmx:["-Xmx256M"] xms:["-Xms64M"] maxMetaspace:["-XX:MaxMetaspaceSize=196M"] maxDirect:["-XX:MaxDirectMemorySize=4096M"] args : ["-server"]' Install the new JDK. Do not remove the old version. Disable all workflow groups from the Execution Manager in Desktop, or by entering an mzsh command, for example: $ mzsh mzadmin/<password> wfgroupdisable * Note! If you use the wfgroupdisable command, make sure that you enable all system tasks again when you are done. Stop all workflows that are not disconnected and let them finish execution. Ensure that all users shut down any connected Desktops. If you want to see which Desktops that are connected, you can use the following command: $ mzsh mzadmin/<password> pico -view Note! This command will also display other pico instances, such as Execution Contexts. Shut down the Platform and all ECs. $ mzsh shutdown Platform <ec_name> If you have set the environment variable JAVA_HOME, change it to the install path of the new JDK. Set the variable in both the current shell and the login script. This is only required if you want to override the default value in the mzsh script. Example - Setting JAVA_HOME in the Shell export JAVA_HOME="/usr/lib/jvm/java-17-openjdk-17.0.6.0.9-0.3.ea.el8.x86_64" Set JAVA_HOME in MZ_HOME/bin/mzsh to the install path of the new JDK. Example - Setting JAVA_HOME in mzsh $ mzsh topo env --update-java-home "/usr/lib/jvm/java-17-openjdk-17.0.6.0.9-0.3.ea.el8.x86_64" Start the Platform and ECs. $ mzsh startup Platform <ec_name> Start the real-time workflows. Enable the workflow groups. Execution Container To change the Java version on an Execution Container: Install the new JDK. Do not remove the old version. If you have set the environment variable JAVA_HOME, change it to the install path of the new JDK. Set the variable in both the current shell and the login script. This is only required if you want to override the default value in the mzsh script. Example - Setting javahome javahome="/usr/lib/jvm/java-17-openjdk-17.0.6.0.9-0.3.ea.el8.x86_64" Set JAVA_HOME in MZ_HOME/bin/mzsh to the install path of the new JDK. Example - Setting JAVA_HOME in mzsh $ mzsh topo env --update-java-home "/usr/lib/jvm/java-17-openjdk-17.0.6.0.9-0.3.ea.el8.x86_64" Restart the ECs and Stop the real-time workflows. To minimize downtime, restart the ECs one by one. To restart an EC: $ mzsh shutdown <ec name> $ mzsh startup -f <ec name> Start the workflows on the restarted ECs. Legacy Desktop Client To change the Java version for the Legacy Desktop: Shut down all connected Legacy Desktops. Uninstall the existing JDK or JRE. Install the new JDK or JRE. Set the environment variable JAVA_HOM to the new JDK/JRE install path.

---

# Document 104: KPI Management Quick-Start Guide - Non-Distributed - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611353/KPI+Management+Quick-Start+Guide+-+Non-Distributed
**Categories:** chunks_index.json

The input data in this example use case consists of sales numbers in CSV format. This dataset is from here on, referred to as "sales". The data is collected in real-time from the regions "APAC", "AMERICAS", and "EMEA". We want to calculate the total-, average, and number of sales per minute. These numbers will be our KPIs, broken down per country and region. Example - Input Data timestamp region country amount timestamp region country amount 2017-03-08T13:53:52.123 EMEA Sweden 123.50 2017-03-08T13:53:56.123 APAC India 12.12 2017-03-08T13:53:59.123 AMERICAS US 425.23 Step-by-Step Instructions Getting started with KPI Management is done in three parts: 1 Configure the Service Model 2 Create the Workflow 3 Start the Workflow Configure the Service Model The service model describes your data, which KPIs to generate and how to calculate them. A JSON representation is used to describe the model, which includes the following top-level objects: dimension tree metric kpi threshold (optional) To configure the service model in a KPI profile: Click the New Configuration button in the Build view, and then select KPI Profile . Start with the dimension and tree objects. The dimensions describe the fields of your data that are used for grouping and the tree the relation between them. The identifying fields in the input data are region and country . A region has one or more countries. The data type is sales . In the dimension object we specify each of our identifying fields as separate objects, with the datatype and field. Open Configuring Dimensions and Tree Dimension JSON "dimension": { "Region": { "sales": "region" }, "Country": { "sales": "country" } }, "tree": { "tree1": { "Region": { "Country": { } } } } Define the metrics using the amount field in the input data: totalSales - For total sales, sum up the amount for each record by using the sum function on the expression expr , which contains the amount field. avgSales - For average sales use the avg function instead of sum . numSales - To count the number of records, use the conditional function isSet in the expression. This function evaluates to 1 if there is a value in amount or 0 if there is no value. Use the function sum to sum up the 1s and 0s. Open Configuring Metrics Metrics JSON "metric": { "totalSales" : { "fun": "sum", "expr": { "sales": "amount" } }, "avgSales" : { "fun": "avg", "expr": { "sales": "amount" } }, "numSales" : { "fun": "sum", "expr": { "sales": "isSet(amount)" } } } Define the KPIs. The expected output is the total sales, average sales, and number of sales per region and country in 60 second periods. Use the property node to describe where in the topology the KPI should be calculated and windowSize to set the period length. Use the names of the metrics defined above in the expr property. Open Configuring KPIs KPI JSON "kpi": { "Region.TotalSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "totalSales" }, "Region.AvgSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "avgSales" }, "Region.NumberOfSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "numSales" }, "Country.TotalSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "totalSales" }, "Country.AvgSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "avgSales" }, "Country.NumberOfSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "numSales" } } Select the Aggregated Output checkbox and set the Window Size to 60, and save the profile with the name "SalesModel" in the folder "kpisales". Full Service Model in JSON { "dimension": { "Region": { "sales": "region" }, "Country": { "sales": "country" } }, "tree": { "tree1": { "Region": { "Country": { } } } }, "metric": { "totalSales" : { "fun": "sum", "expr": { "sales": "amount" } }, "avgSales" : { "fun": "avg", "expr": { "sales": "amount" } }, "numSales" : { "fun": "sum", "expr": { "sales": "isSet(amount)" } } }, "kpi": { "Region.TotalSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "totalSales" }, "Region.AvgSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "avgSales" }, "Region.NumberOfSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "numSales" }, "Country.TotalSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "totalSales" }, "Country.AvgSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "avgSales" }, "Country.NumberOfSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "numSales" } } } Create the Workflow Create the real-time workflow. In this guide we use Pulse agents to simulate sales data coming from three different sources, EMEA, AMERICAS, and APAC. To create the workflow: Add three Pulse agents and an Analysis agent. Open Workflow - Pulse Agents Configure the Pulse agents as follows: AMERICAS will send 1000 TPS - Set Time Unit to MILLISECONDS and Interval to 100 EMEA will send 500 TPS - Set Time Unit to MILLISECONDS and Interval to 200 APAC will send 250 TPS - Set Time Unit to MILLISECONDS and Interval to 400 To be able to identify the data, set the data to the region name. Open Pulse agent configuration The Pulse agents only sends us a simple event containing the name of the region, the other data that will be used in the KPI calculations are generated in the connected Analysis agent. In the Analysis agent, click on the Set to Input button beneath UDR Types and paste the APL code below to create the input to KPI Management. list<string> americas = listCreate(string, "US", "Canada", "Mexico", "Brazil", "Argentina", "Cuba", "Colombia"); list<string> emea = listCreate(string, "Sweden", "UK", "Portugal", "Italy", "France", "Germany", "Norway", "Spain", "Finland", "Denmark"); list<string> apac = listCreate(string, "India", "China", "Japan", "Thailand", "Australia", "Indonesia", "Malaysia","South Korea"); consume { // create KDR - the input for the KPI CLusterIn agent kpimanagement.KDR kdr = udrCreate(kpimanagement.KDR); // The KDR has a type field - we set this to the value we had for our data type in the model kdr.type = "sales"; // It also has a timestamp field - lets populate that from the current time but using seconds. kdr.timestamp = dateCreateNowMilliseconds() / 1000; string region = baToStr(input.Data); // the data in our use case (country, city, amount) we will put in the values field of the kdr. map<string, any> sales = mapCreate(string,any); mapSet(sales, "region", region); // lets create a random amount between 1 and 1000 int amount = randomInt(1000); // set amount and city depending on the region if (region == "AMERICAS") { mapSet(sales, "amount", amount * 1.25d); mapSet(sales, "country", randomCountry(americas)); } else if (region == "EMEA") { mapSet(sales, "amount", amount * 1.0d); mapSet(sales, "country", randomCountry(emea)); } else if (region == "APAC") { mapSet(sales, "amount", amount * 0.65d); mapSet(sales, "country", randomCountry(apac)); } else { mapSet(sales, "amount", 0.0d); mapSet(sales, "country", "UNKNOWN"); debug("Unknown region:" + region); } kdr.values = sales; udrRoute(kdr); } // pick a random country from a list string randomCountry(list<string> countries) { int index = randomInt(listSize(countries)); return listGet(countries, index); } Add a KPI agent and configure it to use the KPI profile that you created in step 1. Set Delay to 0 . Open Workflow - KPI agent Open KPI agent configuration Add another Analysis agent for debugging of the KPIs. Open Final workflow configuration In the Analysis agent, click on the Set to Input button beneath UDR Types and paste the APL code below to the Analysis agent. string format = "yyyy-MM-dd'T'HH:mm:ss:S"; consume { // input is KPIAggregatedOutput which contains a list of // KPIOutput list<kpimanagement.KPIOutput> kpis = input.kpiOutputUDRs; // loop the KPIs and debug string dateStr = ""; for (int i = 0; i < listSize(kpis); i++) { kpimanagement.KPIOutput kpi = listGet(kpis, i); dateToString(dateStr, dateCreateFromMilliseconds(kpi.periodStart * 1000), format); debug("Period start: " + dateStr + ", instance: " + kpi.instancePath + ", KPI: " + kpi.kpi + ", Value:" + kpi.value + ", Samples: " + kpi.sampleCount); } } Save the workflow. Start the Workflow To start the workflow: Open the workflow configuration in the Workflow Monitor. Enable debugging and select events for the KPI agent and the Analysis agent that produces the debug output. Ensure to have an ec running and click on the Start button. The calculated KPIs will be displayed in the debug output in the Workflow Monitor. Note! It will take a minute before the output is displayed due to the configuration of the windowSize property in the service model. Open Debug output

---

# Document 105: Usage Scenarios External Version Control - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611152/Usage+Scenarios+External+Version+Control
**Categories:** chunks_index.json

This chapter contains a few example scenarios of how different user groups may work with configurations using a version control system. The possible ways of working with external version control systems are by no means limited to these scenarios. This chapter includes the following sections: Simple Version Control Release of New Functionality Emergency Fix in Production System Upgrade

---

# Document 106: Python Connector Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740022/Python+Connector+Agent
**Categories:** chunks_index.json

The Python Connector agent is a collection agent for real-time workflows, that allows you to explore and test workflows using an exploration tool. The Python Connector agent is similar to the Python processing agent but the Python Connector agent enables interaction between an exploration tool, e.g. Jupyter Notebook, and workflows during exploration and test. This gives you access to all the UDRs, routes, APL functions etc that are part of MediationZone with the flexibility that you can choose an interactive console or script that suits you, in which to test and explore your code. Open Example workflow including the Python Connector agent This section includes the following subsections: Python Connector UDR Types Python Connector Agent Configuration Python Connector Agent Input/Output Data and MIM Python Connector Agent Events - Real-Time Connecting Using SSL

---

# Document 107: Categorized Grouping Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999281/Categorized+Grouping+Agent
**Categories:** chunks_index.json

This section describes the Categorized Grouping profile and agent. The agent is a processing agent for batch workflow configurations. Prerequisites The reader of this information has to be familiar with: Analysis Programming Language UDR structure and contents User Documentation The Analysis Programming Language description and syntax is listed in the APL Reference Guide . The Ultra Format Definition La ngu age is described in the Ultra Reference Guide . Loading

---

# Document 108: Managing the Authorization Server - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849609/Managing+the+Authorization+Server
**Categories:** chunks_index.json

To use the Authorization Server service, there are several steps to take to enable the server as described in Enabling Authorization Server . For more information on the parameters found in the Authorization Server template, refer to Authorization Server Properties . Loading

---

# Document 109: CCCycleUDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609165/CCCycleUDR
**Categories:** chunks_index.json

The CCCycle UDR is a container used to correlate a charging request with the corresponding charging answer received from SAP Convergent Charging. The following fields are included in the CCCycleUDR : Field Description answer (AbstractChargingAnswer) One of the CC Charging Answer UDRs defined in Charging Answer UDRs freeField (any) This free field can be used to store contextual information regarding the request. For example, it can be used to store the diameter request context associated with a charging request, which will be required to push the answer back on the diameter stack. Latency (long - readOnly) This readonly field will only be populated when the agent emits the answer in the workflow. It contains the latency introduced by the processing of this UDR. request (AbstractChargingRequest) One of the CC Charging Request UDRs defined in Charging Request UDRs requestId (string) A free field which is intended to contain the requestId provided by the external system startProcessingTime (long - readOnly) This readonly field is only populated when the agent emits the answer in the workflow. It contains a timestamp which corresponds to the time when the UDR has been submitted to SAP Convergent Charging.

---

# Document 110: Black Box for Deprecated or Unsupported Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204996985/Black+Box+for+Deprecated+or+Unsupported+Agents
**Categories:** chunks_index.json

When importing configurations from MediationZone 8.x to 9.x, there is a possibility that there will be agents that are invalid due to changes in the code or agents that are no longer supported on version 9.x. During import of these configurations, these invalid agents will cause the system import to fail, which will result in the workflow containing said agents to not be imported into MediationZone 9.x. Open Example workflow with a Black Box agent. To allow you to import the configuration without fail, there will now be a black box to replace any invalid or unsupported agents. The workflow itself will still be imported but it will be invalid and it will be up to you to replace the invalid agents with a new agent of your choosing. The following scenarios will see the agents be replaced as a Black Box agent: An agent that was previously supported in version 8.x but is now deprecated in version 9.x. An agent that is licensed for use in version 8.x but is not included in the license for version 9.x. Custom Developed DTK agents in version 8.x that are not ported. Note! The Black Box agent is meant as a last resort measure when importing configurations from version 8.1 that are not supported, it is highly advisable that you ensure that your MediationZone 9.x installation can support the configuration that you plan to import. Warning! All saved configuration in the agent to be replaced by a Black Box agent will not be retained after importing it into MediationZone 9.x. When importing a workflow that has any agent that will be replaced, you will see the following messages in the import log for the System Importer. The log will display the name of the agent that will be replaced with the Black Box agent. Open System Importer - Import Log Replacing a Black Box agent To replace a Black Box agent with a suitable replacement agent, you will have to open the workflow that contains the Black Box agent. By selecting the Black Box agent, you will be presented with a menu with options to select from a list of possible replacement agents. Open Example Workflow with APN_2 agent selected Select the replacement agent and click on Yes to confirm the selection. The Black Box agent will then be replaced with your chosen agent. You will be required to configure the agent as if it was a new agent. Open Example Workflow - agent replacement confirmation pop up

---

# Document 111: Aggregation Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646187
**Categories:** chunks_index.json

This chapter describes the functions, function blocks, and variables that are available for use in the Aggregation Agent APL code. Loading Loading

---

# Document 112: Always Available - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646366
**Categories:** chunks_index.json

Command line tool commands that are available even when the Platform is not running, vary both in argument requirements and in behavior, depending on the logged on user's user permissions. This section includes a detailed description of each command, its required or optional arguments, and its operation. The following commands are available: desktop encryptpassword exit help kill pcreate pcommit picoversion quit restart shutdown startup status version For information about the commands that or only available when the Platform is running, see Available When the Platform Is Running .

---

# Document 113: Management API - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205784151
**Categories:** chunks_index.json



---
**End of Part 5** - Continue to next part for more content.
