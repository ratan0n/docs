# RATANON/MZ93-DOCUMENTATION - Part 82/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 82 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~57.6 KB
---

This section describes how to configure the agents. When you select one of the agents, the configurations include the same tabs that are displayed in a batch version of the configuration, with the exception of the Filename Sequence tab, which is not included as it is irrelevant in a real-time workflow. For details on the configuration of the agents, see the relevant section: Amazon S3 Collection Agent Configuration Amazon S3 Forwarding Agent Configuration Disk Collection Agent Configuration - Batch Disk Forwarding Agent Configuration - Batch FTP Collection Agent Configuration FTP Forwarding Agent Configuration SCP Collection Agent Configuration SCP Forwarding Agent Configuration SFTP Collection Agent Configuration SFTP Forwarding Agent Configuration Additional Configurations for Collection Agents When the collection agents are included in a real-time workflow, there are also three additional tabs for each of the agent configurations: Decoder , Decompression and Execution . See the example image below. Open Disk agent configuration dialog - Agent tab Decoder Tab In the Decoder tab you configure the settings related to the decoding of the collected data. Open Disk agent configuration dialog - Decoder tab Setting Description Setting Description Decoder Click Browse to select from a list of available decoders created in the Ultra Format Editor, as well as the default built-in decoders: CSV Format JSON Format MZ Tagged Format Different settings are available depending on the Decoder you select. CSV Format Open Decoder CSV Format JSON Format Open Decoder JSON Format MZ Tagged Format Open Decoder MZ Tagged Format Full Decode This option is only available when you have selected a decoder created in the Ultra Format Editor. Select this option to fully decode the UDR before it is sent out from the decoder agent. This action may have a negative impact on performance, since not all fields may be accessed in the workflow, making decoding of all fields in the UDR unnecessary. If it is important that all decoding errors are detected, you must select this option. If this checkbox is cleared (default), the amount of work needed for decoding is minimized using "lazy" decoding of field content. This means that the actual decoding work may be done later in the workflow, when the field values are accessed for the first time. Corrupt data (that is, data for which decoding fails) may not be detected during the decoding stage, but can cause a workflow to abort later in the process. MZ Tagged Specific Settings Tagged UDR Type Click the Add button to select from a list of available internal UDR formats stored in the Ultra and Code servers, to reprocess UDRs of an internal format and send them out. If the compressed format is used, the decoder automatically detects this. JSON Specific Settings UDR Type Click Browse to select the UDR type you want the Decoder to send out. You can either select one of the predefined UDRs or the DynamicJson UDR, which allows you to add a field of type any for including payload. Unmapped Fields If you have selected DynamicJson as UDR Type , you can select the option data in this field in order to include payload. If you have selected another UDR type that contains an any, or a map field, you can select to put any unmapped fields into the field you select in this list. All fields of any or map type in the selected UDR type will be available. If set to (None), any unmapped fields will be lost. Schema Path Enter the path to the JSON schema you want to use in this field. CSV Specific Settings UDR Type Click Browse to select the UDR type you want the Decoder to send out. You can either select one of the predefined UDRs or the DynamicCsv UDR if the CSV format is not known. Format Select the CSV format you want to use; Unix , Mac , Windows , or Excel , or select to define your own customized format. If you select Custom , the following four settings will be enabled. Delimiter Enter the delimiter character(s) for the fields in the CSV. Use Quote Select this option if quotes are used in the CSV. Quote If Use Quote is selected, enter the type of quotes used in the CSV. Line Break Enter how line breaks are expressed in the CSV. Decompression Tab In the Decompression tab you specify if you want to decompress the files or not. Open Disk agent configuration dialog - Decompression tab Setting Description Setting Description Compression Type Select the required decompression algorithm: No Compression : The agent will not decompress the files. This is the default setting. GZIP : The agent will decompress GZIP formatted files. No configuration required. LZO : The files are compressed using LZO compression. If you select this option, the Command Line field is displayed and populated by default with the command lzop -d -c -q . This command allows you to use the lzop unix command to decompress standard input with the LZO algorithm and write the output on standard output. You can enter an alternative lzop command if required. For further information on the lzop command, see https://www.lzop.org/lzop_man.php . Execution Tab In the Execution tab, you configure how often the workflow will be executed, how decoding errors will be handled and what action is to be taken when Cancel Batch messages are called. For furher information on Cancel Batch, see Batch-Based Real-Time Agents - Transaction Behavior . Open Disk agent configuration dialog - Execution tab Note! Cancel Batch messages are sent: If a decoding error occurs, and you have selected the Cancel Batch option in the agent configuration If you have selected for files to be decompressed in the Decompression tab of agent configuration and it fails, e g because a file is corrupt or is not compressed. Setting Description Setting Description Run Once Select this option if you want the workflow to run once. Repeat Every X Seconds Select this option if you want the workflow to be run repeatedly with an interval of a specific number of seconds. The default value is 5 seconds. If you have selected this option, when the agent encounters an error, the workflow does not abort. The error is reported in the System Log and the agent retries at the next repeat. Decoding Error Handling Select one of the error handling options to control how to react upon decoding errors: Cancel Batch - Processing of the current batch is stopped and skips to the next batch. Route Raw Data - Route the remaining, undecodable, data as raw data. This option is useful if you want to implement special error handling for batches that are partially processed. Abort Immediately If enabled, the workflow immediately aborts on the first Cancel Batch message from any agent in the workflow. The erroneous data batch is kept in its original place and must be moved/deleted manually before the workflow can be started again. Abort After X Consecutive Cancel Batch If enabled, the value of X indicates the number of allowed Cancel Batch calls, from the collection agent before the workflow will be aborted. The counter is reset between each successfully processed data batch. Thus, if 5 is entered, the workflow will abort on the 6th data batch in a row that is reported erroneous. Never Abort The workflow will never abort. However, as with the other error handling options, the System Log is always updated for each Cancel Batch message. Additional Configurations for Forwarding Agents When the forwarding agents are included in a real-time workflow, there is also one additional tabs for each of the agent configurations: File Closing Criteria . See the example image below. Open Disk agent configuration dialog Setting Description Setting Description (bytes) When the file size has reached the number of bytes entered in this field, the file will be closed as soon as the current bytearray has been included, and stored in the storage directory. This means that the file size may actually be larger than the set value since the system will not cut off any bytearrays. If nothing is entered, this file closing criteria will not be used. Volume (UDRs) When the file contains the number of UDRs entered in this field, the file will be closed and stored in the storage directory. If nothing is entered, this file closing criteria will not be used. Timer (sec) When the file has been open for the number of seconds entered in this field, the file will be closed and stored in the storage directory. If nothing is entered, this file closing criteria will not be used.

---

# Document 1937: APL Plugins - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655740/APL+Plugins
**Categories:** chunks_index.json

A new APL plugin is defined by extending the DRAPLPlugin class. The code server will locate all such classes and make their functions available within the APL language. This class will be instantiated by the APL compilation engine to extract information about the available APL functions from getFunctions and the Executor class, which is responsible for executing the function, from getExecutorClass . If the plugin routes UDRs into the workflow, this should be declared by implementing getOutputTypes and returning all types that the plugin may route for each outgoing route. If the plugin expects any hardcoded route names to be present in the workflow, it must also implement getExpectedRouteNames . The presence of these names on outgoing routes will then be validated. Open APL APL functions developed using DTK need to be imported. If the function get_package_name() returns a package name, that name must be used to import the included functions, as in the following example: Example - The function get_package_name() import apl. package_name; If the returned name is not imported, the included functions cannot be accessed and will not be highlighted. If the function get_package_name() returns null instead, the functions will be globally available and automatically highlighted regardless of where they are used. Note! Since Function Overloading is not supported, all APL plugin function names must be unique. This also applies if the functions have different parameters, for example, a(int x) and a(string x) . If an Aggregation/Analysis agent or APL module imports two functions with the same names, the calling agent or APL module will become invalid, even if the functions are located in different APL modules. DRAPLFunctionSignature A DRAPLFunctionSignature object describes each new function. The function name is returned by the getName method. The number of parameters and their data types are returned by the getArgumentTypes method and the return type by the getReturnType method. If the function supports dynamic parameters, the isDynamic method will return true . In this case, the function must have an additional parameter of the Java type List that, when called, will contain the additional parameters. The APL compiler will automatically make the necessary syntax validations regarding the name, parameters and return type when APL code is compiled. If additional validation is required, the validate method must be implemented. It is essential that the Executor class has a function that exactly matches the name, the return type and the number of parameters stated by this object. DRAPLExecutor When a workflow is executing, the APL function call will be delegated to the corresponding function in the APL plugins Executor object. An Executor class must implement the interfaces DRAPLExecutor or DRAPLTxnExecutor , where the latter allows the APL plugin to interact with the transaction handling. Executor classes will have their initialize method called once, prior to any invocation of the function(s) they hold. Transaction Executors may implement the methods from DRBatchTransactionResource , such as cancelBatch or commit . To route UDRs from the plugin, a router must first be obtained by calling getRouter on the environment.

---

# Document 1938: Aggregation Agents Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737976/Aggregation+Agents+Configuration
**Categories:** chunks_index.json

The Aggregation agent is available in both batch and real-time workflows. Pages containing the suffix -Batch indicates that the page contains information pertaining to the batch version of the agent. A -Real-Time suffix indicates the page contains information pertaining to the real-time version of the agent. A page with no suffix will contain information for both. The Aggregation agent is also configured with APL code, see the APL Reference Guide for further information about available functions for the Aggregation agent. The following are the storage types that are supported for agents in batch, real-time or both workflows: File Storage and Memory Only can be used in batch and real-time workflows. Elasticsearch Storage and SQL Storage can only be used in batch workflows. Couchbase Storage and Redis Storage can only be used in real-time workflows. Loading

---

# Document 1939: FTP DX200 Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641364
**Categories:** chunks_index.json

You open the FTP DX200 collection agent configuration dialog from a workflow configuration. To open the configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select FTP DX200 from the Collection tab of the Agent Selection dialog. The dialog contains three tabs; Switch , Advanced, and TTSCOF Settings . Switch Tab The Switch tab includes configuration settings that are related to the remote host and the directory where the control files are located. On this tab you specify from which VDS device the control files are retrieved, and the time zone location of the VDS device. Open FTP DX200 collection agent configuration - Switch tab with FTP Open FTP DX200 collection agent configuration - Switch tab with SFTP Setting Description Setting Description Host Name Enter the name of the Host or the IP address of the switch that is to be connected. Transfer Protocol Choose transfer protocol. Authenticate With Choice of authentication mechanism. Both password and private key authentication are supported. When you select Private Key , a Select... button will appear, which opens a window where the private key may be inserted. If the private key is protected by a passphrase, the passphrase must be provided as well. For further information about private keys, see Authentication in 9.31.2 FTP DX200 Agent Preparations . User Name Enter the name of the user from whose account on the remote Switch the FTP session is created. Password Enter the user password. Root Directory Enter the physical path of the source directory on the remote Host, where the control files are saved. Path To Data Files Enter the path to the directory where the data files are located, if they are not located in the Root Directory, in this field. Switch Time Zone Select the time zone location. Timezone is used when updating the transaction control file. VDS Device No. Enter the network element device from where the control files are retrieved. Advanced Tab The Advanced tab includes configuration settings that are related to more specific use of the FTP service. Open FTP DX200 collection agent configuration - Advanced tab Setting Description Setting Description Prefix Enter the data files name prefix Number Positions Select the length of the number-part in the data file name as follows: 01 for a two digit number 001 for a three digit number 0001 for a four digit number For example: If you select 0001, data file number 99 will include the following four digits: 0099 . Ends With VDS Device No. Select this check box to create data file names that end with VDS device No. Server Port Enter the port number for the server to connect to, on the remote Switch . Note! Make sure to update the Server Port when changing the Transfer Protocol . Number Of Retries Enter the number of attempts to reconnect after temporary communication errors. Retry Interval (ms) Enter the time interval, in milliseconds, between connection attempts. Force Delay (s) Select this option to force a delay (in seconds) between the TTTCOF write and the TTSCOF read. The maximum value is 3600. The default value is 0. Decompress Files Data files are decompressed by default. If you want the data files to be compressed, deselect this check box. Local Data Port Enter the local port number that the agent will listen to for incoming data connections. This port will be used when communication is established in Active Mode. If the default value, zero, is not changed, the FTP server will negotiate about the port the data communication will be established on. Active Mode (PORT) Select this check box to set the FTP connection mode to ACTIVE. Otherwise, the mode is PASSIVE. Transfer Type Select either Binary or ASCII transfer of the data files. Note! Setting Transfer Type to the wrong type might corrupt the transferred data files. FTP Command Trace Select this check box to generate a printout of the FTP commands and responses. This printout is logged in the Event Area of the Workflow Monitor. Use this option only to trace communication problems, as workflow performance might deteriorate. Accept New Host Keys Select this check box if you want the existing host key to be overwritten when the host is represented with a new key. The default behavior is to abort when the key mismatches. Warning! Selecting this option causes a security risk since new keys are accepted regardless of if they belong to another machine. TTSCOF Settings Tab Open FTP DX200 collection agent configuration - TTSCOF Settings tab The TTSCOF Settings tab includes configuration settings that allows you to adjust the default settings for the FTP DX200 agent Setting Description Setting Description Collect when file is present on only one WDU With this setting you can select to allow files to be collected, even though they are only present on one WDU. This may be useful if one of the WDUs cannot be reached for some reason. Default is No, which means that files will only be collected if they are present on both WDUs. Note! WDU is short for Winchester Drive Unit, and each VDS (Virtual Data Storage) has two WDUs. WDU0 Path In this field you can specify the path to WDU0. This setting is optional. WDU1 Path In this field you can specify the path to WDU1. This setting is optional. Select default collection WDU Select the WDU you want to use as default in this list. WDU 1 is default. Collect files with bit 5 In this list you can select if you only want to collect files where bit 5 IS NOT set ( Must not be set ), or where bit 5 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 5 is set or not ( May be set ). Collect files with bit 6 In this list you can select if you only want to collect files where bit 6 IS NOT set ( Must not be set ), or where bit 6 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 6 is set or not ( May be set ). Collect files with bit 7 In this list you can select if you only want to collect files where bit 7 IS NOT set ( Must not be set ), or where bit 7 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 7 is set or not ( May be set ).

---

# Document 1940: Prometheus Forwarding Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674058/Prometheus+Forwarding+Agent+Example
**Categories:** chunks_index.json

In this example, we have *.csv input files containing information about registered members and we want to produce metrics for the number of members from each country present in each processed file and forward the metrics to Prometheus to display them in a graph on a dashboard. Below is a snippet of the input records, the corresponding Ultra Format Definition, and the graph that we want to produce. Open *.csv file, Ultra Format Definition and graph In this case it is recommended to use PrometheusUDRs (instead of MIMs) for the following reasons: The labels of the metrics (countries in this case), is not known in advance. The regularity of publishing the metrics needs to be decided by the user. Designing the Workflow The workflow handling the data in this example contains an Aggregation agent that consolidates the metrics for each country present in the input data since we do not want to publish metrics for each record processed. The consolidated metrics are then forwarded to an Analysis agent that maps the aggregated data to a PrometheusUDR before sending it to a Prometheus Agent. Open Workflow design Analysis Agent Configuration The Analysis Agent is configured with the following code: consume { debug(input); PrometheusUDR promUDR = udrCreate(PrometheusUDR); map<string,string> labels = mapCreate(string,string); mapSet(labels, "country", input.countryOfOrigin); promUDR.Description = "Sum of members' country of origin."; promUDR.Labels = labels; promUDR.MetricType = "GAUGE"; promUDR.Name = "NoOfMembersPerCountry"; promUDR.Value = input.count; debug(promUDR); udrRoute(promUDR); } Debug Examples The debug output from the Analysis agent for one of the countries (France) will look like this: Field Values for: Prometheus.UFL_input.countryAggregate countryOfOrigin: France count: 1265 Field Values for: PrometheusUDR Description: Sum of members' country of origin. MetricType: GAUGE Value: 1265.0 Labels: {country=France } Name: NoOfMembersPerCountry The first entry is the incoming aggregated UDR, and the second is the PrometheusUDR that is forwarded to the Prometheus Agent.

---

# Document 1941: Troubleshooting - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001068/Troubleshooting
**Categories:** chunks_index.json

This section contains important information for troubleshooting that are essential for errors that you might encounter. Note! Prior to testing the MSMQ agent, ensure that MSMQ is installed and running on the target Windows machine. In order to troubleshoot the agent, you must take into consideration the following aspects: Installing MSMQ Service To install the MSMQ service, refer to https://docs.microsoft.com/en-us/dotnet/framework/wcf/samples/installing-message-queuing-msmq . Windows Firewall By default, Windows Firewall may block the ports that are required by the MSMQ server. You must configure the Windows Features to enable the connectivity of the MSMQ agent. For more information, refer to https://support.microsoft.com/en-ca/help/183293/how-to-configure-a-firewall-for-msmq-access . Errors When an agent is aborted during the startup of the workflow, an error is displayed. The following shows a list of error messages with explanation. Error Message Description Error Message Description AutomationException: 0x5 Access denied. In Invoke, this is usually due to incorrect username/password. AutomationException: 0xc00e0014 The queue path name specified is invalid. EC and Workflow stop working Restart the EC and Workflow to recover.

---

# Document 1942: Workflow Bridge Example Real-Time to Real-Time Scenario with Load Balancing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002887/Workflow+Bridge+Example+Real-Time+to+Real-Time+Scenario+with+Load+Balancing
**Categories:** chunks_index.json

This section will show an example of a scenario where a real-time forwarding workflow will split the execution of UDRs between three real-time collection workflows depending on the incoming data. Each collection workflow will add information and send back the consumeCycleUDR to the real-time forwarding workflow for further execution. The following configurations will be created: An Ultra Format A Workflow Bridge profile A Workflow Bridge real-time forwarding workflow A Workflow Bridge real-time collection workflow Open Example of a real-time to real-time scenario Define an Ultra Format A simple Ultra Format needs to be created in order to forward the incoming data and enable the collection workflows to populate it with more information. For more information about the Ultra Format Editor and the UFDL syntax, r efer to the Ultra Reference Guide . Create an Ultra Format as defined below: internal myInternal { string inputValue; string executingWF; }; Define a Profile The profile is used to connect the forwarding workflow towards the three collection workflows. See Workflow Bridge Profile for information how to open the Workflow Bridge profile dialog. Open Example of a profile configuration In this dialog, the following settings have been made: The Send Reply Over Bridge is selected which means that all ConsumeCycleUDR s will be returned to the Workflow Bridge forwarding agent. Force serialization is not used since there will be no configuration changes during workflow execution. The Workflow Bridge real-time collection agent must always respond to the WorkflowState UDRs. The Response Timeout (s) has been set to "60" and this means that the Workflow Bridge real-time forwarding agent that is waiting for a WorkflowState UDR reply will timeout and abort (stop) after 60 seconds if no reply has been received from the real-time collection workflow. Enter the appropriate timeout value to set the timeout for the Workflow Bridge real-time forwarding agent. The Bulk Size has been set to "0". This means that the UDRs will be sent from the Workflow Bridge real-time forwarding agent one by one, and not in a bulk. Enter the appropriate bulk size if you wish to use bulk forwarding of UDRs. The Bulk Timeout (ms) has been set to "0" since there will be no bulk forwarding. Enter the appropriate bulk timeout if you wish to use bulk forwarding of UDRs. Bulk timeout can only be specified if the bulk functionality has been enabled in the Bulk size setting. Since the UDRs in this example will be split between three different workflows, the Number of Collectors has been set to "3". Create a Real-Time Forwarding Workflow In this workflow, a TCP/IP agent collects data that is forwarded to an Analysis agent. The Analysis agent will define the receiving real-time collection workflow before the ConsumeCycleUDR is sent to the Workflow Bridge forwarding agent. The Workflow Bridge forwarding agent will distribute the UDRs to the correct collection workflow and forward the returning ConsumeCycleUDR to another Analysis agent for further execution. Open Example of a real-time forwarding workflow The workflow consists of a TCP/IP agent, an Analysis agent named Analysis , a Workflow Bridge real-time forwarding agent named Workflow_Bridge_FW and a second Analysis agent named Result . TCP/IP TCP/IP is a collection agent that collects data using the standard TCP/IP protocol and forwards it to the Analysis agent. Double-click on the TCP_IP agent to display the configuration dialog for the agent: Open Example of a TCP/IP Agent Configuration In this dialog, the following settings have been made: Host has been set to "10.46.20.136". This is the IP address or hostname to which the TCP/IP agent will bind. Port has been set to "3210". This is the port number from which the data is received. Allow Multiple Connections has been selected and Number of Connections Allowed has been set to "2". This is the number of TCP/IP connections that are allowed simultaneously. Analysis The Analysis agent is an Analysis agent that receives the input data from the TCP/IP agent. It defines which real-time collection workflow should be chosen and forwards the ConsumeCycleUDR to the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { wfb.ConsumeCycleUDR ccUDR = udrCreate(wfb.ConsumeCycleUDR); WFBridge.myFormat.myInternal data = udrCreate(WFBridge.myFormat.myInternal); data.inputValue = baToStr(input); debug("First character is: " + strSubstring(data.inputValue,0,1)); if (strStartsWith(data.inputValue,"1") || strStartsWith(data.inputValue,"2")) { int wfId; strToInt(wfId,strSubstring(data.inputValue,0,1)); ccUDR.LoadId = wfId; } else { ccUDR.LoadId = 3; } ccUDR.Data = data; udrRoute(ccUDR); } In this dialog, the APL code for handling input data is written. In the example, the incoming data is analyzed and depending on the first character in the incoming data, the receiving real-time collection workflow is chosen by setting the LoadId in the ConsumeCycleUDR , which is sent to the Workflow_Bridge_FWD agent. Adapt the code according to your requirements. Workflow_Bridge_FWD Workflow_Bridge_FWD is the Workflow Bridge real-time forwarding agent that sends data to the Workflow Bridge real-time collection agent. Double-click on Workflow_Bridge_FWD to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. Result The Result agent is an Analysis agent that receives the returning ConsumeCycleUDR s and potential ErrorCycleUDR s from the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, wfb.ErrorCycleUDR)) { debug("Something went wrong"); } else if (instanceOf(input, wfb.ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; string msg = ("Value " + data.inputValue + " was executed by " + data.executingWF); debug(msg); } } } In this dialog, the APL code for further handling of the UDRs is written. In the example, only simple debug messages are used as output. Adapt the code according to your requirements. Create the Real-Time Collection Workflows In this workflow, a Workflow Bridge real-time collection agent collects the data that has been sent in a ConsumeCycleUDR from the Workflow Bridge real-time forwarding agent and returns an updated ConsumeCycleUDR . Open Example of a real-time collection workflow Workflow_Bridge_Coll Workflow_Bridge_Coll is the Workflow Bridge real-time collection agent that receives the data that the Workflow Bridge real-time forwarding agent has sent over the bridge. Double-click on the Workflow_Bridge_Coll agent to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. The default port that the collector server will listen on for incoming requests has been set to default value "3299". Analysis The Analysis agent is the Analysis agent that receives and analyses the data originally sent from the Workflow Bridge real-time forwarding agent in the ConsumeCycleUDR , as well as the workflow state information delivered in the WorkflowStateUDR . Double-click on the agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, WorkflowStateUDR )) { udrRoute((WorkflowStateUDR)input, "response"); } else if (instanceOf(input, ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; debug("Incoming data: " + data.inputValue); data.executingWF = (string)mimGet("Workflow","Workflow Name"); ccUDR.Data = data; udrRoute(ccUDR, "response"); } else { debug(input); } } In this example, each ConsumeCycleUDR will populate the data field executingWF with the name of the executing workflow. Also WorkflowStateUDR s are routed back. Adapt the code according to your requirements. Instance Table Since this example will load balance between three workflows, additional workflows is added in the workflow table. Right-click in the workflow template and choose Workflow Properties to display the Workflow Properties dialog. Open Example of Workflow Properties In this dialog, the following settings have been made: Workflow - Execution - Execution Settings and Workflow_Bridge_Coll - WFB_Collector - Port have default checked, which means they will use the configured value in the template unless a new value is given in the Workflow Table. Workflow_Bridge_Coll - WEB_Collector - loadID has Per Workflow set, which means that the value must be specified in the Workflow Table. Number of Workflows to Add has been set to "2", since one is already existing and the example needs two additional workflows. The Workflow Table will contain three workflows, that all will communicate with the real-time forwarding workflow. Populate the Workflow Table with the correct settings for each workflow: Name should be set to a unique name for each workflow. Set which EC each workflow will execute on in Execution Settings . Each workflow needs to have a unique port assigned for communication with the Workflow_Bridge_FWD agent. In case workflows are configured to run on different ECs (configured on a different server), then the workflows can share the same ports. The loadID need to correspond with the APL code and should be "1", "2" and "3" in this example Load Balancing There are two types of load-balancing strategies that can be used. Only one type may be active at a time. Static  This load balancing method allows for a specific number of LoadIDs to be defined in the workflow instances. Dynamic  The dynamic balancing method will automatically distribute the load.

---

# Document 1943: Upgrade - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736188
**Categories:** chunks_index.json

This section contain separate instructions for a standard upgrade procedure and for a simplified upgrade. The Standard upgrade procedure should be used when it is required to minimize downtime during upgrade and to ensure that configurations, persisted data (in the file system or database), and system properties are properly migrated. The Standard upgrade procedure needs to be done step-wise via every minor and major. The Simplified Upgrade procedure should be used when this is not required, and you only wish to migrate configuration data. The Simplified upgrade procedure can be done over multiple majors and minors in one upgrade. Note! If you have workflow packages containing workflows using Web Services in your system, you need to follow the steps described in Workflow Packages Containing Web Services before the upgrade. Note! In MediationZone 8, Keystore-related fields in an agent configuration can be configured in the Workflow Table. However, in MediationZone 9, these settings are moved to the Security Profile configuration and the Workflow Table does not support configuring the Security Profile fields directly. Standard Upgrade Procedure Simplified Upgrade Procedure Workflow Packages Containing Web Services

---

# Document 1944: mzcli - slowmethods - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979753/mzcli+-+slowmethods
**Categories:** chunks_index.json

Usage slowmethods [-class regexp] [-all] [-threshold size] This command enables you to list too-long methods including more than 8000 byte code instructions (or the size set by the -threshold <byte code size> option). When there are too many byte code instructions in a method, they tend to run very slowly because the JVM will not be able to perform a proper JIT compilation. This will result in lower runtime performance. Hint! Two methods including 7000 byte code instructions is a better alternative than one method with 14000 instructions. Example - mzadmin/dr slowmethods output: Scanned 20 classes The following methods have too many byte code instructions: 9393 Folder.Workflow.Agent.method: Default.myWorkflow.RequestAnalysis.consume 9393 is the method size in bytes. Default is the folder where the workflow is stored, myWorkflow is the name of the workflow, RequestAnalysis is the agent that includes the too long method, and consume is the method. Note! If the workflow configuration cannot be retrieved using an internal key, the class and method name will be returned instead, as in the following example: Example - class and method name 8930 com.mysql.jdbc.DatabaseMetaData.getTypeInfo 8930 - the method size in bytes com.mysql.jdbc.DatabaseMetaData - the class name getTypeInfo - the too-long method. Options Option Description Option Description [-class <regexp>] Use this option to add a regular expression to be matched when searching through the code. For example, mzcli slowmethods -class com.<product>.* will only search for classes and packages in com.<product> . [-all] Use this option to search through all code in the cache, not only the APL code. [-threshold <byte code size>] Use this option to set a byte code threshold, to be able to list slow methods with a byte code size lower than "8000". Return Codes Listed below are the different return codes for the slowmethods command: Code Description Code Description 1 Will be returned if an argument is invalid.

---

# Document 1945: Routing Control Data Model - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677437/Routing+Control+Data+Model
**Categories:** chunks_index.json

The UDRs in PCC.Routing store information about destinations, routing attributes, and use cases. The main objects of PCC Routing are: Object Description Object Description Routing_Details Contains the name of the workflow route to a specific Diameter Request Agent. It also contains the routing use cases to execute. Routing_Destination Contains the available routes for the incoming UDRs. Each route corresponds to a Hostname or Realm defined in the Diameter Routing Profile used by the Diameter Requested agent specified by the Routing_Details. RouteMapping Maps message attributes to an entry in Routing_Details . If a match is found by the routing logic, it will be used to retrieve a set of destinations from Routing_Destination . Use_Case A use case triggers additional routing functions. Overview of the Routing Data Model The different UDRs within the Routing Control data model are connected as follows: Open Overview of the Routing Data Model RouteMapping UDR In the RouteMapping UDR (PCC.Routing.RouteMapping) you set the arguments that are used by the routing logic to map incoming UDRs to entries in the Routing_Details. This makes it possible to route e g Diameter messages to specific destinations based on a custom set of attributes associated with the session or the subscriber. Field (name and type) Description Field (name and type) Description Arguments (list<string>) The name of the attributes to map. For information about the attributes that are used by the default workflow configurations, see the section below, RouteMapping Arguments. ID (int) The unique id of the RouteMapping entry. Priority (int) The priority of the RouteMapping entry, a low value indicates a high priority. Determines the order in which the matched entries are returned. StopFallThrough (boolean) The routing logic will match the attributes of incoming UDRs against RouteMapping entries in priority order. If StopFallThrough is set to true the routing logic will not iterate through all matching entries. Targets (list<Routing_Details>) The Routing_Details to be mapped against the message attributes. Below is a screenshot of the UDR Assistance displaying the RouteMapping UDR: Open RouteMapping UDR RouteMapping Arguments The RouteMapping arguments are implementation specific and are typically adapted to meet specific customer requirements. The following attributes are used as arguments in default workflow configurations: Argument Description Argument Description Group Id The group of a subscriber-identifier. The default workflow configuration extracts the value from an external database. Origin Source The value of the Origin-Host AVP. Origin Destination The value of the Destination-Host AVP. Protocol Set to diameter in the workflow. Application The value of the Auth-Application-Id AVP. Routing_Details UDR In the Routing_Details UDR (PCC.Routing.Routing_Details) you set the workflow route that is to be used by the routing logic. The UDR contains one or several instances of Routing_Destination , which holds information about the destination hosts or realms. Field (name and type) Description Field (name and type) Description Active (int) Indicates if the Routing_Details entry is active. ID (int) The unique id of the Routing Details entry. Misc (map<string, any>) See the section below, Misc Field, for more information. Route_Name (string) The name of the workflow route to a specific Diameter Request Agent. Routing_Destination (list<Routing_Destinations>) The Routing_Destinations associated with the Route_Name. Routing_Type (string) Indicates session-based or subscriber based routing. Use_Cases (list<Use_Case>) A list of use cases to be executed. Open Route_Name Below is a screenshot of the UDR Assistance displaying the Routing_Details UDR: Open Routing_Details UDR Routing_Destination UDR In the Routing_Destination UDR (PCC.Routing.Routing_Destination) you set the peer, realms and weights that is to be used by the routing logic. Field (name and type) Description Field (name and type) Description Active (int) Indicates if the Routing_Destination is active. Routing_Destination (string) The host or realm name. ID (int) The unique id of the Routing Destination. Misc (map<string, any>) See the section below, Misc Field, for more information. Weight (int) Used to achieve load balancing when routing UDRs to a list of destinations. The weight indicates the probability for a message to be routed to a specific destination relative to other destinations in the same list. The weight is expressed as a percentage and the sum of the weights in a list of destinations should be 0% or 100%. When the sum of the weights are not 100%, the individual weights are normalized. For instance, the weights 1, 3, and 5 approximately corresponds to 11% 33%, and 56%. Below is a screenshot of the UDR Assistance displaying the Routing_Destination UDR: Open Routing_Destination UDR Use Case UDR In the Use Case UDR (PCC.Routing.Use_Case) you set the use cases that trigger the workflow logic to perform additional routing functions. Field (name and type) Description Field (name and type) Description ID (int) The unique id of the use case. Misc (map<string, any>) See the section below, Misc Field, for more information. Use_Case (string) The name of the use case. Acceptable values in the default workflow configurations are: NULL for no additional functions wfb for routing via Back-End Workflow realm for realm-based routing. For information about realm-based routing see 9.17 Diameter Agents in the Desktop User's Guide. The default workflow configurations can be extended to handle additional use cases. Below is a screenshot of the UDR Assistance displaying the Use_Case UDR: Open Use Case UDR Misc Field Some objects in the PCC Routing Control data model include a field for storing miscellaneous data. The field is called Misc and has the type map<string, any> . Even though the Misc field is stated to support storing values of type any there are some limitations to what type the values can have. The following types can be used: bigint bitset boolean byte bytearray char date float/ double int ipaddress long short string

---

# Document 1946: Global Code - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677778/Global+Code
**Categories:** chunks_index.json

Global Code can be either APL code, as entered in the APL Code Editor; or UFDL code, as entered in the Ultra Format Language Editor. Global APL Code is called packages and is referred to through the name under which it was saved. Correspondingly, Global UFDL Code is called modules. Using Global APL Code Before Global APL Code can be used in the Analysis and Aggregation agents, it must be imported: import apl.<foldername>.<APL Code configurationname> The package is the name under which the Global APL code was saved. Note that single function blocks from within a package may not be imported - the whole package must be imported. Note that function names are resolved first in the imported scope, and second in the local scope. The import statement may also be used to include UFDL modules, that is, UDR types (see the section below, Using Global UFDL Code). Using Global UFDL Code The lookup of type names is slightly different in APL than in Ultra definitions. In APL, the module must either be explicitly specified or imported. The explicitly specified module import is done by: import ultra.<folder>.<module>; The module is the name under which the Ultra format was saved. The modules of any entries in the UDR types section of the Analysis agent are implicitly imported. For instance, if the input type myUDR(myFolder.module1) is configured, the complete module myFolder.module1 is imported. Example - Using global UFDL code /* APL code without import statement */ consume { myFolder.module1.MyUDR newUDR = udrCreate( myFolder.module1.MyUDR ); newUDR.number = input.seqNum; newUDR.sub = udrCreate( myFolder.module1.MySubUDR ); newUDR.sub.subNum = 17; udrRoute( newUDR ); } /* APL code with import statement */ import ultra.myFolder.module1; consume { MyUDR newUDR = udrCreate( MyUDR ); newUDR.number = input.seqNum; newUDR.sub = udrCreate( MySubUDR ); newUDR.sub.subNum = 17; udrRoute( newUDR ); }

---

# Document 1947: JSON Decoding Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743771/JSON+Decoding+Functions
**Categories:** chunks_index.json

The functions described in this section are used to decode JSON-formatted strings. The following functions for JSON Decoding described here are: 1 jsonDecode 2 jsonDecodeUdr 3 jsonParse jsonDecode This function decodes a JSON formatted string to a json.JsonObject UDR or a json.JsonArray UDR . The field asMap in a json.JsonObject UDR contains key-value pairs where the value is a string, another json.JsonObject UDR or a json.JsonArray UDR. The field asList in a json.JsonArray UDR contains a list of json.JsonObject and json.JsonArray UDRs. any jsonDecode ( string jsonString) Parameter Description Parameter Description jsonString The JSON formatted string to decode Returns A json.JsonObject UDR or a json.JsonArray UDR Note! jsonDecode can only decode valid JSON. If the input is split due to size, the JSON may be invalid. The workaround for this is to use the baAppend function as shown in the example below. Example - Decoding JSON formatted string to json.JsonObject and json.JsonArray import ultra.json; bytearray ba; consume { ba = baAppend(ba, input); } drain { if (ba != null) { string s = baToStr(ba); any js = jsonDecode(s); if (isJsonObject(js)) { debugJsonObject((JsonObject) js, ""); } else if (isJsonArray(js)) { debugJsonArray((JsonArray) js, ""); } } } void debugJsonObject(JsonObject js, string parent) { map<string, any> mapObj = js.asMap; list<string> ikeys = mapKeys(mapObj); string path; if (strLength(parent) > 0) { path = path + "/" + parent; } for (string i : ikeys) { any value = mapGet(mapObj, i); if (isJsonObject(value)) { debugJsonObject((JsonObject) value, i); } else if (isJsonArray(value)) { debugJsonArray((JsonArray) value, i); } else { debug(path + "/" + i + ":" + value); } } } void debugJsonArray(JsonArray ja, string parent) { list<any> array = ja.asList; for (any i : array) { if (isJsonObject(i)) { debugJsonObject((JsonObject) i, parent); } else if (isJsonArray(i)) { debugJsonArray((JsonArray) i, parent); } } } boolean isJsonObject(any value) { return instanceOf(value, JsonObject); } boolean isJsonArray(any value) { return instanceOf(value, JsonArray); } jsonDecodeUdr This function decodes a JSON formatted string to a DRUDR. void jsonDecodeUdr ( string jsonString, DRUDR udr, boolean requireExactMapping (optional) ) Parameter Description Parameter Description jsonString The JSON string to decode udr The UDR to store the decoded data requireExactMapping Set to true if there must be matching UDR fields for all fields in the JSON string. The default value is false . When requireExactMapping is set to true and the function fails to match a field, it will throw an exception. If the function is used in batch workflow, the exception will cause it to abort. In a real-time workflow, the function does not decode the string, but the workflow does not abort. When requireExactMapping is set to false, the function will decode all JSON fields that can be mapped to a UDR field, the rest of the string will be ignored. Returns Nothing Example - Encoding UDR to JSON and decoding the result: Ultra internal itemUDR { int itId; string itDescription; string itMisc1 : optional; list<string> itMisc2 : optional; }; internal transactionUDR { int trId ; string trDescription; float amount; long trDate; map<string, itemUDR> trItems; boolean trProcessed; ipaddress trSourceIP; }; APL import ultra.JSON_EXAMPLE.ULTRA_JSON_EXAMPLE; consume { //Item 1 itemUDR item1 = udrCreate(itemUDR); item1.itId = 1; item1.itDescription = "Item1"; item1.itMisc2 = listCreate(string); listAdd(item1.itMisc2, "abc"); listAdd(item1.itMisc2, "def"); listAdd(item1.itMisc2, "ghi"); //Item2 itemUDR item2 = udrCreate(itemUDR); item2.itId = 1; item2.itDescription = "Item2"; item2.itMisc1 = "abc"; //Transaction1 transactionUDR transaction1 = udrCreate(transactionUDR); transaction1.trId = 1; transaction1.trDescription = "Transaction1"; transaction1.amount = 999.99; transaction1.trDate = dateCreateNowMilliseconds(); transaction1.trItems = mapCreate(string, itemUDR); mapSet(transaction1.trItems, "item1Key",item1); mapSet(transaction1.trItems, "item2Key",item1); transaction1.trProcessed = true; transaction1.trSourceIP = ipLocalHost(); //Encode with JSON string json; json = jsonEncodeUdr(transaction1); bytearray ba; strToBA(ba, json); debug("Encoded JSON (" + baSize(ba) + "bytes):n" + json + "n" ); //Decode from JSON transactionUDR transactionIn = udrCreate(transactionUDR); jsonDecodeUdr(json, transactionIn); debug("Decoded JSON:n" + transactionIn); }; jsonParse This function decodes a JSON formatted string to an any. any jsonParse(string jsonString) Parameter Description Parameter Description jsonString The JSON string to decode Returns any Example Decoding JSON consume { // Define json with a multi-line string literal string json = """ { "menu": { "header": "Document Viewer", "items": [ {"id": "Open"}, {"id": "OpenNew", "label": "Open New"}, null, {"id": "ZoomIn", "label": "Zoom In"}, {"id": "ZoomOut", "label": "Zoom Out"}, {"id": "OriginalView", "label": "Original View"} ] } } """; // Parse the json to get an object any obj = jsonParse(json); debug(obj); // Access the object values as fields for (any item : obj.menu.items) { if (item != null) { debug("label for " + item.id + " is " + item.label); } }

---

# Document 1948: HTTP/2 Server Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685893/HTTP+2+Server+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input Data Push , RequestCycle and NRFSpecificationUDR will only be required when Custom Specification is enabled on the 5G Profile. Refer to 5G Profile or more information about enabling Custom Specifications for 5G. Output Data RequestCycle NRFSpecificationUDR will only be required when Custom Specification is enabled on the 5G Profile. Refer to 5G Profile or more information about enabling Custom Specifications for 5G. MIM For information about the MIM and a list of the general MIM parameters, see MIM . This agent does not publish or access any MIM parameters.

---

# Document 1949: wfcommand - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657137/wfcommand
**Categories:** chunks_index.json

usage: wfcommand <pattern matching expression for workflow name> <'Workflow' | workflow service | agent> <instructions> ... Debug The command accepts wild cards, such as '*' and '?'. For further information see Textual Pattern Matches . To turn the debug on for a workflow: mzsh <username>/<password> wfcommand <workflow> Workflow debug on To turn the debug for a workflow off: mzsh <username>/<password> wfcommand <workflow> Workflow debug off Supervision By stating the Supervision service you can select to trigger and clear supervision actions with the wfcommand command. If you want to use the wfcommand to trigger the Supervision service, you can append the following instructions: manual which has the following syntax: Usage: manual [-action actionName actionParameters] Simply entering manual will deactivate the Supervision Service configurations and switch to manual mode, which means that no conditions will be evaluated and no actions will be executed. When using the -action option, the syntax will be as follows: Usage: manual -action overload ( <ratio> | -trigger | -clear ) [strategy] Option Description ratio If you want to trigger overload and reject a specific ratio of incoming requests, e g reject every fifth request, you can enter the ratio number here. 2 equals every second request, 5 equals every fifth request, etc. -trigger Use this option if you want to trigger overload for everything. An event will then be sent out for monitoring purposes. This will disable the automatic Supervision configuration. -clear Use this option if you want to clear overload for everything. Any Diameter/Radius overload protection strategies will be cleared and no requests will be rejected. The automatic Supervision configuration will then be disabled. Note! Using this option will not enable the automatic Supervision configuration. [strategy] Use this option to state a specific Diameter/Radius strategy, e g Diameter_AbortSessionRequest, Diameter_CCEventRequest, etc. See the Desktop User's Guide for further information about these strategies. automatic is used to revert to automatic Supervision configuration. When switching from automatic to manual mode, all active overload protection will remain active. However, in the opposite direction, when switching from manual to automatic mode, all active overload protection will be reset and the appropriate overload protection will be set by the automatic rules. Each time you switch between manual an automatic mode, an entry will be logged in the System Log for monitoring purposes. Example. To trigger overload protection for a workflow: mzsh <username>/<password> wfcommand <workflow> "Supervision" manual -action overload -trigger An event will then be sent out for monitoring purposes. To clear overload protection for a workflow: mzsh <username>/<password> wfcommand <workflow> "Supervision" manual -action overload -clear To reject every fifth Diameter AbortSessionRequest in a workflow: mzsh <username>/<password> wfcommand <workflow> "Supervision" manual -action overload 5 Diameter_AbortSessionRequest To switch back to automatic mode: mzsh <username>/<password> wfcommand <workflow> "Supervision" automatic See the Desktop User's Guide for further information about Supervision Service, Diameter, Radius, and overload. Aggregation and Diameter agents The Aggregation and Diameter agents allows the user to interact with the flow of data via the wfcommand. For further information, see 9.3 Aggregation Agent and 9.17 Diameter Agents in the Desktop User's Guide . GTP agent The GTP agent allows the user to view the MIM counters via the wfcommand. For further information, see 9.36 GTP' Agent in the Desktop user's guide. The request counters and the timestamp, which are published as MIM values are also possible to view from the commandline tool. Syntax to run the wfcommand function in mzsh: mzsh wfcommand <Workflow Name> <Agent Name> printcounters Parameters: Workflow Name Name of the workflow which contains the GTP agent. Agent Name Name of the GTP agent in the workflow. Example. Executing the wfcommand $ mzsh mzadmin/dr wfcommand "Default.myWorkflow" "GTP_1" printcounters Default.myWorkflow (3): Data Record Count: 138 Cancel Data Count: 2 Message Error Count: 0 Possible Duplicate Count: 5 Release Data Count: 1 Out of Sequence Count: 0 Duplicate Message Count: 0 Redirect Count: 0 Last Request Timestamp: Thu Mar 07 14:38:12 CET 2013 Return Codes Listed below are the different return codes for the wfcommand command: Code Description 1 Will be returned if command line arguments are missing. 2 Will be returned if no matching workflows can be found, or if the workflow is not running. 3 Will be returned if the target agent cannot be found, or if the agent is not accepting/supporting the command.

---

# Document 1950: Amazon SQS Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/299663494/Amazon+SQS+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Events An agent event is an information message from the agent that can be used when configuring an event notification in an Event Notifications configuration, see Agent Event for more information about this event type. There are no agent events for this agent. Debug Events A debug event is an event that is dispatched when debug is used, and can be used when configuring an event notification in an Event Notifications configuration, see Debug Event for more information about this event type. There are no debug events for this agent.

---

# Document 1951: Aggregation Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031685/Aggregation+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json



---
**End of Part 82** - Continue to next part for more content.
