# RATANON/MZ93-DOCUMENTATION - Part 53/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 53 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~68.9 KB
---

Killing spark applications are required when you have updated a kpi model or the configurations for Kafka, Zookeeper, or Spark. Note! As prerequisite, the script kpi_params.sh must be prepared according to 5.2 Preparing and Creating Scripts for KPI Management , as it contains connection information for spark and kafka etc. Stopping the Spark Cluster $ stop.sh You can also use the spark web interface by clicking on the kill buttons, next to the Application Id in the Spark Master web interface. Example using the web interface; Kill a submitted application by clicking on the kill link next to the Application Id . Open Spark Master UI - Running Applications Identify the Spark driver that coordinates application. This driver must be killed manually, ensuring that the application can be submitted again. When you are running one application there will be only one driver. However, when you are running multiple applications, you must click on the Worker name of each driver to find the name of the coordinated application. Open Spark UI - Running drivers The Spark application is listed in the Job Details column. Open Spark Worker - Running Executors To kill a driver, click the kill link next to the Submission Id . Open Spark Master UI - Running drivers

---

# Document 1244: SAP JCo Uploader Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001857/SAP+JCo+Uploader+Agent+Transaction+Behavior
**Categories:** chunks_index.json

The SAP JCo Uploader agent performs some extra maneuvers to ensure that records, for a particular file, that are already committed to the SAP System are skipped if the workflow aborts before the batch is completed successfully. Records are committed in sub batches. For further information, see the Commit Size setting in SAP JCo Uploader Agent Configuration . For every successful commit, an entry is inserted into the database, consisting of the WF_NAME , FILENAME , START_POSITION and END_POSITION of the sub batches. If the file is successfully processed, at workflow commit, the agent removes all entries inserted into the database for the current file. If the workflow aborts before the file is successfully processed, on the next re-run, the agent performs a database lookup, and gets the list of records that are already processed. Record numbers that are found in the database are skipped. For information about the general transaction behavior, see Workflow Monitor . Emits This agent does not emit anything. Retrieves Command Description Consume As this agents's action depends on the data of the HeaderUDR , nothing is done in BeginBatch . Processing begins when the HeaderUDR is received. This initial consume instance is only when the agent receives the HeaderUDR . The agent gets the RemoteFunction from the SAP System based on the name in the HeaderUDR . This RemoteFunction is used to insert records into the SAP System. When every RemoteFunction is committed, an entry of stateInfo is inserted into the database. Commit Removes all stateInfo data for the file from the database.

---

# Document 1245: TCP/IP Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643410/TCP+IP+Forwarding+Agent+Configuration
**Categories:** chunks_index.json

You open the TCP/IP forwarding agent configuration dialog from a workflow configuration: you can right-click the agent icon and select Configuration... , or double-click the agent icon. To open the TCP/IP collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select the Processing tab in the Agent Selection dialog. Select the Tcp Ip agent. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. General Tab Open TCP/IP forwarding agent configuration dialog - General tab Setting Description Setting Description Host The IP address or hostname to which the agent should connect. Port The port number to send the data to. Make sure the port is not used by other applications. Receive Response Select this checkbox if you want the agent to receive request responses. Note! The visual string that contains the Host and Port act as an identifier for the connection. Advanced properties Tab In the Advanced tab you can configure additional properties to optimize the performance of the TCP/IP forwarding agent. Open TCP/IP forwarding agent configuration dialog - Advanced properties tab See the text in the tab for further information about the properties.

---

# Document 1246: Analysis Agent Assignment and Cloning - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606422/Analysis+Agent+Assignment+and+Cloning
**Categories:** chunks_index.json

All agents handling UDRs forward UDR references on all outgoing links from the agent, that is, the same instance of the UDR is referred to from all agents in a workflow. This is accepted if all agents only read the UDR content. If an agent alters the content, it effectively alters the content for all other agents that receive the UDR. To avoid this behavior, the UDR must be cloned. Refer to the following examples for more infor mat ion. Note! Cloning is a costly operation in terms of performance, therefore it must be used with care. Example - Analysis agent assignment and cloning, case 1 Open Assignment case 1 In this example, you want to alter the UDR in the Analysis agent and send it to Encoder_1, while still sending its original value to Encoder_2. To achieve this, the UDR must be cloned. The following code creates, alters, and routes a cloned UDR on r_2 and leaves the original UDR unchanged. input=udrClone(input); input.MyNumber=54; udrRoute(input); Note that input is a built-in variable in APL, and must be used for all UDRs entering the agent. Example - Analysis agent assignment and cloning, case 2 An alternative solution to the one presented in the previous example is to clone the UDRs in an Analysis agent and then route the UDRs to another Analysis agent in which amendment is performed. Open Assignment case 2 Configurations in the Analysis_1 agent: udrRoute(input,"r_3",clone); input.MyNumber=54; udrRoute(input,"r_2"); The incoming UDR is cloned and the clone is routed on to r_3. After that the original UDR can be altered and routed to r_2.

---

# Document 1247: Simplified Downgrade Procedure - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/550731833
**Categories:** chunks_index.json

A system downgrade is a user controlled downgrade to an older version of MediationZone. This procedure can be used after a simplified upgrade. To revert to a previously installed version, execute the following procedure: Ensure that the environment variables are set correctly: Variable Description Variable Description MZ_HOME This environment variable specifies where the software is installed. JAVA_HOME This environment variable specifies where the JDK is installed. PATH This environment variable specifies where the search path must contain the following directories: $JAVA_HOME/bin:$MZ_HOME/bin Example - Extracting the Upgrade Software export MZ_HOME=/opt/mz export JAVA_HOME=/opt/jdk/jdk-17.0.2 export PATH=$JAVA_HOME/bin:$MZ_HOME/bin:$PATH Disable all workflow groups, either from the Execution Manager in Desktop, or by entering the following command: mzsh mzadmin/<password> wfgroupdisable * Note! If you use the wfgroupdisable command, make sure that you enable all system tasks again when you are done. Stop all workflows. Shut down all connected Desktops. If you want to see which Desktops that are connected, you can use the following command: mzsh mzadmin/<password> pico -view Note! This command will also display other pico instances, such as ECs.. Shut down the Platform and all ECs. mzsh shutdown platform <ec name> ... Remove MZ_HOME. rm -rf $MZ_HOME Restore the backup of MZ_HOME. cp -r <backup directory> $MZ_HOME If you are downgrading a system not using Derby, restore the database backup. Note! If you are using SAP HANA as the Platform database, you will need to disable TLS/SSL on the SAP HANA database before you downgrade your Platform. Resume Workflow Execution Set the environment variables JAVA_HOME and PATH according to the system requirements of the restored version. Note! You must rollback to Java 8 before you downgrade to MediationZone 8.3 or earlier. Example - Setting Environment Variables export MZ_HOME=/opt/mz export JAVA_HOME=/opt/jdk/jdk1.8.0_121 export PATH=$JAVA_HOME/bin:$MZ_HOME/bin:$PATH Start the Platform and all ECs. mzsh startup platform <ec names> ... Import the configurations that were exported in the old version prior to the upgrade. mzsh mzadmin/<password> systemimport <backup directory>/<filename> Start the real-time workflows. Enable the Workflow Groups. mzsh mzadmin/<password> wfgroupenable *

---

# Document 1248: Inter Workflow Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685963/Inter+Workflow+Forwarding+Agent
**Categories:** chunks_index.json

The Inter Workflow forwarding agent is responsible for sending data to the storage server. The agent can be part of both batch and real-time workflows. In the latter case, the user has to define batch closing criteria for the data to be saved in the storage directory. This is based on UDR count, byte count, or elapsed time. This section contains the following subsections: Inter Workflow Forwarding Agent in a Batch Workflow Inter Workflow Real-Time Forwarding Agent Events Inter Workflow Batch Forwarding Agent Transaction Behavior Inter Workflow Batch Forwarding Agent Input/Output Data and MIM Inter Workflow Batch Forwarding Agent Events Inter Workflow Real-Time Forwarding Agent Transaction Behavior Inter Workflow Real-Time Forwarding Agent Input/Output Data and MIM Inter Workflow Forwarding Agent in a Real-Time Workflow

---

# Document 1249: Database Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783237/Database+Properties
**Categories:** chunks_index.json

This section describes the properties related to Oracle, PostgreSQL, and SAP HANA that you can set. Derby The following properties are applicable to the Derby database. Property Description Property Description derby.restore.path Default value: "MZ_HOME/dbrestore" Set this property to set the Derby restore directory. Oracle The following properties are applicable to the Oracle database. These settings are used by the Platform for the connection towards the Platform database, and for ECs for connections towards external databases. Property Description Property Description oracle.pool.connectionwaittimeout Default value: 900 The number of seconds to wait for a free connection if the pool has reached max-connections used. oracle.pool.maxlimit Default value: -1 Maximum number of connections in the pool. It should be set to no less than "3". A usual setting would be about 10 to 100. The default value "-1" means that the number of connections is unlimited. Oracle Advanced Security Support The following properties are supported for Oracle 12.2, which includes support for Oracle Advanced Security. Set the properties below if you want to enable encryption for Platform for the connection towards the Platform database and for ECs for connections towards external databases. More information about Oracle Advanced Security can be found in the Oracle Database Advanced Security Administrator's Guide . Note! MediationZone only supports Oracle Advanced Security for Oracle 12.2. Property Description Property Description oracle.net.encryption_client The level of security of the client that will connect to the Oracle database is determined with this parameter. Accepted values include, REJECTED, ACCEPTED, REQUESTED, and REQUIRED. Example oracle.net.encryption_client="REQUIRED" oracle.net.encryption_types_client The encryption algorithm to be used is determined with this parameter. oracle.net.crypto_checksum_client The level of security regarding data integrity for the connection with the Oracle database is determined with this parameter. Accepted values include, REJECTED, ACCEPTED, REQUESTED, and REQUIRED. Example oracle.net.crypto_checksum_client="REQUIRED" oracle.net.crypto_checksum_types_client The data integrity algorithm to be used is determined with this parameter. PostgreSQL The following properties are applicable to the PostgreSQL database. These settings are used by the Platform for the connection towards the Platform database, and for ECs for connections towards external databases. Property Description Property Description postgresql.connectionpool.defaultQueryTimeout Default value: 300 Sets the maximum allowed duration in ms of any statement. postgresql.connectionpool.maxlimit Default value: 10 Maximum number of connections in the pool for the PostgreSQL database. It should be set to no less than the sum of the values of postgresql.connectionpool.maxlimit configured for Platform and database profiles connecting to this database. These settings are used by the Platform for the connection towards the Platform database and can also be used for ECs connections towards external databases and it is set on Database profile level. The following settings are used by the PostgreSQL Database for the connection to MediationZone, and are stored in the postgresql.conf file in the server that is hosting the PostgreSQL database. These settings are updated from the Psql Terminal using the ALTER SYSTEM sql command. Changes to the settings will require a restart of the PostgreSQL service. $ psql postgres=# ALTER SYSTEM SET max_connections = 110; ALTER SYSTEM Property Description Property Description max_connections Default value: 100 Maximum number of connections in the pool for the PostgreSQL database. It should be set to no less than the value set for postgresql.connectionpool.maxlimit. shared_buffers Default value: 32 Amount of memory (in MB) for caching data that should be dedicated to the PostgreSQL database. effective_cache_size Default value: 128 Amount of memory (in MB) for disk caching. The recommended effective size should be set to 1/2 of the total memory of the server. work_mem Default value: 1 Memory (in MB) for performing in-memory complex sorts in the database. random_page_cost Default value: 4.0 Multiplier value for determining the length of time it takes for the disk to seek a random disk page. SAP HANA The following properties are applicable to the SAP HANA database. These settings are used by the Platform for the connection towards the MediationZone database, and by ECs for connections towards external databases. Property Description Property Description saphana.connectionpool.maxlimit Default value: 10 This property specifies the maximum number of connections in the pool. It is recommended that the value is set to 3 or greater. A usual setting would be between 10 and 100.

---

# Document 1250: KPI Management - Distributed Processing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742761/KPI+Management+-+Distributed+Processing
**Categories:** chunks_index.json

The following scheme demonstrates a KPI Management provisioning and processing scenario. Open Provisioning and processing The steps used (the subsequent pages will describe this in more detail) A user provisions a service model configuration via a KPI profile in the Desktop. The user then submits an application, which performs the KPI calculations, to the Spark cluster. Input data is received by the KPI Cluster In agent as KDR UDRs. The agent then transfers the content through Kafka to the Spark Cluster. The Spark cluster periodically polls the input topic and performs the KPI calculations that are based on the service model and the input data. The polling interval depends on the duration of the Spark batch intervals. When the timestamps of the input data indicate that a configurable time period has elapsed, the Spark cluster sends the calculated KPIs to a dedicated output topic. There is also a separate topic for alarm output. If the service model has been configured to produce immediate alarms, the Spark cluster sends the KPIs that hit an alarm level, within a Spark batch, potentially before their KPI period closes. The data on the output and alarm topics are collected via KPI Cluster Out agents extracts and decodes the KPI data to KPIAggregatedOutput UDRs. This chapter includes the following sections:

---

# Document 1251: HTTPD_Deprecated Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641704/HTTPD_Deprecated+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no message events for this agent. Debug Events There are no debug events for this agent.

---

# Document 1252: mzcli - wfgroupstop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547980191/mzcli+-+wfgroupstop
**Categories:** chunks_index.json

Usage usage: wfgroupstop [-immediate] <pattern matching expression for workflow group name> ... Stops one or more workflow groups. The workflow group does not stop instantly but rather waits for all the workflow group members to stop running before the whole group is fully stopped and returns to the Idle state. Options Option Description Option Description [-immediate] When this option is used, the workflow group is stopped without waiting for a batch to finish. Return Codes Listed below are the different return codes for the wfgroupstop command: Code Decsription Code Decsription 0-> Will indicate the number of failed group stops or the number of arguments that are incorrect. 1 Will be returned if no user is logged in.

---

# Document 1253: Error Search and Examination - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849138/Error+Search+and+Examination
**Categories:** chunks_index.json

Using ECS, it is possible to search for UDRs or batches matching any filter criteria with the Search ECS window as shown below Open Search ECS dialog The result of the search is displayed in the ECS Inspector window shown below, where it is possible to view the UDR and edit the fields by clicking on UDR Type. Open Search results shown in ECS inspector In the Inspector you can also add tags on selected UDRs. These tags can then be used as a search criteria in the Search ECS dialog. The different search settings you make in the Search ECS dialog can also be saved and reused at a later stage. If you have a filter saved, you can also use this when configuring what the ECS Collection agent should collect.

---

# Document 1254: IBM MQ Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739381/IBM+MQ+Preparations
**Categories:** chunks_index.json

The following jar files are required by the IBM MQ collection agent: com.ibm.mq.allclient-x.x.x.x.jar The classpath for the jar file is specified for each configuration file for each EC. For example: <classpath path="/opt/mqm/java/lib/com.ibm.mq.allclient-9.3.2.0jar"/> You can download the jar file from https://mvnrepository.com/artifact/com.ibm.mq/com.ibm.mq.allclient . It is recommended to use the jar version relevant to the MQ service that you are using. For further information on how to update a pico configuration, see Updating Pico Configurations . After the classpath has been set, the jar file should be manually distributed to be in place when the ECs are started.

---

# Document 1255: GCP PubSub Subscriber Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000330/GCP+PubSub+Subscriber+Agent
**Categories:** chunks_index.json

Subscribers are registered listeners on a subscription to receive messages. Subscribers interact with Pub/Sub service to read messages from the pull subscriptions. In order to receive messages published on a topic, a subscription must be created for that topic. Only messages that are published to the topic after the subscription is created, are available to subscriber applications. When a message is sent to a subscriber, the subscriber should acknowledge the message. Without the acknowledgment, the message is considered to be outstanding and PubSub will repeatedly attempt to redeliver these messages to the subscriber. This, However, will not be delivered to other subscribers within the same subscription. GCP PubSub Subscriber agent is available as a collection agent in Real-Time workflows. Open This section contains the following subsections: GCP PubSub Subscriber Agent Events GCP PubSub Subscriber Agent Configuration GCP PubSub Subscriber Agent Input/Output Data and MIM

---

# Document 1256: Error Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638531/Error+Tab
**Categories:** chunks_index.json

The Error tab contains configurations related to error handling of the workflow. Since the batch concept does not exist for real-time workflows, batch related options in the tab are only valid for batch workflows. If you decide not to abort after one cancel batch you can choose whether to save errors to 1) the ECS (Error Correction System) or 2) Data Veracity . These two options are both described below. Open Error tab - Data Veracity batch error type Item Description Item Description Abort after one cancel batch If enabled, the workflow immediately aborts on the first Cancel Batch message from any agent in the workflow. The erroneous data batch is kept in its original place and must be moved or deleted manually before the workflow can be started again. Abort after one cancel batch followed by <configured amount> c onsecutive cancel batches If enabled, the value of <configured amount> indicates the number of allowed cancel batch calls, from any agent in a workflow before the workflow is aborted. The counter is reset between each successfully processed data batch. Thus, if 5 is entered, the workflow aborts on the 6th data batch in a row that is reported erroneous. All erroneous files, but the first one, are removed from the stream and placed into Data Veracity or the ECS . Do not abort after cancel batch The workflow will never abort. However, as with the other error handling options, the System Log is always updated for each cancel batch message, and files are sent to Data Veracity or the ECS . Error Batch Type When selecting Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch options above, the Error Batch Type radio buttons are enabled. Choosing Data Veracity will send all erroneous batches to Data Veracity while choosing ECS will send the erroneous batches to the ECS. Data Veracity Batch Error UDR Data Veracity allows for the handling of erroneous batch files received from the input source. You must set up a database table prior to setting up the error batch handling for your workflows. For more information see Data Veracity . The Data Veracity Error UDR sections are grayed out until one of the Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch alternatives is selected and Error Batch Type is set to Data Veracity . Item Description Item Description Error Code Select from a drop-down list of error codes that have been defined from the error code web interface. You can refer to Error Codes for instructions. The Error Codes are shared between Data Veracity and the ECS . Data Veracity Profile The Data Veracity profile to be used. Refer to Data Veracity Profile for instructions to set up the profile for use. Named MIMs MIM values to be associated with the erroneous batch file when sent to Data Veracity. The Named MIMs added in the Data Veracity Profile should be listed in the table. MIM Resource The Mim Resource column is populated with the MIM values that are based on the MIM parameters which are selected from the MIM Browser dialog. ECS Batch Error UDR A UDR that contains information on selected MIMs can be associated with the batch. This is useful when reprocessing a batch from the ECS, as the fields of the Error UDR will appear as MIMs in the collecting workflow. The batch UDR may be populated from Analysis or Aggregation agents as well. This is useful if you want to enter other values than MIMs. The ECS Batch Error UDR section is grayed out until one of the Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch alternatives is selected and Error Batch Type is set to ECS . Open Error tab - ECS Error Bath Type Item Description Item Description Error Code Select from a drop-down list of error codes that have been defined in the Error Correction System Inspector. The Error Codes are shared between Data Veracity and ECS . Refer to Error Codes for instructions. Error UDR Type The error UDR to be associated with the batch. The appropriate format can be selected from the UDR Internal Format Browser dialog opened by selecting the Browse... button. The columns UDR Field and MIM Resource are populated depending on the selected UDR type. UDR Field A list of all the fields available for the selected Error UDR Type. MIM Resource The Mim Resource column is populated by clicking the MIM button. The preferred MIM to map to the Error UDR Type fields can then be selected from the MIM Browser dialog. Logged MIMs The column Error Mim holds information on what MIM resources to be logged in the System Log when the workflow aborts or sends UDRs and batches to ECS. These values may also be viewed from ECS (the MIM column). The most relevant resources to select are things that identify the data batch, such as the source filename, if available. Note that this is only a short summary of the functionality description. For further information, see Error Correction System .

---

# Document 1257: Amazon S3 Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672052
**Categories:** chunks_index.json

To open the Amazon S3 collection agent configuration dialog from a workflow configuration, you can do the following: right-click the agent icon and select Configuration... select the agent icon and click the Edit button Note! If you are using the Amazon S3 collection agent in a Batch workflow, part of the configuration may be done in the Filename Sequence and Sort Order tabs in Workflow Template . For Real-Time workflow, additional configurations in Decoder , Decompression and Execution tabs are available in Batch-Based Real-Time Agents - Agent Configuration . The Amazon S3 tab contains settings related to the placement and handling of the source files to be co llecte d by the agent. Open Amazon S3 collection agent configuration - Amazon S3 tab Setting Description Setting Description Profile Select the File System profile you want the agent to use, see File System Profile for further information about this profile. File Information Directory Enter the absolute pathname of the source directory on the location stated in the File System profile, where the source files reside. Include Subfolders Select this check box if you have subfolders in the source directory from which you want files to be collected. If you select Enable Sort Order in the Sort Order tab, the sort order selected will also apply to subfolders. Filename Enter the name of the source files in the location stated in the File System profile. Regular expressions according to Java syntax applies. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILE , type: TTFILE.* Compression Select the compression type of the source files: No Compression - agent does not decompress the files. This is the default setting. Gzip - agent decompresses the files using gzip. This determines if the agent will decompress the files before passing them on in the workflow. Before Collection Move to Temporary Directory If enabled, the source files will be moved to the automatically created subdirectory DR_TMP_DIR in the source directory, prior to collection. This option supports safe collection of a source file reusing the same name. Append Suffix to Filename Enter the suffix that you want to be added to the file name prior to collecting it. Important! Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (h) If the specified value is greater than zero, and if no file has been collected during the specified number of hours, the following message is logged: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ) to the directory specified in the >Destination field, after the collection. If the Prefix or Suffix fields are set, the file will be renamed as well. Note! It is only possible to move files within the same bucket. Destination Enter The absolute pathname of the directory on the location specified in the referenced File System profile into which the source files will be moved after collection. This field is only enabled if Move to is selected. Rename If enabled, the source files will be renamed after the collection, remaining in the source directory from which they were collected (or moved back from the directory DR_TMP_DIR , if using the Move To Temporary Directory setting). Prefix/Suffix Enter the Prefix and/or suffix that will be appended to the beginning respectively the end of the name of the source files, after the collection. These fields are only enabled if Move to or Rename is selected. Note! If Rename is enabled, the source files will be renamed in the current directory (source or DR_TMP_DIR ). Be sure not to assign a prefix or suffix, giving files new names, still matching the filename regular expression, or else the files will be collected over and over again. Search and Replace Select this option if you want to apply the Search and Replace function. Select either the Move to or Rename setting. Search : Enter the part of the filename that you want to replace. Replace : Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note! A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Remove If enabled, the source files will be removed from the source directory (or from the directory DR_TMP_DIR , if using the Move To Temporary Directory setting), after the collection. Keep (days) Number of days to keep source files after the collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, determining when the file may be removed. This field is only enabled if Move to or Rename is selected. Ignore If enabled, the source files will remain in the source directory after collection. UDR Type Route FileReferenceUDR Select this check box if you want to forward the data to an SQL Loader agent. See SQL Loader Agent for further information.

---

# Document 1258: Flushing spark - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611619/Flushing+spark
**Categories:** chunks_index.json

Note! As prerequisite, the scripts must be prepared according to Preparing and Creating Scripts for KPI Management . If you stop running a KPI-management system and want to make sure all the pending KPIs are sent to the Kafka output topic, use this spark flush command. $ flush.sh

---

# Document 1259: MySQL - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639479/MySQL
**Categories:** chunks_index.json

This section contains information that is specific to the database type MySQL. Supported Functions The MySQL database can be used with: Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Event Notifications Prepared Statements (APL) SQL Collection/Forwarding Agents SQL Loader Agent Task Workflows Agents (SQL) Properties When selecting the MySQL database type, you can configure the following property using the Properties tab in the Database profile: mysql.connectionpool.maxlimit Preparations The MySQL driver has to be downloaded to the Platform in order to connect to a MySQL database from MediationZone. You must proceed as follows: Go to the MySQL web page and download Connector/J from MySQL Connectors: https://dev.mysql.com/downloads/connector/j/ Place the downloaded jar file in the $MZ_HOME/3pp directory . Restart the Platform and ECs for the change to take effect. In addition, you must carry out the following preparations before attempting to connect to a MySQL database. The next step is to set the necessary transaction isolation levels. Edit the mysqld configuration files by adding this line: mysqld Configuration File transaction-isolation = READ-COMMITTED To decrease re-connection overhead, database connections are saved in a connection pool. To set the connection pool size, set the Execution Context property mysql.connectionpool.maxlimit in the relevant <pico> .conf file: $ mzsh topo set topo://container:<container>/pico:<pico name>/val:config.properties.mysql.connectionpool.maxlimit 45

---

# Document 1260: Shared Table Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737922
**Categories:** chunks_index.json

This section describes the Shared Table profile. This profile en ables workflow instances to share tables for lookups. Open Shared Table Using the Table Lookup Service instead of adding tableCreate in each workflow instance will increase the throughput with fewer duplicated tables, fewer lookups, and reduced memory consumption. The Table Lookup Service comprises a profile in which SQL queries are defined, and two APL functions; one that references the profile and creates a shared table, and one that can be used for refreshing the table data from the APL code. The Shared Table profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow each time you save the profile. Memory Allocation There are three different ways to allocate memory for the created tables. By default, the tables are kept as Java objects in memory. The shared tables can also be configured to keep the tables as raw data either on or off the heap. By using raw data, the overhead of Java objects is removed and less memory is required. The type of memory allocation chosen for the shared tables is configured in the Shared Table profile by selecting a Table Storage parameter. If relevant, you can select an Index Storage parameter and also Variable Width Varchar Columns . For further information about these settings, see the section below, Shared Table Profile Configuration. For further information about memory allocation, see the System Administration Guide . Shared Table Profile Configuration The Shared Table profile configuration is opened by clicking on the New Configuration button in Desktop and selecting the Shared Table Profile option. Open The Shared Table profile dialog The contents of the buttons in the button bar may change depending on which configuration type has been opened. The Shared Table profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The Shared Table profile dialog contains the following configurations: Setting Description Setting Description Database Database Click the Browse... button and select the Database profile you want to use. Any type of database that has been configured in a database profile can be used. See Database Profile for further information. Release Timeout (seconds) If this check box is selected, the table will be released when the entered number of seconds has passed since the workflows accessing the table were stopped. The entered number of seconds must be larger than 0. If this check box is not selected, the table will stay available until the execution context is restarted. Refresh Interval (seconds) Select this check box in order to refresh the data in the table with the interval entered. The entered number of seconds must be larger than 0. If this check box is not selected, the table will only be refreshed if the APL function tableRefreshShared is used. For more information regarding the function, see the section below, tableRefreshShared. Note! The interval for checking if the table needs to be refreshed is 10 seconds, which is the minimum time before a new refresh is performed. If a refresh fails, an error is generated in the system log, but the table is not cleared - the old data remains in the shared table. A new refresh is initiated every 10th second until the refresh has finished successfully. Table Storage Object Select this option to set the Table Storage to Object . If you select this option, the shared tables are stored as Java objects on the JVM heap. Note! If you have selected to use a profile with the CSV database type, this is the only option available if you have not configured properties using Advanced Connection Setup . On Heap Select this option to set the Table Storage to On Heap . If you select this option, the shared tables are stored in a compact format on the JVM heap. If you select On Heap , you must select an option for the Index Storage . Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Off Heap Select this option to set the Table Storage to Off Heap . If you select this option, the shared tables are stored in a compact format outside the JVM heap. Note! You are required to set the JDK parameter in the relevant Execution Context pico configuration, for example: $ mzsh topo set topo://container:<container>/pico:<pico>/obj:config.jvmargs  'maxDirect:["-XX:MaxDirectMemorySize=4096M"]' Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . If you select Off Heap , you must select an option for the Index Storage . Unsafe Select this option to set the Table Storage to Unsafe . If you select this option, the shared tables are stored in a compact format. If you select Unsafe , you must select an option for the Index Storage . Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Primitive Lookup Select this option to set the Table Storage to Primitive Lookup . This provides simple lookup tables with a fast lookup function but they are limited to two columns of type Int/Long for the key (column 1) and type Short/Int/Long for the value (column 2). Lookup operations on Primitive Lookup tables are limited to the equals operation on column 1. Note! If you use the Primitive Lookup option with a database profile that is configured for Oracle, using the Oracle column type NUMBER with a precision greater than 10 may cause errors. Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Index Storage Object Select this option to set the Index Storage to Object . If you select this option, the index is stored as Java objects on the JVM heap. This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . Pointer Select this option to set the Index Storage to Pointer . If you select this option, the index is stored as pointers to the table data. This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . Cached Long/Int Pointer Select this option to set the Index Storage to Cached Long/Int Pointer . This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . For numeric index columns, the Cached Long/Int Pointer can be used for faster lookups, but at the cost of slightly higher memory consumption. Variable Width Varchar Columns Select this check box to enable variable width storage of varchar columns. This reduces memory usage for columns that are wide and of varying widths. SQL Load Statement SQL Load Statement In this field, an SQL SELECT statement should be entered in order to create the contents of the table returned by the tableCreateShared APL function. The following statement will return a table named MyTable with the columns key and value when the tableCreateShare function is used together with this profile. Example - SQL SELECT statement SELECT key,value FROM MyTable If no data has been fetched from the database, SQL errors in the table lookup will cause runtime errors (workflow aborts). However, if data has already been fetched from the database then this data will be used. This will also be logged in the System Log. Whenever possible, use values of the type long instead of the type date or string. This may improve performance. Example - Using SQL SELECT statement SELECT to_number(user_id) as USER_ID, to_number(to_char(nvl(start_date,to_date( '19000101010101', 'yyyymmddhh24miss')), 'yyyymmddhh24miss')) as START_DATE, to_number(to_char(nvl(end_date, to_date( '99990101010101', 'yyyymmddhh24miss')), 'yyyymmddhh24miss')) as END_DATE FROM MyTable Table Indices If you want to create an index for one or several columns of the shared table, these columns can be added in this field by clicking the Add button and add the columns for which you want to create an index. The index will start with 0 for the first column. Note! An index will not be created unless there are at least five rows in the table. Create indices in the Shared Table profile based on the data that is fetched from the database. Even if you look up multiple columns in APL, using one index instead of several may result in improved performance. Note! Some Database Management systems provide character column types so that you are not required to specify the column width (e g TEXT in PostgreSQL). If you use shared tables with such a column type, you cannot use the types On Heap , Off Heap or Unsafe within Table Storage. APL The following functions are included in the Table Lookup Service: tableCreateShared tableRefreshShared tableCreateShared This function returns a shared table that holds the result of the database query entered in the Shared Table profile. table tableCreateShared ( string profileName boolean disableCommit ) Parameters: Returned Value Description Returned Value Description profileName Name of the Shared Table profile you want to use. disableCommit (Deprecated from MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement being performed at the end of every SQL transaction for this particular function. By default, disableCommit is set to true unless otherwise changed via this parameter. Info! It should be noted that on recent Oracle versions, the DBLink SQL transaction behavior has changed, where every single SQL statement for remote database transactions requires a commit or rollback statement in order to close a connection. Returns A table containing the columns stated with the SQL query in the stated Shared Table profile, that can be shared by several workflow instances. Example - Using the function tableCreateShared This creates a shared table called myTable with the columns returned by the SQL query in the mySharedProfile Shared Table profile. initialize { table myTable = tableCreateShared("Folder.mySharedProfile"); } tableRefreshShared This function can be used for refreshing the data for a shared table configured with a Shared Table profile. The table will be updated for all workflow instances that are using the table and running on the same EC. table tableRefreshShared ( string profileName boolean disableCommit ) Parameters: Returned Value Description Returned Value Description profileName Name of the Shared Table profile you want to refresh data for. disableCommit (Deprecated from MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement being performed at the end of every SQL transaction for this particular function. By default, disableCommit is set to true unless otherwise changed via this parameter. Info! It should be noted that on recent Oracle versions, the DBLink SQL transaction behavior has changed, where every single SQL statement for remote database transactions requires a commit or rollback statement in order to close a connection. Returns A refreshed shared table. Example - Using the function tableRefreshShared This returns the shared table called myTable, which uses the mySharedProfile, with refreshed data. table myTable = tableRefreshShared("Folder.mySharedProfile"); Additional Performance Tuning The Oracle JDBC driver includes a feature built that allows you to set the number of rows that are prefetched while the full result set is being formulated. At the time of writing, the default number of prefetched rows is 10. You can increase this value by setting the Execution Context property oracle.jdbc.defaultRowPrefetch in the relevant Pico configuration file.

---

# Document 1261: APN Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/285638748
**Categories:** chunks_index.json

To use Apple Push Notification functionality, you need an Apple Entitlement certificate specified in the APN profile. This profile is loaded when you start a workflow that relies on it. Any changes to the profile will take effect once you restart the workflow. Configuration In the APN profile configuration, enter the details required to register the Apple Push Certificate. Open APN profile configuration Setting Description Setting Description File Path Enter the path and file name of the Apple Push certificate in this field. Password Enter the password for the certificate in this field. Bundle Id Enter the bundle id for the certificate in this field. Production Select this check box if the certificate is a production certificate. Otherwise, it will be considered a development certificate.

---

# Document 1262: HDFS Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033406/HDFS+Forwarding+Agent
**Categories:** chunks_index.json

The HDFS forwarding agent creates files on the remote file system containing the received data. Files are created when a Begin Batch message is received and closed when an End Batch message is received. In addition, the Filename Template service offers the possibility to compress (gzip) the files or to further process them, using commands. To ensure that downstream systems will not use the files until they are closed, they are stored in a temporary directory until the End Batch message is received. This behavior also applies to Cancel Batch messages. If a Cancel Batch is received, file creation is canceled. The chapter contains the following sections: HDFS Forwarding Agent Events HDFS Forwarding Agent Transaction Behavior HDFS Forwarding Agent MultiForwardingUDR Input HDFS Forwarding Agent Configuration HDFS Forwarding Agent Input/Output Data and MIM

---

# Document 1263: HDFS Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641625/HDFS+Collection+Agent
**Categories:** chunks_index.json

The HDFS collection agent collects files from a HDFS, which is the primary distributed storage used by Hadoop applications, and inserts them into a workflow. A HDFS cluster primarily consists of a NameNode that manages the file system meta data, and DataNodes that store the actual data. Initially, the source directory is scanned for all files matching the current filter. In addition, the Filename Sequence and Sort Order services may be used to further manage the matching of files, although they may not be used at the same time since it will cause the workflow to abort. All files found will be fed one after the other into the workflow. When a file has been successfully processed by the workflow, the agent offers the possibility of moving, renaming, removing or ignoring the original file. The agent can also be configured to keep files for a set number of days. In addition, the agent offers the possibility of decompressing compressed (gzip) files after they have been collected. When all the files are successfully processed, the agent stops to await the next activation, whether it is scheduled or manually initiated. The section contains the following subsections: HDFS Collection Agent Configuration HDFS Collection Agent Events HDFS Collection Agent Input/Output Data and MIM HDFS Collection Agent Transaction Behavior

---

# Document 1264: Syntax description - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646138/Syntax+description
**Categories:** chunks_index.json

All APL functions are described according to the following: returnType function ( type1 parameter1 , type2 parameter2 ) Parameter Description Parameter Description parameter1 A description of parameter1. If necessary, an example is given. parameter2 A description of parameter2. If necessary, an example is given. Returns A description of what the function returns. If necessary, an example is given. Example If there is a need to explain a function in relation to other functions and/or agents, this is done in an 'Example' clause. //Global variable declaration int myVar1; //Function block, entry-point for execution in the Consume state. consume { //Local varaible declaration string myVar2; // A comment. /* This is a comment over several lines */ //Assignments myVar1=1; myVar2="example"; //Function call myFunction(myVar2); } //Function declaration int myFunction(string myParam) { debug(myParam); return 0; }

---

# Document 1265: Disk Forwarding Agent Events  - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607135/Disk+Forwarding+Agent+Events+-+Batch
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification dialog. For further information about the agent message Ready with file: name - Reported along with the name of the target file when it has been successfully stored in the target directory. If an After Treatment Command is specified, the message also indicate that it has been executed. Debug Events There are no debug events for this agent.

---

# Document 1266: Kafka UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138475/Kafka+UDRs
**Categories:** chunks_index.json

Open The Kafka UDR types are designed to exchange data between the workflows and can be viewed in the UDR Internal Format Browser , see Administration and Management in Legacy Desktop | UDR Browser . KafkaRecord KafkaOffset

---

# Document 1267: DRRealtimeAsynchRouter - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676639/DRRealtimeAsynchRouter
**Categories:** chunks_index.json

Interface that indicates that records, routed out from the agent, will be turned over to a global workflow thread for further processing. The interface also defines a notification method to be called when the backlog of unprocessed records grows over a certain limit, as defined in the Workflow Preferences as Limit. The notification can also be called before the limit is reached. For information regarding how to control this, see the section DRRealtimeServerEnv in Environment Interfaces . An important limitation is that an agent, implementing this interface, is not allowed to route raw data to other agents. Only records are allowed. If raw data needs to be routed, encapsulate it in a record.

---

# Document 1268: SQL Loader Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654215/SQL+Loader+Agent+Configuration
**Categories:** chunks_index.json

To open the SQL Loader agent configuration dialog from a workflow configuration, you can do either one of the following : double-click the agent icon select the agent icon and click the Edit button The Agent Configuration contains configurations related to the SQL query used for populating the database with data from external files, as well as error handling. Open The SQL Loader agent configuration dialog The SQL Loader tab Setting Description Setting Description Database Profile name of the database that the agent will connect to and forward data to. The supported database profiles are: MySQL Netezza PostgreSQL SAP HANA Sybase IQ SQL Statement In this field you enter the SQL statement to be used for stating where the files containing the data are located, into which table in the database the data should be inserted, as well as the formatting of the data. See SQL Statements for information about how to write the statements. Abort if exception Select this check box if you want the workflow to abort in case of an exception.

---

# Document 1269: Database Agents General Information - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606638/Database+Agents+General+Information
**Categories:** chunks_index.json

You will find information regarding stored procedures, and descriptions on how to configure the mapping for the Database agents to the respective fields in the Database table and vice versa under this section. The section contains the following subsections: Tables and Stored Procedures Inter-Workflow Communication, Using Database Agents Mapping Assignments between Database Fields and UDR Fields SQL Server Considerations

---

# Document 1270: System Architecture and Requirements PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783357/System+Architecture+and+Requirements+PCC
**Categories:** chunks_index.json

This chapter contains a brief overview of the PCC architecture. For further information about the architecture and system requirements, see the PCC Installation Instructions . The Policy Charging and Control solution is designed for high availability with high throughput and low latency. The setup of the solution depends on which type of data repository you have selected: Couchbase, Redis (ElastiCache Cluster on AWS), or MySQL Cluster. PCC Setup when Using Couchbase When using Couchbase, the setup is divided into the following parts: Open PCC Architecture when using Couchbase [CZ01] and [CZ02] - Two separate Platforms Containers, their databases, and various monitoring and management processes for the collection of statistics. Two [CZs] are required for failover purposes, and one of them will be on standby. [EZ01] and [EZ02] - Two Execution Containers, hosting the pico instances, i.e ECs, that run the workflows that are required by the PCC solution. Two are required for failover purposes. [DR01], [DR02] and [DR03] - A Couchbase cluster that contains the Data Repository where all data pertaining to processing is stored. The minimum cluster should consist of three nodes with one replication per bucket. Names within [ ] are used throughout the document to describe on what machines the corresponding function is to be installed, e.g [CZ01] or [EZ01] . When the number is omitted, e.g [CZ], the name refers to both [CZ01] and [CZ02] etc. PCC Setup when Using Redis When using Redis for PCC in AWS, you use Amazon ElastiCache. Monitoring functions are not included in this setup. The setup is divided into the following parts: Open PCC Architecture when using Redis [CZ01] and [CZ02] - Contain the Control Zone and various management processes for the management of the data repository. Two [CZs] are required for failover purposes, and one of them will be on stand-by. [EZ01+DR01], [EZ02+DR02] - Contain the Execution Zones, where the workflows are executed, as well as the Data Repository where all data pertaining to processing is stored. Two [EZs] are required for failover purposes. PCC Setup when Using MySQL Cluster When using MySQL Cluster, the setup is divided into the following parts: Open PCC Architecture when using MySQL Cluster [CZ01] and [CZ02] - Two separate Platforms Containers, their databases, and various monitoring and management processes for the collection of statistics. Two [CZs] are required for failover purposes, and one of them will be on stand-by. [EZ01] and [EZ02] - Two Execution Containers, hosting the pico instances, i e ECs, that run the worfklows that are required by the PCC solution. Two are required for failover purposes. [DR01] and [DR02] - Contains the Data Repository where all data pertaining to processing is stored. Names within [ ] are used throughout the document to describe on what machines the corresponding function is to be installed, e g [CZ01] or [EZ01] . When the number is omitted, e g [CZ], the name refers to both [CZ01] and [CZ02] etc.

---

# Document 1271: Resolving Hostnames - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848731/Resolving+Hostnames
**Categories:** chunks_index.json

The section describes how to resolve hostnames that associated with pico instances in a high availability deployment. For a description of the high availability properties that are used by the HA Monitor Server, see High Availability Properties . Cell Properties Property Description pico.rcp.platform.host On a Platform Container host, the hostname specified in this property must resolve to: Virtual IP address of Platform node Local IP address of Platform node in an OpenStack environment Local IP address of Platform node in an Amazon Web Services environment On an Execution Container host, the hostname specified in this property must resolve to: Virtual IP address of Platform node Floating IP address of Platform node in an OpenStack environment Elastic IP address of Platform node in an Amazon Web Services environment Note! The Platform hostname must be static. Container Properties Property Description pico.rcp.server.host On a Platform Container host, the hostname specified in this property must resolve to: Virtual IP address of Platform node Local IP address of Platform node in an OpenStack environment Local IP address of Platform node in an Amazon Web Services environment Note! The Platform hostname must be static On an Execution Container host, the hostname specified in this property must resolve to: Virtual IP address of pico node Local IP address of pico node in an OpenStack environment Local IP address of pico node in an Amazon Web Services environment Loading

---

# Document 1272: REST Client_Deprecated Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674408/REST+Client_Deprecated+Agent+Events
**Categories:** chunks_index.json

Agent Events There are no message events for this agent. Debug Events There are no debug events for this agent.

---

# Document 1273: Examples of Non-Automatic Maps - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744259/Examples+of+Non-Automatic+Maps
**Categories:** chunks_index.json

Non-automatic maps provide explicit mapping between the external and internal format. Such mappings work on fields with primitive types and/or fields that are instances of other formats. In the latter case, the maps can be nested indefinitely. The following two examples demonstrate these concepts. Example - Non-Automatic Maps, all primitives In the following example a decoder, based on the in_map , does the following: Reads an instance of the external record ExtFormat . Associates ef1 with if1. That is, any reference to if1 forces decoding of the information of ef1 and in the process converts it to an integer. Associates ef2 with if2. That is, any reference to if2 forces decoding of the information of ef2 and in the process converts it to a string. external ExtFormat { ascii ef1 : static_size(4), padded_with(" "); int ef2 : static_size(2); }; internal IntFormat { int if1; ascii if2; }; in_map InMapFormat: external(ExtFormat), internal(IntFormat) { e:ef1 and i:if1; e:ef2 and i:if2; }; Example - Non-Automatic Maps, externals in externals In the following example a decoder, based on the in_map , does the following: Reads an instance of the external record ExtFormat2 . Associates ef1 with if1. That is, any reference to if1 forces decoding of the information of ef1 . The conversion between ExtFormat1 and IntFormat1 is, in this case, dictated by the in-map named InMapFormat1 as declared in InMapFormat2. external ExtFormat1 { ascii ef1 : static_size(4); }; external ExtFormat2 { ExtFormat1 ef1; }; internal IntFormat1 { int if1; }; internal IntFormat2 { IntFormat1 if1; }; in_map InMapFormat1: external(ExtFormat1), internal(IntFormat1) { e:ef1 and i:if1; }; in_map InMapFormat2: external(ExtFormat2), internal(IntFormat2) { e:ef1 and i:if1 using in_map InMapFormat1; }; As seen in the example, explicit map specifications for complex formats can become very large, even describing all the internals to map data too, can be a huge task. This is why automatic mapping is normally used.

---

# Document 1274: SNMP OID Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674811/SNMP+OID+Profile
**Categories:** chunks_index.json

Open With the SNMP OID profile, you can configure which OIDs, that is UDR types and fields to poll, outside of the SNMP Request agent itself. This way, several agents can use the same configuration. Configuration To create a new SNMP OID profile configuration, click the New Configuration button in the upper left part in Desktop, and then select SNMP OID Profile from the menu. The SNMP OID profile configuration contains the following settings: Setting Description Setting Description SNMP Collection Profile Select which SNMP Collection profile the OID profile should use. Poll/Field/Instances Add the UDR types and fields you want to have available for polling in this section, and select the Poll check box for the ones you want to poll. Continuous requests for the specified OIDs will be sent out. Do not enable this if you want to send out dynamic requests from APL to the OID by using the generated structure.

---

# Document 1275: mzcli - desktopadmin - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979447/mzcli+-+desktopadmin
**Categories:** chunks_index.json

Usage usage: desktopadmin [options] This command can be used for shutting down connected Desktops, Legacy Desktops or for displaying an administrator message to all users. When shutting down connected Desktops or Legacy Desktops, locked configurations are released as well. Any mention of desktops in the description below will represent both the Desktop and Legacy Desktop unless specified. Note! Only users belonging to the Administrator group are authorized to execute this command. Options The command accepts the following options: Option Description Option Description [-l, --list] List all connected desktop sessions include SSO login. Returns a table with session details. Format: <Name>, <Session ID>, <Username> Example, +----------------+---------------------+----------+ | Name | Session ID | Username | +----------------+---------------------+----------+ | legacy-desktop | 192.168.0.224:53631 | john | | desktop | 154379321 | sarah | +----------------+---------------------+----------+ For Legacy Desktop, Session ID column will display <IP Address:Port>. For Desktop, Session ID column will display Session ID. [-k, --locks] Retrieves a list of locked configurations for each currently connected desktop. Must be used with the -l option. Example, +----------+------------+----------+-------------------------+ | Name | Session ID | Username | Locked Configuration(s) | +----------+------------+----------+-------------------------+ | desktop | 1363198261 | john | | | desktop | 1545132119 | sarah | autotest.asserts | +----------+------------+----------+-------------------------+ [-m, --message] Broadcasts a message to all currently logged-in desktops and those that log in later. Use with -n to send the message to a specific desktop. This message can be removed using the -r option. [-r, --reset] Cancel and clear scheduled shutdown and message, if any. [-n, --session] Apply the command to a specific desktop session obtained using the -l option. Use with one of the following options: -x , -m , -r , or -s . [-x, --shutdown] Schedules a forced shutdown for all desktops after the specified number of minutes (0 for immediate shutdown). Use it with -n to shutdown a specific desktop An optional message can be included using -m . Example, desktopadmin -n 15300940 -x 5 "Shutting down in 5minutes" This message will pop up on the desktop and count down of 5 minutes to shutdown. [-s, --status] Use this option to displays the current shutdown timeout and message, if any. Return Codes Listed below are the different return codes for the desktopadmin command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if there was a problem parsing command parameters. 2 Will be returned if there was a failure scheduling shutdown of Desktops

---

# Document 1276: Kafka Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301137921/Kafka+Agents
**Categories:** chunks_index.json

This section describes the Kafka agents and profile. The agents are used to forward and collect messages using Kafka. Kafka is a cluster-based software executed externally outside of MediationZone. Kafka uses a high throughput publish-subscriber messaging model. It persists all messages and by design, it decouples the publisher from the subscriber. This means that the forwarding agent will keep writing to the log even if the collection agent terminates. The Kafka forwarding agent is available in real-time workflows and is listed among the processing agents while the Kafka collection agent is available in both batch and real-time workflows and is listed among the collection agents. In this release we have built two new Kafka agents that will replace the previous ones in a later release. In this documentation, the previous agents and profile are referred to as, Legacy Kafka agents and Legacy Kafka profile. Subsections This sections contains the following subsections: New Kafka Agents Legacy Kafka Agents

---

# Document 1277: Disk Collection Agent Input/Output Data and MIM  - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607118/Disk+Collection+Agent+Input+Output+Data+and+MIM+-+Batch
**Categories:** chunks_index.json



---
**End of Part 53** - Continue to next part for more content.
