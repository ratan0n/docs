# RATANON/MZ93-DOCUMENTATION - Part 89/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 89 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.4 KB
---

The Search ECS dialog allows the user to filter out and locate erroneous UDRs and batches in the ECS. Any search settings you make can also be saved as filters that you can use for future searches. The Search ECS dialog is displayed when the Search option is selected in the ECS Inspector. Select UDRs or Batches to display UDR or batch specific options. Open Search ECS dialog Search Options The entries in the ECS can be either UDRs or batches. Depending on the selected type, different search options are available. Common Search Options The following settings are available for both UDRs and batches in the ECS. Search Option Description Search Option Description Saved Filters This is the column to the left in the dialog. This column lists any previously saved filters. For more information about how to create a filter, see section Saving Search Settings as a Filter below. Workflow The name of the workflow that sent the entry to the ECS. Agent The name of the agent that sent the entry to the ECS. Error Code An Error Code that has been defined in the ECS Inspector. See ECS Error Codes for further information. Error Case A list displaying the Error Cases associated with the selected Error Code. If the entry is too long to fit in the field, the field can be expanded by enlarging the ECS Inspector in order to display the entire error case text. An Error Case is a string, associated with a defined Error Code. Error Cases can only be appended via APL code: udrAddError( <UDR>, <Error Code>, <Error Case> ); Note! When Batches is selected, the <UDR> parameter is the error UDR. MIM It is possible to configure a workflow to send descriptive MIM values with the actual data (in the Workflow Properties window). This can be used to refine the search in the ECS. Insert Period Select this checkbox to search for UDRs or batches that have been inserted into the ECS during a specific time period. Choose User Defined and specify start time ( From ) and end time ( To ), or use one of the predefined intervals, e.g. Last Hour , Today , This Week . Reprocessing Group Contains a list of all reprocessing groups. Unassigned UDRs or Batches list all entries not associated with any reprocessing group. Reprocessing State Filter based on state, which can be New or Reprocessed . Only entries in state New may be collected. Search Options for Batches The following settings are only available when searching for batches in the ECS Search Option Description Search Option Description Cancel Agent The name of the agent that cancelled the batch. Cancel Message The error message sent as an argument with the cancelBatch function. Error UDR Type The type of Error UDR that can optionally be sent with a batch, containing important MIM information (or any other desired information when the UDR is populated via APL). Search Options for UDRs The following settings are only available when searching for UDRs in the ECS. Search Option Description Search Option Description UDR Type The type of UDR you want to search for. Tag If you have tagged UDRs, this option can be used for displaying only UDRs with the stated tag. Reprocessing State Change Period Select this checkbox to search for UDRs that last changed state (to New or Reprocessed ) during a specific time period. Choose User Defined and specify start time ( From ) and end time ( To ), or use one of the predefined intervals, e.g. Last Hour , Today , This Week . Search Fields If you have selected to search for UDRs , there is an Advanced tab, where you can select to Search Fields . Here, you can enter specific values for different UDR labels that you have configured, see Configuring Searchable Fields in the ECS . Only UDRs containing the specified values will be displayed in the ECS Inspector. Example For example, with the following setting: Open Only UDRs with IMSI 2446888776 are displayed in the ECS Inspector, provided that the label IMSI has been mapped to the IMSI field in the UDRs. Wild cards and intervals can also be used when entering the values for the fields; " * " can be used for matching any or no characters, and intervals can be set by using brackets " [ ] ". When using the * or [ ] , the following rules apply: Only one wild card and one interval can be used per value. An interval is defined with a start value and an end value. If the interval consists of the same number of digits as start and end values, the match is made based on the specified number of digits, e.g. (a[001-002]) matches a001 but not a01 . If the interval consists of different numbers of digits as start and end values, the match is made based on an appropriate number of digits in the UDR, e.g. (a[1-20]) matches a1 and a20 , but not a01 or a020 . Only one interval can be stated within the " [ ] ". The start and end values have to consist of numbers. The start value cannot be larger than the end value. The start value cannot have a larger amount of numbers than the end value, e.g. [0001-3] . If a setting for a value is not correct, an error message is displayed as a tooltip. In order for a UDR to pass the filter, all the defined values have to match. Example For example, with the following setting: Open Only UDRs with: A Number starting with 468 AND B Number starting with 47 AND Location Area Code 10 to 20 will be displayed. Saving Search Settings as a Filter When the search settings have been made, you can select to save these settings as a filter. Warning! If you have included search criteria that refer to parameters previously defined in your system, such as error codes, tags, search fields, etc, such filters will not work properly if you delete any of the defined parameters. To save a filter: Set the search options you want and click Save (bottom left corner button). A dialog opens asking you to enter a name for the filter. Open Entering Filter Name Enter a name in the Filter Name field and click OK . The dialog closes and the new filter appears in the Saved Filters list. The next time you want to use the same search settings, click the filter name in the Saved Filters list and the saved search settings are displayed. Hint! Any saved filters can be renamed or deleted by selecting the filter and clicking the Rename or Delete buttons.

---

# Document 2102: change-password - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/543719541/change-password
**Categories:** chunks_index.json

Usage: change-password --password <new password> [--validate-only] This command allows users to update their current login password. Additionally, it provides an option to validate a new password before applying the changes. Option Description Option Description [-p, --password <new password>] This is the mandatory option. You must specify the new password with this option. [-vo, --validate-only] Validates the new password without actually applying the change. Examples - Using the change-password command Changing the user password: mzsh <username>/<password> change-password --password MyNewPass@ or mzsh <username>/<password> change-password -p MyNewPass@ Validating the new password without applying changes: mzsh <username>/<password> change-password --password MyNewPass@ --validate-only or mzsh <username>/<password> change-password -p MyNewPass@ -vo Return Codes Listed below are the different return codes for the configuration command: Code Description Code Description 0 Will be returned if the command executed successfully. 1 Will be returned if the password change failed. 10 Will be returned if the password validation failed.

---

# Document 2103: Rollback for Standard Upgrades - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/550731808/Rollback+for+Standard+Upgrades
**Categories:** chunks_index.json

The rollback procedure is designed to restore the system to its original state before the upgrade. If the upgrade fails, a rollback will be triggered automatically to revert the file system, configuration files, and database to their previous states. This ensures system stability and consistency. You can do a manual rollback but this is not recommended if changes have been made to the system or database after the upgrade, since it may result in inconsistencies or corruption. Automatic Rollback When an upgrade fails, the rollback process begins automatically. Below is an example of the logs you might encounter during an upgrade from version 9.3.1.9 to 9.3.2.0: === Starting rollback in /mzhomes/mz1 === Old version: 9.3.1.9 === Rolling back from attempted upgrade to 9.3.2.0 Once the rollback process is complete, you will see the following confirmation: ============================== ROLLBACK COMPLETE =============================== Manual Rollback If you have completed an upgrade and want to do a rollback, you can initiate a rollback manually. It is assumed that no configurations have been updated or added in the upgraded version and that the installation directory has not been removed. If the installation directory has been removed, you need to follow the instructions in General Preparations Platform before performing the rollback. You can perform a manual rollback by running the following command: ./setup.sh rollback During a rollback, the process uses backup files stored in the following directory: <my_mz_home>/tmp/upgrade/backup This directory is created during the upgrade process and is essential for rollback in case of any failure. The directory above is the default directory, you can also define another directory by using the property upgrade.temp.dir , see Upgrade Preparations .

---

# Document 2104: Radius Client Agent Messages - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674360/Radius+Client+Agent+Messages
**Categories:** chunks_index.json

Agent Message Events There are no message events for this agent. Agent Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see 4.3.23 Debug Event . The agent produces the following debug events: Event Description Event Description Access request with invalid passwd/secret from <ipaddress> Indicates an invalid password was entered or is missing for the connection. Accounting request with invalid signature from <ipaddress> Indicates the calculated sum, based on secret specified for the agent, is not equal to the secret in the incoming packet. Incoming invalid code <xx> from <ipaddress> Indicates the incoming request is not ACCESS_REQUEST or ACCOUNTING_REQUEST. Maximum number of resends (<x>) reached for packet sent to <ipaddress> on port <port> Indicates that the maxumim number of retries have been reached and no more attempts will be made to resend the message. Request message has remote IP set to <ipaddress> and there is no entry for this address in the Radius Servers table. Rejecting message. Indicates that the request destination that is set in the workflow has not been configured for the agent. Access response with invalid passwd/secret from <ipaddress> Indicates an incorrect Response Authenticator. Accounting response with invalid signature from <ipaddress> Indicates an incorrect Response Authenticator. Incoming invalid code(<Code>) from <ipaddress> Indicates that the incoming response code is not the expected one. Maximum number of resends reached for Radius Request [packet id ='ID'] to host : <ipaddress>, port : <port> Indicates that the maximum number of repeated attempts had been reached and that no more attempts will be made to send the message. Radius Request [packet id =<ID>] to host : <ipaddress>, port : <port> has been resent. Number of resends for this request: <NUM>. Indicates that another attempt is being transmitted.

---

# Document 2105: Supervision Service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/496730317/Supervision+Service
**Categories:** chunks_index.json

The Supervision Service allows you to define actions based on specific conditions using decision tables. Depending on which conditions are met, the service can log messages in the System Log, apply an overload protection configuration, or generate an event. You can configure these conditions using any available MIMs in the workflow, such as throughput or queue size. You can also manually trigger the Supervision Service using mzsh commands for maintenance or other purposes. After manual activation, you must switch it back to automatic mode for the settings in the Services tab to retake effect. Open Supervision Service concept Configuration of the Supervision Service To open the Supervision Service configuration, open the Workflow Properties dialog in a real-time workflow configuration, click on the Services tab, click click Add Service , select the Supervision option, and click OK . Open Supervision configuration - Workflow properties Services tab Setting Description Setting Description Execution Interval (ms) Enter the time interval, in milliseconds, with which current MIM values should be checked against the conditions in the decision tables. This configuration will be valid for all decision tables. Decision Tables A Decision Table holds the conditions you set and the actions that are meant to happen when those conditions are met. All the decision tables you configure will be listed in this section. Click Add to add a new decision table. We recommend configuring different decision tables for different purposes, for example, one for aggregation rules and one for data enrichment. Note! You can change the order of your decision tables, but this does not affect the functionality. All decision tables will be applied. Creating a Decision Table Click to add a new decision table, the Add Decision Tables dialog opens. This is where you configure a Decision Table. In a Decision Table, actions are triggered based on predefined conditions. Conditions and actions are configured separately. Open Add Decision Table dialogue Setting Description Setting Description Decision Table Enter a name for your decision table in the Name field. Table Parameters Click the buttons Action Lists and Conditions Lists to configure the different conditions and actions for this table. Decisions Each configured condition list will be displayed in the Conditions column. Each of these conditions can be set to either True , False or - . If you want to add more columns to set up different combinations of conditions, you can right-click the Action column heading and select to add more columns. For each column with a condition combination, you can then select which action to take in the drop-down list containing all configured actions. Configure Decision tables - high level steps Configure the conditions. The conditions you configure are based on different MIM parameters having defined values. Configure the actions. Depending on your workflow configuration you can select different options when configuring the actions. These options are: Generate a Supervision event to reject messages Log an entry in the System Log Add Overload Protection to reject messages (this option is only available if you have Radius or Diameter collection agents in your workflow). Note! If you are using Diameter or Radius Collection agents in your workflow, you can also select from a range of Diameter and Radius specific overload protection strategies in order to only reject specific types of messages. See https://infozone.atlassian.net/wiki/x/RJIyD and https://infozone.atlassian.net/wiki/x/14dCD for further information about these strategies. Set the newly created conditions to True , False or - (ignore), then select an action for each one. Note! When a condition is evaluated to true, the corresponding action will be performed only once, until any other condition is also evaluated to true. Generally, this means that a minimum of two conditions are required in the decision table. Set a name for the decision table if you havent already done so. Click Add . The decision table is added and you can repeat steps 1 to 4 to add additional tables. Open Multiple Decision Tables added in the Services tab Configure Conditions 1. From the Add Decision Tables dialog, click Condition Lists. Open 2. Click Add to open the Add Conditions dialog. Open Condition List dialogue 3. Click Add to add conditions. Open Add Condition List dialogue In the Add Condition dialogue: Open Add Condition dialogue Add Condition option Description Add Condition option Description Left Operand Select the MIM Field that you want to add your condition to. Operator list This is the drop-down list located between the two operands. Select from: > (greater than) < (less than) == (equals) != (does not equal) Right Operand Select either the MIM Field against which the selected left operand will be compared, or add a constant value. Click Add to add the condition to the condition list. Repeat steps 4 to 5 until you have added all the conditions you want to have in the condition list and then click Close when you are finished. You will return to the Add Condition List dialog. Enter a name for the condition list in the Name field. In the Match field select if you want all the conditions in the list to be matched or, if only one condition is required to match by selecting either of the buttons Any of the Following or All of the Following . Open Add Condition List dialogue Click Add to add the condition list in the Add Decision Table s dialog. Repeat steps 3 to 9 until you have created all the condition lists you want to have and then click Close when you are finished. You will return to the Add Decision Tables dialog. Open Add Decision Tables dialogue - with a Condition List Now that you are back in the Decision Table dialog you can create the actions for the condition lists. Configure Actions To configure actions: In the Add Decision Tables dialog, click the Action Lists button. Click the Add button to open the Add Actions dialog. Click the Add button to add actions. Open Add Actions process In this dialog, select which type of action you want to use; System Log , Overload Protection, or Supervision Event . Depending on what you choose, the options in the dialog differ. Add Action option Description Add Action option Description Action Select from the following: System Log - add a notification to the system log. Supervision Event - To generate an event that can be sent to various targets depending on how you configure your Event Notifications, see https://infozone.atlassian.net/wiki/x/u4Q4D . Overload Protection - Overload Protection - Add an overload protection service Note! You will only see the option for an Overload Protection If you have Diameter or Radius Collection agents in your workflow. Description Enter a description for the action. Reject This option is only available when you have selected to configure an Overload Protection action and determines the percentage of requests that should be rejected; 0, 25, 50, or 100 %. Strategy This option is only available when you have selected to configure an Overload Protection action and determines if you want to apply this action on all types of requests or if you only want to apply them to the requests following a selected Diameter or Radius overload protection strategy. Content This option is only available when you have selected to configure a Supervision Event action and it determines the content of the event. See Event Types for more information. Severity This option is only available when you have selected to configure a System Log action and it determines the severity of the log entry; Information, Warning, Error, or Disaster. Message This option is only available when you have selected to configure a System Log action and allows you to enter a message that will be visible in the System Log. Expand the sub-steps for the option you selected If you have selected an Overload Protection action: If you have selected a Supervision Event action: If you have selected a System Log action: Click Add to add the action to the action list. Repeat steps 4 to 5 until you have added all the required actions to the action list and then click Close. You will return to the Add Action List dialogue. Open Add Action List dialogue Add a name and click the Add button to add the action list in the Create Decision Tables dialog. Setting Description Setting Description List Enter a name for the action list in the Name field. Action This section contains all the actions you have added to this list. Repeat steps 1 to 8 until you have created all the necessary action lists then click Close. You will return to the Create Decision Tables dialog. Configuring a Decision Table In the Decision Table tab you will now have two columns; Conditions and Actions . Open Add Decision Table dialogue The Conditions column lists all the condition sets you have created. In the Actions column, you can set each condition to True , False , or - (Ignore), and then choose which action to trigger when the decision table criteria are met. If you have multiple conditions, various combinations may require different actions. To add another action column, right-click the Actions column heading and select Add Column... . You can repeat this process to define actions for all necessary condition combinations. Note! Only one action can be selected for each set of combinations. Note! When a condition is evaluated to true, the corresponding action will be performed only once, until any other condition is also evaluated to true. Generally, this means that a minimum of two conditions are required in the decision table. Example - Decision Table Open Decision Table configuration example The following actions will be taken during the following conditions: Case 1 If the Incoming messages exceed 50, a Supervision Event will be triggered. Case 2 If the Incoming messages exceed 100, 25 % of the incoming Diameter Credit-Control Initial requests will be rejected. Case 3 If the Incoming messages exceed 150, 100 % of the incoming Diameter Credit-Control Initial requests will be rejected. MZSH Commands If you need to manually trigger or clear the supervision service with action overload, for example, for maintenance or other purposes, you can use the mzsh wfcommand . See https://infozone.atlassian.net/wiki/x/MRRCD for more information.

---

# Document 2106: APN Agent MIM and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/285638810/APN+Agent+MIM+and+Events
**Categories:** chunks_index.json

MIM This agent does not publish nor access any MIM parameters. For information about the MIM and a list of the general MIM parameters, see MIM . Agent Message Events There are no agent message events for this agent. For information about the agent message event type, see Agent Event . Debug Events There are no debug events for this agent.

---

# Document 2107: FTP Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641329/FTP+Forwarding+Agent+Configuration
**Categories:** chunks_index.json

You open the FTP forwarding agent configuration dialog from a workflow configuration. To open the FTP forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select FTP from the Forwarding tab of the Agent Selection dialog. This dialog contains two tabs: FTP and Filename Template . The FTP tab in turn contains four tabs; Connection Target , Advanced , and Backlog , which will be described on this page. For information about Filename Template configuration, see Workflow Template . Connection The Connection tab is used to configure the remote server connection. Open The FTP forwarding agent configuration - Connection tab Setting Description Setting Description Connection Information Host Enter the hostname or IP address of the remote host. If a connection cannot be established to this host, the Additional Hosts specified in the Advanced tab, are tried. Username Enter the username for the remote host account. Password Enter the associated password. Transfer Type Select the data transfer type to be used during file retrieval. Binary - agent uses binary transfer type. Default setting. ASCII - agent uses ASCII transfer type. File System Type Specify the file system type of the remote host. You can choose between Unix , Windows NT, and VAX/VMS . Forwarding Retries Enable Select this check box to enable forwarding connection retry attempts. When this option is selected, the agent will attempt to connect to the host as many times as is stated in the Retry Intervals(s) field. The maximum number of attempts is defined in the Max Entries field. Retry Interval(s) Enter the retry time interval in seconds. Max Entries Enter the maximum number of forwarding retry attempts. Target Tab Open The FTP forwarding agent configuration - Target tab Setting Description Setting Description Input File Handling Input Type The agent can act on two input types. Depending on which one the agent is configured to work with, the behavior will differ. The default input type is byte array , that is the agent expects byte arrays. If the input type is MultForwardingUDR , the behavior will change. For further information about the agent's behavior with MultiForwardingUDR input, refer to FTP Forwarding Agent MultiForwardingUDR Input . File Information Directory Enter the absolute pathname of the target directory on the remote host. The pathname may also be given relative to the home directory of the user's account. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR . When an endBatch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this check box to create the directory, or the directory structure, in the specified path. Note! The directories are created during workflow execution. Compression Select the compression type of the target files. When enabled this will compress the files before they are stored on the server. No Compression - The agent will not compress the files. Gzip - The agent will compress using Gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. Target File Handling Produce Empty Files Enable this option if you need to create empty files. Handling of Already Existing Files Select the behavior of the agent when handling existing files. The available options are: Overwrite - The old file will be overwritten and a warning entry will be created in the System Log. Add Suffix - If the file already exists the suffix ".1" will be added. An ascending order of numeral suffixes will be applied if it already exists, starting from ".2". Abort - This is the default option and it is applied for upgraded configurations. Temporary File Handling Use Temporary Directory If this option is selected, the agent will move the file to a temporary directory before placing it in the target directory. After the whole file has been transferred to the target directory, and the endBatch message has been received, the temporary file is deleted from the temporary directory. Use Temporary File If there is no write access to the target directory and, hence, a temporary directory cannot be created, the agent can move the file to a temporary file that is stored directly in the target directory. After the whole file has been transferred, and the endBatch message has been received, the temporary file will be renamed. The temporary filename is unique for every execution of the workflow. It consists of a workflow and agent ID, and a file number. Abort Handling Select the file handling behavior in case of cancelBatch or rollback is received. The options are to Delete Temporary File or Leave Temporary File . Note! When a workflow aborts, the file will not be removed until the next time the workflow is run. Advanced Tab Open The FTP forwarding agent configuration - Advanced tab Setting Description Setting Description Advanced Settings Command Port Select the port number of the remote FTP server. Timeout(s) The maximum time, in seconds, to wait for a response from the server. A value of "0" will wait indefinitely. Passive Mode (PASV) This enables Passive Mode. In passive mode, the channel for data transfer between the client and server is initiated by the client instead of by the server. This is useful when a firewall is situated between the client and the server. Additional Hosts Here you can specify additional host names or IP addresses that can access the data storage. Connections attempts to these hosts are initiated, in sequence from top to bottom, if the agent fails to connect to the remote host set in the Connection tab. Use the Add , Edit , Remove , Up , and Down buttons to configure the host list. Note! The names of the created files are determined by the settings in the Filename Template tab. The use and setting of private threads for an agent, enabling multi-threading within a workflow, is configured in the Thread Buffer tab. For further information, see Thread Buffer Tab in Workflow Template . Backlog Tab The Backlog tab contains configurations related to backlog functionality. If the backlog is not enabled, the files will be moved directly to their final destination when an end batch message is received. If the backlog however is enabled, the files will first be moved to a directory called DR_POSTPONED_MOVE_DIR and then to their final destination. Refer to Retrieves in FTP Forwarding Agent Transaction Behavior for further information about transaction behavior. When the backlog is initialized and when backlogged files have been transferred a note is registered in the System Log. Open The FTP forwarding agent configuration - Backlog tab Setting Description Setting Description Enable Backlog Check to enable the backlog option. Directory Base directory in which the agent will create sub-directories to handle backlogged files. Absolute or relative path names can be used. Max Size Type Files is the maximum number of files allowed in the backlog folder. Bytes refer to the total sum of all files that reside in the backlog folder. If a limit is exceeded the workflow will abort. Size Enter the maximum number of files or bytes that the backlog folder can contain. Processing Order Determine the order by which the backlogged data will be processed once the connection is reestablished, and select between First In First Out (FIFO) or Last In First Out (LIFO). Duplicate File Handling Specifies the behavior if a file with the same file name as the one being transferred is detected. The options are Abort or Overwrite and the action is taken both when a file is transferred to the target directory or to the backlog.

---

# Document 2108: Platform Container Installation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669841/Platform+Container+Installation
**Categories:** chunks_index.json

This chapter describes how to prepare and install the Platform Container. This is done in three steps: General Preparations which include configuration of the software environment and setting values for predefined parameters that may vary between installations. For further information, see General Preparations Platform . Database-specific preparations. Depending on the software you have, either Oracle, PostgreSQL, SAP HANA, or Derby should be installed. For further information, see Oracle Preparations , PostgreSQL Preparations , SAP HANA Preparations , or Derby Preparations . Software installation. For further information, see Platform Software Installation . Post Installation configuration. For further information, see Post Platform Installation Configuration . Hint! This section covers a small installation with both Platform and EC in the same installation. For separate Platform and EC container installations, first perform a Platform installation and then refer to Execution Container Installation . This chapter includes the following sections: General Preparations Platform Platform Database Preparations Preparing JDBC Drivers Platform Software Installation Post Platform Installation Configuration

---

# Document 2109: quit - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646433/quit
**Categories:** chunks_index.json

usage: quit [ N ] Exits the command line tool. The argument N is the final exit code returned to the calling process. If no argument is supplied, the result of the previous operation will be returned to the calling process. Return Codes Listed below are the different return codes for the quit command: Code Description Code Description N The value of the supplied argument or the final exit code from the last command.

---

# Document 2110: Event Fields - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997812
**Categories:** chunks_index.json

An event is an object containing information related to an event. For example, there are Workflow State events that are emitted each time the state of a workflow is changed. The Event Notification configuration subscribes to these events and routes them to notifiers, for example, log files or a database. An Event Type is comprised of a set of fields containing the original event message, a set of standard workflow related information, and event specific fields that are the parameters the original event message. An Event Message contains information ordered in fields All event types in the system inherit fields from the Base event type. Workflow related events inherit fields related to the Workflow event as well, such as agentName . In addition to these, User Defined Events will receive any fields as defined by the user. There are two types (hierarchies) of events: User Defined events. Events inherited from a Base event (all other events), with additional information added. The user-defined event must be configured in an Ultra Format configuration. Other than the fields entered by the user, the system will automatically add basic fields. User Defined Events may only be dispatched from an agent using APL, i e Analysis or Aggregation. User defined events are sent from Analysis or Aggregation agents Fields added by the user must be populated manually by using APL commands, while the basic fields are populated automatically. From the basic fields, only category and severity may be assigned values. The other basic fields are read only, hence it is not possible to assign values to them.

---

# Document 2111: Diameter Routing Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607031/Diameter+Routing+Profile
**Categories:** chunks_index.json

The Diameter Routing profile enables you to define the Peer Table and the Realm Routing Table properties for the Diameter Stack agent. You can also enable throttling, which allows you to prevent more than the specified number of UDRs per second to be forwarded. The throttling functionality uses the token bucket algorithm. The Diameter Routing profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. It is also possible to make changes effectively while a workflow is running. For more information about this, see the section below, To Dynamically Update the Diameter Routing Profile. To define a Diameter Routing profile, click on the New Configuration button and then select Di ameter Routing profile in the menu. Routing Tab The Diameter Routing profile configuration - Routing tab Peer Table A Diameter Stack agent that uses the Diameter Routing profile maintains transport connections with all the hosts that are defined in the Peer table list. Connections and handshakes of hosts that are not in this list are rejected with the appropriate protocol errors. Note that the system will actively try to establish connections to any hosts that are included in this list, unless the Do Not Create Outgoing Connections option is checked in the Diameter Stack agent. Item Description Item Description Hostname The hostname (case sensitive) or IP address of a Diameter Identity. For example, ggsn01.vendor.com Note! The content of the Origin-Host AVP in the answer commands from the specified peer should be identical to this value. If the values do not match, the MIM values published by the Diameter Stack agent that contain counters are not updated correctly. This may occur, for instance, if you have specified a hostname in this text box but the Origin-Host AVP contains an IP address. It is recommended that you consistently use either IP addresses or hostnames when configuring the Diameter profiles and agents. Port The port to connect to when initiating transport connections with a peer. For example, 3868 . Protocol The transport protocol to use when initiating a peer connection. The following settings are available: TCP TCP/TLS SCTP When TCP/TLS is selected, the Diameter Stack requires a secure connection from this host. You configure this feature by setting the Keystore Path and the Keystore Password in the Diameter Stack agent. For further information, see the section, Advanced Tab, in Diameter Stack Agent Configuration . Note! SCTP must be installed on every EC host that uses the SCTP protocol. For installation instructions, see your operating system documentation. Throughput Threshold If throttling has been enabled for the peer, this field will show the configured threshold for when transmissions of request UDRs should be throttled. Throttled UDRs will be routed back into the workflow. For example 1.000 (which means a maximum of 1.000 UDRs/second will be transmitted). Note! Throttling will determine if and how the workflow will limit the number of requests and UDRs sent out from the workflow. For information regarding how to configure the Diameter agent to reject incoming requests or UDRs to the workflow, see the section, Diameter Too Busy in Diameter Stack Agent Configuration . On Connection from Unknown Peer When unknown peers try to connect to the Diameter Stack, you can configure how such connection attempts are to be handled. You can choose from three options from the drop box: Reject - Select this option to reject all connection attempts from unknown peers. This is the default option. Accept Secure Connection - Select this option to accept connection attempts from unknown peers with TLS authentication. Accept All - Select this option to accept all connection attempts from unknown peers. To Add a Host In the Diameter Routing Profile , click on the Add button beneath the Peer Table . The Add Host dialog o pens . Open The Diameter Routing Profile - Adding a Host Enter the host name and port for the host in the Hostname and Port fields. Select protocol in the Protocol drop-down-list. If you want to enable throttling for the peer, select the Enable Throttling check box, and then enter the maximum number of request UDRs per second you want the Diameter Stack agent to transmit to the peer in the Throughput Threshold (UDR/s) field. Click on the Add button and the host will be added in the Peer Table , and then click on the Close button to close the dialog when you are finished adding hosts. Realm Routing Table Realm-based routing is performed when the Destination-Host AVP is not set in a Diameter message. All realm-based routing is performed based on lookups in the Realm Routing Table. When the lookup matches more than one set of keys, the first result from the lookup will be used for routing. For this reason, the order of the rows in the Realm Routing Table must be considered. You can control the order of the rows by using the arrow buttons. Clicking on the table columns to change the displayed sort order does not have any effect on the actual order of the rows in the Realm Routing Table . Item Description Item Description Realm Routing Strategy Diameter requests are routed to peers in the realms in accordance with the selected Realm Routing Strategy. The following settings are available: Failover : For each realm, Diameter requests are routed to the first specified peer (primary) in the Hostnames cell, or the first host resolved by a DNS query. If the connection to the first peer fails, requests to this realm are routed to the next peer (secondary) in the cell, or the next host resolved by a DNS query. Failback to the first peer (primary) is performed when possible. Round Robin : Diameter requests are evenly distributed to all the specified peers in the Hostnames cell, or peers resolved by DNS queries. If the connection to a peer fails, the requests are distributed to the remaining hosts. This also applies when UDRs are throttled due to the settings in the Peer Table . The table below contains examples of how Diameter requests are routed to the peers of a realm, with the Round Robin strategy, depending on the peer connection state: Peer 1 Status Peer 2 Status Peer 3 Status Route Distribution OKAY OKAY OKAY Peer 1, Peer 2, or Peer 3 OKAY OKAY SUSPECT Peer 1 or Peer 2 REOPEN SUSPECT OKAY Peer 3 DOWN DOWN SUSPECT Peer 3 DOWN REOPEN DOWN Peer 2 DOWN DOWN DOWN None Diameter request are not routed to peers that are specified in the ExcludePeers field of a RequestCycle UDR. For more information about the RequestCycle UDR, see the section, RequestCycleUDR in The Diameter Base Protocol . Enable Dynamic Peer Discovery Select this check box when you want to use DNS queries (dynamic peer discovery) to find peer hosts in realms. The queried peer host information is buffered by the Diameter Stack agent according to the TTL (time to live) parameter in the DNS records. When the TTL has expired, the agent will attempt to refresh the information. If the refresh fails, the buffered information will be deleted. When Enable Dynamic Peer Discovery is selected, DNS queries are performed at: Workflow start After TTL Expiration Dynamic update of Diameter routing profile Note! To make changes to this setting effective, you must restart the workflow(s). If the DNS service is unavailable (server available but service down) when starting the workflow(s), the system log entry will indicate errors in realm lookups. In order to resume lookups in DNS, you need to dynamically update the routing table in the Diameter Stack agent when the DNS is available again. For information about how to dynamically update the routing table, see the section below, To Dynamically Update the Diameter routing profile. For information about how to select DNS servers, see the section below, DNS tab. Realm The realm name (case sensitive). Realm is used as the primary key in the routing table lookup. If left empty, all the destination realms are valid for this route. For example, address.com Applications The applications that this route serves. This entry is used as a secondary key field in the routing table lookup. If left empty all the applications are valid for this route. For example, 3,4 . Hostnames A list of all the peer hosts in the realm. The hostnames must be selected from the Peer Table . When Node Discovery is set to Dynamic, you should leave this field empty. Node Discovery The method of finding the peer hosts in the realm: Static - The peer hosts are specified in the Hostnames field of the Realm Routing Table. Dynamic - The Diameter Stack agent uses DNS queries (dynamic peer discovery) to find the peer hosts. These queries may resolve to multiple IP addresses or hostnames. In order to use this setting, you may need to add DNS servers to the network interfaces of your system. Note! Entries in the Realm Routing Table that have the Dynamic setting are ignored (not matched), unless Enable Dynamic Peer Discovery is selected. When a DNS server resolves a realm to peer hosts, it may return fully-qualified DNS domain names with a dot at the end. These trailing dots are removed by the Diameter Stack agent. To Add a Realm In the Diameter Routing Profile , click on the Add button beneath the Realm Routing Table. The Add Route dialog opens. Open The Diameter Routing Profile - Adding a Realm Enter the realm name in the Realm text box. If the realm serves specific applications, click on the Add button beneath the Applications list box and specify the Application Id. Repeat this step for each application. You should only perform this step if Peer Discovery is set to Static and the peer hosts are to be specified in the Realm Routing Table. Click on the Add button beneath the Hostname list box and select a host from the drop-down list. Repeat this step for each host in the realm. If you specified the peer hosts of the realm in the previous step, select Static from Peer Discovery . If you want to use dynamic peer discovery instead, select Dynamic from this drop-down list. Click on the Add button and the realm will be added to the Realm Routing Table , and then click on the Close button to close the dialog when you are finished a ddin g realms. DNS Tab Open The Diameter Routing profile - DNS tab You can use the DNS tab to configure the DNS settings used for looking up peer hosts of realms. For information about how to configure your DNS for dynamic peer discovery, see the Diameter Base Protocol (RFC 6733). Note! To make changes to this tab effective, you must restart the workflow(s). Avoid configuring the same peer host in both DNS and the Peer Table , this may cause duplicate instances of Diameter peers. The host- and realm names in the Diameter Stack agent are case-sensitive. Retry Interval Time (ms) Enter the time (in milliseconds) that the Diameter Stack agent must wait before retrying a failed DNS connection. Max Number Of Retries Enter the maximum number of times that the Diameter Stack agent should retry to connect to the servers in the DNS Servers list before it gives up. When the agent has attempted to connect to all servers (after an initial failed attempt), it counts as a retry. DNS Servers Enter the hostname or IP address of the DNS servers that can be queried. The topmost available server will be used. If the DNS Servers list is empty, the Diameter Stack agent will use the file /etc/resolv.conf on the Execution Context host to select the DNS server. For information about how to configure resolv.conf, see your operating system documentation. To Dynamically Update the Diameter Routing Profile You can refresh the routing table of a Diameter Stack agent while a workflow is running. When the agent refreshes the routing table, it reads the updated Peer Table , Realm Routing Table and Realm Routing Strategy from the selected Diameter Routing Profile. The setting Enable Dynamic Peer Discovery in the Routing tab and the settings in the DNS tab is not read from the Diameter Routing table at refresh. To make changes to these settings effective, you must restart the workflow(s). The routing table can be refreshed from the Workflow Monitor or from the Command Line Tool. In the Workflow Monitor, double-click the Diameter Stack agent to open the Workflow Status Agent configuration. In the Command tab, select the Update Routing Table button to refresh the routing table. Command Line Tool Run the following command: mzsh mzadmin/<password> wfcommand <workflow name> <Diameter Stack agent name> Example - Update Routing Table mzsh mzadmin/<password> wfcommand Default.my_workflow Stack1 When Round Robin is the selected Realm Routing Strategy , you can reset the selection cycle by running the following command: mzsh mzadmin/<password> wfcommand <workflow name> <Diameter Stack agent name> clearstrategystate Example - Reset Round Robin Selection mzsh mzadmin/<password> wfcommand Default.my_workflow Stack1 clearstrategystate To Read the Realm Routing Table in APL You can use the Diameter Stack MIM value Realm Routing Table to read the realm routing table of a Diameter Stack Agent from APL. The MIM value is of the map<string,map<string, list<string>>> type and is defined as a global MIM context type. The string values in the outer map contain the realm names (primary key). The string values of the inner map contain the applications (secondary key). The lists in the map contain the hostnames of the peers in the realm. The string values in the outer map contain the realm names (primary key). The string values of the inner map contain the applications (secondary key). The lists in the map contain the hostnames of the peers in the realm. Open Asterisks (*) are used in the strings to denote unspecified realm name or unspecified applications. The values in the inner and outer maps are sorted exactly as the Realm Routing Table of the selected Diameter routing profile. Example Realm Routing Table MIM Assume that the following realm routing table is defined for a Diameter Stack agent: Realm Applications Peers dr peer1, peer2 100,200 peer3, peer4 peer5, peer6 The following APL code can be used to read the table: initialize { //Note the space between the angle brackets! map<string, map<string, list<string> > > realmTable = (map<string, map<string, list<string> > >) mimGet("Stack1", "Realm Routing Table"); //Check the size of the table if (mapSize(realmTable) != 2) abort("Realm table incorrect size"); //Check that realms are included if (mapKeys(realmTable) != listCreate(string, "dr", "*")) abort("Wrong realms"); //Get the inner map for realm name "dr" map<string, list<string> > drMap = mapGet(realmTable, "dr"); //Get the inner map for realm name "*" (unspecified realm) map<string, list<string> > starMap = mapGet(realmTable, "*"); //Any Application Id debug(mapGet(drMap, "*")); //Application Id 100 debug(mapGet(starMap, "100")); //Any Application Id debug(mapGet(starMap, "*")); } Example debug output: 12:11:40: [peer1, peer2] 12:11:40: [peer3, peer4] 12:11:40: [peer5, peer6] For more information about MIM values published by the Diameter Stack agent, see Diameter Stack Agent Input/Output Data and MIM .

---

# Document 2112: Dimensioning of the Database - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029896/Dimensioning+of+the+Database
**Categories:** chunks_index.json

The parameters included in this section are depending on the amount of BucketDataHolders that is going to be stored in the database, and they may be dimensioned as follows: Parameter Description Parameter Description DataMemory DataMemory is the amount of memory used for storing the data. This amount of physical memory will be required for each of the data nodes in, addition to memory required for OS etc, and is calculated as follows: (40b + average usage per BucketDataHolder) x NumberOf BucketDataHolder = DataMemory The maximum size of a BucketDataHolder is 14 kb. However, if you use 5 buckets per BucketDataHolder, and 1-2 sessions, you may expect a size of about 1-2 kb. Example - DataMemory If you plan to have 50 000 BucketDataHolders with an average usage of 2 kb, the required DataMemory will be: (40b + 2048) x 50 000 = 104 400 000 bytes IndexMemory IndexMemory is the amount of memory used for the index of each table/entry. This data will also be stored on the data nodes, which means that this amount of memory needs to be available on each data node. Usually, this parameter is configured to be 1/5 or 1/6 of the DataMemory. However, the required amount of memory may also be calculated as follows: 20b x TotalNumberOfRecords (one record for each BucketDataHolder) = IndexMemory Example - IndexMemory If the DataMemory is configured to be 104 400 000 bytes, as in the example for DataMemory, configuring the IndexMemory to 1/5 would be 20 880 000 bytes, or to 1/6 would be 17 400 000 bytes. If the IndexMemory is calculated based on the number of records, and you plan to have 50 000 BucketDataHolders, the required IndexMemory will be: 20b x 50 000 = 1 000 000 bytes FragmentLogFileSize and NoOfFragmentLogFiles These two parameters in combination make out the total amount of redo space. The redo space is the amount of space required for keeping a copy of all changes made to the database on disk in order to make it possible to recover the database after a crash, for example. The redo space should be about six times the available data memory and is calculated as follows: 4 x FragmentLogFileSize x NoOfFragmentLogFiles = 6 x DataMemory The reason for multiplying with four is that the number of configured files times four are created for the backup. NoOfFragmentLogFiles has a minimum value of 3, and it is generally recommended to set FragmentLogFileSize to 256. Example - NoOfFragmentLogFiles If you have 10 000 M in DataMemory, you will need 60 000 M in redo: 4 x 256 M x NoOfFragmentLogFiles = 60 000 M which in turn means that the NoOfFragmentLogFiles should be at least 59 DiskCheckPointSpeed The DiskCheckPointSpeed is used for specifying the number of Mb per second that should be written to disk during local checkpoint (a complete backup to disk of the database which is done on a regular basis). When configuring this parameter, the amount of data in the database as well as the speed of the hard drive should be considered. MaxNoOfConcurrentTransactions With this parameter you can specify the maximum number of concurrent transactions that may be performed. Each thread will use at least one transaction during load, but typically this parameter can be set higher. The recommended setting is 1024. However, this may need to be adjusted during functional and performance testing. MaxNoOfConcurrentOoperations With this parameter you can specify the maximum number of concurrent operations. One operation constitutes one update of one line, which means that deleting 10 000 entries will result in 10 000 concurrent operations. This parameter may be set to 32 768 and then be adjusted during functional and performance testing. TimeBetweenGlobalCheckPoints This parameter determines how often the redo log should be flushed to disk. The default value is 2 000 ms and the recommended value is 1 000 ms. TimeBetweenLocalCheckPoints With this parameter you can specify how often you want to make a local checkpoint. By default this is set to 20, which is usually a reasonable configuration. The calculation is based on the amount of changes, which means that if this parameter is configured to 20, the effect will be as follows: 4 x 220 = 4 Mb A rewrite will be made when at least 4 Mb of changes have been written to the database. If less has been written, then a local checkpoint will be made after 57 minutes. Only one rewrite will be active at the time, which means that even if there is more traffic this configuration will still work.

---

# Document 2113: MediationZone User Interfaces - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816602/MediationZone+User+Interfaces
**Categories:** chunks_index.json

MediationZone offers the following types of user interfaces aimed for different purposes: Desktop is a graphical user interface for development, management, and operation of configuration. This is the legacy UI, and will be replaced with the MediationZone WebUI in the future. WebUI is a graphical user interface for development, management, and operation of configuration. This is the new version standardized for web browsers, replacing the legacy MediationZone Desktop . Shell is a command line based interface used for workflow deployment scenarios such as platform launch/update control, start/stop and import/export of workflows and dynamic provisioning. UI Builder Agent enables users to create external customized responsive web UIs.

---

# Document 2114: KafkaOffset - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138521
**Categories:** chunks_index.json

The kafkaOffset UDR is used to control from which position (offset) you want to start collecting data. It only applies if you have selected the Start At Request option in the Kafka Real-Time Collection Agent Configuration . The Kafka Collection agent waits for the kafkaOffset UDR before starting to consume data. The offset information can be persisted and tracked by, for example, using the Aggregation agent, or forwarding to an external database. Field Description Field Description offsets (map<string,map<int,long>>) This field is populated with offset information. The string identifies the topic. As offsets in Kafka are unique per partition, this maps partition identifiers (int) to offsets (long).

---

# Document 2115: FTP EWSD Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033157/FTP+EWSD+Agent
**Categories:** chunks_index.json

This section describes the FTP/EWSD agent. This is a collection agent for batch workflow configurations. Prerequisites The reader of this information should be familiar with: The Siemens SAMAR OneFileView file handling Standard FTP (RFC 959, http://www.ietf.org/rfc/rfc0959.txt The FTP/EWSD agent enables collection of cyclic files from Siemens EWSD switches into the workflow, by using the FTP protocol. When the workflow is activated, the FTP/EWSD agent connects to the configured FTP service and requests information about the cyclic file by using the LIST FTP command. The switch returns information about the cyclic file. The agent compares the returned information field values COPY-DATA-BEGIN and LAST-RELEASE with its internal state. If LAST-RELEASE has the same value that the agent holds, the agent deletes the cyclic file with the FTP delete command DELE , and thereby releases the currently established copy space. The agent then retrieves the file contents with RETR and inserts it into the workflow. The RETR command establishes a new copy space. When all the data is successfully collected into the workflow, the agent generates another LIST command in order to retrieve the value of COPY-DATA-BEGIN and of LAST-RELEASE . The agent saves these values and then generates the DELE command to release the space of the collected data. Multiple File View Option In the Multiple File View mode, the cyclic file is sliced into sections (slices). By checking Automatic Seq No Assignment you enable the agent to automatically set these sequence numbers. The agent connects to the configured FTP service and searches from the ACTIVE file slice, backwards, to the oldest file slice in the FILLED state. The agent then collects all the file slices, from the oldest to the most recent file slice. Note! When collection is complete, release all the file slices and continue to collect the currently ACTIVE file slice. The section contains the following subsections: FTP EWSD Transaction Behavior FTP EWSD Configuration FTP EWSD Input/Output Data and MIM FTP EWSD Events

---

# Document 2116: Workflow Bridge Example with Static or Dynamic Load Balancing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675628/Workflow+Bridge+Example+with+Static+or+Dynamic+Load+Balancing
**Categories:** chunks_index.json

This section will show an example of a scenario where a real-time forwarding workflow will split the execution of UDRs between three real-time collection workflows depending on the incoming data. Each collection workflow will add information and send back the consumeCycleUDR to the real-time forwarding workflow for further execution. The following configurations will be created: An Ultra Format A Workflow Bridge profile A Workflow Bridge real-time forwarding workflow A Workflow Bridge real-time collection workflow Open Example of a real-time to real-time scenario Define an Ultra Format A simple Ultra Format needs to be created in order to forward the incoming data and enable the collection workflows to populate it with more information. For more information about the Ultra Format Editor and the UFDL syntax, re fe r to the Ultra Reference Guide . Create an Ultra Format as defined below: internal myInternal { string inputValue; string executingWF; }; Define a Profile The profile is used to connect the forwarding workflow towards the three collection workflows. See Workflow Bridge Profile for information on how to open the Workflow Bridge profile dialog. Open Example of a profile configuration In this dialog, the following settings should be made: The Send Reply Over Bridge is selected which means that all {{ConsumeCycleUDR}}s will be returned to the Workflow Bridge forwarding agent. Force serialization is not used since there will be no configuration changes during workflow execution. The Workflow Bridge real-time collection agent must always respond to the WorkflowState UDRs. The Response Timeout (s) has been set to "60" and this means that the Workflow Bridge real-time forwarding agent that is waiting for a WorkflowState UDR reply will timeout and abort (stop) after 60 seconds if no reply has been received from the real-time collection workflow. Enter the appropriate timeout value to set the timeout for the Workflow Bridge real-time forwarding agent. The Bulk Size has been set to "0". This means that the UDRs will be sent from the Workflow Bridge real-time forwarding agent one by one, and not in a bulk. Enter the appropriate bulk size if you wish to use bulk forwarding of UDRs. The Bulk Timeout (ms) has been set to "0" since there will be no bulk forwarding. Enter the appropriate bulk timeout if you wish to use bulk forwarding of UDRs. Bulk timeout can only be specified if the bulk functionality has been enabled in the Bulk size setting. Since the UDRs in this example will be split between three different workflows, the Number of Collectors has been set to "3". Transport is set to TCP. We can also test Aeron later to see if it gives a better perormance. Load Balancing Strategy is set to Static.Since the number of collectors are more than 1 in this case it must be set. With Static we control the Load balance to which workflow by defining LoadIDs in the workflow instances. The alterntive is Dynamic loadbalancing, that will automatically distribute the load. Select the Load Balance Strategy for prefered scalability and redundance. Create a Real-Time Forwarding Workflow In this workflow, a TCP/IP agent collects data that is forwarded to an Analysis agent. The Analysis agent will define the receiving real-time collection workflow before the ConsumeCycleUDR is sent to the Workflow Bridge forwarding agent. The Workflow Bridge forwarding agent will distribute the UDRs to the correct collection workflow and forward the returning ConsumeCycleUDR to another Analysis agent for further execution. Open Example of a real-time forwarding workflow The workflow consists of a TCP/IP agent, an Analysis agent named Analysis , a Workflow Bridge real-time forwarding agent named Workflow_Bridge_FW and a second Analysis agent named Result . TCP/IP TCP/IP is a collection agent that collects data using the standard TCP/IP protocol and forwards it to the Analysis agent. Double-click on the TCP_IP agent to display the configuration dialog for the agent: Open Example of a TCP/IP Agent Configuration In this dialog, the following settings have been made: Host has been set to "10.46.20.136". This is the IP address or hostname to which the TCP/IP agent will bind. Port has been set to "3210". This is the port number from which the data is received. Allow Multiple Connections has been selected and Number of Connections Allowed has been set to "2". This is the number of TCP/IP connections that are allowed simultaneously. Analysis The Analysis agent is an Analysis agent that receives the input data from the TCP/IP agent. It defines which real-time collection workflow should be chosen and forwards the ConsumeCycleUDR to the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { wfb.ConsumeCycleUDR ccUDR = udrCreate(wfb.ConsumeCycleUDR); WFBridge.myFormat.myInternal data = udrCreate(WFBridge.myFormat.myInternal); data.inputValue = baToStr(input); debug("First character is: " + strSubstring(data.inputValue,0,1)); if (strStartsWith(data.inputValue,"1") || strStartsWith(data.inputValue,"2")) { int wfId; strToInt(wfId,strSubstring(data.inputValue,0,1)); ccUDR.LoadId = wfId; } else { ccUDR.LoadId = 3; } ccUDR.Data = data; udrRoute(ccUDR); } In this dialog, the APL code for handling input data is written. In the example, the incoming data is analyzed and depending on the first character in the incoming data, the receiving real-time collection workflow is chosen by setting the LoadId in the ConsumeCycleUDR , which is sent to the Workflow_Bridge_FWD agent. Adapt the code according to your requirements. Workflow_Bridge_FWD Workflow_Bridge_FWD is the Workflow Bridge real-time forwarding agent that sends data to the Workflow Bridge real-time collection agent. Double-click on Workflow_Bridge_FWD to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. Result The Result agent is an Analysis agent that receives the returning ConsumeCycleUDR s and potential ErrorCycleUDR s from the Workflow_Bridge_FWD agent. Double-click on the Analysis agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, wfb.ErrorCycleUDR)) { debug("Something went wrong"); } else if (instanceOf(input, wfb.ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; string msg = ("Value " + data.inputValue + " was executed by " + data.executingWF); debug(msg); } } } In this dialog, the APL code for further handling of the UDRs is written. In the example, only simple debug messages are used as output. Adapt the code according to your requirements. Create the Real-Time Collection Workflows In this workflow, a Workflow Bridge real-time collection agent collects the data that has been sent in a ConsumeCycleUDR from the Workflow Bridge real-time for wardin g agent and returns an updated ConsumeCycleUDR . Open Example of a real-time collection workflow Workflow_Bridge_Coll Workflow_Bridge_Coll is the Workflow Bridge real-time collection agent that receives the data that the Workflow Bridge real-time forwarding agent has sent over the bridge. Double-click on the Workflow_Bridge_Coll agent to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. The default port that the collector server will listen on for incoming requests has been set to default value "3299". Analysis The Analysis agent is the Analysis agent that receives and analyses the data originally sent from the Workflow Bridge real-time forwarding agent in the ConsumeCycleUDR , as well as the workflow state information delivered in the WorkflowStateUDR . Double-click on the agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, WorkflowStateUDR )) { udrRoute((WorkflowStateUDR)input, "response"); } else if (instanceOf(input, ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR)input; WFBridge.myFormat.myInternal data = (WFBridge.myFormat.myInternal)ccUDR.Data; debug("Incoming data: " + data.inputValue); data.executingWF = (string)mimGet("Workflow","Workflow Name"); ccUDR.Data = data; udrRoute(ccUDR, "response"); } else { debug(input); } } In this example, each ConsumeCycleUDR will populate the data field executingWF with the name of the executing workflow. Also WorkflowStateUDR s are routed back. Adapt the code according to your requirements. Instance Table Since this example will load balance between three workflows, additional workflows is added in the workflow table. Right-click in the workflow template and choose Workflow Properties to display the Workflow Properties dialog. Open Example of Workflow Properties In this dialog, the following settings have been made: Workflow - Execution - Execution Settings and Workflow_Bridge_Coll - WFB_Collector - Port have default checked, which means they will use the configured value in the template unless a new value is given in the Workflow Table. Workflow_Bridge_Coll - WEB_Collector - loadID has Per Workflow set, which means that the value must be specified in the Workflow Table. The Workflow Table will contain three workflows, that all will communicate with the real-time forwarding workflow. Populate the Workflow Table with correct settings for each workflow: Name should be set to a unique name for each workflow. Set which EC each workflow will execute on in Execution Settings . Each workflow needs a unique port for communication with the Workflow_Bridge_FWD agent. The loadID need to correspond with the APL code and should be "1", "2" and "3" in this example

---

# Document 2117: Python Processing Agent Input/Output Data and MIM - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642459/Python+Processing+Agent+Input+Output+Data+and+MIM+-+Batch
**Categories:** chunks_index.json

Input/Output Data

---

# Document 2118: Merge Files Collection Meta Information Model and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641921/Merge+Files+Collection+Meta+Information+Model+and+Events
**Categories:** chunks_index.json



---
**End of Part 89** - Continue to next part for more content.
