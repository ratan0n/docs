# RATANON/MZ93-DOCUMENTATION - Part 64/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 64 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.8 KB
---

**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652812/GCP+PubSub+Publisher+Agent
**Categories:** chunks_index.json

A GCP PubSub Publisher agent interacts with the GCP PubSub service to create messages containing data and send a request to the service to publish the messages to the subscribed topic. In a real-time manner, the Publisher allows the submission using the high-performance streaming API. Each message are submitted through the API which will then deliver them according to the batch configurations. When using a client library such as Java, the transmission details can be configured to wrap messages in batch for delivery based on the numbers, byte size or time, and options to use blocking or non-blocking calls for publishing. If non-blocking calls (e.g. asynchronous) are used, you can specify the flow control and retry settings. While callbacks are also available for the client library to report if messages have been accepted by the PubSub service. GCP PubSub Publisher agent is available as a forwarding agent in Real-Time workflows. Open This section contains the following subsections: GCP PubSub Publisher Agent Configuration GCP PubSub Publisher Agent Events GCP PubSub Publisher Agent Transaction Behaviour GCP PubSub Publisher Agent Input/Output Data and MIM

---

# Document 1502: Conditional Trace User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676782/Conditional+Trace+User+s+Guide
**Categories:** chunks_index.json

Search this document: Conditional Trace is a trouble-shooting function that allows you to trace data in real-time workflows. Conditional Trace templates, defining what you want to trace, are created in the Legacy Desktop, and these templates can then be used in the regular Desktop. The Conditional Trace templates can be configured to match certain workflows, certain UDR types, and fields, as well as using matches for specific parameters, using either specific values or regular expressions. The following sections are included: Conditional Trace Templates Tracing in Desktop Conditional Trace Access Permissions

---

# Document 1503: importrollback - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743887/importrollback
**Categories:** chunks_index.json

usage: importrollback <rollback file> This command reverses the effects of the systemimport command by removing the imported configuration and reverting to an older configuration. Note! Use the importrollback command only to revert the systemimport command and not for the purpose of a general system rollback. To be able to properly rollback the entire system, make sure that you occasionally create a backup file with systemexport . This way, you can revert to an earlier system configuration by simply using systemimport . Note! The ECS Reprocessing Groups and ECS error codes are not removed by the importrollback command. The rollback file parameter provides importrollback with information that enables the system to reconstruct the status of your system prior to applying the systemimport command. For further information about systemimport see systemimport . Return Codes Listed below are the different return codes for the importrollback command: Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect. 2 Will be returned if the rollback file does not exist. 3 Will be returned if the import rollback could not be started due to locked import. 1-> If import rollback is started, a return code greater than zero is returned if the rollback of any of the configurations fail. The exit code is then the number of failed rollbacks, i e return code is 1 if one rollback fails, 2 if two rollback fails, etc.

---

# Document 1504: Pulse Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001359/Pulse+Agent
**Categories:** chunks_index.json

This section describes the Pulse agent. This is a collection agent for real-time workflow configurations. The Pulse agent is useful for testing purposes since it functions both as a collection agent and an internal data source. You can use this agent to generate random or specific data while simulating various traffic patterns through stochastic distributions. Open Workflow with Pulse agent The section contains the following subsections: Pulse Agent Configuration Pulse Agent Events Pulse Agent Input/Output Data and MIM PulseUDR

---

# Document 1505: Appendix 1 - Profiles - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639259/Appendix+1+-+Profiles
**Categories:** chunks_index.json

This appendix contains descriptions of the profiles that are not related to specific agents. Agent-specific profiles are described in the documentation of the respective agents. Parameterizable Profile Fields To make profiles reusable in an efficient way, some of them support a concept called Parameterizable Fields. Such fields makes it possible to have one configuration only, reusable both in development, test, and production systems. It also makes it easy to add support in CI/CD pipelines. For instructions on how to define basic parameters, refer to External Reference Profile . A parameterizable profile field can be used as a template and its value can be set per workflow. The parameter s ynt ax is ${category.name}. By using this syntax, a Dynamic Fields Tab of type string is created. The value of the field is set as a workflow parameter and the value will be replaced when the workflow is executed. Profile fields supporting parameterization are marked with a $(dollar sign) icon. Currently, the following profiles support use of parameterization: 5G Profile Couchbase Profile File System Profile Kafka Profile This chapter contains the following sections: 5G Profile Audit Profile Azure KeyVault Profile Couchbase Profile Database Profile Distributed Storage Profile Elasticsearch Profile External Reference Profile File System Profile Google Secret Manager Profile IPDR SP Template Profile Open API Profile Redis Profile Secrets Profile Security Profile Shared Table Profile

---

# Document 1506: Batch-Based Real-Time Agents - Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644273/Batch-Based+Real-Time+Agents+-+Agent+Events
**Categories:** chunks_index.json

For information on Agent Message Events and Debug Events, see the Agent Events section for the relevant agent: Disk Collection Agent Events - Batch FTP Collection Agent Events SCP Collection Agent Events SFTP Collection Agent Events

---

# Document 1507: Batches Data Model - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743132/Batches+Data+Model
**Categories:** chunks_index.json

The UDRs in PCC.Batches contain information about the work orders, work logs and updates made in the Usage Management Workflow Templates package. Workorder UDR Field Description Field Description Key (string) The unique key for the work order. Updates (string) Contains a list of updates to be made in the work order. Below is a screenshot of the UDR Assistance displaying the Workorder UDR: Open Workorder UDR Update UDR Field Description Field Description Product (int) The product for which usage should be updated. Timestamp (date) The date and time of the usage update. Usage (map<byte,long>) The latest reported usage. Misc (map<string,any>) This field can contain optional other types of information. Below is a screenshot of the UDR Assistance displaying the Update UDR: Open Update UDR Workflow UDR Field Description Field Description Key (string) The unique key for the work log. Misc (map<string,any>) This field can contain optional other types of information. Below is a screenshot of the UDR Assistance displaying the Worklog UDR: Open Worklog UDR

---

# Document 1508: SAP RFC Processor Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642985/SAP+RFC+Processor+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. For information about the agent message event type, see Agent Event . Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 1509: Disk Collection Agent Events  - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685140
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor. For further information about the agent message event type please refer to Agent Event . Ready with file: filename - Reported along with the name of the source file that has been collected and inserted into the workflow. File cancelled: filename - Reported along with the name of the current file, each time a Cancel Batch message is received. This assumes the workflow is not aborted; refer to Disk Collection Agent Input/Output Data and MIM - Batch for further information. Debug Events There are no debug events for this agent.

---

# Document 1510: Workflow Bridge Real-time Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610133/Workflow+Bridge+Real-time+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events

---

# Document 1511: Redis Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998705/Redis+Profile
**Categories:** chunks_index.json

A Redis profile is used to read and write data in a Redis database and can be accessed by real-time workflows using Aggregation agents. The Redis profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. The Redis profile has been verified on Redis Labs Enterprise Cluster (RLEC). Note! To ensure aggregation functions correctly with the Redis profile, you must have the Enterprise edition of Redis. Configuration To create a new Redis profile, click the New Configuration button from Build View , and then select Redis Profile from the selection screen. Th e contents of the buttons bar may change depending on which configuration type has been opened in the currently displayed tab. The Redis profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit button content is specific to the Redis profile configurations. Item Description Item Description External References Open Select this menu item to enable the use of External References in the Redis profile configuration. This can be used to configure the use of the following fields, and the respective external reference keys available: Host Port Max Retries Retry Interval Operation Timeout Password For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . The Redis configuration contains two tabs: Connectivity and Advanced . Connectivity Tab The Connectivity tab is displayed by default. Open Redis profile - Connectivity tab The following settings are available in the Redis profile: Setting Description Setting Description Host Enter the IP address/DNS name of the cluster. Port Enter the listening port of the cluster. Password Enter the password of the cluster. Operation Timeout (ms) Enter the number of milliseconds after which Redis "CRUD" operations, i e create, read, update, and delete, should timeout. Setting a lower value than the default 1000 ms may have a positive impact on throughput performance. However, if the value is set too low, indicated by a large number of operation timeouts errors in the EC logs, a lower throughput can be expected. Retry Interval Time (ms) Enter the time interval, in milliseconds, that you want to wait before trying to read the cluster configuration again after a failed attempt. The default value is 5. Max Number of Retries Enter the maximum number of retries. The default value is 30. Advanced Tab The Advanced tab contains additional properties that you can use for performance tuning. Open Redis profile - Advanced tab The property mz.redis.socket.timeout specifies how long the Redis client will try to open a socket connection before it times out, while the property mz.redis.connection.timeout determines the maximum amount of time that the Redis client waits to read data from the Redis server before it times out. Idle connections will timeout after the time specified in mz.redis.connection.time.limit has expired. These connections will be returned to the pool and are considered unused. The profile will periodically remove (evict) unused connections to the Redis cluster. Use mz.redis.connection.time.limit to control the time between the eviction intervals. Connections in use are periodically and unconditionally reset. Use mz.redis.connection.time.limit to control the time between the reset intervals. In case of topology changes the connections are reset after a delay. Use mz.redis.topology.limit.time to set the length of this delay.

---

# Document 1512: Creating Client Keystore and Certificate - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002550
**Categories:** chunks_index.json

After generating the key pair for server, the next step is to generate a key pair for the client. Run the following command: $ keytool -genkey -alias client -keyalg RSA -keystore ./Client.jks -storetype PKCS12 alias = name of the key, for example, client keystore = name of the keystore, for example, Client.jks Note! When prompted for first and last name the hostname where the certificate is valid should be entered, e.g. localhost. Other values can be anything. Generate a Certificate Signing Request (CSR) so that we can get client's certificate signed by a CA. keytool -certreq -alias client -keystore Client.jks -file Client.csr Get the certificate signed by our the CA, Test CA in these example. See Setting Up a Certificate Authority for instructions on how to set up a CA. $ openssl x509 -CA caroot.cer -CAkey cakey.pem -CAserial serial.txt -req -in Client.csr -out Client.cer -days 365 Note! CA , CAkey and CAserial are files generated when setting up the CA. Import the Test CA root self signed certificate in client key store as a trusted certificate. $ keytool -import -alias TestCA -file caroot.cer -keystore Client.jks Import client's certificate signed by Test CA in client key store with the same alias name that was used to generate the key pair during genkey. $ keytool -import -alias client -file Client.cer -keystore Client.jks We also need to import server's public key in the client key store, because client is the first one who need to initiate a conversation with server or the service. And it needs to encrypt the request message (some part of it) using sever's public key. Server does not need client's public in its keystore if the Binary Security Token is used, server is going to get the client public key in the SOAP message itself. $ keytool -import -alias server -file Server.cer -keystore Client.jks

---

# Document 1513: Open API Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639682
**Categories:** chunks_index.json

If you want to use Open API 3.0 with HTTP/2 agents, you require an Open API profile configuration. You select the profile that you configure in the HTTP/2 Server agent configuration. In the Open API profile configuration, you import your OpenAPI specification file and view any other included files defined by the spe cificatio n. All schemas that require a UDR must be named. Due to a limitation in the third-party parsing library used by OpenAPI, unnamed schemas cannot be detected and will not generate a corresponding UDR. Therefore, you must name all schemas that require a UDR. Field names in the yaml specification file containing the following symbols will be replaced with unique string of characters during the UDR generation process as shown below: @ -> _40_ . -> _2E_ - -> _2D_ For example, the field "test-name" will be converted into "test_2D_name" as a UDR. All tab indentations found in the Open API yaml files will be replaced with two spaces. Configuration The Open API profile consists of the following tabs: 1 General Tab 2 Advanced Tab General Tab Open Open API Profile Configuration - General tab Setting Description Setting Description Import Imports the Open API specification file from the local file directory of the desktop or desktop client. Click on this button again after a file has been imported, to import another Open API specification file to overwrite the previously imported specification file. View Opens the selected OPEN API schema file. Referenced Files Lists the yaml files that are already imported into the Open API profile. Clicking on any of the records in the list will allow you to view the yaml file or remove it from the profile. Clicking on the Import button on the Reference Files will allow you to import the reference files for the main Open API specification file as well as any other reference files that are required. Importing Open API Specification Files Users can choose to import or upload the yaml specification files via the following: Browse and select from local directory Drag and drop the file(s) to the Import Open API file dialog box Open Import Open API file dialog box For any files that contain missing dependencies, you can choose to import or upload the referenced files via the following: Browse and select from local directory Drag and drop the file(s) to the Import referenced Open API file(s) dialog box Open Import referenced Open Api file(s) dialog box To view each Referenced file , select the .yaml file from the table and click View Schema . To remove the .yaml file from the table, click Remove . Advanced Tab Open Open API Profile Configuration - Advanced tab Setting Description Setting Description Ignore Read Only Tag Select this option to ignore the readOnly tag in the schema file. Info! When UDRs are generated from the OpenAPI specification file, some UDR fields found in the response body are marked as read-only. This prevents HTPP/2 Server from setting these fields in the APL to generate a proper response. By selecting this option, it allows HTTP/2 Server agents to be able to set the readOnly fields in the APL for use cases that require a response from the HTTP/2 Server agent. Add additionalProperties: false to component schemas Select this option to add the additionalProperties: false to each component in the schema file. Limitations This section lists the limitations that users may encounter when using the OpenAPI profile. OpenAPI specification schema which contains oneOf tag will be decoded as a map instead of a UDR In the following example, the SubscriptionData schema contains the subscrCond property with oneOf tag: Example: SubscriptionData schema contains the subscrCond with oneOf tag SubscriptionData: description: Information of a subscription to notifications to NRF events, included in subscription requests and responses type: object required: - nfStatusNotificationUri - subscriptionId properties: nfStatusNotificationUri: type: string reqNfInstanceId: $ref: 'TS29571_CommonData.yaml#/components/schemas/NfInstanceId' subscrCond: oneOf: - $ref: '#/components/schemas/NfInstanceIdCond' - $ref: '#/components/schemas/NfInstanceIdListCond' - $ref: '#/components/schemas/NfTypeCond' - $ref: '#/components/schemas/ServiceNameCond' - $ref: '#/components/schemas/AmfCond' - $ref: '#/components/schemas/GuamiListCond' - $ref: '#/components/schemas/NetworkSliceCond' - $ref: '#/components/schemas/NfGroupCond' - $ref: '#/components/schemas/NfSetCond' - $ref: '#/components/schemas/NfServiceSetCond' - $ref: '#/components/schemas/UpfCond' - $ref: '#/components/schemas/ScpDomainCond' - $ref: '#/components/schemas/NwdafCond' - $ref: '#/components/schemas/NefCond' The subscrCond is a schema of NfSetCond but it is decoded as a map with key value pair as shown below: [openapi.issue_http.OAPI_NrfMgt.udr.SubscriptionData] nfStatusNotificationUri: http://localhost/dummy subscriptionId: 123456 subscrCond: {nfSetId=MU01} Example: SubscriptionData schema decoded in the APL: To retrieve the value of the map, enter the following code in APL: string ID = mapGet((map<string, any>)subscriptionData.subscrCond, "nfSetId"); debug(ID); The debug output is as follows: MU01 View Included Files Tab Setting Description Setting Description Included Files A list of files that are referenced in the imported OpenAPI specification file will be shown here. Selecting from this list will have its contents be displayed in the box below.

---

# Document 1514: Resetting the mzadmin Password - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647170/Resetting+the+mzadmin+Password
**Categories:** chunks_index.json

If the password for mzadmin is lost, it can either be changed by another user belonging to the Administration group or be set according to the following procedure: Add a Platform property for setting the password for mzadmin. $ mzsh topo set topo://container:<container>/pico:platform/val:config.properties.mz.user.emergency.unlock  `mzsh encryptpassword <new password>` Restart the Platform: $ mzsh restart platform When the platform is restarted, the password for mzadmin is reset. Note! Remember to remove the property mz.user.emergency.unlock, or the password will be reset at every restart of the platform. $ mzsh topo unset topo://container:<container>/pico:platform/val:config.properties.mz.user.emergency.unlock

---

# Document 1515: Python Processing Agent Events - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642474/Python+Processing+Agent+Events+-+Batch
**Categories:** chunks_index.json

Agent Message Events

---

# Document 1516: Automatic Maps - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678267/Automatic+Maps
**Categories:** chunks_index.json

Automatic mapping is used when all the fields of the external format should be included in the internal format. This does not preclude explicit mapping of selected fields or types. Automatic mapping complies with the following steps for each field in the external format: Evaluate if the in_map contains an explicit mapping for the external field. If so, use that explicit mapping. If the preceding step fails, evaluate if the internal format (if specified) contains a field name that is identical to the external field name. If so, create, with appropriate type conversions, a mapping between the two field names. If the preceding step fails and the external field is not marked as external_only , create a new field (with the same field name and the appropriate type) in the target_internal . Then create a mapping from the external field to this new internal field. The type of the created internal field is controlled by the external field type. External Type Ultra Type mapped to External Type Ultra Type mapped to Primitive external Maps to the corresponding primitive internal type. This depends on the external format specification. External format (sub-record) Unless a using in_map specification is applicable, a new in_map is automatically created to handle the mapping between the external sub-record and the internal field. The automatic map has by default no specified internal or target_internal name, however this can be modified with automatic mapping specifications. List Maps to an internal list type, where the list element type is deduced from the external list element type according to these rules (recursively). target_internal When using automatic , a new internal format may be automatically generated if any new internal fields are needed for the automatic mapping. The name of this new internal format is given by the target_internal specification of an in_map declaration. If no target_internal name specification is given, the format is still generated if needed, however without usable name (that is, the format cannot be directly referred to in APL). If a target_internal name has been given, this format is created even if no new fields are created. If the in_map has a specified internal format, the generated format is a subtype of this format (hence, inherits all its fields). Automatic Mapping Specifications It is possible to modify the behavior of the automatic mapping algorithm when generating the sub-maps. The most direct way to do this is to specify how each external type will be mapped, either by explicitly specifying the sub-map or specifying the internal and/or target_internal name. The syntax for the two forms of mapping specifications is declared as follows: <external type>: <in_map (target_)internal options> ; and <external type>: using in_map <map name>; Example - Automatic mapping specifications internal IntBase { int recordSequenceNumber; }; asn_block { ExtDataFile ::= CHOICE( mc MobileCall, dp DataPacket) MobileCall ::= SET ( ... ) DataPacket ::= SET ( ... ) }; in_map InMapDataFile : external(ExtDataFile), target_internal(IntDataFile) { automatic; }; In this case two new internal types are automatically generated for MobileCall and DataPacket . However, they must usually be given a name or in other cases inherit the fields of some declared base internal format. This can be accomplished by adding some automatic mapping specifications: Example - Adding automatic mapping specifications in_map InMapDataFile : external(ExtDataFile), target_internal(IntDataFile) { automatic { MobileCall: internal(BaseInt), target_internal(TIMobileCall); DataPacket: internal(BaseInt), target_internal(TIDataPacket); }; }; This can also be done by specifying a using in_map declaration. For instance, the previous example is equivalent to the more complicated one: Example - using in_map declaration in_map MobileCallMap: internal(BaseInt), target_internal(TIMobileCall) { automatic; }; in_map DataPacketMap: internal(BaseInt), target_internal(TIDataPacket) { automatic; }; in_map InMapDataFile : external(ExtDataFile), target_internal(IntDataFile) { automatic { MobileCall: using in_map MobileCallMap; DataPacket: using in_map DataPacketMap; }; }; The type map specifications are inherited by every automatically generated sub-map: Example - type map specifications inherited asn_block { ... MobileCall ::= SET (location Location;) ... }; external Location { int longitude; int latitude; }; in_map InMapDataFile : external(ExtDataFile), target_internal(IntDataFile) { automatic { MobileCall: internal(BaseInt), target_internal(TIMobileCall); DataPacket: internal(BaseInt), target_internal(TIDataPacket); Location: internal(BaseInt), target_internal(TILocation); }; }; In this example, even though Location is not a part of the direct mapping of ExtDataFile itself, the type map information is still considered when creating TIMobileCall . use_external_names Automatic Option When declaring a target_internal name for a large number of sub-formats the use_external_names option on the automatic block is a useful option. It gives all generated maps a target_internal name identical to the name of the external format. Example - use_external_names option on the automatic block in_map AMARecord_Map : external(AMARecord), target_internal(AMARecord) { automatic { RecordOwnerType : target_internal(RecordOwnerType); RecordOwnerDN : target_internal(RecordOwnerDN); Package_100 : target_internal(Package_100); Package_101 : target_internal(Package_101); Package_102 : target_internal(Package_102); }; }; The use_external_names specification eases the syntax by removing the need of all the target_internal specifications. The general syntax for a use_external_names declaration is as follows: use_external_names(<external 1>, <external 2>, ...) The previous example includes all of the specified external definitions within the file. Hence, the in_map can be rewritten as: Example - in_map in_map AMARecord_Map : external(AMARecord), target_internal(AMARecord) { automatic : use_external_names; }; It is possible to combine this with automatic mapping specifications. In this case the explicit specifications override the use_external_names behavior. Automatic Block Internal Specification It is also possible to append internal type specifications to the constructed in_maps : Example - Internal type specifications in_map AMARecord_Map : external(AMARecord), target_internal(AMARecord) { automatic : use_external_names, internal(myInternal); }; If automatic mapping specifications are in use, these override the internal specifications on the automatic level (even if they only specify target_internal ).

---

# Document 1517: FTAM EWSD Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033053/FTAM+EWSD+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent produces bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop .. Publishes The agent publishes the following MIMs: MIM Parameter Description File Retrieval Timestamp This MIM parameter contains a timestamp, indicating when the file transfer starts. File Retrieval Timestamp is of the date type and is defined as a header MIM context type. Source Filename This MIM parameter contains the Filename , merged with an automatic generated file sequence number of five (5) digits (00001-99999). Source Filename is of the string type and is defined as a header MIM context type. Source Host This MIM parameter contains the Host Name . Source Host is of the string type and is defined as a header MIM context type. Source User Name This MIM parameter contains the User Name . Source User Name is of the string type and is defined as a header MIM context type. Accesses The agent does not itself access any MIM resources.

---

# Document 1518: HTTP/2 Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743754
**Categories:** chunks_index.json

The HTTP/2 functions are used to exchange data over HTTP/2 as a client. However, the functions can also be used for HTTP/1. The client functions are used to exchange data with a HTTP server. There are specific functions for GET and POST as well as functions for general HTTP requests. Either plain text or encrypted communication can be used. Basic authentication is supported, as well as the use of a keystore, and if required a truststore, for the functions with an encrypted communication channel. Note! In all parameter descriptions below, "HTTP" may refer to both HTTP and HTTPS, and both HTTP/1 and HTTP/2. The following functions for HTTP/2 described here are: 1 httpGet 2 httpPost 3 httpReq 4 httpMultipartPost 5 TLS/SSL Encryption httpGet This function uses the GET method to retrieve content from an HTTP server, see Response(9.3) . Response httpGet ( string host, string path, string protocol, //Optional int port, //Optional boolean secure, //Optional int requestTimeout, //Optional int connectionTimeout, //Optional string username, //Optional string password, //Optional map<string, string> headers ) //Optional Parameter Description Parameter Description host The name or IP address of the HTTP server. path The path. Example - path /api/v2/doc protocol The protocol used: HTTP/1 or HTTP/2. The default value is HTTP/1. To use HTTP/2, you must set this value to "h2". port The port number to contact the HTTP server on. Port 80 is used for HTTP connection and 443 is used for HTTPS connection by default. secure Indicates whether the data should be sent in secure mode or not. requestTimeout The number of milliseconds to wait for a response. If the value is not specifically specified, the default timeout is used. The default value is 15000 milliseconds. connectionTimeout The number of milliseconds to wait for a connection to be established. If the value is not specifically specified, the default timeout is used. The default value is 3000 milliseconds. username A username for an account on the HTTP server. password Password associated with the username. headers Custom HTTP request headers. Returns A response from the HTTP server. It will be null if any part of the communication fails. httpPost This function uses the POST method to send content to an HTTP/2 server and receives the response. Response httpPost ( string host, string path, bytearray content, string contentType, string protocol, //Optional int port, //Optional boolean secure, //Optional int requestTimeout, //Optional int connectionTimeout, //Optional string username, //Optional string password, //Optional map<string, string> headers ) //Optional Parameter Description Parameter Description host The name or IP address of the HTTP server. path The path Example - path /api/v2/doc bytarray content The body of the request in bytearray format. contentType The MIME type of the content. Example - contentType application/json protocol The protocol used: HTTP/1 or HTTP/2. The default value is HTTP/1. To use HTTP/2, you must set this value to "h2". port The port number to contact the HTTP server on. Port 80 is used for HTTP connection and 443 is used for HTTPS connection by default. secure Indicates whether the data should be sent in secure mode or not. requestTimeout The number of milliseconds to wait for a response. If the value is not specifically specified, the default timeout is used. The default value is 15000 milliseconds. connectionTimeout The number of milliseconds to wait for a connection to be established. If the value is not specifically specified, the default timeout is used. The default value is 3000 milliseconds. username A username for an account on the HTTP server. password Password associated with the username. headers Custom HTTP request headers. Returns A response from the HTTP server. It will be null if any part of the communication fails. httpReq This function makes an HTTP request and uses the specified method, for example GET, POST, and PUT. Response httpReq ( string method, string host, string path, bytearray content, string contentType, string protocol, //Optional int port, //Optional boolean secure, //Optional int requestTimeout, //Optional int connectionTimeout, //Optional string username, //Optional string password, //Optional map<string,string> headers )//Optional Parameter Description Parameter Description method The HTTP method. host The name or IP address of the HTTP server. path The path Example - path /api/v2/doc content The body of the request in bytearray format. contentType The MIME type of the content. Example - contentType application/json protocol The protocol used: HTTP/1 or HTTP/2. The default value is HTTP/1. To use HTTP/2, you must set this value to "h2". port The port number to contact the HTTP server on. Port 80 is used for HTTP connection and 443 is used for HTTPS connection by default. secure Indicates whether the data should be sent in secure mode or not. requestTimeout The number of milliseconds to wait for a response. If the value is not specifically specified, the default timeout is used. The default value is 15000 milliseconds. connectionTimeout The number of milliseconds to wait for a connection to be established. If the value is not specifically specified, the default timeout is used. The default value is 3000 milliseconds. username A username for an account on the HTTP server. password Password associated with the username. headers Custom HTTP request headers. Returns A response from the HTTP server. It will be null if any part of the communication fails. httpMultipartPost This function uses the POST method to send multipart binary contents to an HTTP server and receives the response. Response httpMultipartPost ( string host, string path, list<MultipartSegmentUDR> content, string protocol, //Optional int port, //Optional boolean secure, //Optional int requestTimeout, //Optional int connectionTimeout, //Optional string username, //Optional string password, //Optional map<string, string> headers ) //Optional Parameter Description Parameter Description host The name or IP address of the HTTP server. path The path on the server to which we should do the POST. content The body of the the request. protocol The protocol used: HTTP/1 or HTTP/2. The default value is HTTP/1. To use HTTP/2, you must set this value to "h2". port The port to be used for the HTTP server. Port 80 is used for HTTP connection and 443 is used for HTTPS connection by default. secure Indicates whether the data should be sent in secure mode or not. requestTimeout The number of milliseconds to wait for a response. If the value is not specifically specified, the default timeout is used. The default value is 15000 milliseconds. connectionTimeout The number of milliseconds to wait for a connection to be established. If the value is not specifically specified, the default timeout is used. The default value is 3000 milliseconds. username Username for the account to be used on the HTTP server. password Password associated with the username. headers Custom HTTP request headers. Returns A response from the HTTP server. It will be null if any part of the communication fails. Note! For optional parameters, you need to state null in case you supply subsequent optional parameters. If there are no subsequent parameters, you do not have to state anything. TLS/SSL Encryption When selecting secure, a keystore is required. In this case, the use of a truststore is supported. Configure Java Keystore for Secure URL Functions Keystore is used to store HTTP Clients credential. This certificate is sent to a server for authentication if required. To specify a Keystore file that you want to use, set the properties https.apl.keystore_location and https.apl.keystore_passphrase in the relevant ECs. See the example below for how to set these properties using the mzsh topo command. Example - Setting Java keystore properties $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.https.apl.keystore_location <location of the Keystore file> $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.https.apl.keystore_passphrase <DR encrypted password> If the two properties above are not set in the relevant Execution Context <pico> .conf , MZ Default Keystore is used. The property https.apl.keystore_location represents the location of the keystore and the property https.apl.keystore_passphrase represents the passphrase for that keystore. The following command can be used to create a keystore with the Java keytool program. keytool -keystore /var/opt/mz/HttpdExec.keystore -genkey -keyalg RSA Note! The Keystore passphrase must be the same as the passphrase used by the certificate. See the JVM product documentation for more information about how to use the keytool. Configure Java Truststore for Secure URL Functions A truststore is a keystore that is used when deciding what to trust - truststore stores certificates from third parties. If you receive data from an entity that you already trust, and if you can verify that the entity is what it claims to be, you can assume that the data does in fact come from that entity. By Default, MediationZone uses its own truststore, which always trusts any server connection. If you want to use a specific truststore, use the mzsh topo command to add the property https.apl.userdefined.truststore to the required ECs and set the value to true: $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.https.apl.userdefined.truststore true The default value of this property is false . After setting the property https.apl.userdefined.truststore to true , if you want to use a specific truststore, use the mzsh topo command to set the following properties in the relevant ECs: https.apl.truststore_location https.apl.truststore_passphrase Example - Setting properties to use a specific truststore $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.https.apl.truststore_location <location of the truststore file> $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.https.apl.truststore_passphrase <DR encrypted password> If you do not set these two properties, the Java Default Truststore is used.

---

# Document 1519: ADLS2 File Collection Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671890/ADLS2+File+Collection+Agent+Transaction+Behavior
**Categories:** chunks_index.json

The transaction behavior for the ADLS2 file collection agent is described here. For more information about general transaction behavior please refer to the section Transactions in Workflow Monitor . Emits The agent emits commands that changes the state of the file currently processed. Command Description Begin Batch Emitted before the first part of each collected file is fed into a workflow. End Batch Emitted after the last part of each collected file has been fed into the system. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on the workflow level is configured to abort the workflow, the agent will never receive the last Cancel Batch message. In this situation, ECS will not be involved, and the file will not be moved, but left at its current place. Hint End Batch If a Hint End Batch message is received, the collector splits the batch at the end of the current block processed (32 kB), If the block end occurs within a UDR, the batch will be split at the end of the preceding UDR. After a batch split, the collector emits an End Batch message, followed by a Begin Batch message (provided that there is data in the subsequent block).

---

# Document 1520: Post Platform Installation Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/546635777
**Categories:** chunks_index.json

This section covers post-installation steps that are required when using a platform database. It includes the following subsections: Post Configuration for Oracle RAC Connection

---

# Document 1521: Radius Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740146/Radius+Agents
**Categories:** chunks_index.json

This section describes the Radius Server and Radius Client agents. These are collection and processing agents for real-time workflow configurations. Prerequisites The reader of this information should be familiar with: RADIUS (RFC 2865, http://www.ietf.org/rfc/rfc2865.txt ) RADIUS Accounting (RFC 2866, http://www.ietf.org/rfc/rfc2866.txt ) This section includes the following subsections: Radius Server Agent Radius Client Agent Radius Related UDR Types A Radius Example

---

# Document 1522: Shared Table Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737922/Shared+Table+Profile
**Categories:** chunks_index.json

This section describes the Shared Table profile. This profile en ables workflow instances to share tables for lookups. Open Shared Table Using the Table Lookup Service instead of adding tableCreate in each workflow instance will increase the throughput with fewer duplicated tables, fewer lookups, and reduced memory consumption. The Table Lookup Service comprises a profile in which SQL queries are defined, and two APL functions; one that references the profile and creates a shared table, and one that can be used for refreshing the table data from the APL code. The Shared Table profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow each time you save the profile. Memory Allocation There are three different ways to allocate memory for the created tables. By default, the tables are kept as Java objects in memory. The shared tables can also be configured to keep the tables as raw data either on or off the heap. By using raw data, the overhead of Java objects is removed and less memory is required. The type of memory allocation chosen for the shared tables is configured in the Shared Table profile by selecting a Table Storage parameter. If relevant, you can select an Index Storage parameter and also Variable Width Varchar Columns . For further information about these settings, see the section below, Shared Table Profile Configuration. For further information about memory allocation, see the System Administration Guide . Shared Table Profile Configuration The Shared Table profile configuration is opened by clicking on the New Configuration button in Desktop and selecting the Shared Table Profile option. Open The Shared Table profile dialog The contents of the buttons in the button bar may change depending on which configuration type has been opened. The Shared Table profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The Shared Table profile dialog contains the following configurations: Setting Description Setting Description Database Database Click the Browse... button and select the Database profile you want to use. Any type of database that has been configured in a database profile can be used. See Database Profile for further information. Release Timeout (seconds) If this check box is selected, the table will be released when the entered number of seconds has passed since the workflows accessing the table were stopped. The entered number of seconds must be larger than 0. If this check box is not selected, the table will stay available until the execution context is restarted. Refresh Interval (seconds) Select this check box in order to refresh the data in the table with the interval entered. The entered number of seconds must be larger than 0. If this check box is not selected, the table will only be refreshed if the APL function tableRefreshShared is used. For more information regarding the function, see the section below, tableRefreshShared. Note! The interval for checking if the table needs to be refreshed is 10 seconds, which is the minimum time before a new refresh is performed. If a refresh fails, an error is generated in the system log, but the table is not cleared - the old data remains in the shared table. A new refresh is initiated every 10th second until the refresh has finished successfully. Table Storage Object Select this option to set the Table Storage to Object . If you select this option, the shared tables are stored as Java objects on the JVM heap. Note! If you have selected to use a profile with the CSV database type, this is the only option available if you have not configured properties using Advanced Connection Setup . On Heap Select this option to set the Table Storage to On Heap . If you select this option, the shared tables are stored in a compact format on the JVM heap. If you select On Heap , you must select an option for the Index Storage . Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Off Heap Select this option to set the Table Storage to Off Heap . If you select this option, the shared tables are stored in a compact format outside the JVM heap. Note! You are required to set the JDK parameter in the relevant Execution Context pico configuration, for example: $ mzsh topo set topo://container:<container>/pico:<pico>/obj:config.jvmargs  'maxDirect:["-XX:MaxDirectMemorySize=4096M"]' Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . If you select Off Heap , you must select an option for the Index Storage . Unsafe Select this option to set the Table Storage to Unsafe . If you select this option, the shared tables are stored in a compact format. If you select Unsafe , you must select an option for the Index Storage . Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Primitive Lookup Select this option to set the Table Storage to Primitive Lookup . This provides simple lookup tables with a fast lookup function but they are limited to two columns of type Int/Long for the key (column 1) and type Short/Int/Long for the value (column 2). Lookup operations on Primitive Lookup tables are limited to the equals operation on column 1. Note! If you use the Primitive Lookup option with a database profile that is configured for Oracle, using the Oracle column type NUMBER with a precision greater than 10 may cause errors. Note! If you have selected to use a profile with the CSV database type, this option is only available if you have configured properties for this using Advanced Connection Setup . Index Storage Object Select this option to set the Index Storage to Object . If you select this option, the index is stored as Java objects on the JVM heap. This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . Pointer Select this option to set the Index Storage to Pointer . If you select this option, the index is stored as pointers to the table data. This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . Cached Long/Int Pointer Select this option to set the Index Storage to Cached Long/Int Pointer . This option is only available if you have selected On Heap , Off Heap or Unsafe for Table Storage . For numeric index columns, the Cached Long/Int Pointer can be used for faster lookups, but at the cost of slightly higher memory consumption. Variable Width Varchar Columns Select this check box to enable variable width storage of varchar columns. This reduces memory usage for columns that are wide and of varying widths. SQL Load Statement SQL Load Statement In this field, an SQL SELECT statement should be entered in order to create the contents of the table returned by the tableCreateShared APL function. The following statement will return a table named MyTable with the columns key and value when the tableCreateShare function is used together with this profile. Example - SQL SELECT statement SELECT key,value FROM MyTable If no data has been fetched from the database, SQL errors in the table lookup will cause runtime errors (workflow aborts). However, if data has already been fetched from the database then this data will be used. This will also be logged in the System Log. Whenever possible, use values of the type long instead of the type date or string. This may improve performance. Example - Using SQL SELECT statement SELECT to_number(user_id) as USER_ID, to_number(to_char(nvl(start_date,to_date( '19000101010101', 'yyyymmddhh24miss')), 'yyyymmddhh24miss')) as START_DATE, to_number(to_char(nvl(end_date, to_date( '99990101010101', 'yyyymmddhh24miss')), 'yyyymmddhh24miss')) as END_DATE FROM MyTable Table Indices If you want to create an index for one or several columns of the shared table, these columns can be added in this field by clicking the Add button and add the columns for which you want to create an index. The index will start with 0 for the first column. Note! An index will not be created unless there are at least five rows in the table. Create indices in the Shared Table profile based on the data that is fetched from the database. Even if you look up multiple columns in APL, using one index instead of several may result in improved performance. Note! Some Database Management systems provide character column types so that you are not required to specify the column width (e g TEXT in PostgreSQL). If you use shared tables with such a column type, you cannot use the types On Heap , Off Heap or Unsafe within Table Storage. APL The following functions are included in the Table Lookup Service: tableCreateShared tableRefreshShared tableCreateShared This function returns a shared table that holds the result of the database query entered in the Shared Table profile. table tableCreateShared ( string profileName boolean disableCommit ) Parameters: Returned Value Description Returned Value Description profileName Name of the Shared Table profile you want to use. disableCommit (Deprecated from MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement being performed at the end of every SQL transaction for this particular function. By default, disableCommit is set to true unless otherwise changed via this parameter. Info! It should be noted that on recent Oracle versions, the DBLink SQL transaction behavior has changed, where every single SQL statement for remote database transactions requires a commit or rollback statement in order to close a connection. Returns A table containing the columns stated with the SQL query in the stated Shared Table profile, that can be shared by several workflow instances. Example - Using the function tableCreateShared This creates a shared table called myTable with the columns returned by the SQL query in the mySharedProfile Shared Table profile. initialize { table myTable = tableCreateShared("Folder.mySharedProfile"); } tableRefreshShared This function can be used for refreshing the data for a shared table configured with a Shared Table profile. The table will be updated for all workflow instances that are using the table and running on the same EC. table tableRefreshShared ( string profileName boolean disableCommit ) Parameters: Returned Value Description Returned Value Description profileName Name of the Shared Table profile you want to refresh data for. disableCommit (Deprecated from MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement being performed at the end of every SQL transaction for this particular function. By default, disableCommit is set to true unless otherwise changed via this parameter. Info! It should be noted that on recent Oracle versions, the DBLink SQL transaction behavior has changed, where every single SQL statement for remote database transactions requires a commit or rollback statement in order to close a connection. Returns A refreshed shared table. Example - Using the function tableRefreshShared This returns the shared table called myTable, which uses the mySharedProfile, with refreshed data. table myTable = tableRefreshShared("Folder.mySharedProfile"); Additional Performance Tuning The Oracle JDBC driver includes a feature built that allows you to set the number of rows that are prefetched while the full result set is being formulated. At the time of writing, the default number of prefetched rows is 10. You can increase this value by setting the Execution Context property oracle.jdbc.defaultRowPrefetch in the relevant Pico configuration file.

---

# Document 1523: Analysis Agent Transaction Behavior - Batch Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639988/Analysis+Agent+Transaction+Behavior+-+Batch+Workflows
**Categories:** chunks_index.json

This section includes information about the Analysis agent transaction behavior. For information about the general transaction behavior, see 3.1.11 Workflow Monitor . Input/Output Data The agent retrieves commands that changes the state of the file currently processed. Command Description End Batch The agent itself does not emit End Batch, however it can trigger the collector to do so by calling the hintEndBatch method. See the sections about beginBatch and endBatch in the APL Reference Guide for information about the hintEndBatch method. Cancel Batch The agent itself does not emit Cancel Batch however, it can trigger the collector to do so by calling the cancelBatch method. See the section about cancelBatch in the APL Reference Guide for further information. Hint End Batch If the code contains a call to the method hintEndBatch this will make the agent emit a Hint End Batch. Note! Not all collectors can act upon a call on a hintEndBatch request. Please refer to the user's guide for the respective Collection agent for information. The agent acquires commands from other agents and, based on them, generates a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, the agent calls the beginBatch function block, if present in the code. Cancel Batch When a Cancel Batch message is received, the agent calls the cancelBatch function block, if present in the code. End Batch When an End Batch message is received, the agent calls the drain and endBatch function blocks, if present in the code.

---

# Document 1524: Details - Virtual Machine Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998095/Details+-+Virtual+Machine+Tab
**Categories:** chunks_index.json

The Virtual Machine tab contains the information listed below. Memory Tab Open Virtual Machine with the Memory tab displayed Tab Description Tab Description Memory Memory information will be displayed here and Garbage Collection Functions are available. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html VM Arguments Tab Open Virtual Machine with the VM Arguments tab displayed Tab Description Tab Description VM Arguments VM arguments listed. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html System Properties Tab Open Virtual Machine with the System Properties tab displayed Tab Description Tab Description System Properties System Properties. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html MBeans Tab Open Virtual Machine with the MBeans tab displayed Tab Description Tab Description MBeans MBeans. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Diagnostics Commands Tab Open Virtual Machine with the Diagnostics Commands tab displayed Tab Description Tab Description Diagnostic Commands For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html

---

# Document 1525: Diameter Stack Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999657/Diameter+Stack+Agent+Events
**Categories:** chunks_index.json

When you run a Diameter workflow, the peer connections of the Diameter Stack agents are monitored through the standard Diameter watchdog as described in RFC 6733 and RFC 3539. Possible states of the connection are: OKAY, SUSPECT, DOWN, REOPEN, and INITIAL. The Diameter peer state changed event is triggered whenever there is a change of peer state. You can configure an event notification that is triggered whenever a state change occurs. For more information about this event, see the section Diameter Peer State Changed Event . Diameter Dynamic Event You can configure an event notification that is triggered in the following cases when dynamic peer discovery is enabled: A Diameter workflow is started. The routing table of a Diameter Stack agent is dynamically updated. The TTL (time to live) of a cached DNS record expires. For more information about this event, see Diameter Dynamic Event . Agent Message Events An information message from the agent, is generated according to the configuration in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Generic Diameter Server initialized for realm XXXX The message is generated when the workflow is started. RFC 6733 compliance warning: Passive mode enabled The message is generated if the workflow is started with the agent in the passive mode. Generic Diameter Server stopping. The message is generated when the workflow is stopping. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 1526: Upgrade Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669993
**Categories:** chunks_index.json

During the upgrade process, you must use the default application user, mzadmin , and correct password whenever you are prompted to enter it. The preparation steps will not affect the running system and can be done in advance. Before proceeding with the upgrade, make the following preparations: Verify that you are on a version from which upgrade is supported. You can check the current version by opening the About window in the Desktop, where the current version is listed in the Pico Version section. See the documentation space for the version you are currently running for more information. Install either Oracle JDK 17 or OpenJDK 17 on the server(s) where the Platform Container and Execution Containers are running. Verify that the System Requirements are met. If the operating system, or database need to be upgraded, this should be done prior to the upgrade. This has to be done for all machines that are hosting the system. Ensure that the environment variables are set correctly: Variable Description MZ_HOME This environment variable specifies where the software is installed. JAVA_HOME This environment variable specifies where the JDK is installed. PATH This environment variable specifies the search path and must contain the following directories: $JAVA_HOME/bin:$MZ_HOME/bin Example - Setting environment variables export MZ_HOME=/opt/mz export JAVA_HOME=/opt/jdk/jdk-17.0.2 export PATH=$JAVA_HOME/bin:$MZ_HOME/bin:$PATH Make an online backup of the database(s). For further information regarding how to perform an online backup, see Backup and Disaster Recovery Procedures . Note! If you are upgrading from 9.x, you can find the database backup instructions in the documentation for 9.x. It is important to make a backup of MZ_HOME for rollback purposes. To make a backup of your MZ_HOME, you can use the following command: Note! Ensure that all processes in your installation are shutdown prior to the backup. You can check which processes are running by using the mzsh status command. cd $MZ_HOME/../ tar -zcvf mzhome_backup.tgz <MZ_HOME directory> mv mzhome_backup.tgz <backup directory> Caution! Use of MZ_HOME backup The MZ_HOME backup is needed in case the upgrade fails. Caution! Use of Filebase persistence When you have the platform property mz.userserver.filebased set to true , to ensure a seamless upgrade process, it is imperative to export all the config before the upgrade and re-import it afterward. Failure to do so may result in the missing configuration data in the upgrade.The MZ_HOME backup is needed in case the upgrade fails. Create a directory to use when unpacking this release and future releases. For the purpose of these instructions, this designated directory is referred to as the staging directory . Important! The staging directory should not be the same directory as the one you created and set up as the MZ_HOME directory. Place the *.tgz file from your release delivery into the staging directory . Use a command line tool, go to the staging directory , and unpack the *.tgz file by running the following command: tar xvzf <filename>.tgz A directory is then created in the staging directory , containing the software to be installed. For the purpose of these instructions, this directory is referred to as the release content directory . Now copy the MZ license file into the release content directory . Note! If you are upgrading from an earlier major or minor version, you need a new license file. Contact Support | DigitalRoute on getting the MZ license file. cp mz.license <release content directory> Enter the release content directory and prepare the install.xml file by running the following command: cd <release content directory> ./setup.sh prepare The *.mzp packages have now been extracted, and the install.xml has been extracted into the release content directory . The install.xml file will automatically be populated with information from your existing installation. Note! Refer Updating the Installation Properties for Platform to know more about these properties and their default values. Important! When upgrading to this release, the install.admin.password property must be set according to your current admin password before proceeding with the ./setup.s h upgrade step. Important! If your existing MZ_HOME platform database is configured to use other database type than the default Derby, you need to check and update the respective database related properties from the install.xml file to match your current setup. Oracle See Properties for Oracle for details. The following properties require manual update: <property name="install.ora.owner" value="mzowner"/> <property name="install.ora.password" value="mz"/> PostgreSQL See Properties for PostgreSQL for details. The following properties require manual update: <property name="install.pg.owner" value="mzowner"/> <property name="install.pg.password" value="mz"/> SAP HANA See Properties for SAP HANA for details. Important! Oracle ojdbc.jars If you are using Oracle as the system database, you need to use Oracle 19c (19.20.0.0) JDBC drivers ( ojdbc8.jar/ojdbc10.jar ). Replace the jar files in $MZ_HOME/3pp and $MZ_HOME/lib folders The jar file can be downloaded from JDBC and UCP Downloads page .

---

# Document 1527: SAP CC Online Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642693/SAP+CC+Online+Agent
**Categories:** chunks_index.json

SAP C onvergent Charging provides a rating and charging solution for high-volume processing in service industries. It delivers pricing design capabilities, high-performance rating, and convergent balance management. The SAP CC Online agent provides an easy way to integrate MediationZone with SAP Convergent Charging. The SAP CC Online agent is a real-time agent that can be used to send charging requests to SAP Convergent Charging and to handle the response from the server. The SAP CC Online agent communicates with the workflow by using a dedicated set of UDRs. The SAP CC Online agent accepts any charging request UDR as input. A CCCycleUDR, containing both the request and the answer will be returned as output. For further information about the SAP CC UDR types, see SAP CC UDRs . Asynchronous Communication To provide a fast and efficient charging capability, the SAP CC Online agent relies on the Asynchronous Charging Client (AsynchStatefulServiceClient). For further information about the SAP CC AsynchStatefulServiceClient API, see the SAP AsynchStatefulServiceClient documentation . Note! The SAP CC APIs that the SAP CC Online agent depend on do not support setting the itemImmediatelyLoaded flag for ChargingOutputContext . As a result, the SAP CC Online agent cannot support scenarios such as CIT creation via charging requests. In order to reduce the number of connected clients on an SAP Convergent Charging core server, a pool of shared clients exists on the MediationZone side. It means that all the workflows running in the same EC will not instantiate their own connection with SAP Convergent Charging, but instead use one shared client. Based on its configuration (Host, Port, Timeout, and Flow Control), the SAP CC Online agent will pick a mapping client up from the pool when the workflow is initialized. If there is no client matching a particular configuration, a new client will be added to the pool. For further information about how to configure the client, see SAP CC Online Agent Configuration . Note! Due to the asynchronous behavior of the client, requests and answers are not grouped together, and the order of the requests and answers cannot be guaranteed. However, they share a unique ID for the workflow execution. The section contains the following subsections: SAP CC Online Agent Configuration SAP CC Online Agent Events SAP CC Online Agent Input/Output Data and MIM

---

# Document 1528: Package Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744622/Package+Management
**Categories:** chunks_index.json



---
**End of Part 64** - Continue to next part for more content.
