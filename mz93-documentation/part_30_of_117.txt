# RATANON/MZ93-DOCUMENTATION - Part 30/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 30 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~68.1 KB
---

A Duplicate UDR agent is configured in two steps. First, a profile has to be defined, then the regular configurations of the agent are made. The Duplicate UDR profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Duplicate UDR profile configuration, click the New Configuration button from the Configuration dialog available from Build View , and then select Duplicate UDR Profile from the menu. To open an existing Duplicate UDR profile configuration, click on the configuration in the Configuration Navigator, or right-click on the configuration and then select View Configuration . The contents of the menus in the menu bar may change depending on which configuration type that has been opened. The Duplicate UDR profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . Item Description Item Description External References Select this menu item to Enable External References in an agent profile field. Refer to Enabling External References in an Agent Profile Field in External Reference Profile for further information. General Tab The General tab is displayed by default. In the General tab, the File Storage is displayed as the default Storage for a New Configuration. Open Duplicate UDR New Configuration The General tab is split into two sections, the Storage settings and the UDR settings. To begin, first select either File Storage or SQL Storage with the Storage selector. When a Storage is selected, only settings relevant to the Storage will be displayed. Storage Settings The Storage settings is the top section of the General Tab. It contains settings to setup the Duplicate UDR cache storage and settings for managing the cache size and data expiration. File Storage Open General Tab Storage settings for File Storage The Duplicate UDR profile configuration contains the following Storage settings specific to File Storage: Setting Description Setting Description Storage Host In the drop-down menu, the preferred storage host, where the duplicate UDRs are to be stored, can be selected. The choice for storage of duplicate repositories is either on a specific EC Group or Automatic . If Automatic is selected, the same EC Group used by the running workflow will be selected. When the Duplicate UDR Inspector is used, the EC Group is automatically selected. Note! The workflow must be running on the same EC Group as its storage resides, otherwise, the Duplicate UDR Agent will refuse to run. If the storage is configured to be Automatic, its corresponding directory must be a file system shared between all the EC Groups. Directory An absolute path to the directory on the selected storage host, in which to store the duplicate cache. If this field is greyed out with a stated directory, it means that the directory path has been hard-coded using the mz.present.dupUDR.storage.path property. This property is set to false by default. Example - Using the mz.preset.dupUDR.storage.path property To enable the property and state the directory to be used: mzsh topo set val:common.mz.preset.dupUDR.storage.path '/mydirectory/dupudr' To disable the property: mzsh topo unset val:common.mz.preset.dupUDR.storage.path For further information about all available system properties, see System Properties . External References specific to File Storage can be used with the following field: Directory SQL Storage Open General Tab Storage settings for SQL Storage The Duplicate UDR profile configuration contains the following Storage settings specific to SQL Storage: Setting Description Setting Description Database Profile This is the database in which to store the Duplicate UDR cache. Click the Browse... button to get a list of all the database profiles that are available. For further information see Database Profile . Duplicate UDR SQL Storage is supported for use with the following database: SAP HANA 2.0 SP 7 PostgreSQL 12 and above Note! If no changes are made to the Duplicate UDR profile, changes to the settings of selected Database Profile will only be detected during Duplicate UDR Agent workflow run. Generate SQL Click this button to bring up a dialog that will contain the SQL statements for the table schema generated for the Duplicate UDR profile. Note! The Duplicate UDR profile Configuration Key is used for generating the names of the Duplicate UDR database tables. You will need to save the profile at least once for the profile to have a Configuration Key, so that proper database table names can be generated. Warning! Users will have to copy the SQL script generated in the dialog and execute the SQL script separately to create the Duplicate UDR tables in the database selected with the Database Profile selector. The Duplicate UDR Profile screen will not automatically create the tables in the database for you. Generate SQL Dialog Box When a user clicks on the SQL Storage Generate SQL button, the associated dialog box will open. The Copy button is a convenient way to copy the whole Create Tables SQL Script. Open SQL Storage Generate SQL Dialog box More Storage Settings The Duplicate UDR profile configuration contains the following Storage settings common to both File Storage and SQL Storage. Setting Description Setting Description Max Cache Age (days) The maximum number of days to keep UDRs in the cache. The age of a UDR stored in the cache is either calculated from the Indexing Field (timestamp) of a UDR in the latest processed batch file, or from the system time, depending on whether Based on System Arrival Time or Based on Latest Time Stamp in Cache is selected. If the Date Field option in the UDR settings section below, is not enabled for the Indexing Field , Max Cache Age setting will be disabled and ignored, and cache size can only be configured using the Max Cache Size settings. If enabled, the default value is 30 days. Note! If enabled and the Duplicate UDR Agent receives UDRs that are too old (exceeded Max Cache Age ), the UDRs will not be processed and will simply be routed to the usual route. Duplicate checking is not performed for these UDRs and a warning will be logged in the System Log. Note! The age calculation cannot be performed if the cache is empty. Based On System Arrival Time When Max Cache Age is enabled, this radio button is selected by default, the calculation of cached UDR's age will be based on the time when a new batch is being processed. In case of a longer system idle time, this setting may have a major impact on which UDRs that are removed from the cache. For more information about the difference between Based on System Arrival Time and Based on Latest Time Stamp in Cache when calculating the UDR age, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Based on Latest Time Stamp in Cache When Max Cache Age is enabled and this radio button is selected, the UDR cache age calculation will be made toward the latest Indexing Field (timestamp) of a UDR that was included in the previously processed batch files. For more information about the difference between Based on System Arrival Time and Based on Latest Time Stamp in Cache when calculating the UDR age, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Max Cache Size (thousands) The maximum number of UDRs to store in the duplicate cache. The value must be in the range of 100-9999999 (thousands), the default is 5000 (thousands). The cache will be made up of containers partitioned by the key from the Indexing Field below. For every incoming UDR, it will be determined in which cache container the UDR will be stored. During the initialization phase of each batch, the agent checks if the cache is full. If the check indicates that there will be less than 10% of the cache available, cache containers will start to be cleared until at least 10% free cache is reached, starting with the oldest container. Note! Depending on how many UDRs are stored in each container, this means that different amounts of UDRs may be cleared depending on the setup. If the Indexing Field of all the UDRs happens to have the same value, then all the UDRs in the cache will be cleared. Note! If you have a very large cache size, it may be a good idea to split the workflows in order to preserve performance. Enable Separate Storage Per Workflow This option enables each workflow to have a separate storage that is checked for duplicates. This allows multiple workflows to run simultaneously using the same Duplicate UDR profile. However, if this checkbox is selected, a UDR in a workflow will not be checked against UDRs in a different workflow. Note! Duplicate UDR Inspector currently does not support Duplicate UDR profiles with Enable Separate Storage Per Workflow enabled For both File Storage and SQL Storage, External References can be used with the fields: Max Cache Age Max Cache Size UDR Settings The UDR settings is the bottom section of the General tab. It contains settings to select which UDR to apply duplicate checks, the UDR field used to segment the Duplicate UDR cache into containers and information to manage the scope and contents in the Duplicate UDR containers. Open General Tab UDR Settings The Duplicate UDR profile configuration contains the following UDR settings common to both File Storage and SQL Storage. Setting Description Setting Description Type The UDR type the agent will process. Indexing Field The UDR field is used as an index in the duplicate comparison. Fields of type long (in milliseconds) and date are valid for selection. The cache will be made up of containers partitioned by the key from this Indexing Field . If Date Field below is disabled, each container will cover an interval of 50 seconds. If Date Field is enabled, each container will cover an interval of 10 minutes. For every incoming UDR, it will be determined in which cache container the UDR will be stored. For performance reasons, this field should preferably be either an increasing sequence number or a timestamp with good locality. This field will always be implicitly evaluated. For further information, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Date Field If selected, the Indexing Field will be treated as a timestamp instead of a sequence number, and this must be selected to enable the Max Cache Age (days) field above to be configured. Note! If the UDR Indexing Field value is a timestamp that is configured to be 24 hours or more ahead of the system time, the workflow will abort. Checked Fields In addition to the Indexing Field , the Checked Fields will be used for the duplication evaluation when deciding if a UDR is a duplicate. Note! If the Checked Fields or Indexing Field are modified after an agent is executed, the already stored information will be considered useless the next time the workflow is activated. Hence, duplicates will never be found amongst the old information since another type of metadata has replaced them. Advanced Tab The Advanced tab is available when you have selected SQL Storage for your Duplicate UDR Storage . It contains properties that can be used for performance tuning. For information about setting up SQL Storage for better performance, see Duplicate UDR SQL Storage Setup Guide . Open Advanced Tab for SQL Storage

---

# Document 634: mzcli Exit Codes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547980426/mzcli+Exit+Codes
**Categories:** chunks_index.json

Below are the defined exit codes listed. Exit Code Message Info Exit Code Message Info 0 Run completed. 50 <Prints the help command> No parameters to the command. 51 <varying, error description> Error parsing the parameters. 60 Timed out. Workflow/group did not completed/aborted within the defined timeout. 70 No matching workflow. Workflow/group not found. If wfgroupstart is used the message will be: No matching group found. 71 Only one workflow is allowed to be started when option -w is used. Workflow is replaced with workflowgroup if wfgroupstart is used. [-w] is replaced with [-b] in case it is used instead. 80 <varying, depending on why it didn't start> Reason why the workflow/group did not start. 90 Unexpected error: <error message + stack trace> Unexpected error, abort command. 100 Aborted. 101 Platform shutdown, group aborted. 110 Unknown status of run. Command has been out of contact with platform and lacks information about how the run completed. 230 Already active (ignoring request). 231 Permission denied. 232 No such configuration. 240 Configuration is invalid.

---

# Document 635: SAP CC Batch Agent Configuration - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642736/SAP+CC+Batch+Agent+Configuration+-+Batch
**Categories:** chunks_index.json

You open the SAP CC Processing agent configuration dialog from a workflow configuration. To open the SAP CC Batch agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select SAP CC Batch from the Processing tab of the Agent Selection di alog . Batch Workflow only. Using the SAP CC Batch agent in a Batch workflow will allow the agent to send a call synchronously to the SAP server. Open SAP CC Batch agent batch workflow configuration - Co nne ction tab Setting Description Setting Description Hosts In this section, add the IP address/hostname and external charging port of at least one SAP Convergent Charging Core server Dispatcher Instance. Enable Transaction Safe Select this check box to enable transaction safety. This is a batch workflow feature. SAP CC Batch agent Batch workflow functionality. SAP CC does not support rollback on operations that has already been executed. In order to achieve transaction safety, the agent will utilize a database to keep track of which record that has been processed. In a scenario where the workflow aborts, the agent will be able to refer to the database to determine which records were processed before the abort and prevent duplicated records from being processed. Database The database profile to commit the records to. Prior to running this workflow, a table needs to be created in the respective database. For example SQL scripts to create tables for the database, refer to Prepare Database for Transaction Safe Setup . Supported Databases We support Oracle 9 onward, PostgreSQL 9 onward and SAP Hana version 2.0 SPS 05 onward. DB Commit Size The size of the commit to be performed to the database. Enable Authentication Select this check box to enable charging API authentication. Note! Charging API Authentication is only available for SAP CC version 4.1 SP2 and later. User Name SAP CC user id Password The password for the SAP CC user Timeout The timeout (in milliseconds) to apply for each connection Enable Secured Connection Select to use TLS encrypted communication with Hosts . For more information about setting up Secured Connection, refer to SAP CC Secured Connection . Keystore Path The path to the keystore on an Execution Container host. The path must be the same for all hosts. Note! The keystore format to be used by this particular agent is PKCS12 only. Keystore Password The password for the keystore Advance d Setti ngs Tab The Advanced Settings tab contains extended con fig uration options for the agent. Open SAP CC Batch agent batch workflow configuration - Advanced Settings Tab Setting Description Setting Description Enable Debug Events Select this check box to enable debug mode. This option is useful for testing purposes. Convert BigDecimal Response to String Select this check box to allow conversion of the BigDecimal data type from the SAP Convergent Charging response into string. By default, the check box is empty. Note! In terms of backwards compatibility, enabling this option prevents any compatibility issues with workflows that support the configurations from before the BigDecimal data type was introduced in version 8.0.5.0. Prepare Database for Transaction Safe Setup Oracle To create the table in Oracle, follow the command below: CREATE TABLE MZ_SAP_CC_BATCH_STATE ( WF_NAME VARCHAR2(128) NOT NULL , NODE_NAME VARCHAR2(128) NOT NULL , POSITION NUMBER NOT NULL , ANSWER_UDR BLOB NOT NULL , CONSTRAINT MZ_SAP_CC_BATCH_STATE_PK PRIMARY KEY ( WF_NAME , NODE_NAME , POSITION ) ENABLE ); PostgreSQL To create the table in PostgreSQL, follow the command below: CREATE TABLE mz_sap_cc_batch_state ( wf_name VARCHAR(128) NOT NULL, node_name VARCHAR(128) NOT NULL, position NUMERIC(19) NOT NULL, answer_udr bytea NOT NULL, PRIMARY KEY(wf_name, node_name, position) ); SAP Hana SAP Hana Schema You need to create a schema if your default schema is SYSTEM (which seems to be the case for SAP Hana). The MZ_SAP_CC_BATCH_STATE table must not be within schema SYSTEM or SYS. To create the table in SAP Hana, follow the command below: SET SCHEMA <your_schema_name>; CREATE TABLE mz_sap_cc_batch_state ( wf_name VARCHAR(128) NOT NULL, node_name VARCHAR(128) NOT NULL, position DECIMAL(19) NOT NULL, answer_udr BLOB NOT NULL, PRIMARY KEY(wf_name, node_name, position) );

---

# Document 636: service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657098/service
**Categories:** chunks_index.json

usage: service start [-s, --scope <all | custom | standard>] | restart [--publish-only] | dump | info [-d, --detailed] [-i, --instance <provider/service instance>] | list | update [-c, --command] [-i, --instance <provider/service instance>] Note! This command is valid only for the MZ_HOME owner. The command enables you to manage the services which are configured and hosted in Service Contexts. start Use service start to start the platform and Service Contexts. When you call service start, the platform orchestrates the start-up of defined service instances ( standard-services.conf and custom-services.conf ) and saves the configuration produced. The command accepts the following option: [-s, --scope <all | custom | standard> Use this option to choose which services you want to start. Example. To start both standard and custom services, you may call the following: mzsh service start or mzsh service start --scope all To start only custom or standard services, run mzsh service start --scope custom or mzsh service start --scope standard restart Use service restart when the platform is restarted while some or all of the Service Contexts are kept running, e g, after Platform failure and recovery, or after scheduled maintenance of the Platform. When you call service restart , the command applies the configuration state saved by the service start command, so that all service instances are configured the same way as when the service start was issued. At the very least, service restart republishes the information required to connect to a service instance, e g, from a workflow). However, it also causes any service members found to be missing in the Service Contexts, to be restarted - this may happen when a failure and recovery affects the platform and some of the Service Contexts. When the Platform is restarted, while Service Contexts are active, you must also restart the embedded services using this command. The command accepts the following option: [--publish-only] Use this option to only publish the service configuration in the platform registry. This option can be used if you first upgrade the platform and then execute a rolling upgrade of Service Contexts. Using the option will not restart any service members. dump Use service dump to display very detailed information in HOCON format on all the services running. This information is mainly intended for support and troubleshooting. info Use service info to display information on all of the services running or a specific service instance. The command accepts the following options: [-d, --detailed] Use this option to display detailed information on all of the services running. [-i, --instance <provider/service instance>] Use this option to display detailed information on a specific service instance. Example. For information on all of the services running, you call the following: mzsh service info For detailed information on all of the services running, you call the following: mzsh mzadmin/dr service info --detailed For information on a specific service, you must specify the service provider/service instance, for example: mzsh service info --instance kafka/kafka1 list Use service list to display information on all available service providers. update Use service update to send custom commands to the service instance. [-c, --command] Use this option to specify a custom command. [-i, --instance <provider/service instance> ] Use this option to specify a specific service instance that should be updated. Return Codes Listed below are the different return codes for the service command: Code Description Code Description 0 Will be returned if the command was successful 1 Will be returned if an argument is invalid 2 Will be returned when a known error occurs 3 Will be returned when a general error occurs

---

# Document 637: Configuring PCC Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736361
**Categories:** chunks_index.json

The following steps are required in order for MediationZone to connect to MySQL Cluster: Copy the following files from located in the /[MYSQLDIR]/lib/mysql folder in [CZ], to the /usr/lib directory in each [EZ]: libndbclient.so private/libcrypto.so.3 private/libssl.so.3 and ensure the subdirectory in the path is retained. Once installed, the pcc.properties file, located in the $ MZ_HOME/etc folder in [EC] , needs to be edited to connect to the host and port of the Management Node. Configure the following properties: In order for the Execution Context to be able to locate the pcc.properties file, the Execution Context property mz.pcc.properties must be set in the STR: $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.pcc.properties <path> Restart the ECs.

---

# Document 638: Upgrade Execution Container - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638008/Upgrade+Execution+Container
**Categories:** chunks_index.json

Important! If the existing installation has additional JVM arguments that are obsolete in Java 17, remove them and update to use supported options. Refer to Java 17 documentation for details on  Obsolete Java Options  and  Removed Java Options  since Java 8. Refer to Managing Picos with Topo for JVM Arguments configuration. To upgrade an Execution Container, follow these steps: Before starting the upgrade process, ensure you have performed all necessary backups. Copy the MZ license file to the release directory and prepare the install.xml file. To do this, follow the steps in Upgrade Preparations to change the necessary fields to the respective Execution Container values. Note! Ensure that the install.type value is set to ec only when upgrading the Execution Container. These properties should be configured according to the platform container setup. For more information, refer to Execution Container Properties . Pre-upgrade validations are performed to verify the environment and configuration, ensuring a seamless upgrade experience. To perform validation without starting the upgrade, run the following command: $ ./setup.sh upgrade -validate-only Run the following command to initiate the upgrade process: $ ./setup.sh upgrade Note! If the upgrade validation fails, there is no need to perform a rollback or restore, as the upgrade process has not yet begun. If validation fails but you choose to proceed, you can run the following command: $ ./setup.sh upgrade -skip-validate Caution! You can choose to skip validation, but be aware that this bypasses checks for potential issues in your environment and configuration. If there are undetected problems, the upgrade may fail or cause unexpected behavior. Proceed with caution and ensure you have backups before continuing.

---

# Document 639: Real-Time Disk_Deprecated Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676047/Real-Time+Disk_Deprecated+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events

---

# Document 640: Operating System Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676201/Operating+System+Tab
**Categories:** chunks_index.json

The Operation System tab contains information as listed below. Open Operating System with the OS tab displayed Tab Description Tab Description Operating System For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Operating System with Environment Variables tab Tab Description Tab Description Environment Variables Displays the list of environment variables applicable to your system. Open Operating System with the Threads tab displayed Tab Description Tab Description Threads For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html

---

# Document 641: Checksum Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743709/Checksum+Functions
**Categories:** chunks_index.json

Use the functions described below to generates a checksum for a supplied argument. The following functions for Checksum described here are: 1 cksumReset 2 cksumUpdate 3 cksumResult 4 cksumLength 5 cksum Example 6 crc32 cksumReset Resets the cksum info and prepare to start new cksum calculation. void cksumReset() cksumUpdate Updates the cksum using the specified array of bytes. void cksumUpdate( bytearray data ) Parameter Description Parameter Description bytearray Any bytearray Returns Nothing cksumResult Gets the result of cksum (using all data that has been entered into cksumUpdate). Returns an unsigned 32 bit integer (as a long). long cksumResult() cksumLength Gets the total length of all data that has been entered into cksumUpdate. long cksumLength() cksum Example Example - cksum Simple APL code (Disk_1 is the input agent in the workflow) beginBatch { cksumReset(); } consume { cksumUpdate( input ); } endBatch { debug( ((string)cksumResult()) + " " + cksumLength() + " " + ((string)mimGet("Disk_1","Source Filename")) ); } crc32 Function that computes the CRC-32 of supplied argument. long crc32( any source ) Parameter Description Parameter Description source The argument used to compute the CRC-32. Supported types are string, bytearray and number types. Returns The CRC-32 value

---

# Document 642: System Importer - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998199
**Categories:** chunks_index.json

System Importer enables you to import data to your system from a ZIP file or a remote source like Git or AWS S3. The import contains data about your system, its configurations, and run-time information. System Importer imports data that has been exported by the System Exporter. Note! In Legacy Desktop, you can also import from a directory. The file exported by the System Exporter can contain data from the following categories: Configuration: Workflow configurations, Profiles, Workflow groups, Ultra formats, Event Notifiers and Alarms etc. Run-Time Data: Data that is produced by the system during workflow execution, for example, ECS UDRs, Aggregation Sessions, etc. System: Other customizable parts such as Error Codes, Reprocessing Groups, Configuration Folders, Pico Host, Ultra, User, or Workflow Alarm Value. Workflow Packages: Workflow Packages are compiled versions of configurations, and are created in the Workflow Package mode in the System Exporter. To open the System Importer, go to Manage  Tools & Monitoring and then select System Importer. Open Source Type: Select the source of the export from either: Local : A locally stored export can be selected or dropped in the selected area. Remote: An export stored in a remote source. Currently available for Git and AWS S3. Git is only available in the Desktop interface . Available entries : Contains a tree layout view of the data you can select to import. Logs : Contains a log of the import process Importing from Local Source To import data/configurations from your local machine: Select the appropriate options according to your preferences by clicking the Options button. Click the Select File... button to select the directory where the data to be imported is located. Alternatively, you can also drag and drop the file in the Drop file here area. Open System Importer with log In the Available Entries field, expand the folders and select the check boxes for the entries you want to import. You can also use the Search field to find the appropriate entry to import. Click on the Import button to start the import process. After the import is completed, the Available entries field is cleared of all directories, and the selected file is also removed. You can perform an initial test import to check for any errors. To do this, click the Dry run button. This does not import the entry into Configurations . When the Dry Run is complete, the Available entries field is not cleared off and the selected file is not removed. You can proceed to click the Import button to import data. Open Dry run logs Upon clicking the Import button, it immediately changes into the Abort button, which enables you to cancel the Import process. After clicking abort, the file is removed and you will need to reimport the file to continue the import. If the directory structure of the imported file is not identical to that of the exported material, the import will fail. Update the dynamic configuration data in the collectors with the file sequence numbers that you had noted down before performing the Export. For further information, see the section, To Export Data, in System Exporter . Enable all the workflows that are configured with Scheduling. Before importing Inter Workflow and Aggregation profiles, empty the Workflow data stream. Otherwise, these agent profiles will be overwritten by the profiles that are included in the imported bundle and might not recognize or reprocess data. Imported Workflow groups are disabled by default. You need to activate all the members, their respective sub-members, and the workflow group itself. When you import a User it is disabled by default. A User with Administrator permissions must enable the user and revise which Access groups the user should be assigned to. Imported Alarms are disabled by default. You enable an Alarm from the Alarm Detection. Imported Event Notifications are disabled by default. You enable an Event notification from the Event Notification Configuration. Importing from Remote Source To import data/configurations from a Git branch or AWS S3 bucket: Select the appropriate options according to your preferences by clicking the Options button. Click the Browse button to choose a File System Profile with type Git or AWS S3. Open Choose File System Profile Click the Select Source to choose the folder to import from. The following image shows the import using Git. Open The following image shows the import using Amazon S3. Open Continue from step 4 in Importing from Local Source . Options Click the Options button to open the Options menu. Open The Options menu The Options menu has multiple settings described in the following table: Setting Description Setting Description Import options Abort On Error Select this option to abort the import process if an error occurs. If an error occurs and you do not select this option, the import will be completed, but the imported data might contain erroneous components. Note! Invalid Ultra and APL definitions are considered erroneous and result in aborting the import. Import External Reference Database Values Select this option if you want to import external reference values. Select Dependencies Select this option to have dependencies follow along with the entries that you actively select. Preserve Permissions Select this option to preserve user permissions in the current system when importing a configuration. Clear this option to accept overwriting of user permissions in the current system. Import Configuration From Package Select this option if you would like to import the Workflow Package (MZP) as a configuration instead of a Workflow Package. New Owner Use this drop-down menu to reassign the ownership of configurations to another user, during an import. Workflow groups executions suppress option No Suppress Select this if you do not want to suppress the workflow group execution. Hold Execution Select this option to prevent scheduled Workflow groups from being executed while importing configurations. Restart For information, see systemimport . Stop and Restart For information, see systemimport . Stop Immediately and Restart For information, see systemimport . Wait for Completion and Restart For information, see systemimport . Beside t he Options button, these buttons exist: Button Description Button Description Expand All Select this option to expand all of the folders to display the folders and all the configurations that they contain. Collapse All Select this option to collapse the folders so only the folders are visible.

---

# Document 643: Authorization Server Storage Database Schema - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204648145/Authorization+Server+Storage+Database+Schema
**Categories:** chunks_index.json

When the Authorization Server storage is set to Database, users are required to create two new tables manually in the Database where the Authorization Server will store the scope and details of the client registration. Users can refer to the following example of a table schema to generate the tables for Authorization Server. Example - Table Schema CREATE TABLE oauth_scope ( scope VARCHAR(64) NOT NULL PRIMARY KEY, description VARCHAR(128) ); CREATE TABLE oauth_client ( client_name VARCHAR(128) NOT NULL PRIMARY KEY, client_id VARCHAR(64) NOT NULL, client_secret VARCHAR(64) NOT NULL, client_scope VARCHAR(2048) NOT NULL );

---

# Document 644: Executing Shell Commands When OS Level Access Is not Available - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744067/Executing+Shell+Commands+When+OS+Level+Access+Is+not+Available
**Categories:** chunks_index.json

MediationZone has some capabilities and activities that can only be performed with OS level access, either through direct shell access or through a shared folder, for example execution of mzsh commands or handling of External References property files. Using the APL Shell Script Execution Function ScriptExec can address most needs for shell commands that normally would require OS access. The ScriptExec function is executed as a part of the normal workflow execution principles so it can be used by any user who has be granted permissions to design workflow logic and execute workflows. This is done in 3 steps: Create the script that contains the command and its parameters. The script must match any required dependencies in the filesystem. Create a workflow that uses the ScriptExec function to call the script , run it, and capture the execution events. Execute the workflow and analyse the debug output for any errors. When successful, the command is executed in the same way as when regular shell command is been used. Any errors and information about the execution are shown in the workflow debug events.

---

# Document 645: Error Correction System - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032833
**Categories:** chunks_index.json

The Error Correction System (ECS) is used when UDRs fail validation and manual intervention is needed before they can be successfully processed. Batches are sent to the ECS using an APL command. To send UDRs, the ECS forwarding agent is required. To collect data from the ECS, the ECS collection agent is used. UDRs may be examined, deleted, or updated in the ECS Inspector. This section contains the following subsections: ECS Maintenance System Task ECS Forwarding Agent ECS Collection Agent ECS Inspection ECS Statistics Example - ECS handling of UDRs Example - ECS handling of Batches

---

# Document 646: Netia FTP Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738146/Netia+FTP+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about the Netia FTP collection agent transaction behavior. For information about the general transaction behavior, see Workflow Monitor . Emits The agent emits commands that change the state of the file currently processed. Command Description Begin Batch Emitted before the first byte of each collected file is fed into a workflow. End Batch Emitted after the last byte of each collected file has been fed into the system. Retrieves Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on workflow level is configured to abort the workflow, the agent will never receive the last Cancel Batch message. In this situation, ECS will not be involved, and the file will not be moved. APL code where Hint End Batch is followed by a Cancel Batch will always result in workflow abort. Make sure to design the APL code to first evaluate the Cancel Batch criteria to avoid this sort of behavior. Hint End Batch If a Hint End Batch message is received, the collector splits the batch at the end of the current block processed (32 kB), provided that no UDR is split. If the block end occurs within a UDR, the batch will be split at the end of the preceding UDR. After a batch split, the collector emits an End Batch Message, followed by a Begin Batch message (provided that there is data in the subsequent block).

---

# Document 647: IntegerField UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643700/IntegerField+UDR
**Categories:** chunks_index.json

The IntegerField UDR is used to create an input field that only accepts Integers. To create an integer field that only accepts even numbers between 0 - 100 this code can be used. IntegerField num = udrCreate(IntegerField); num.label = "Even Numbers"; num.step = 2; num.min = 0; num.max = 100; The following fields are included in the IntegerField UDR : Field Description attributes (map<string,string>) This field may contain extra attributes to be added. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap . disabled (boolean) This field may contain a boolean if the component should be disabled or enabled. id (string) This field may contain the id of the component label (string) This field may contain the label for the integer field. labelCssClasses (list<string>) This field may contain a list of extra values added to class attribute of the label. This is typically used to style the component. Please read more on Bootstrap . max (int) This field may contain a max value. min (int) This field may contain a min value. name (string) This field may contain the name of the component. If the component is present in a Form UDR , the name will be submitted with the form as the key in the Params Map in Request UDR . placeholder (string) This field may contain a placeholder can be used as a help text. readonly (boolean) This field may contain a boolean if the field is readonly. required (boolean) This field may contain a boolean if the component is required. Typically used inside a Form UDR. step (int) This field may contain a value to specifies the legal number intervals value (int) This field may contain a value.

---

# Document 648: mzcli - wfdisable - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979944/mzcli+-+wfdisable
**Categories:** chunks_index.json

Usage usage: wfdisable <pattern matching expression for workflow names> ... This command disables one or more workflows. Return Codes Listed below are the different return codes for the wfdisable command: Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect. 2 Will be returned if the user is not found or not logged in. 3 Will be returned if no matching workflow was found.

---

# Document 649: HDFS Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673462/HDFS+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent consumes bytearray or MultiForwardingUDR types. MIM For information about the MIM and a list of the general MIM parameters, s ee Administration and Management in Legacy Desktop . Publishes MIM Value Description File Transfer Timestamp This MIM parameter contains a timestamp, indicating when the target file was created in the temporary directory. File Transfer Timestamp is of the date type and is defined as a trailer MIM context type. MultiForwardingUDR's FNTUDR This MIM parameter is only set when the agent expects input of MultiForwardingUDR type. The MIM value is a string representing the sub path from the output root directory on the target file system. The path is specified by the fntSpecification field of the last received MultiForwardingUDR . For further information on using input of MultiForwardingUDR type, refer to HDFS Forwarding Agent Configuration . This parameter is of the string type and is defined as a batch MIM context type. Target Filename This MIM parameter contains the name of the target filename, as defined in Filename Template . Target Filename is of the string type and is defined as a trailer MIM context type. Target Pathname This MIM parameter contains the path to the output directory, as defined in the HDFS tab. Target Pathname is of the string type and is defined as a global MIM context type. Accesses This MIM parameter contains various resources from the Filename Template configuration that are accessed to construct the target filename.

---

# Document 650: NewConnection UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644043/NewConnection+UDR
**Categories:** chunks_index.json

When the Websocket Server agent receives a handshake request, it routes a NewConnection UDR into the workflow. The following fields are included in the NewConnection U DR: Field Description Field Description URI (string) This field contains the Uniform Resource Identifier of the client that has established a connection, in string format. securityInformation (TLSInformation (web soc ket)) This field contains information about the certificate chain and also which protocol and cipher suite are used.

---

# Document 651: HDFS Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000543/HDFS+Collection+Agent+Configuration
**Categories:** chunks_index.json

You open the HDFS collection agent configuration dialog from a workflow configuration. To open the HDFS processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select HDFS from the Collection tab of the Agent Selection dialog. Part of the configuration may be done in the Filename Sequence or Sort Order service tab described in Workflow Template . HDFS Tab The HDFS tab contains confi guratio ns related to the placement and handling of the source files to be collected by the agent. Open HDFS collection agent configuration - HDFS tab, General Item Description Item Description Profile Select the File System profile you want the agent to use, see File System Profile for further information about this profile. Collection Strategy If there is more than one collection strategy available in the system a Collection Strategy drop-down list will also be visible. For more information about the nature of the collection strategy please refer to Appendix 4 - Collection Strategies . File Information Settings Directory Enter the absolute pathname of the directory on the remote file system, where the source files reside. Filename Enter the name of the source files on the local file system. Regular expressions according to Java syntax apply. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILE , type: TTFILE.* Compression Select the compression type of the source files. Determines if the agent will decompress the files before passing them on in the workflow. No Compression - agent does not decompress the files. Default setting. Gzip - agent decompresses the files using gzip. Before Collection Settings Move to Temporary Directory If enabled, the source files will be moved to the automatically created subdirectory DR_TMP_DIR in the source directory, prior to collection. This option supports safe collection of a source file reusing the same name. Append Suffix to Filename Enter the suffix that you want added to the file name prior to collecting it. Important Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (hours) If the specified value is greater than zero, and if no file has been collected during the specified number of hours, the following message is logged: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Settings Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ) to the directory specified in the Destination field, after the collection. If the Prefix or Suffix fields are set, the file will be renamed as well. Note It is possible to move collected files from one file system to another however it causes negative impact on the performance. Also, the workflow will not be transaction safe, because of the nature of the copy plus delete functionality. If it is desired to move files between file systems it is strongly recommended to route the HDFS collection agent directly to a HDFS forwarding agent, configuring the output agent to store the files in the desired directory, HDFS Forwarding Agent . This is because of the following reasons: It is not always possible to move collected files from one file system to another. Moving files between different file systems usually cause worse performance than having them on the same file system. The workflow will not be transaction safe, because of the nature of the copy plus delete functionality. Rename If enabled, the source files will be renamed after the collection, remaining in the source directory from which they were collected (or moved back from the directory DR_TMP_DIR , if using Move to Temporary Directory ). Remove If enabled, the source files will be removed from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ), after the collection. Ignore If enabled, the source files will remain in the source directory after collection. Destination Enter the absolute pathname of the directory on the local file system of the EC into which the source files will be moved after collection. The pathname might also be given relative to the $MZ_HOME environment variable. This field is only enabled if Move to is selected. Prefix/Suffix Enter the Prefix and/or suffix that will be appended to the beginning respectively the end of the name of the source files, after the collection. These fields are only enabled if Move to or Rename is selected. Note If Rename is enabled, the source files will be renamed in the current directory (source or DR_TMP_DIR ). Be sure not to assign a Prefix or Suffix, giving files new names, still matching the filename regular expression, or else the files will be collected over and over again. Search and Replace Note To apply Search and Replace , select either Move to or Rename . Search : Enter the part of the filename that you want to replace. Replace : Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Note that the search value divides the file name into two parts by using brackets. The replace value applies the second part by using the place holder $2. Keep (days) Specify the number of days to keep source files after the collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note, a date tag is added to the filename, determining when the file may be removed. This field is only enabled if Move to or Rename is selected.

---

# Document 652: FTP DX200 Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738919/FTP+DX200+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about the FTP DX200 collection agent transaction behavior. For information about the general transaction behavior, see 3.1.11 Workflow Monitor . Emits The agent emits commands that changes the state of the file currently processed. Command Description Begin Batch Will be emitted right before the first byte of each collected file is fed into a workflow. End Batch Will be emitted just after the last byte of each collected file has been fed into the system. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on the workflow level is configured to abort the workflow, the agent will never receive the last Cancel Batch message. In this situation, ECS will not be involved, and the file will not be deleted.

---

# Document 653: Desktop Plugins - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676719/Desktop+Plugins
**Categories:** chunks_index.json

DTK provides the ability to integrate customer implemented user interface windows to be part of the Desktop. Open Desktop DRDesktopPlugin Desktop plugins are introduced by implementing the interface DRDesktopPlugin . The Code Server will locate all such classes and add them in a menu called Extensions in Desktop. The purpose of DRDesktopPlugin is to act as a pure representation class of the actual Swing implementation. It is therefore recommended to separate the plugin class and the actual Swing implementation, since all DRDesktopPlugins are instantiated during startup and may allocate memory from the JVM. Based on the result from the method getMenuName , the plugin can be put into its own submenu. When a user selects the menu item, the startPlugin method is called where the returned Java Swing internal frame is displayed. For a Desktop plugin example, see: com.digitalroute.devkit.examples.udrlister.*

---

# Document 654: Data Veracity Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032283/Data+Veracity+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the Data Veracity collection agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button You can select to collect data from an Error Code that has been defined in Data Veracity in Data Management or from a filter that has been saved in Data Veracity Search. The available settings in the Agent Configuration dialog depends on the the UDR type option. Data Veracity - UDR Type Data Veracity collection configuration for UDR type Setting Description Setting Description Profile Click Browse to select a predefined Data Veracity Profile. The profile contains details about the Data Veracity For further information, see Data Veracity Profile . Type UDR and Batch radio button here will be enabled according to the selected Data Veracity Profile. Select either UDR or Batch to determine if the agent will be collecting UDRs or Batch files from the Data Veracity tables. The Collection agent can only collect either UDR or Batch at any one time. Filtering Open Collection Configuration dialog for UDR type Click Add to configure the following Collection Configuration: UDR Type - This dropdown field lists the UDR Type based on the selected Data Veracity Profile Error Code - This dropdown field lists all the Error Codes available. For more information, see Error Codes . Saved Filters - This dropdown fields lists all the Saved Filters in the selected Data Veracity Profile. For more information, see Filters . Reprocessing Collection Size Collection Size is the value that defines how many UDRs will be collected before the collection agent finishes the current batch and starts a new one. The default value is 5000. The range for the value is 1 - 100000. This field is only available for collecting UDR. SQL Bulk Size To improve performance, data records are retrieved in bulk. The SQL Bulk Size value specifies how many records that will be included in each bulk. The default value is 1000. The range for the value is 1 - 100000. State This combo box displays a list of states that indicates which UDRs with the corresponding state to collect from the database. The choices presented in the list are NEW, UPDATED and NEW or UPDATED. The default value is NEW or UPDATED. This field is only available for collecting UDR. Read Only Select this option to have the agent retrieve the Data Veracity UDRs or Batch records without updating the state of the records from the database. Such as, UDRs that were collected with the state NEW will not have its state changed to REPROCESSED when Read Only is enabled. Data Veracity - Batch Type Open Data Veracity collection configuration for Batch type Setting Description Setting Description Profile Click Browse to select a predefined Data Veracity Profile. The profile contains details about the Data Veracity For further information, see Data Veracity Profile . Type UDR and Batch radio button here will be enabled according to the selected Data Veracity Profile. Select either UDR or Batch to determine if the agent will be collecting UDRs or Batch files from the Data Veracity tables. The Collection agent can only collect either UDR or Batch at any one time. Filtering Open Collection Configuration dialog for batch type Click Add to configure the following Collection Configuration: Error Code - This dropdown field lists all the Error Codes available. For more information, see Error Codes . Saved Filters - This dropdown fields lists all the Saved Filters in the selected Data Veracity Profile. For more information, see Filters . Reprocessing SQL Bulk Size To improve performance, data records are retrieved in bulk. The SQL Bulk Size value specifies how many records that will be included in each bulk. The default value is 1000. The range for the value is 1 - 100000. Read Only Select this option to have the agent retrieve the Data Veracity UDRs or Batch records without updating the state of the records from the database. Such as, UDRs that were collected with the state NEW will not have its state changed to REPROCESSED when Read Only is enabled.

---

# Document 655: Parquet Decoder Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608468/Parquet+Decoder+Agent+Configuration
**Categories:** chunks_index.json

You open the Parquet Decoder agent configuration dialog from a workflow configuration. To open the Parquet Decoder agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Parquet Decoder from the Processing tab of the Agent Selection dialog. Open Parquet Decoder agent configuration dialog for a batch workflow The only configuration setting for a Parquet Decoder is the choice of a Parquet Profile. Note! Choosing a profile for decoding is optional. Parquet documents include embedded schemas, so setting a schema with a profile is only needed when filtering the full set of columns - selecting a subset of the columns actually available in the document. Setting Description Setting Description Parquet Profile Optional choice. When set, this is the profile that specifies a Parquet schema for column filtering. For information on how to configure a Parquet profile , see Parquet Profile . To select a profile, click on the Browse... button, select the profile to use, and then click OK .

---

# Document 656: Web Service Profile Advanced Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675244/Web+Service+Profile+Advanced+Tab
**Categories:** chunks_index.json

In the Advanced tab, you can configure the advanced XML binding options. Open Web Service profile - Advanced tab See the text in the Properties field for further information about the XML binding options. If possible, avoid renaming . If you must rename the profile, it has to be saved again in its new location, regenerating the UDRs there. For further information about viewing the UDR type structure, see Web Service UDR Type Folder Structure .

---

# Document 657: Inter Workflow Batch Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000702/Inter+Workflow+Batch+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent consumes bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see the section Meta Information Model in Administration and Management in Legacy Desktop . Publishes The agent does not publish any MIM parameters. Accesses The agent accesses various resources from the workflow and all its agents to configure the mapping to the Named MIMs (that is, what MIMs to refer to the collection workflow).

---

# Document 658: Change Procedures - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677156/Change+Procedures
**Categories:** chunks_index.json

This section describes Spark-related procedures that are required for configuration changes in KPI Management. Updating the Spark Cluster- or Application Configuration Stop the KPI Management workflows Kill the Spark application Stop the Spark cluster Make whatever changes are needed Start the Spark cluster Submit the Spark application Start the KPI Management workflows Note! When you have changed properties in the configuration related to the service model or Kafka, you also need to clear the check-point directory before starting the Spark cluster. Updating the Kafka or Zookeeper Configuration Stop KPI Management workflows Kill the Spark application Shutdown Spark, Kafka and Zookeeper Clear all the runtime data. Update the Kafka and/or Zookeeper configuration Start Zookeeper, Kafka and Spark Submit the Spark application Start the KPI Management workflows

---

# Document 659: Function Blocks for the Python Collection Agent - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686625/Function+Blocks+for+the+Python+Collection+Agent+-+Batch
**Categories:** chunks_index.json

When writing code for the Python collection agent for batch, the function blocks in this section apply. For examples and further information on writing code in the Python collection agent, see Python Writer's Guide . The following function blocks are supported by the Python collection agent: Function Block Description def initialize() This function block initializes resources and state. def execute() This function block is the main entry point for collection. def commit() This function block is executed for each batch when the transaction is successful. def rollback() This function block is executed for each batch when a transaction fails. def stop() This function block is called when the workflow is about to stop. def deinitialize() This function block will cleanup resources and state. def cancelBatch() This function block is executed if a Cancel Batch is emitted anywhere in the workflow. def endBatchHinted() This function block is called when another agent has called hintEndBatch . The collection agent may choose to ignore this method, that is, not to implement it if it cannot be supported.

---

# Document 660: Platform Database Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669877/Platform+Database+Preparations
**Categories:** chunks_index.json

Make the necessary preparations for the platform database you intend to use. Derby Preparations Oracle Preparations Oracle RAC PostgreSQL Preparations SAP HANA Preparations

---

# Document 661: Notification UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674566
**Categories:** chunks_index.json

NotificationArgumentUDR This UDR inherits all the fields from PropertyUDR and is used in the NotificationAlertAmount , NotificationExpirationAlert , and NotificationUserAlert notifications types. NotificationUDR A notification describes an event, exceptional or not, that takes place in the instances. It provides information on where, when and what happened. This UDR is included in: CheckLimitResultUDR PurchaseOrderUDR StartSessionResultUDR StopSessionResultUDR TransactionSetUDR TransactionUDR UpdateSessionResultUDR. Field Description Field Description instanceId (string) The instance id where the event described by this notification took place timestamp (long) The date when the event described by this notification occurred descUid (int) Returns the unique identifier of the notification name (string) Returns the name of the notification prettyName (string) Returns the pretty name of the notification notifArguments (string) The arguments of the notification serviceProvider (string) The service provider of the subscriber account subscriberAccountCode (string) The code of the subscriber account code code (string) The user message of the alert

---

# Document 662: Workflow Validation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638608
**Categories:** chunks_index.json

Workflow configurations may be designed, configured, and saved step-by-step, but are still not valid for activation until fully configured and valid. A valid workflow configuration contains three types of configuration data: Workflow data : General information related to the workflow configuration, for instance, error handling. Workflow structure data : Contains the agents and routes. A route indicates the flow of data depending on the name of the route and the internal behavior of its source agent. Agent specific data : Each agent has a different behavior. Thus, each agent in the workflow configuration requires different configuration data in order to operate. When a workflow is saved, it is silently validated and if some of its configuration is invalid or missing, a dialog will state this and ask whether to still save the workflow or not. Validity is not necessary in order to save a workflow configuration. The workflow can be incomplete or the agent configuration can be faulty. The only exception is that all workflows in the workflow configuration must have unique names. When data is imported to the workflow table, the content is not validated, only the correct number of columns and types are checked. If validation errors occur during the import, the user is asked whether the import should be aborted or continued (that is, importing with errors). Aborting an import results in restoration/rollback to the previous table. How Workflow Validation Works When you click the ( Validate ) button, the workflow configuration validation is started. The validation is done in two steps: Validation of the workflow configuration. If the workflow configuration is invalid, an Information dialog is opened showing details, such as if configuration data or routes are missing, if referenced MIM resources are no longer available, or if configuration data in an agent is missing. The details can be changed by modifying the agent configuration, after clicking OK . If the workflow configuration is invalid the validation process ends there. Example of an invalid workflow Information dialog If the workflow configuration is valid, the validation of the workflow table starts. The values in the table are validated according to each agent's specifications. There is also a check that values have been added for all cells in the per workflow columns. The result is presented in a validation dialog and possible workflow errors are indicated in the workflow table. You can view the validation message for a specific workflow by selecting the corresponding action in the pop-up menu. If none of the workflows in the workflow configuration are valid, you get a dialog saying none of the workflows are valid. If it is not evident why the workflow(s) is erroneous, you can select one or more rows in the workflow table and then click on Validation Message to display a dialog with error message(s). Open Validation dialog for workflow table Note! External References are validated only during runtime. How to tell when the workflow is valid or invalid There is a label at the top of the workflow template, to the right of the workflow template name, that indicates if the workflow configuration is valid or invalid. Open Open Validity labels for workflow template For workflows, the workflow table for each row has the following symbols: Open which indicates that the workflow and related fields in that row are valid Open which indicates that the workflow and related values for the configuration in that row are invalid For example, data type can be invalid (if there are words in a numeric value only field), or a mandatory field could be empty. You can also tell if a workflow is invalid in the Configuration Browser in the Build view if the workflow configuration name is RED in color. Open Configuration Browser with an invalid workflow configuration

---

# Document 663: wfexport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/298614810/wfexport
**Categories:** chunks_index.json



---
**End of Part 30** - Continue to next part for more content.
