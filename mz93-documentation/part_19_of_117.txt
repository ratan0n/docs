# RATANON/MZ93-DOCUMENTATION - Part 19/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 19 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~68.2 KB
---

Data Hub allows MediationZone to integrate into Big Data solutions, by allowing the agent to forward large amounts of data to be stored in data lakes or data storage. Data Hub leverages Cloudera Data Platform (CDP) for storage access along with HDFS and Apache Impala. It also allows for any data manipulation, such as enrichment, formatting, normalization and correlation of data to be done before the data is sent into the big data storage. The stored data can then be made accessible and searched for using the Data Hub web UI. Data Hub task agent will allow for removal of old records by way of partitions in Impala. The removal of old data will be based on the date and time set in the task agent, and then have that value correlate with the values stored in the table partition. Currently all testing was done on CDP version 7.1.7. Overview of Data Hub solution Open Data Hub Profile The Data Hub Profile contains settings for Impala connection details, selecting databases, HDFS details as well as advanced configurations for LDAP and Kerberos. Example of Data Hub profile Open Currently, Data Hub supports connection to Cloudera with LDAP, Kerberos or both. The table below will indicate how we will support the LDAP and Kerberos authentication in the Cloudera framework. LDAP Kerberos No Authentication LDAP Kerberos No Authentication Impala Supported Supported Supported HDFS Supported Supported Mapping of the UDR's to the designated table in Impala is performed once the table name is selected from the Database section of the Impala Tab. The mapping can be done automatically if the field names in the ultra decoder matches the one in the Impala table. Example of Data Hub Profile - Tables Mapping Tab Open The Data Hub forwarding agent will be able to send any UDRs collected from any particular source and connect to the Impala table to insert and commit the UDRs. A standard Data Hub Forwarding workflow will like involve a collection agent, an analysis or aggregation agent to perform the UDR enrichment, followed by the Data Hub forwarding agent itself to output the UDRs into Impala. Example of Workflow with the Data Hub forwarding agent Open The Data Hub forwarding agent will perform the following series of tasks when initiated by the workflow. A temporary CSV file is created locally in MediationZone, where once the file is complete, it will be uploaded to the HDFS staging area. Following from that, the JDBC driver will call the Impala database to load the CSV file into the designated parquet table. It is only after the contents of the file is fully committed inside the table, does the agent remove the temporary file. Any workflow aborts will result in the temp file existing locally in MediationZone, much like the standard behavior of most forwarding agents. Data Hub Forwarding Agent process Open You can use the Data Hub profile to select the table that should be available for query or export data in one of the Impala tables specified in the Data Hub profile, without any knowledge about SQL. Example of Data Hub UI Open

---

# Document 408: Legacy KafkaUDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138683
**Categories:** chunks_index.json

KafkaUDR is the UDR that is populated via APL and routed to the Kafka Producer agent, which in turn writes the data to the specified partition, and the topic set in the Kafka profile. The Kafka Collection agent consumes the data from the Kafka log, from the specified partition(s). The topic is set in the Kafka profile, and places the data in a KafkaUDR . The following fields are included in the KafkaUDR : Field Description Field Description data (bytearray) Producer: This field holds data to be passed to the Kafka log by the Kafka Forwarding Agent (producer). Collector: This field is populated with the data read from the Kafka log. key (bytearray) This field can be used to set a key for the broker's messages. offset (long) This is a read only field. This field is populated by the Kafka Collection agent and contains the offset in the Kafka log from where the message was consumed. partition (short) Producer: This field holds the partition to which the Kafka Forwarding agent (producer) writes the message. If this field is not populated, the partition is chosen randomly. Collector: This field holds the partition from which the message was consumed by the Kafka Collection agent (consumer). timestamp (long) Producer: This is an optional field. A timestamp can be assigned before the UDR to a Kafka producer. Consumer: If you set a timestamp on the producer it will be shown in the corresponding consumer.

---

# Document 409: Workflow Table Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604819
**Categories:** chunks_index.json

This tab lets you select information to display in the workflow configuration table, and set specific table cells to different value modes such as Final , Default , or Per Workflow . The Workflow Table tab Item Description Item Description Field Selection Use this drop-down list to display the following in the Workflow Table: Fields of a certain agent Show All to show all the fields Show Final Select this checkbox to list in the table below only the fields that are set to Final . Show Unavailable Select this checkbox to list the fields that are write-protected. The unavailable fields are grayed-out and set to Final . Name The field names of the Workflow Table are listed in this column and include: Execution Settings Throughput MIM Debug Type Dynamic Fields All the agents and their fields in the workflow configuration Final Select this checkbox to block this variable from appearing on the workflow table, Adding Workflow dialog and Edit Workflow dialog. For more information regarding the workflow table, refer to Workflow Table . Note! This variable can still be modified, but only from its configuration. For example, if the variable belongs to an agent, open the agent configuration dialog to modify the variable. Default You can select this checkbox to set the field value to Default only if it is already set to a certain value in the configuration. You can modify a default value from the workflow table; the default value remains in the field and appears grayed-out, and the new value appears in black text on its left, within the same field. Per Workflow Select this checkbox to be able to set the value of the relevant field for each workflow on the workflow table, separately. Note! If you cannot set Per Workflow for profiles in the workflow table, refer to the user guide of the agent that the profile is assigned to for further information. Enable External Reference Enable this checkbox to enable the use of External Reference values from within the workflow table. For further information, see External Reference Profile . Profile Click Browse to specify the External Reference profile. For further information see the section To Create an External Reference Profile in External Reference Profile . To Edit the Execution Context Field: In the Workflow Table tab, select the checkbox for either Default or Per Workflow for the Execution Settings field. Click OK . In the Workflow Table, select the checkbox for the workflow you want edit. Click the Edit button on the table toolbar or right-click and select Edit to bring up the Edit Workflow dialog. Check the Enable checkbox if Execution Settings has not been enabled for the workflow properties. Otherwise, select the Distribution for that particular workflow as well as any EC Groups that you want to associate with this workflow. For a detailed description see Execution Settings in Execution Tab . Example of Edit Workflow dialog with Execution Settings set to Per Workflow in the workflow properties.

---

# Document 410: ECS Forwarding Workflow (batch) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032944/ECS+Forwarding+Workflow+batch
**Categories:** chunks_index.json

The forwarding workflow contains an Analysis agent which validates batches and sends them to the ECS in case of failure. Open The Analysis agent can call the cancelBatch function . For further information, see cancelBatch workflow function. Workflow Properties MIM values to be associated with the batch are mapped in the Workflow properties dialog. Also, the number of allowed cancelled batches is set here. Note that if Abort after one cancel batch is enabled, no batch is sent to the ECS if the workflow aborts. Open Workflow properties - Error tab The error UDR is handled from the Analysis agent. For further information, see Analysis Agent in ECS Collection Workflow (UDR) . APL code always overrides any Desktop settings. Hence, the set Error Code has no effect on this. ECS Inspection Automatic assignment to reprocessing groups is done in exactly the same way as for UDRs in section ECS Forwarding Workflow (UDR) , that is, via the ECS Inspector dialog ( Reprocessing Groups button). Make sure to select the appropriate Error UDR Type. Then the UDR fields will be included as MIMs in the collection workflow. Open Add Error Code dialog - where reprocessing groups can be selected Analysis Agent The Error UDR can be mapped from the Workflow properties dialog as well, however in this case APL code must be used, since other values than MIM values need to be inserted into the error UDR fields. Also, an Error Case must be assigned, and this is not possible from the Workflow properties dialog. For further information, see ECS Error Codes , and ECS Reprocessing Groups . Example - Mapping the Error UDR in APL code E.myErrorUDR eUDR = udrCreate( E.myErrorUDR ); eUDR.FileSize = (long)mimGet( "IN", "Source File Size"); eUDR.TS = (date)mimGet( "IN", "File Modified Timestamp"); eUDR.message = "PROCESSED ONCE."; udrAddError( eUDR, "switch_ERROR", "Switch not found."); cancelBatch( "Incorrect source.", eUDR ); Note! To send error UDRs with the batch is optional. However, it is necessary if access to application-specific information is needed when reprocessing the batch. Error UDR fields will appear as MIM values in the reprocessing workflow. Also, the only possibility to associate an Error Code with the batch is by appending an Error UDR.

---

# Document 411: SQL Loader Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002240/SQL+Loader+Agent+Transaction+Behavior
**Categories:** chunks_index.json

Transaction Behavior Emits The agent does not emit any commands. Retrieves Cancel Batch If a cancelBatch is emitted by any agent in the workflow, all data in the current transaction will be disregarded. No closing conditions will be applied.

---

# Document 412: Data Masking Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640439/Data+Masking+Agent
**Categories:** chunks_index.json

This section describes the Data Masking profile and the Data Masking agent. The agent is a processing agent for batch and real-time workflow configurations. The Data Masking agent can be used to either mask or unmask specified fields in different UDRs in order to protect the data. It can be used when data is going to be processed in the cloud without involving personal data, for example. The agent enables compliance with regulations around data protection, ensuring personal data is accessed in a controlled manner. Open The agent uses a profile in which you can define the masking method; Crypto, Database, or Hash, as well as which fields to mask/unmask along with additional settings specific for each masking method. When selecting Database, Oracle, Postgres, and SAP HANA can be used. In the agent itself, you can select which of the UDR types configured in the used profile(s) that you want the agent to process, as well as how you want the agent to handle unmatching data. Supported features: Encryption/decryption with AES-128 and AES-256 Reading a key from a specific JCEKS keystore Generation and storage of replacement data in a Database Converting input data to random data based on SHA-256 hash Use of multiple profiles in the agent Error handling Logging Unsupported Data Type for Search Searching, filtering, repairing and masking of UDRs with list and map data types are currently not supported by Data Veracity. The section contains the following subsections: Data Masking Profile Masking Methods Data Masking Agent Configuration Data Masking UDRs Data Masking Agent Input/Output Data and MIM Data Masking Agent Events

---

# Document 413: Amazon SQS Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/299663620/Amazon+SQS+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Events An agent event is an information message from the agent that can be used when configuring an event notification in an Event Notifications configuration, see Agent Event for more information about this event type. There are no agent events for this agent. Debug Events A debug event is an event that is dispatched when debug is used, and can be used when configuring an event notification in an Event Notifications configuration, see Debug Event for more information about this event type. There are no debug events for this agent.

---

# Document 414: Inter Workflow Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000657/Inter+Workflow+Agents
**Categories:** chunks_index.json

This section describes the Inter Workflow agents. The agents are used for temporary storage of stream data, merging stream data, and as gateways between real-time and batch workflows. Both agents are available in batch and real-time workflow configurations. The Inter Workflow agents allow files to be distributed between workflows within the same system. It is especially useful when transferring data from real-time workflows to batch workflows. The Inter Workflow agents use an Inter Workflow storage server to manage the actual data storage. The storage server can either run on an EC or on the Platform. The storage server and base directory to use are configured in an Inter Workflow profile. Open The Inter Workflow agents distribute files from one workflow to another Several forwarding workflows may be configured to distribute batches to the same profile, however, only one collection workflow at a time can be activated to collect from it. The Inter Workflow agents support IPv4 and IPv6 environments. Inter Workflow-Related UDR Type The UDR type created by default in the Inter Workflow agent can be viewed in the UDR Internal Format Browser in the InterWorkflow folder. To open the browser, open an APL Editor, right-click on the text pad in the editing area and select UDR Assistance... . The browser opens. The chapter contains the following sections: Inter Workflow Profile Inter Workflow Forwarding Agent Inter Workflow Collection Agent

---

# Document 415: GCP PubSub Publisher Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738975/GCP+PubSub+Publisher+Agent+Configuration
**Categories:** chunks_index.json

To configure the GCP PubSub Publisher Agent, double-click the agent or select the Edit agent button after left-clicking on the agent in the Workflow Editor. After opening the Agent Configuration , you can set a name for the agent using the Name field. Main Tab The GCP PubSub Publisher Main tab contains settings related to the profile and handling of messages for submission by the agent. Open GCP PubSub Publisher Configuration - Main tab Setting Description Setting Description Google GCP Profile This is the profile to use for subscription. For more information, see GCP Profile . Project This is to identify the project that contains the subscribed PubSub subscription and associated Topic. Topic This is to identify existing PubSub topics within a project. Enable Message Ordering Check this to publish messages in the same order as the orderingKey. Endpoint The custom URL that you must specify for using the GCP services. The Endpoint must be specified in the format host:port . Advanced Tab Open GCP PubSub Publisher Configuration - Advanced tab Setting Description Setting Description Group Messages Threshold This is to set the number of messages to be allowed in a batch for publishing. Group Milliseconds Threshold This is to set the time elapse from the first message retrieval before publishing subsequent batches in one instance. Group Kilobytes Threshold This is the threshold for messages to be published based on the configured kilobytes. Once the size is met, all messages within the threshold will be published in a batch. Max Outstanding Messages Maximum number of messages to be published from PubSub. Max Outstanding Kilobytes Maximum size for messages to be published from PubSub. Executor Threads The number of threads for each stream to handle outgoing messages.

---

# Document 416: vcexport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743964/vcexport
**Categories:** chunks_index.json

usage: vcexport [options] Options Options Description [-d, --directory] Use this option to specify in which directory you want to place the exported data. If the directory does not exist, it will be created. [-es, --excludesysdata] Use this option to exclude system data from the export. For example, you can exclude ECS related data, event categories, and workflow alarm values. [-f, --folders] Use this option to specify which folders you want to include in the export. For example, mzsh <user name>/<password> vcexport -d MyDirectory/ -f Default Alarms will export the configurations in the folders Default and Alarms to the directory MyFolder . If this option is not used, the configurations in all folders will be exported. [-im, --includemeta] Use this option to include meta data in the export. This will not be done by default, since not including meta data will make it easier to make a file compare between the exports. [-o, --overwrite] Use this option to enable existing exports in the stated directory to be overwritten. This command exports configurations in a format that is adapted for version control systems. The vcexport command exports the configurations into a flat structure, i e with file extensions instead of directories. For each exported configuration, a .xsd file will be generated in which the structure of the data is stored, which will produce a more compact format than the other export commands can offer. Return Codes Listed below are the different return codes for the vcexport command: Code Description Code Description 0 Will be returned if the export was successful. 1 Will be returned if the command could not be interpreted, e g if a option that does not exist has been entered. 2 Will be returned if the export failed. 3 Will be returned if the folder you want to export to, stated with the -d, --directory option, could not be created. 5 Will be returned if the folder(s) stated, when using the -f, --folders option, does not exist.

---

# Document 417: PCC Installation Instructions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029812/PCC+Installation+Instructions
**Categories:** chunks_index.json

Search this document: Chapters The following chapters are included: Architecture Overview PCC System Requirements PCC Installation Overview PCC Installation of Control Zone and Execution Zone for PCC Installation of the Data Repository for PCC High Availability in PCC This document describes the installation procedure for the Policy and Charging Control solution for 3GPP and non-3GPP environments. The installation procedure comprises two steps, see Installation Overview PCC for further information. Terms and Acronyms This section contains glossaries for all terms and acronyms used throughout the PCC and MediationZone documentation. PCC Terms and Acronyms Term/Acronym Definition Term/Acronym Definition PCC Policy and Charging Control PCRF Policy and Charging Rules Function 3GPP 3rd Generation Partnership Project General Terms and Acronyms For information about general Terms and Acronyms used in this document, see the Terminology document.

---

# Document 418: Data Distribution Report - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816802/Data+Distribution+Report
**Categories:** chunks_index.json

Below is an example of a customized data distribution report produced either as scheduled or on demand. Each entity creates its own log of what has been distributed to the next. Open Example of data distribution report

---

# Document 419: Workflow Packages - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030320/Workflow+Packages
**Categories:** chunks_index.json

Concept A Workflow Package is a configuration of workflows containing a set of configurations that can be bundled together and imported to different staging or integration environments. Once a certain configuration is exported from a current set, it can be assigned a version number thereby creating a version control system. Workflow Packages ar e used to facilitate the transportation of data between different environments, such as development, test, and production environments. The key features of Workflow Packages are the following: Version control : Development, test, and production environments are integrated into the same artifact repository, where they are version-controlled. Rollback : Changes are easy to roll back. Predictable : The same content is guaranteed in all environments. Dependent : Simple dependency handling. Workflow Packages are accessed and managed from the System Exporter ( Manage  System Exporter ). Workflow Packages Management In the System Exporter , set the Export type to Workflow Package . There are two main sections  the Available entries list on the left and the Logs screen on the right. Open Workflow Packages Selection Screen At the top to the right, there are buttons used to manage Workflow Packages. Open Workflow Package management buttons Button Description Button Description Export Opens the Workflow Package export pop-up dialog. Refresh Refreshes the Available entries list. Option Not available when the Workflow package is selected. Expand all Expands all available workflows in the entries list. Collapse all Collapses all available workflows in the entries list. To create a Workflow Package select the list of workflows to include and click the Export button. This opens a pop-dialog. Open Workflow package export dialog Setting Description Setting Description Package name Specify the name of the Workflow Package. Package version Designate the Workflow Package version. Only numbers can be entered. Output option You can specify either a Download or a Commit Workflow Package export format. You can cancel the Workflow Package creation by clicking Close at any time or create the Workflow Package by filling in all the necessary information and clicking the Export button. When a successful export has been created a pop-up dialog is shown with the title and download link of the Workflow Package. Follow the download link to retrieve a copy of the exported workflow, and Close it by clicking the button.

---

# Document 420: Extract Database Definition Files to SAP HANA - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736126/Extract+Database+Definition+Files+to+SAP+HANA
**Categories:** chunks_index.json

When SAP HANA and MediationZone are not installed on the same machine (the database may be installed on any machine within the network), the database definition files need to be transferred to the SAP HANA machine before the database can be created. The database definition files are created in the subdirectory ./database/saphana and archived in the file database-setup.tar file. On the SAP HANA machine (as SAP HANA UNIX user): transfer the database-setup.tar file from the MediationZone machine and extract the files: $ tar -xvf ./database-setup.tar Enter the database directory $ cd database

---

# Document 421: FTPS Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033236/FTPS+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Ready with file: filename Reported, along with the name of the target file, when the file is successfully written to the target directory. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For more information about the debug event type, see Debug Event . The agent produces the following debug events: Command trace A printout of the control channel trace either in the Workflow Monitor or in a file. Loading

---

# Document 422: FTPS Collection Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607649/FTPS+Collection+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about the FTPS collection agent transaction behavior. For information about the general transaction behavior, see Transactions in Workflow Monitor . Emits The agent emits commands that change the state of the currently processed file. Command Description Begin Batch This command is emitted before the first byte of each collected file is fed into a workflow. End Batch This command is emitted after the last byte of each collected file is fed into the system. Retrieves Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on workflow level (set in the workflow properties) is configured to abort the workflow, the agent never receives the last Cancel Batch message. In this situation ECS is not involved, and the file is not moved. APL code where Hint End Batch is followed by a Cancel Batch always results in workflow abort. Make sure to design the APL code to first evaluate the Cancel Batch criteria to avoid this sort of behavior. Hint End Batch If a Hint End Batch message is received, the collector splits the batch at the end of the current block processed (32 kB), provided that no UDR is split. If the block end occurs within a UDR, the batch is split at the end of the preceding UDR. After a batch split, the collector emits an End Batch Message, followed by a Begin Batch message (provided that there is data in the subsequent block).

---

# Document 423: Syslog Collection Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034765/Syslog+Collection+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The Syslog Collection agent produces SyslogMessageUDR s. MIM This agent does not publish or access any additional MIM parameters. For information about the MIM and a list of the general MIM parameters, see Meta Information Model in MIM .

---

# Document 424: Tables and Stored Procedures - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640360/Tables+and+Stored+Procedures
**Categories:** chunks_index.json

Working Table Following list holds information to be taken into consideration when creating the database table that a Database collection agent collects from, or a Database forwarding agent distributes to. The table must have a Transaction ID column , dedicated to the Database agent's internal use. The column could be named arbitrary however it must be numeric with at least twelve digits. It must also not allow NULL . Reading from or writing to columns defined as BLOB will have a negative impact on performance for both Database agents. It has been proved inefficient to put an index on the Transaction ID column . Entries with Transaction ID column set to -1 (Mark as Collected) or -2 (Cancel Batch) must be attended to manually at regular intervals. The following example shows a working table with a Transaction ID column named txn_id . Example - Working table with Transaction ID column named txn_id CREATE TABLE my_tab ( txn_id NUMBER(12) NOT NULL, a_num VARCHAR2(25) NOT NULL, b_num VARCHAR2(25) NOT NULL, duration NUMBER(10) ); After Collection Stored Procedure If a Database collection agent has been configured to call a stored procedure after collection, it will be called when each batch has been successfully collected and inserted into the workflow. The procedure is expected to take one (1) parameter. The parameter must be declared as a NUMBER and the agent assigns the current Transaction ID to the parameter. The procedure must ensure that the rows with the supplied transaction ID are removed from the table, or their Transaction ID column is set to -1. The following example shows such a procedure that moves the rows to another table. Example - After collection stored procedure CREATE or REPLACE PROCEDURE my_move_sp (txn IN NUMBER) IS BEGIN --copy collected rows to another table INSERT INTO my_collected_data_tab (txn_id, a_num, b_num, duration) SELECT txn_id, a_num, b_num, duration FROM my_tab WHERE txn_id = txn; --now delete the rows DELETE FROM my_tab WHERE txn_id = txn; END; Note! It is recommended for the previously described stored procedure to use an internal cursor with several commits, not to overflow the rollback segments. Database Forwarding Target Stored Procedure If a Database forwarding agent has been configured to use a stored procedure as the Access Type the agent will call this procedure for each UDR that is to be distributed. The stored procedure must be defined to take the parameters needed, often including a parameter for the Transaction ID. In the dialog these parameters are assigned their values. When the procedure is called, the agent will populate each parameter with the assigned value. The following example shows a stored procedure that selects the number of calls made by the a_number subscriber from another table, calls_tab , and uses that value to populate the target table. Example - Database forwarding target stored procedure CREATE OR REPLACE PROCEDURE my_insert_sp (a_num IN CHAR, b_num IN CHAR, txn IN NUMBER) IS BEGIN DECLARE cnt_calls NUMERIC(5); BEGIN SELECT COUNT(*) INTO cnt_calls FROM calls_tab WHERE anumber=a_num; INSERT INTO my_tab (from_num, to_num, txn_id, num_calls) VALUES (a_num, b_num, txn, cnt_calls); END; END; / Cleanup Stored Procedure If a Database forwarding agent uses a stored procedure to populate the target table, a cleanup stored procedure must be defined, that will remove all inserted entries in case of a Cancel Batch in the workflow. The procedure is expected to take one parameter. The parameter must be declared as a NUMBER and the agent will assign the current Transaction ID to the parameter. The following example shows such a procedure that removes all the entries with the current Transaction ID. Example - Cleanup stored procedure CREATE OR REPLACE PROCEDURE my_clean_sp (txn IN NUMBER) IS BEGIN DELETE FROM my_tab WHERE txn_id = txn; END; / After Forwarding Stored Procedure The following example shows a stored procedure that marks the row as safe to read by another system. Example - After forwarding stored procedure CREATE TABLE billing_data ( customer_id varchar2(100) NULL, number_of_calls number(5) NULL, money_to_pay number(9) NULL, txn_id number(12) NULL, txn_safe_indicator varchar2(10) DEFAULT 'UNSAFE' NOT NULL ); CREATE or REPLACE PROCEDURE mark_billing_data_as_safe (txn IN number) IS BEGIN LOOP --updates 5000 rows at the time to spare rollback segments update billing_data set txn_safe_indicator = 'SAFE' where txn_id = txn and rownum <= 5000; EXIT WHEN SQL%ROWCOUNT < 5000; COMMIT; END LOOP; COMMIT; END; The billing system must avoid reading rows that contains 'UNSAFE' in the txn_safe_indicator column, to ensure no data is read that could be rolled back later on.

---

# Document 425: Workflow Bridge Profile Configuration Advanced Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654807/Workflow+Bridge+Profile+Configuration+Advanced+Tab
**Categories:** chunks_index.json

In the Advanced tab you can configure additional properties for optimizing the performance of the Workflow Bridge. Depending on the selected transport method, Aeron or TCP, you will have s different set of properties. In the case of Aeron, the option to apply a pre-defined set of parameter values from a Template is available. Select Template type in the drop-down and press Apply to apply the template to the properties. Open The Workflow Bridge profile - Advanced tab Setting Description Setting Description Properties See the text in the Properties field for further information about the properties.

---

# Document 426: Access Control - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849317/Access+Control
**Categories:** chunks_index.json

MediationZone applications are managed on group level through the Access Controller application. Every application contains a set of base permissions, but can also define own permissions based on how they interoperate with the user. Every user is a member of one or several user groups. There are two standard groups available: All  All users are members of this group Administrator  Users of this group will have full authority, no matter the configuration For every MediationZone application, one or several groups can be granted various types of access. The following figure shows an example of users and permissions management in Access Controller. All actions performed by users are logged to the System Log. A user group is granted Maintenance access to all workflows. Since configuration changes should be controlled, an Event Notification configuration could be set up to create an entry in a log file each time the user updates a workflow. Open Users and permissions management in Access Controller

---

# Document 427: Querying A Table In Reference Data Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743433
**Categories:** chunks_index.json

To query a table, you can do the following: click the Get Started button from the initial dashboard page click the Query button on the result view. A Query dialog will open where you will configure the criteria for your query. Note! If a query was done previously, last applied query settings will be reloaded. If it is unable to fully reload, a notification will prompt users to verify and update the values accordingly. Opening the Query dialog will discard any uncommitted changes and refresh the Reference Data Profiles if there are any changes to the profiles. Open Full Query dialog with Query Options and Query Expressions Setting Description Setting Description Saved filters Allows you to manage saved filters within the Query dialog. Update - Allows you to modify existing saved filter by updating the options. To update, select the desired filter, edit the options and click Update from the Acrion Select the desired filter, edit the options, click Action > Update to update the saved filter. Delete - Allows you to delete saved filter. Select desired filter, click Action > Delete to delete the saved filter. Save New - You are only able to save new filter once you have selected a profile and a table. Upon saving, the filter will be added to the list of saved filters for future use. Action Contains the following actions to manage the saved filters: Update - Allows you to modify existing saved filter by updating the options. To update, select the desired filter, edit the options and click Update from the Action dropdown. Delete - Allows you to delete saved filter. To delete, select the desired filter, click Delete from the Action dropdown. Save New - You are only able to save new filter once you have selected a profile and a table. Upon saving, the filter will be added to the list of saved filters for future use. Reference Data Profile Allows you to select a Reference Data Profile that contains the table you want to query from. For more on Reference Data profiles, you can refer to Reference Data Profile . Search Type There are two types of search that you can select as follows: Standard - Query the results from a table based on the Reference Data Management profiles in the system. This query supports the following operations on query result: Insert Row Edit Delete Export Import Join Table - Query the results from join tables based on the Reference Data Management profiles in the system, supports following operations on query result: Export Table This option appears once you have selected a profile and Standard Search Type . This is a dropdown list of tables configured in the selected Reference Data profile. You select from one table in the list to perform your query on. Main Table This option appears once you have selected a profile and Join Table Search Type . This is a dropdown list of tables configured in the selected Reference Data profile. You select from one table in the list to perform your query on. The alias of the selected main table will always be t1 which will be used for Join Constraints. Join Conditions This option appears once you have selected a profile and Join Table Search Type . Add join conditions to combine two or more tables by selecting Table Name, Join Type and configure Join Constraints. The alias of the selected table will be stated next to the Table Name label, for e.g: The screenshot below indicates that t2 will be the alias of the table. The following join types are supported: JOIN INNER JOIN LEFT JOIN RIGHT JOIN CROSS JOIN Open Reference Data Management - Join Condition Dialog Join Constraint is used to specify the conditions for joining tables, for e.g: t1.column_id=t2.column_id. Show Columns This option appears once you have selected a profile and a table. This displays all the column names available in the selected table. You can remove and add the columns at your choosing to be displayed in the query result. Clicking on the All button will select all the columns in the table. All columns are selected by default. Data Set Size This option appears once you have selected a profile and a table. The value in this field controls the maximum row count per data set. The queried data is fetched from the Platform in data sets of configurable size. A large size typically results in a faster query but it may take longer to display the first data set. Note! Data set is the scope in which query results are displayed and modifications are applied. When switching data set, any uncommitted changes will be discarded. Users switch between data sets with buttons on the footer bar in the query result view. Query Expressions This option appears once you have selected a profile and a table. Query Expressions allow you to filter the query results according to the criteria you configure in this option. Click on New Expression to add one to the query. Sort Expression This option appears once you have selected a profile and a table, defaulted to first column in ascending direction. Sort Expressions allow you to sort the query results according to the criteria you configure in this option. Click on New Sort Expression to add additional expression. Query Expressions Open Query Expressions configuration in the Query dialog Setting Description Setting Description Column This is a column in the selected database table. Note! Only columns with supported data types are available for selection. Operator This is a comparison operator that is applied to the values in the specified column and the value. The available operators are: equals not equals like not like greater than greater than or equals lesser than lesser than or equals between Value This field contains a value that is used in the expression. When between is the selected operator, two value fields are displayed. You can use SQL wildcards such % and _ with the operators like and not like. An underscore (_) in the pattern matches exactly one character (as opposed to one byte in a multibyte character set) in the value. A percent sign (%) in the pattern can match zero or more characters (as opposed to bytes in a multibyte character set) in the value. The pattern '%' cannot match a null. Sort Expressions Open Sort Expressions configuration in the Query dialog Setting Description Setting Description Column This is a column in the selected database table which data needs to be sorted. Note! Only columns with supported data types are available for selection. Direction You can select to view the query results in the following manner: Ascending - Query result is sorted from lowest to highest value Descending - Query result is sorted from highest to lowest value Querying a Result To query a result: On the Query dialog, choose a Reference Data Profile by clicking on the Browse button. Look for and select the profile that has the tables you want to search from using the Select Reference Data Profile dialog. Open Reference Data Profile selection dialog Select a Search Type and a table from the Table dropdown list. The tables configured in the Reference Data Profile will appear here. Open Selecting a table from the Query dialog. Select the columns to include or view in the query results and configure the maximum data set value should you require it to be set to a different number. Open Selecting columns You can also add query expressions to refine the result of the query by filtering out certain data. If you did not configure any expressions, the entire table will be retrieved and up to the number set in Data Set Size will be displayed at a time in the query result view. Add sort expressions to specify sort criteria for retrieving data in ascending or descending order. Open Selecting Sort Expression column Click on Apply and the result will be shown in a view. The status bar at the top and the footer bar displays information about the current query configuration. Click on the status bar to reveal the list of configured query expressions if any. From this view, you can then query again, filter with the Search Bar , switch to another data set, insert a new row, edit the fields in the view, delete rows, refresh the view, export the data set, export the entire table or import from data from a CSV file. Note! Drag and drop columns to change the order of the columns. The result of Export action will match the order of the columns. Column sorting is allowed one column at a time. String and number sorting is supported and uncommitted inserted rows are excluded. Open An example of the query result view JSON Viewer The JSON Viewer is available to assist you by displaying JSON data in a more readable form. This is only available for columns that contain valid JSON format. Follow the steps below to view the JSON data. In the Query Result View , click the View Value button. Open Query Result View - View Value If the column contains a valid JSON format, the Viewing Value dialog contains the following tabs: Grid - Displays the JSON data in a grid form. Open Viewing Value Dialog - Grid tab (for valid JSON data) Text - Displays the JSON data in a text form. Open Viewing Value Dialog - Text tab (for valid JSON data) For columns that do not contain valid JSON format, the Viewing Value dialog will display the Text tab only. Open Viewing Value - Text tab (for invalid JSON data)

---

# Document 428: FTAM EWSD Interface Service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673119
**Categories:** chunks_index.json

The interface is a stand-alone program for Linux that communicates with MediationZone through the specified TCP port. The interface consists of the following binaries: start_ftaminterface - Starts the interface. status_ftaminterface - Reports status of the interface. stop_ftaminterface - Terminates the interface. ftamagent - Internal communication agent. Open It is important to start the interface by using a full path name. If the binaries are placed in /opt/mz/ftam/bin the same path must be used when the interface is started. The following command can be used to start the interface: $ /opt/mz/ftam/bin/start_ftaminterface ROOT_DIR=/var/ftamroot PORT=16702 The port name mzftam not found in /etc/services, the default value 16702 will be used. Binary directory: /opt/ftam_isode_if FTAM root directory: /var/ftam_root FTAM interface port: 16702 INFO: The interface has been started. $ Info! The binaries can be found in the installation package that you used to install MediationZone. They are located in: <release content directory>/release/complete/other/FTAM_LINUX_ISODE.tar. The release content directory will be where you have extracted your installation files from the tgz. Note! If you are using a 64-bit OS, you will be required to install the 32-bit versions of the OS system libraries. This is because the ftam binaries requires the 32-bit versions of the OS system libraries to be present. The root directory contains internal state information for recovery and log files.

---

# Document 429: Installation of the Data Repository for PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604374/Installation+of+the+Data+Repository+for+PCC
**Categories:** chunks_index.json

This section describes the installation of the data repository. Installing Data Repository for PCC This chapter includes the following sections and subsections: Installation of Couchbase Installation of Redis DirectoryStorage for Testing Installation of MySQL Cluster

---

# Document 430: Installation of Control Zone and Execution Zone for PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736306/Installation+of+Control+Zone+and+Execution+Zone+for+PCC
**Categories:** chunks_index.json

Install two Platform Containers with separated databases on two separate machines. These installations constitute the [CZ]. For each Platform Container, install an Execution Container. These installations constitute the [EZ]. Open Installing [CZ] and [EZ]

---

# Document 431: SCP Forwarding Agent MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674690/SCP+Forwarding+Agent+MultiForwardingUDR+Input
**Categories:** chunks_index.json

When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the FNT folder. The declaration follows: internal MultiForwardingUDR { // Entire file content bytearray content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is checked, if not a runtime error will occur. When MultiForwardingUDR s are expected configuration options in the Filename Template referring to bytearray input will be ignored. For information about Filename Template see Workflow Template . Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDRs . import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previous in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the user defined directory. The Create Non-Existing Directories check box under the Filename Template tab in the configuration of the forwarding agent must be checked if the directories do not exist.

---

# Document 432: Archiving Agents Transaction Behavior, Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999177/Archiving+Agents+Transaction+Behavior+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Transaction Behavior This section includes information about the Archiving agents' transaction behavior. For information about the general transaction behavior, see 3.1.11 Workflow Monitor . Emits The agents do not emit any commands. Retrieves The agents retrieve commands from other agents and based on them generate a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, if the temporary directory DR_TMP_DIR is not already in the base directory, the agent creates it. Then, the agent creates a target file in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is closed. Finally, the file is moved from the temporary directory to the target directory. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory. Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The Archiving agent consumes bytearray types. The Local Archiving agent consumes either bytearray or MultiForwardingUDR types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes The agents publish the following MIMs: MIM Value Description Target Filename This MIM parameter contains the name of the target filename, as defined in Filename Template. Target Filename is of the string type and is defined as a trailer MIM context type. MultiForwardingUDR's FNTUDR This MIM parameter is set only when the agent expects input of MultiForwardingUDR type. The MIM value is a string that represents the sub- path from the output root directory on the target file system. The path is specified by the fntSpecification field of the last received MultiForwardingUDR . For further information about the MultiForwardingUDR type, see Archiving Agents MultiForwardingUDR Input . This MIM parameter is of the string type and is defined as a batch MIM context type. Accesses Various MIM resources are accessed depending on the MIM value selection in the Agent Directory Name and Logged MIM Data lists. The MIM values are read at End Batch.

---

# Document 433: UI Builder Examples - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654559/UI+Builder+Examples
**Categories:** chunks_index.json

This section provides the following examples of UI created using UI Builder Agent. 1 FileBrowser 2 DB Viewer 3 Grid Example FileBrowser This example presents files from directories accessible by the Execution context running the Workflow. . Open Open FileBrowser workflow The FileBrowser workflow consists of one UI Builder Agent and two Analysis Agents. The Buildpage analysis agent builds up the table to present the page. The buildResponse Agent adds common static parts to the response before it is sent back to the UI Builder. Open The export consists of the filebrowser workflow and some APL constants. In APL constants there is a Map that consists of an alias and a real file system path. In the export, there is only one alias called MZHOME with path /opt/mz . This can be changed to reflect the correct path on the running system, there can also be added more aliases and paths. All aliases will be added as menuItems in the responseUDR. DB Viewer The DB Viewer example, shows how a UI can be created to display data from a database. The example consists of two views, Table view and Query view. This example is only tested on Postgres, it might need to be changed to work with other databases. In the Table view, the table name is read from the Ext_Ref profile and you can apply a filter to the query by filling in the input fields. More filter rows are added by clicking Add Filter . This is performed by using Javascript added in the buildResponse Agent. Open Table View In Query view, you can write the SQL queries. Only the Select statement is allowed. Open Query View The DB Viewer workflow consists of one UI Builder agent and two Analysis Agents. The buildPage analysis agent builds up the views and sends the request to the database. The buildResponse Agent adds common static parts to the response before sending it back to the UI Builder. Open DB Viewer Workflow The export consists of five configurations. In the Ext_Ref profile, the default table name is present, and the default limit of rows. The DB_Prof handles the connection to the database, this needs to be changed to match a real database. FilterType is an Ultra format for the filter in the Table view. Open Configuration in Export Grid Example The Grid Example, is a simple example to demonstrate how the grid works. The workflow produces two grids on a single page. Both grids have four rows and different numbers of columns, but the second grid has a responsive design, which means, it adopts smaller screens. Open

---

# Document 434: System Administrator's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205815899/System+Administrator+s+Guide
**Categories:** chunks_index.json

Search this document: This document describes the MediationZone system and its various options regarding maintenance, adjustments, and security that you need to know as a system administrator. Chapters The following chapters are included: System Overview log4j APL Logging Configurations HTTP Proxy Support Basic Administration Command Line Tool Network Security Java Configuration High Availability Setup Backup and Disaster Recovery Procedures Database Configuration Couchbase Launcher Service Interface PCC System Administration Setting up Prometheus and Grafana Log Forwarding

---

# Document 435: Data types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612125/Data+types
**Categories:** chunks_index.json

The following table describes the different data types available in APL. The second column states if the data type is a primitive, or an object. The third column indicates the default value that an uninitialized variable will obtain. Data type P/O Def val Range Description Example boolean P false 1 Byte Boolean value ( true|false ). boolean aBool=true; byte P 0 1 Byte, -128 to +127 Single byte integer value. byte aNumber=127; char P '0' 1 Byte A single character value. char aCharacter='a'; short P 0 2 Bytes, -32,768 to +32,767 Short integer value. short aNumber=31000; int P 0 4 Bytes, -2,147,483,648 to +2,147,483,647 Integer values have normal Java syntax, for instance: 42, 0x2A An integer literal is always of the most narrow integer type that can contain the value. This normally works as expected, since numeric types can be implicitly converted to any wider numeric type. int aNumber=2147483640; float P 0/0 4 Bytes, 1.40129846432481707e-45 to 3.40282346638528860e+38 (positive or negative) Single-precision floating point. float aValue=0.5; double P 0/0 8 Bytes, 4.94065645841246544e-324d to 1.79769313486231570e+308d (positive or negative). Double-precision floating point. When using floating-point literals, (that is, float (f) or double (d) values) all floating-point literals without trailing d is considered float literals. Note! For instance 0.5 and 0.5d will not give identical values Double types may contain positive or negative infinity. double aValue=432482943.1; double POS_INFINITY=1.0/0.0; double NEG_INFINITY=-1.0/0.0; long P 0 8 Bytes, -263 to +(263 -1) Long integer value. long aNumber=92233720368547; bigint O 0 Unlimited Provides storage for any integer value. bigint aNumber; bigdec O 0 Unlimited Provides storage for any integer value with a decimal point. Values assigned to bigdecimal variables in APL must be suffixed with b. Example bigdec aNumber = 123.4567b; bigdec aNumber; string O null Unlimited Values assigned to string variables in APL must be surrounded with double quotes. A character preceded by a backslash () is an escape sequence and has a special meaning to the compiler. The following sequences are available: t - Tab b - Backspace n - Newline r - Carriage return f - Formfeed ' - Single quote character " - Double quote character  - Backslash character Some string comparison functions use regular expressions. If special characters such as "*", "?" are to be used as characters in the regular expression, they must be escaped with two backslashes in the APL since these strings will be parsed twice. For instance, the following function will return the index of the question mark in the string: strREIndexOf("Am I right?", "?") String values surrounded with triple double quotes are referred to as multiline strings. You can span the content of these string across line boundaries. Multiline strings does not support, or need, escape characters. All characters, including double quotes, are assigned as they appear. string aString = "foobar"; string mlString = """{ "key1": "value 1", "key2": "value 2" }"""; any O null Can be assigned any value of any type. any aVar; aVar = 1; aVar = "foobar"; bitset O null Represents a bit string that grows as needed. The bits are indexed by non-negative integers. bitset aBitset; bytearray O null Bytearray type. bytearray anArray; date O null Holds date and time, including the timezone. date aDate; ipaddress O null Holds IP address information. Both IPv4 and IPv6 are supported. ipaddress anIp = ipLocalHost(); list <type> O null List of objects of the specified type. If a list of lists is to be declared the ">" characters must be separated by at least one space, not to conflict with the ">>" operator. That is, list<list<int> >intLists; list<int>intList; map O null Hash maps allowing key/value pairs to be defined. map<string, int> aMap = mapCreate(string, int); table O null Special type used to hold table data used by the table commands. table aTable = tableCreate("Default", anSQL); UDRType O null Holds a reference to a UDR. The UDRType must be a defined format or compilation will fail. There is also a special format available for generic UDRs called drudr and is of drudr type. MyDefinedType aUDR; drudr anyUDR; uuid O null 128 bits Immutable universally unique identifier (UUID). uuid u=uuidCreateRandom(); The variable types follow Java standard regarding object reference vs. value semantics. All types use value semantics when compared through the '==' and '!=' boolean operators and objects use reference semantics for all other operators, like assignments '='. Example - Reference Semantics Two date variables (objects) and two long variables (primitives) are used to illustrate the reference semantics. In the case of date variables, they will always contain the same value regardless of being updated via myDate_A or myDate_B since they point to the same object. date myDate_A = dateCreateNow(); date myDate_B = myDate_A; long myLong_A = 10; long myLong_B = myLong_A; if (myDate_A == myDate_B ) { // true } if (myLong_A == myLong_B) { // true } // Change value on A variables dateAddHours(myDate_A, 22); if (myDate_A == myDate_B) { // still true, points to same object } myLong_A = 55; if (myLong_A == myLong_B) { // false, primitives always hold their own values } Type Casting It is possible to cast between different types with regular Java syntax. The instanceOf function is used to evaluate dynamic types since an incorrect type conversion will cause the workflow to abort with a runtime error. Example - Type casting /* APL code for two UDR types */ consume { if( instanceOf( input, UDRType1 ) ) { UDRType1 udr1 = (UDRType1) input; // Handle UDRType1 ... } else { UDRType2 udr2 = (UDRType2) input; // Handle UDRType2 ... } } As an extension to the regular conversions, all types may be casted to string type. Note that some types are subtypes of other types and can be directly assigned. Examples are: All types are subtypes of the any type. int is a subtype (narrowing) of long . All UDR types are subtypes of the general base type drudr . Some UDR types are subtypes of other UDR types (depending on Ultra definitions). There is also a number of built-in type conversion functions (refer to the section below, Access of UDR Fields). Access of UDR Fields UDR fields are accessed with the '.' separator. For instance, to access the field myField within the UDR type variable theUDR , the syntax would be: theUDR.myField = 5; debug( theUDR.myField ); If the field name is a variable instead of a string, udrGetValue/udrSetValue can be used to set/get field values dynamically. Access of non-present fields will return the default value for the specific type. Field presence is evaluated with udrIsPresent. When writing to a UDR field on a null object, a NullPointerException will be thrown, see example: Example - Access UDR fields myUDR theUDR = null; int a = theUDR.myField // Will Not throw NullPointerException theUDR.myField = 0; // Will throw NullPointerException To avoid exceptions, first make sure that the UDR is not a null object: Example - Check for null value if( theUDR != null ) { theUDR.myField = 0; } For further information about UDR related functions, see UDR Functions . Access of Input UDR Within the consume block, the incoming UDR can be accessed using the special variable input . The instanceOf function may be useful in case of dealing with several UDR types.

---

# Document 436: Prometheus Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739862/Prometheus+Forwarding+Agent+Configuration
**Categories:** chunks_index.json

To open the Prometheus forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select Prometheus in the Forwarding tab in the Agent Selection di alo g. To publish metrics using the Prometheus forwarding agent, create a Prometheus UDR and send it to the agent. For further information on the Prometheus UDR, see Prometheus UDR Type . Each UDR results in one cached metric, to be scraped by a Prometheus instance. In case the cache becomes full, the oldest metric is removed when a new one arrives. Only numeric values can be used to populate the value field of the Prometheus UDR.

---

# Document 437: Suppressed Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670812/Suppressed+Event
**Categories:** chunks_index.json

This event is triggered by a system import with the Hold Execution setting enabled. The following fields are included: Name - The name of the workflow group that is mentioned in the eventMessage . Message - A textual description of the events that take place while systemimport -holdexecution is executing. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents eventName origin receiveTimeStamp severity timeStamp

---

# Document 438: Encoder Agent Services Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641019
**Categories:** chunks_index.json

The agent utilizes two specialized services allowing the user to add header and trailer information into each data batch. They both offer the possibility of using MIM values, constants, and user-defined values in the header or trailer. When selecting MIM resources, note that MIM values used in the data batch header are gathered when a new batch begins, while MIM values used in the data batch trailer are gathered when a batch ends. Thus, the numbers of outbound bytes, or UDRs, for any agent will always be zero if they are referred to in data batch headers. The windows for both header and trailer configuration are identical. Open Encoder configuration dialog - Header tab Setting Description Setting Description Suppress On No Data Indicates if the header/trailer will be added to the batch even if the batch does not contain any data (UDRs or byte arrays). Value Click on the Add button to populate the columns with items to the header or trailer of the file. They will be added in the order they are specified. Open Add Header/Trailer Content dialog Setting Description Setting Description MIM Defined If enabled, a MIM value will be part of the header. Size and Padding must be entered as well. Note! For data batch headers, the MIM values are gathered at beginBatch . User Defined If enabled, a user defined constant must be entered. If Size is empty or less than the number of characters in the constant, Size is set to the number of characters in the constant. If Size is greater than the length of the constant, Padding must be entered as well. Pad Only If enabled, a string is added according to the value entered for Size , filled with Padding characters. Size Size must always be entered to give the item a fixed length. It can only be omitted if User Defined is selected, in which case it will be calculated automatically. Padding Character to pad any remaining space with. Either a user defined character can be entered, or one of the four predefined/special characters can be selected (Carriage return, Line feed, Space, Tabulator). Alignment Left or right alignment within the allocated field size. Date Format Enabled when a MIM of type date is selected. A Date Format Chooser dialog is opened, where a date format may be entered.

---

# Document 439: Diameter Configuration and Design Considerations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606975/Diameter+Configuration+and+Design+Considerations
**Categories:** chunks_index.json



---
**End of Part 19** - Continue to next part for more content.
