# RATANON/MZ93-DOCUMENTATION - Part 7/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 7 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.9 KB
---

This section describes the Encryption agent. The agent is a processing agent for batch and real-time workflow configurations. The Encryption agent can be used to either mask or unmask complete files in order to protect the data. It can be used when data is going to be processed in the cloud without involving personal data, for example. The agent enables compliance with regulations around data protection, ensuring personal data is accessed in a controlled manner. Open An example Encryption Agent Workflow The agent uses a profile in which you can make your encryption configurations, see Encryption Profile for further information about this profile. In the agent itself, you can determine whether the agent should encrypt or decrypt files and if you want the agent to handle empty data or not. The section contains the following subsections: Encryption Profile Encryption Agent Events Encryption Agent Configuration Encryptor Agent Input/Output Data and MIM

---

# Document 141: Reference Data Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612029/Reference+Data+Profile
**Categories:** chunks_index.json

In a Reference Data profile configuration, you can select the tables that should be available for query and editing via the Reference Data Management dashboard or RESTful interface. For information about these interfaces, refer to the Reference Data Management Dashboard or the RESTful Interface for Reference Data Management . This profile is loaded when you start the Platform. To create a new Reference Data profile configuration, click the New Configuration button from the Configuration dialog available from Build View , and then select Reference Data Profile from the menu . The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently active tab. The Reference Data profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . To open an existing Reference Data profile configuration, click on the configuration in the Configuration Navigator, or right-click on the configuration and then select View Configuration . General The General tab is displayed by default. Open Reference Data profile - General tab Setting Description Setting Description Database This is the database the agent will connect and send data to. Click the Browse... button to get a list of all the database profiles that are available. For further information see Database Profile . If changes have been made in the selected database, click the Refresh button to retrieve updated db metadata that will be reflected in the table selector when adding entries into the Tables section of the configuration dialog. The Reference Data profile is supported for use with the following databases: Oracle PostgreSQL MariaDB SAP HANA Enable Enhanced Logging When you perform any of the following operations, entries are written to the System Log, and DB Ref event notifications are generated: Commit changes to the database Perform import operations Select Enable Enhanced Logging to include information about executed SQL statements in the event notifications. For more information about DB Ref events, see DB Ref Event . Tables Configurations of selected tables are listed here. Open Reference Data Profile - Add Table Dialog Setting Description Setting Description Table Select a table for the selected database with this table selector. Read Select this check box to enable read access to the selected tables. Users of the profile can query and export data. Write Select this check box to enable write access to the table data. If Read is also selected, users of the profile can insert, update, delete and import data. Last Update Select this check box to enable the Last Update feature. Last Update allows users to specify two columns in the table that will be used to automatically record the username and timestamp of when an entry was updated. Note! When creating tables to include the User column and Timestamp column, ensure that these columns are not UNIQUE and does not have any PRIMARY KEY constraints. The datatype for the User column should be set as VARCHAR and datatype of the Timestamp column should be set as TIMESTAMP. User Column This is to specify the name of the User column to be used for the Last Update feature. This column will be updated with the name of the last user to perform an operation on the record. This column can not be edited from Reference Data Management UI Timestamp Column This is to specify the name of the Timestamp column to be used for the Last Update feature. This column will be updated with the the timestamp when performing an operation on the record. This column can not be edited from Reference Data Management UI To Add a Table Note! Reference Data Management does not support tables that contain a column that has both GENERATED ALWAYS AS IDENTITY and PRIMARY KEY constraints. Having one or the other constraint in a column in your table will not affect Reference Data Management. Note! Modifications to a reference data entry are supported either on Oracle (based on ROWID pseudo column) or Postgres tables containing Primary Key constraint. Postgres tables without a Primary Key are not supported for data modifications. Note! Reference Data Management does not support the use of case sensitive identifiers. Click on the Add button. Select a table from the drop down list. Always select the Read access check box. If you want to enable editing of the table table, also select the Write access check box. If you want to enable Last Update, select the Last Update check box. Then fill up the User column and Timestamp column with the column names of your choice. Advanced In the Advanced tab, you can configure additional properties. You can use these parameters to tune the performance of database operations. Open Reference Data profile - Advanced tab prefetchSize - This setting defines the number of rows retrieved from the database in a single network round trip. Increasing the fetch size can improve query performance by reducing the number of trips between the application and the database, which is especially beneficial over high-latency connections. However, setting the fetch size too high may lead to increased memory consumption and potential out-of-memory errors. If prefetchSize is set to 0, no limit will be applied. The default value is 1000. maxRowsPerQuery - This setting sets the maximum number of rows returned for each query or table. It directly affects operations such as exporting results, for example, when using Manage  Reference Data Management  Export  Export Retrieved Rows (export query results). The default value is 50000.

---

# Document 142: HTTP Batch Appendix - Database Requirements for Duplicate Check - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033447
**Categories:** chunks_index.json

The Duplicate Check feature stores the collected URLs in an external database pointed out by a Database profile. The schema of this database must contain a table definition that matches the needs of the agent. Table and Column Names The schema table name must be "duplicate_check". It must contain all the columns from this table: Table column Description txn The transaction id of the batch that collected the URL (in the case the file is split into several chunks using hintEndBatch, it is the last and final transaction id.) tstamp The timestamp when the URL was committed by the workflow. workflow_key A uniquely identifying id of the workflow collecting the URL. It allows workflows to be renamed without changing the table data. url The full absolute URL collected. Column Types The column types are defined by how the specific JDBC driver converts JDBC types to the database. The txn column is a JDBC VARCHAR. The tstamp column is a JDBC TIMESTAMP type. The workflow_key and url columns are of JDBC VARCHAR type. Oracle Example Oracle Example <![CDATA[-- Table definition usable for ORACLE CREATE TABLE duplicate_check( txn long, tstamp timestamp, workflow_key varchar2(32), url varchar2(256) ); ]]>

---

# Document 143: FNTUDR Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656844/FNTUDR+Functions
**Categories:** chunks_index.json

The functions described below operate on values in UDRs of type FNTUDR. The value in an FNTUDR is a delimited string, representing file system paths. The FNTUDR types are usually used in two different cases: Creation of a dynamic path for the output file from a disk oriented forwarding agent. Using the filename template in the forwarding agent the FNTUDR value is, via a MIM resource, used to publish the respective MIM value. For further information about using the FNTUDR in filename templates, see the relevant agent documentation in the Desktop User's Guide . Grouping of output data using a disk oriented forwarding agent set to expect input of MultiForwardingUDR type. The MultiForwardingUDR has two fields, the first contains the data that will be written in the output file, the second contains filename and path specified by the FNTUDR. Both fields are mandatory unless the Produce Empty Files check box is selected in the agent, in this case the data field is not required. For further information about using the MultiForwardingUDR , please refer to respective file based forwarding agents documentation. Example - How an FNTUDR is constructed The following APL code shows how a FNTUDR is constructed with the help of the functions that are described in this section. import ultra.FNT; consume { FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, "ready_folder"); fntAddDirDelimiter(fntudr); date now = dateCreateNow(); fntAddString(fntudr, ""+dateGetYear(now) + "-"); fntAddString(fntudr, ""+dateGetMonth(now), 2, "0", false); fntAddString(fntudr, "-"); fntAddString(fntudr, ""+dateGetDay(now), 2, "0", false); } The fntudr variable will have a value corresponding to a path like r eady_folder/2018-10-31 , where the character '/' represent a directory delimiter that can be any character, depending on the target system. The following functions for FNTUDR described here are: fntAddString The fntAddString function appends a text string to the specified FNTUDR . void fntAddString( FNTUDR fntudr, string str, int size, (optional) string padding, (optional) boolean leftAlignment (optional) ) Parameter Description fntudr The FNTUDR that the text string is going to be added to. A runtime error will occur, if the parameter is null. str The string that will be added. If the string is null a runtime error will occur. The string can be extended or truncated if the size parameter is specified and the string does not match. Note! Do not include directory delimiters, e g "/", in the string. To add delimiters, use the fntAddDirDelimiter function instead. size The optional parameter size defines a fixed size for the appended string. A runtime error will occur, if the size isn't greater than zero. padding The optional parameter padding defines a padding string that will be repeated to fill the string to the size specified in the size parameter. A default padding will be used if the argument isn't specified or if the padding string is null or an empty string. leftAlignment The optional parameter leftAlignment specifies if the padding shall be kept right or left of the string. If the parameters value is true the string will be left aligned and the padding will added to the right. If the parameter isn't specified, the default setting is left aligned. Returns Nothing fntAddDirDelimiter The fntAddDirDelimiter function adds a directory delimiter to the end of the FNTUDR . void fntAddDirDelimiter(FNTUDR fntudr) Parameter Description fntudr The FNTUDR that the text delimiter is going to be added to. A runtime error will occur, if the parameter is null. Returns Nothing Loading

---

# Document 144: Azure Event Hub Producer Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738238/Azure+Event+Hub+Producer+Agent+Configuration
**Categories:** chunks_index.json

To open the Azure Event Hub Consumer agent configuration dialog from a workflow configuration, you can do one of the following: double-click the agent icon select the agent icon and click the Edit button Open Azure Event Hub Producer agent configuration Setting Description Setting Description Profile Select the Azure profile that has been configured with the connection details for the Azure Event Hub. For more information about how to configure the profile, refer to Azure Profile . Partition Enter the partition id of the event hub to be used by the agent. This field is optional.

---

# Document 145: Netia FTP Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640088/Netia+FTP+Agent
**Categories:** chunks_index.json

This section describes the Netia FTP collection agent. It is a collection agent for batch workflow configurations. Prerequisites The reader of this information should be familiar with: Standard FTP (RFC 959, http://www.ietf.org/rfc/rfc0959.txt ) This section contains the following subsections: Netia FTP Agent Overview Netia FTP Agent Configuration Netia FTP Agent Input/Output Data and MIM Netia FTP Agent Transaction Behavior Netia FTP Agent Events

---

# Document 146: Avro Decoder Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606910/Avro+Decoder+Example
**Categories:** chunks_index.json

This example shows how you can configure Avro Decoder/Encoder to be used together with a schema registry, providing Avro schema. To read more about UDRs used in the example see https://infozone.atlassian.net/wiki/spaces/MD93/pages/205684981 Example configuration: To simplify the setup, the configuration includes a workflow simulating a schema register that returns schemas. See the workflow configuration for the schema content. Open Disk_1 This agent collects files containing one complete binary encoded Avro message. This example assumes that each file contains one Avro message. You can use the following file as input for the Disk_1 agent Open Analysis_1 This agent is responsible for creating an AvroDecoderUDR and filling in the necessary information that are required for Avro Decoder: APL code: consume { debug(input); //Create an udr that will be an input for the decoder Avro.AvroDecoderUDR myUdr = udrCreate(Avro.AvroDecoderUDR); //Binary encoded avro message read from a file myUdr.data = input; myUdr.readerSchemaID = 7; myUdr.writerSchemaID = 7 debug(myUdr); udrRoute(myUdr); } Decoder_1 This is an example of how you can configure an Avro Decoder and a schema register. Open Analysis_2 agent This agent shows an example of how one can read a decoded Avro message and create an AvroEncoder UDR that is an input for the Avro encoder. Reading decoded Avro message is done by inspecting DynamicAvro UDR data field. The type of it depends on an Avro schema used for decoding. This examples shows how to access elements of different types. Preparing data for encoding is done by populating AvroEncoder UDR. APL code: consume { //debug(input); if (instanceOf(input, DecoderErrorUDR)) { abort("Received DecoderErrorUDR"); } else { DynamicAvro dynamicAvroUdr = (DynamicAvro)input; AvroRecordUDR inp = (AvroRecordUDR)dynamicAvroUdr.data; map<string,any> fields = (map<string,any>)inp.fields; debug("MyDouble value: " +mapGet(fields, "myDouble")); //debug(mapGet(fields, "rootUsers")); //Get an enum debug(mapGet(fields,"favorite_fotball_team")); //Print the whole list debug(mapGet(fields, "favoriteFoodList")); //Get the first list element debug(mapGet(fields, "favoriteFoodList").fields.dish); //Preview the second list element debug(mapGet(fields, "favoriteFoodList").fields.next); //Preview the third list element debug(mapGet(fields, "favoriteFoodList").fields.next.fields.next); //Get the fourth element - null means no more elements debug(mapGet(fields, "favoriteFoodList").fields.next.fields.next.fields.next); //Get a record included in a record AvroRecordUDR nameRecord = (AvroRecordUDR)mapGet(fields, "name"); //Get a value of one of the fields of the included record debug(mapGet(nameRecord.fields, "lastName")); //Get a list of maps, print first element list<any> arrayOfMaps = (list<any>)(mapGet(fields, "array_of_maps")); debug(listGet(arrayOfMaps,0)); //Part 2 - Preparing input for encoding //Record as a top element of the schema Avro.AvroRecordUDR record = udrCreate(Avro.AvroRecordUDR); record.fullname = "example.avro.User3"; //Map to keep record's fields map<string,any> fieldz = mapCreate(string,any); // name Avro.AvroRecordUDR name = udrCreate(Avro.AvroRecordUDR); map<string,any> namefields = mapCreate(string,any); mapSet(namefields, "firstName", ""); mapSet(namefields, "lastName", "Kula"); name.fullname = "example.avro.FullName2"; name.fields = namefields; mapSet(fieldz, "name", name); // favorite number mapSet(fieldz, "favorite_number", (int)1632); // colour mapSet(fieldz, "favorite_color", "Green"); // football team (spelled with one "o" in schema) Avro.AvroEnumUDR favorite_fotball_team = udrCreate(Avro.AvroEnumUDR); favorite_fotball_team.fullname = "example.avro.teams.teams"; favorite_fotball_team.symbol = "Djurgarden"; mapSet(fieldz, "favorite_fotball_team", favorite_fotball_team); // IP address list<any> ipAddresslist = listCreate(any); Avro.AvroFixedUDR ipv4Address = udrCreate(Avro.AvroFixedUDR); ipv4Address.fullname = "example.avro.ipv4Address"; ipv4Address.bytes = baCreateFromHexString("00ABCDEF"); listAdd(ipAddresslist, ipv4Address); Avro.AvroFixedUDR ipv6Address = udrCreate(Avro.AvroFixedUDR); ipv6Address.fullname = "example.avro.ipv6Address"; ipv6Address.bytes = baCreateFromHexString("00ABCDEFEFEFEFEFEFEFEFEFEFEFEFEF"); listAdd(ipAddresslist, ipv6Address); mapSet(fieldz, "ipAddresses", ipAddresslist); // favoriteFoodList Avro.AvroRecordUDR favoriteFoodList = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish1 = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish2 = udrCreate(Avro.AvroRecordUDR); Avro.AvroRecordUDR dish3 = udrCreate(Avro.AvroRecordUDR); map<string,any> dish3Map = mapCreate(string,any); mapSet(dish3Map, "dish", "Brown beans"); mapSet(dish3Map, "next", null); dish3.fields = dish3Map; dish3.fullname = "example.avro.favoriteFood"; map<string,any> dish2Map = mapCreate(string,any); mapSet(dish2Map, "dish", "Pannkakor"); mapSet(dish2Map, "next", dish3); dish2.fields = dish2Map; dish2.fullname = "example.avro.favoriteFood"; map<string,any> dish1Map = mapCreate(string,any); mapSet(dish1Map, "dish", "Ramen"); mapSet(dish1Map, "next", dish2); dish1.fields = dish1Map; dish1.fullname = "example.avro.favoriteFood"; favoriteFoodList.fullname = "example.avro.favoriteFood"; favoriteFoodList.fields = dish1Map; mapSet(fieldz, "favoriteFoodList", favoriteFoodList); // salary mapSet(fieldz, "salary", 347857486343); // myFixed Avro.AvroFixedUDR myFixed = udrCreate(Avro.AvroFixedUDR); bytearray myFixed_bytes = baCreateFromHexString("DEADBEEF"); myFixed.bytes = myFixed_bytes; myFixed.fullname = "example.avro.myfixed"; mapSet(fieldz, "myFixed", myFixed); // myFloat mapSet(fieldz, "myFloat", (float)0.4); // myDouble mapSet(fieldz, "myDouble", (double)0.5); // RootUsers map<string,any> rootUsers = mapCreate(string,any); Avro.AvroRecordUDR olleBack = udrCreate(Avro.AvroRecordUDR); olleBack.fullname = "example.avro.RootUsers"; map<string,any> fields1 = mapCreate(string,any); mapSet(fields1, "rootUser", "Allan"); mapSet(fields1, "privileges", (int)3); olleBack.fields = fields1; Avro.AvroRecordUDR nisseHult = udrCreate(Avro.AvroRecordUDR); nisseHult.fullname = "example.avro.RootUsers"; map<string,any> fields2 = mapCreate(string,any); mapSet(fields2, "rootUser", "Guran"); mapSet(fields2, "privileges", (int)2); nisseHult.fields = fields2; mapSet(rootUsers, "olleBack", olleBack); mapSet(rootUsers, "nisseHult", nisseHult); mapSet(fieldz, "rootUsers", rootUsers); // array_of_maps list<any> array_of_maps = listCreate(any); map<string,any> map1 = mapCreate(string,any); Avro.AvroRecordUDR Nyckel = udrCreate(Avro.AvroRecordUDR); map<string,any> nyckelFields = mapCreate(string,any); mapSet(nyckelFields,"favoriteUser", "Sture D.Y."); mapSet(nyckelFields,"favoriteNumber", (int)10); Nyckel.fullname = "example.avro.some_record"; Nyckel.fields = nyckelFields; mapSet(map1, "Nyckel", Nyckel); listAdd(array_of_maps, map1); map<string,any> map2 = mapCreate(string,any); Avro.AvroRecordUDR Nyckel2 = udrCreate(Avro.AvroRecordUDR); map<string,any> nyckel2Fields = mapCreate(string,any); mapSet(nyckel2Fields,"favoriteUser", "Sture D.A."); mapSet(nyckel2Fields,"favoriteNumber", (int)12); Nyckel2.fullname = "example.avro.some_record"; Nyckel2.fields = nyckel2Fields; mapSet(map2, "Nyckel2", Nyckel2); listAdd(array_of_maps, map2); mapSet(fieldz, "array_of_maps", array_of_maps); record.fields = fieldz; debug("-------------- record -------------------"); debug(record); debug("-------------- record -------------------"); Avro.AvroEncoderUDR encode = udrCreate(Avro.AvroEncoderUDR); encode.data = record; encode.writerSchemaID = "7"; //debug(encode); udrRoute(encode); } } Encoder_1 This is an example of how you can configure an Avro Encoder and a schema register to be used. Open

---

# Document 147: Amazon S3 Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999022/Amazon+S3+Forwarding+Agent+Transaction+Behavior
**Categories:** chunks_index.json

The transaction behavior for the Amazon S3 forwarding agent is presented here. For more information about general transaction behavior please refer to the section, Transactions, in Workflow Monitor . Input/Output Data This agent does not emit anything. The agent acquires commands from other agents and based on them generates a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is first created in the target directory, if not already created. Then a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is first closed and then the Command, if specified in After Treatment, is executed. Finally, the file is moved from the temporary directory to the target directory. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 148: Disk Forwarding Agent Configuration - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640925
**Categories:** chunks_index.json

You open the Disk forwarding agent configuration dialog from a workflow configuration. To open the Disk collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Disk from the Forwarding tab of the Agent Selection dialog. Disk Tab Disk forwarding agent configuration - Disk tab Setting Description Setting Description Input Type The agent can act on two input types. Depending on which one the agent is configured to work with, the behavior will differ. The default input type is bytearray, that is the agent expects bytearrays. If nothing else is stated the documentation refer to input of bytearray. If the input type is MultForwardingUDR , the behavior is different. For further information about the agent's behavior in MultiForwardingUDR input, see Disk Forwarding MultiForwardingUDR Input - Batch . Directory Enter the absolute pathname of the target directory on the local file system of the EC, where the forwarded files will be stored. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR , in the target directory. When an End Batch message is received, the files are moved from the subdirectory to the target directory. Note! When the File already exists! error is thrown during a workflow execution, the temporary file created in the DR_TMP_DIR directory will not be removed. If changing to a new path, you will need to remove the temporary file created in the previous path's DR_TMP_DIR before running the workflow with the new directory. Create Directory Check to create the directory, or the directory structure, of the path that you specify in Directory. Note! The directories are created when the workflow is executed. Compression Select the compression type of the target files. Determines if the agent will compress the files or not. No Compression - agent does not compress the files. Default setting. Gzip - agent compresses the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. The configuration of the filenames is managed in the Filename Template tab only. Command If a Command is supplied, it will be executed on each successfully closed temporary file, using the parameter values declared in Arguments. Refer to the section, Commands in Desktop User's Guide . Note! At this point the temporary file is created and closed, however the final filename has not yet been created. The entered Command has to exist in the execution environment, either including an absolute path, or to be found in the PATH for the execution environment. Arguments This field is optional. Each entered parameter value has to be separated from the preceding value with a space. The temporary filename is inserted as the second last parameter, and the final filename is inserted as the last parameter, automatically. This means that if, for instance, no parameter is given in the field, the arguments will be as follows: $1=<temporary_filename> $2=<final_filename> If three parameters are given in the field Arguments, the arguments are set as: $1=<parameter_value_#1> $2=<parameter_value_#2> $3=<parameter_value_#3> $4=<temporary_filename> $5=<final_filename> Produce Empty Files If enabled, files will be produced although containing no data. Filename Template Tab For a detailed description of the Filename Template tab, see Filename Template Tab in Workflow Template .

---

# Document 149: Optional Software - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848800/Optional+Software
**Categories:** chunks_index.json

The following software is optional or is required by optional components: For Couchbase Enterprise Edition support, see the Couchbase Compatibility Matrix (storage for Aggregation, Distributed Storage, and PCC) Cloudera Data Platform 7.1.x (Data Hub agent) MySQL Cluster Carrier Grade Edition with NDB version 7.5.x, 7.6.x and 8.0.x MySQL Cluster CGE version 8 is supported only for PCC functionality All other uses of MySQL Database, MZ supports version 5.x Elasticsearch version 6.2.x (storage for Aggregation) Grafana 7.5.x or newer (visualization tool for System Insight) Prometheus 2.53.1 (storage for System Insight) Kafka version 2.4.x - 3.0.x Python 3 *) (required by Python agents) Redis version 5.0.x (storage for Aggregation, Distributed Storage, and PCC) ElastiCache (Redis service in AWS) (storage for PCC) *) Versions that have not reached end-of-life. For further information, see https://devguide.python.org/#status-of-python-branches .

---

# Document 150: GCP Storage Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652882/GCP+Storage+Forwarding+Agent+Configuration
**Categories:** chunks_index.json

To open the Database collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Batch . Click Add Agent and select GCP Storage from the Forwarding tab of the Agent Selection dialog. The GCP Storage Forwarder tab contains settings related to the placement and handling of the source files to be forwarded by the agent. Open Setting Description Setting Description Profile Select the File System profile you want the agent to use, see File System Profile for further information about this profile. Input Type For an example see GCP Storage Forwarding MultiForwardingUDR Example . Directory The absolute pathname of the target directory on the location stated in the referenced File System profile, where the forwarded files will be stored. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR , in the target directory. When an End Batch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this setting to create the directory, or the directory structure, of the path that you specify in the Directory field. Note! The directories are created when the workflow is executed. Compression A compression type of the target files. Determines if the agent will compress the files or not. - No Compression - agent does not compress the files. Default setting. - Gzip - agent compresses the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. The configuration of the filenames is managed in the Filename Template tab only. Command If a UNIX command is supplied, it will be executed on each successfully closed temporary file, using the parameter values declared in the Arguments field. Note! At this point the temporary file is created and closed, however, the final filename has not yet been created. The entered command has to exist in the execution environment, either including an absolute path, or to be found in the PATH for the execution environment. Arguments This field is optional. Each entered parameter value has to be separated from the preceding value with a space. The temporary filename is inserted as the second last parameter, and the final filename is inserted as the last parameter, automatically. This means that if, for instance, no parameter is given in the field, the arguments will be as follows: $1=<temporary_filename> $2=<final_filename> If three parameters are given in the field Arguments, the arguments are set as: $1=<parameter_value_#1> $2=<parameter_value_#2> $3=<parameter_value_#3> $4=<temporary_filename> $5=<final_filename> Produce Empty Files If enabled, files will be produced although containing no data. Filename Template Tab For a detailed description of the Filename Template tab, see Filename Template Tab in Workflow Template .

---

# Document 151: SFTP Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609385/SFTP+Forwarding+Agent
**Categories:** chunks_index.json

The SFTP forwarding agent forwards files to a remote host using the SFTP protocol over SSH2. Upon activation, the agent establishes an SSH2 connection and an SFTP session towards the remote host. On failure, additional hosts are tried, if configured. To ensure that downstream systems will not use the files until they are closed, they are maintained in a temporary directory on the remote host until the endBatch message is received. This behavior is also used for cancelBatch messages. If a Cancel Batch is received, file creation is canceled. The SFTP forwarding agent supports IPv4 and IPv6 environments. The section contains the following sub-sections: SFTP Forwarding Agent Configuration SFTP Forwarding Agent Memory Management SFTP Forwarding Agent MultiForwardingUDR Input SFTP Forwarding Agent Transaction Behavior SFTP Forwarding Agent Input/Output Data and MIM SFTP Forwarding Agent Events

---

# Document 152: LDAP Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686173/LDAP+Agent+Example
**Categories:** chunks_index.json

This section provides an example of a workflow that sends data from a TCP/IP agent to the LDAP agent in order to request different LDAP operations. The results of the requests are then sent to the Analysis_2 agent, which sends the results via an encoder back to the TCP/IP agent. Example of a workflow with the LDAP agent Analysis_1 The TCP/IP agent sends data to Analysis_1 which in turn routes the data in the form of UDRs to the Ldap_1. Analysis_1 configuration in the Ldap agent workflow example consume { LdapSearchRequestUDR req = udrCreate (LdapSearchRequestUDR); req.baseDN = "dc=com"; req.filter = "cn=John"; req.scope = "subtree_scope"; udrRoute(req); } LDAP_1 LDAP_1 is configured to send the LDAP operation requests to the LDAP server node. The connection details of the server node are configured in the LDAP agent configuration, in the Connections tab, and the default routing logic is selected, RoundRobin. Open LDAP_1 configuration in the LDAP agent workflow example In this example the default settings are used and no security settings are configured. For details on the different UDRs that can be sent by the LDAP agent to the LDAP server to perform different LDAP operations, see LDAP Agent UDRs . Analysis_2 The corresponding result UDRs are sent by the LDAP agent to Analysis_2, which contain the result information of processing different operations. The possible result UDRs are configured in Analysis_2 as shown in the image below. Open Analysis_2 configuration in the LDAP agent workflow example consume { debug(input); udrRoute(input); } The results of the LDAP requests are then sent back to the TCP/IP agent via an encoder.

---

# Document 153: Reference Data Management User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656532/Reference+Data+Management+User+s+Guide
**Categories:** chunks_index.json

Search this document: Reference Data Management delivers the capability to query and edit specific table sets in relational databases while schema permissions remain unaltered. Open Secure access to database table data Reference Data Management includes the following: Profile that enables access via dashboard or RESTful API. Event notification that is triggered by edited data. Dashboard accessible on the Desktop. RESTful interface running on the Platform. Info! The Reference Data profile can be used with a Database profile that is configured for Oracle or PostgreSQL. Prerequisites The reader of this document should be familiar with: Database administration RESTful Web Services Chapters The following chapters and sections are included: Reference Data Profile RESTful Interface for Reference Data Management Reference Data Management DB Ref Event Reference Data Management Dashboard Data Types for Reference Data Management

---

# Document 154: Encryption Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999867
**Categories:** chunks_index.json

In the Encryption Profile you make encryption configurations to be used by the Encryption agent. Configuration To create a new Encryption profile, click the New Configuration button in the upper left part of the Build View , and then select Encryption Profile from the menu. Open Encryption profile Menus The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently displayed tab. The Encryption profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The Edit menu is specific for the Encryption profile configurations. Item Description Item Description External References Select this menu item to enable the use of External References in the Encryption profile configuration. This can be used to configure the following fields: Key KeyStore Path Keystore password Key Name Key Password For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . Settings You can opt to choose a key definition using either a Directly Configured key or to Read Key from Keystore. Select the appropriate setting for the profile. Directly Configured key You can enter a custom key in the Key input box or alternatively click on the Random button to automatically generate an entry. Using the Algorithm setting you can choose either the AES-128 or AES-256 cipher. Read Key from Keystore The keystore must be a JCEKS keystore. Example - How to create a symmetric crypto key $ keytool -keystore test.ks -storepass password -storetype jceks -genseckey -keysize 128 -alias testkey -keyalg AES The following settings are available: Setting Description Setting Description Key Enter the associated directly configured key. Keystore Path Enter the location of the JCEKS-type keystore from which you want to read the key. Keystore Password Enter the relevant keystore password. Key Name If required, enter the key name. Key Password The Key Password fields are optional. You can enter the key password, or if you leave this field empty, the Keystore Password is the default.

---

# Document 155: Decoder Agent Transaction Behavior and Input/Output Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606944/Decoder+Agent+Transaction+Behavior+and+Input+Output+Data
**Categories:** chunks_index.json

This section includes information about the Decoder agent transaction behavior. For information about the general transaction behavior, see Workflow Monitor . The agent sends out commands that change the state of the file currently processed. Command Description Cancel Batch Emitted on failure to decode the received data. The agent acquires commands from other agents and, based on them, generates a state change of the file currently processed. Command Description End Batch Unless this has been generated by a Hint End Batch message, the decoder evaluates that all the data in the batch was decoded. When using a constructed or blocked decoder, the decoder does additional validation of the structural integrity of the batch. Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent produces one or more UDR types depending on configuration and consumes bytearray type or UDR depending on the configuration. In case of error, the agent may produce DecoderErrorUDR if the selected decoder supports it. Field Description Cause (ExeptionDetails) Information about an exception that has occurred Data (DRUDR) UDR input for the selected decoder

---

# Document 156: ECS Reprocessing Groups - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000070
**Categories:** chunks_index.json

In order for the ECS data to be processable, it has to be assigned to a Reprocessing Group and be in status New . The Reprocessing Group is then selected in the ECS collection agent of the reprocessing workflow. To create a Reprocessing Group, click the Reprocessing Groups button in the ECS Inspector. Open ECS Reprocessing Group dialog Click the Add button to display the Add ECS Reprocessing Group dialog. The Reprocessing Group must have a unique name. Open Add ECS Reprocessing Group dialog The setting Error UDR Type is only applicable to Data Type Batch . If no Error UDR is to be used in reprocessing, this information is not required. Note! UDRs with several Error Codes mapped to different reprocessing groups cannot be automatically assigned to a reprocessing group. They must be assigned manually.

---

# Document 157: SCP Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609281
**Categories:** chunks_index.json

Agent Message Events An information message from the agent stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file: filename Reported along with the name of the source file that has been collected and inserted into the workflow. File cancelled: filename Reported along with the name of the current file, each time a cancelBatch message is received. This assumes the workflow has not been aborted. For further information, see the section, Retrieves, in SCP Collection Agent Transaction Behavior . Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 158: SQL Collection Agent in Batch Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654149/SQL+Collection+Agent+in+Batch+Workflows
**Categories:** chunks_index.json

The SQL collection agent collects rows from any database table and inserts them as UDRs into a workflow. When the workflow is executed the agent will execute a query in SQL, based on the user configuration, and retrieve all rows matching the statement. For each row, a UDR is created and populated according to the assignments in the configuration window. Note! Supported database commands depend on the JDBC driver of the database. The section contains the following subsections: SQL Collection Agent Configuration - Batch SQL Collection Agent Transaction Behavior - Batch SQL Collection Agent Input/Output Data and MIM - Batch SQL Collection Agent Events - Batch

---

# Document 159: Real-Time Disk_Deprecated Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676024/Real-Time+Disk_Deprecated+Collection+Agent
**Categories:** chunks_index.json

The Disk_Deprecated collection agent collects files from a local file system and inserts them into a workflow. The agent's source directory is continuously scanned for files that match a filter expression. Files that match are temporarily moved to an automatically created subdirectory before being processed. This is to prevent that the same files are collected by multiple workflows. The collected files are routed to the workflow as partial data sets, i.e. part of a bytearray or a decoded UDR, via FileSend types. These UDRs contain the original data and a sequence number. When all the partial data sets have been sent by the agent, it sends a FileEnd UDR to the workflow. The workflow must acknowledge the reception by routing back the same UDR type, or a timeout error will occur. For further information about timeout errors, see Real-Time Disk_Deprecated Collection Agent Configuration . Open UDR Flow When a file has been successfully processed by the workflow, the agent offers the possibility of moving, renaming, or removing the original file. If the agent fails to read, move, or decode a file, a FileError UDR type is routed to the workflow. In case of decoding errors, the erroneous file is also sent to DR_ERROR_DIR in the source directory. If the EC is terminated while the workflow is running, the temporary files remain on the file system. When the workflow is restarted, these files are moved to the original source directory. Note! Since partial data is routed to the workflow, it may be duplicated when temporary files are moved back to the source directory after a workflow restart. If you are required to run the batch-based Disk collection agent in a real-time workflow, see Batch-Based Real-Time Disk Agents - int . The section contains the following subsections: Real-Time Disk_Deprecated Collection Agent Configuration Real-Time Disk_Deprecated Collection Agent UDR Types Real-Time Disk_Deprecated Collection Agent Events Real-Time Disk_Deprecated Collection Agent Input/Output Data and MIM Real-Time Disk_Deprecated Collection Agent Example

---

# Document 160: Execution Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736697
**Categories:** chunks_index.json

A workflow is loaded and started on an EC group according to configured distribution criteria, for example, based on machine load or by specifying the EC groups where the workflow should run. You can create EC groups using the EC Groups tool in the Manage view. Create EC using pico management or via topo commands. Refer to Pico Management or Managing Picos with Topo for more information. Add ECs into the EC group using the topo command: mzsh topo set topo://container:<container name>/pico:<ec name>/val:config.properties.pico.groups "<EC Group Name>" Note! To run a workflow on a specific EC, an EC group needs to be created. Multiple ECs can be included in an EC group to serve a workflow. There are multiple ways of adding EC groups to the system. Refer to EC Groups for additional information. If only one EC is to be included in the EC Group, you may do the following or do as for multiple ECs: Create the EC group with the same name as the EC from the UI Restart EC If multiple ECs are to be included in one EC Group, you must: Add the property pico.groups to all ECs that should be included by using this command: mzsh topo set topo://container:main1/pico:ec1/val:config.properties.pico.groups "myEcGroup" Restart the ECs. Execution Tab in Batch Workflow The Execution tab has settings that are related to where the workflow will be executed and how often it will execute. Open The Batch Workflow Execution tab Item Description Item Description Execution Settings Select Enable to enable setup of the execution parameters. Distribution A workflow executes on an EC group. You can specify these EC groups, or the system can select them automatically. Note! EC groups will only be used if you have selected them, the reason for this is to avoid the execution of undesired workflows. The following options exist: Sequential - Starts the workflow on the first EC group in the list, if you choose the sequential option and only configure one EC group then the workflow will not run if the EC group is not available. If this EC group is not available, it proceeds with the next in line. Workflow Count - Starts the workflow on the EC group running with the fewest number of workflows. Machine Load - Starts the workflow on the EC group with the lowest machine load. Round Robin - Starts the workflow on the available EC groups in turn, but not necessarily in a sequential order. If ecg1, ecg2 and ecg3 are defined, the workflow may first attempt to start on ecg2. The next time it may start on ecg3 and then finally on ecg1. This order is then repeated. If an EC group is not available, the workflow will be started on any of the other listed EC groups. Kafka Profile Required to enable the Kafka-based batch transaction handler for https://infozone.atlassian.net/wiki/x/GADzEQ . This setting must be selected to use Kafka agents in batch workflows. When selected, MediationZone uses the Kafka cluster identified by this profile for all Kafka agents in the workflow. It also uses a Kafka-based transaction handler to ensure batch transactions across all Kafka agents in the workflow. Debug Type Select Event to enable debug output, such as output from a debug call in the APL code, to appear in the Workflow Monitor . Select File to save debug results in $MZ_HOME/tmp/debug . The file name is made up of the names of the workflow template and of the workflow itself, for example: MZ_HOME/tmp/debug/Default.radius_wf.workflow_2 . Only debug events are written to the file. To save to file all the events that are shown when you select to monitor events in the Workflow Monitor, you must add an event notifier to the relevant Agent Message Event with file output. For further information, see Event Notifications . If you save debug results in a file, and you restart the workflow, this file gets overwritten by the debug information that is generated by the second execution. To avoid losing debug data of earlier executions, set Number of Files to Keep to a number that is higher than 0 (zero). Number of Files to Keep : Enter the number of debug output files that you want to save. When this limit is reached, the oldest file is overwritten. If you set this limit to 0 (zero), the log file is overwritten every time the workflow starts. Example - Debug output The workflow configuration Default.radius_wf includes a workflow that is called workflow_2. Number of Files to Keep is set to 10. The debug output folder contains the following files: Default.radius_wf.workflow_2 (current debug file) Default.radius_wf.workflow_2.1 (newest rotated file) Default.radius_wf.workflow_2.2 Default.radius_wf.workflow_2.3 Default.radius_wf.workflow_2.4 Default.radius_wf.workflow_2.5 Default.radius_wf.workflow_2.6 Default.radius_wf.workflow_2.7 Default.radius_wf.workflow_2.8 Default.radius_wf.workflow_2.9 Default.radius_wf.workflow_2.10 (oldest rotated file) According to this example there are totally 11 files that are being overwritten one-by-one and the rotation order is: Default.radius_wf.workflow_2 | V Default.radius_wf.workflow_2.1 | V Default.radius_wf.workflow_2.2 | V : : | V Default.radius_wf.workflow_2.n | V Deleted Example - Using the option Always Create a New Log File If you have a workflow named Default.radius_wf with an instance called workflow_2 and you create new debug output files every time the workflow executes, the debug output folder contains files like the following: Default.radius_wf.workflow_2.1279102896375 Default.radius_wf.workflow_2.1279102902908 Default.radius_wf.workflow_2.1279102907149 Caution! The system will not manage the debug output files when this option is used. It is up to the user to make sure that the disk does not fill up. Throughput Calculation MediationZone contains an algorithm to calculate the throughput of a running workflow. It locates the first agent in the workflow configuration that delivers UDRs, usually the decoder and counts the number of passed UDRs per second. If no UDRs are passing through the workflow, the first agent delivering raw data will be used. The statistics can be viewed in the System Statistics . If a MIM value other than the default is preferred for calculating the throughput, the User Defined check box is selected. From the browser button, a MIM Browser dialog is opened and available MIM values for the workflow configuration is shown and a new calculation point can be selected. Since the MIM value shall represent the amount of data entered into the workflow since the start (for batch workflows from the start of the current transaction), the MIM value must be of a dynamic numeric type, as it will change as the workflow is running. Execution Tab in Real-Time Workflow Open The Realtime Execution tab Item Description Item Description Execution Settings Select Enable to enable setup of the execution parameters. Distribution A workflow executes on an EC group. You can specify these EC groups, or the system can select them automatically. Note! If you select to configure the distribution using EC groups, the selected distribution type will also be applied on the ECs within the groups. The following options exist: Sequential - Starts the workflow on the first EC group in the list. If this EC group is not available, it proceeds with the next in line. Workflow Count - Starts the workflow on the EC group running the fewest number of workflows. If the Execution Contexts list contains at least one entry, only this/these EC groups will be considered. Machine Load - Starts the workflow on the EC group with the lowest machine load. If the Execution Contexts list contains at least one entry, only this/these EC groups will be considered. Which EC group to select is based on information from the System Statistics sub-system. Round Robin - Starts the workflow on the available EC groups in turn, but not necessarily in a sequential order. If ecg1, ecg2 and ecg3 are defined, the workflow may first attempt to start on ecg2. The next time it may start on ecg3 and then finally on ecg1. This order is then repeated. If an EC group is not available, the workflow will be started on any other available EC groups. Execution Context Type Select an execution context type that the workflow should execute on. The following options exist: Normal - This setting enables execution of the workflow on one or more ECs. If several ECs/EC groups are added to the Execution Contexts list, the selected Distribution is considered. If no EC/EC group is selected the system will consider all available ECs as possible targets. Standalone - This setting enables execution of a stand-alone workflow that is independent of the Platform. To add an EC group, click Add and select an EC Group in the drop-down list. Queue Size The number of unprocessed entries (backlog) that the workflow can store in a buffer before the collector is slowed down. The workflow and its back-end systems might slow its processing activity when the number of requests rises. To avoid congestion, while the records or decoding tasks are in the queue, the queue intake is delayed to limit the backlog from growing too fast. Default value is 1000. The value that you enter here is the size of each route's queue in the workflow. Queue Strategy Blocking queue This is the default method. Ordered Routing To be able to preserve the order of incoming UDRs, this option should be used in combination with Ordered Services in the Service tab. Open To maintain the order of the UDRs as the agent sees it from the source, you can use the function ordered routing. This ensures that you retain the routing order, even if the work is divided over several threads. When using this function you must define how to catch the order of the UDRs. This can be done with APL in the Services tab in Workflow Properties. Via the 'input' value, typically by instanceOf checks for each routed type, Use ordered.addInteger(si, i) etc with values from session defining fields. Hashing will select partition. Alternatively use ordered.setPartition(si, N) to explicitly chose a partition. void route(order.SessionIdentifier si, any input) {....} Queue Worker Strategy By selecting Queue Worker Strategy , you can determine how the workflow should handle queue selection, which may be useful if you have several different collectors. You have the following options: RoundRobin The RoundRobin strategy works in the same way as the InsertionOrder strategy, except that each workflow thread will be given its own starting position in the routing queue list. This means that as long as the number of workflow threads is equal to, or greater than, the number of routing queues, no queue will suffer from starvation. Faster routes will get more load than slower ones. This option provides pretty fair distribution. Use this strategy if the number of workflow threads is equal to, or greater than, the number of routing queues, and it is desirable to prioritize faster routes before slower ones. RoundRobin is the default strategy. DedicatedAndRoundRobin In the DedicatedAndRoundRobin strategy, each queue has one thread dedicated to it by default. The number of workflow threads (given in the Threads column) minus one, serve the queues in round robin fashion. The number of threads indicates the maximum number of threads that can collect from a queue at any one time. One workflow thread guarantees the order of the UDRs in the workflow. InsertionOrder With the InsertionOrder strategy, queues are selected in route insertion order. As long as there are queued UDRs available on the first queue, that queue will be polled. This means that routes with later insertion order may not receive as many UDRs as they have capacity for, and get no or little throughput. This type of condition may be detected by looking at the Queue Throughput for workflows in the System Statistics view. Only use this strategy, if this is not an issue. This is the preferred choice when you work synchronously with responses and process small amounts of UDRs at any given time (which is not the same as low throughput). Note! The insertion order depends primarily on how close the queues are to an "exit", i e an agent without any configured output. The queues that are closest to an exit will be inserted first. The queues that are furthest from an exit, will be inserted last. However, if the distance to an exit is equal for two or more queues, the insertion order is dictated by the sequence of the agents in the workflow configuration, i e the agent that was added first to the configuration has higher priority. Threads The number of workflow threads. The default value is 8. Throughput Calculation MediationZone contains an algorithm to calculate the throughput of a running workflow. It locates the first agent in the workflow configuration that delivers UDRs, usually the decoder and counts the number of passed UDRs per second. If no UDRs are passing through the workflow, the first agent delivering raw data will be used. The statistics can be viewed in the System Statistics . If a MIM value other than the default is preferred for calculating the throughput the User Defined check box is selected. From the browser button a MIM Browser dialog is opened and available MIM values for the workflow configuration is shown and a new calculation point can be selected. Since the MIM value shall represent the amount of data entered into the workflow since the start (for batch workflows from the start of the current transaction) the MIM value must be of a dynamic numeric type, as it will change as the workflow is running. Processed UDRs Count Interval (min) Select this option to specify the interval period in minutes for counting the number of processed UDRs. The default value is 1. The maximum value permitted is 1440 min (one day).

---

# Document 161: JMS Agents Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033745/JMS+Agents+Preparations
**Categories:** chunks_index.json

SBoth the collection and the (processing) request JMS agents use JNDI to acquire the previously configured connection and destination objects. For information about Connection Factory and Destination objects, see the JMS 1.1 specification: http://www.oracle.com/technetwork/java/docs-136352.html . The classpath for the JMS and JNDI jar files are specified for each EC. In the example below, the jar files are located in MZ_HOME/3pp. Note! Ensure that you include existing paths, so that they are not overwritten. $ mzsh topo get topo://container:<container>/pico:<ec>/obj:config.classpath Example - Setting classpath mzsh topo set topo://container:<container>/pico:<ec name> '{ template:mz.standard-ec config { classpath { jars=["jms/apache-activemq-5.0.0/activemq-all-5.0.0.jar"] } } }' After the classpath is set, the jar file must be manually distributed to be in place when the EC is started

---

# Document 162: An XML Format Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205815827/An+XML+Format+Example
**Categories:** chunks_index.json

The example in this section describes how to decode and encode XML data. The example includes the following: Workflow and Ultra configurations ( ultra_xml_example.zip ): CSV_TO_XML_WF - Collects a CSV formatted file and converts it to XML. Optional fields that are not included in the CSV are populated in the workflow. XML_TO_CSV_WF - Collects am XML formatted file and converts it to CSV. Optional fields in the XML are discarded. ULTRA_CSV - CSV decoder and encoder. ULTRA_XML - XML decoder and encoder. Input data in CSV format ( INFILE01.txt ). Follow these steps to run the example: Download the example files to /opt/ultraXML . Create the subdirectory indata in /opt/ultraXML . Copy INFILE01.txt to /opt/ultraXML/indata . Open the System Importer and select /opt/ultraXML/ultra_xml_example.zip . Import all configurations. Start the workflow CSV_TO_XML_WF . This creates an XML file based on the CSV input in the directory /opt/ultraXML/out . Copy the generated XML file to the directory /opt/UltraXML/indata . Start the workflow XML_TO_CSV_WF . A CSV file that is identical to the downloaded input file is created in /opt/ultraXML/out . Below is a description of the Ultra configuration that is used for XML encoding and decoding. Example - ULTRA_XML To decode or encode XML data, a format definition (an XML Schema syntax) is included in Ultra xml_schema block. xml_schema { <?xml version="1.0" encoding="ISO-8859-1"?> <schema xmlns = " http://www.w3.org/2001/XMLSchema "> <element name="TRANSACTION_LOG"> <complexType> <sequence> <element ref="TRANSACTION" maxOccurs="unbounded"/> </sequence> </complexType> </element> <element name="TRANSACTION"> <complexType> <attribute name="TXID" type="string" use="required"/> <sequence> <element name="USER" type="string" minOccurs="1" maxOccurs="1"/> <element name="IP" type="string" minOccurs="1" maxOccurs="1"/> <element name="ITEM" type="string" minOccurs="1" maxOccurs="1"/> <element name="VALUE" type="long" minOccurs="1" maxOccurs="1"/> <element name="TIMESTAMP" type="dateTime" minOccurs="1" maxOccurs="1"/> <element name="CURRENCY" type="string" minOccurs="1" maxOccurs="1"/> <element name="MISC" type="string" minOccurs="0" maxOccurs="unbounded"/> </sequence> </complexType> </element> </schema> }; Collected XML UDRs are often terminated by one or several whitespace characters. When mapping, the whitespace temporary record is identified. Although input data that includes trailing whitespace characters is valid in XML, it is recommended that you eliminate them when decoding the data. To remove any excessive white spaces from the XML UDRs, the external format WhiteSpace is used with the in_map of the decoder. For further information see In-maps and Decoders . external WhiteSpace : identified_by( value == 0x20 || value == 0xA || value == 0xD ) { int value: static_size(1); }; The internal and external formats can be mapped automatically but are mapped explicitly in the example. This is to demonstrate how the interpretation of XML types works. internal TransactionLog { list<Transaction> Transactions; }; internal Transaction { string TxId; string User; string IP; string Item; long Value; date Timestamp; string Currency; list<string> Misc; }; in_map inTransactionLog: external(TRANSACTION_LOG), internal(TransactionLog) { i:Transactions and e:TRANSACTION using in_map inTransaction; }; in_map inTransaction: external(TRANSACTION), internal(Transaction) { i:TxId and e:TXID; i:User and e:USER; i:IP and e:IP; i:Item and e:ITEM; i:Value and e:VALUE; i:Timestamp and e:TIMESTAMP; i:Currency and e:CURRENCY; i:Misc and e:MISC; }; out_map outTransactionLog: external(TRANSACTION_LOG), internal(TransactionLog) { i:Transactions and e:TRANSACTION using out_map outTransaction; }; out_map outTransaction: external(TRANSACTION), internal(Transaction) { i:TxId and e:TXID; i:User and e:USER; i:IP and e:IP; i:Item and e:ITEM; i:Value and e:VALUE; i:Timestamp and e:TIMESTAMP; i:Currency and e:CURRENCY; i:Misc and e:MISC; }; To get rid of white spaces, create a <literal>in_map</literal> for the external format WhiteSpace using the discard_output option. For further information, see the In-maps . in_map WS_map: external(WhiteSpace), discard_output {automatic;}; To remove as many white spaces as possible from the processed data, WS_map is set first in the deocder. encoder encTransactionLog: out_map(outTransactionLog); decoder decTransactionLog: in_map(WS_map), in_map(inTransactionLog);

---

# Document 163: Amazon S3 Collection Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606286/Amazon+S3+Collection+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent produces bytearray types. MIM For information about the MIM and a list of the ge neral MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Value Description File Modified Timestamp This MIM parameter contains a timestamp, indicating when the file is stored in the collection directory. File Modified Timestamp is of the date type and is defined as a header MIM context type. File Retrieval Timestamp This MIM parameter contains a timestamp, indicating when the file processing starts. File Retrieval Timestamp is of the date type and is defined as a header MIM context type. Source File Size This MIM parameter contains the file size, in bytes, of the source file. Source File Size is of the long type and is defined as a header MIM context type. Source Filename This MIM parameter contains the name of the currently processed file, as defined at the source. Source Filename is of the string type and is defined as a header MIM context type. Source Filenames This MIM parameter contains a list of file names of the files that are about to be collected from the current collection directory. Note! When the agent collects from multiple directories, the MIM value is cleared after collection of each directory. Then, the MIM value is updated with the listing of the next directory. Source Filenames is of the list<any> type and is defined as a Header MIM context type. Source File Count This MIM parameter contains the number of files, available to this instance for collection at startup. The value is constant throughout the execution of the workflow, even if more files arrive during the execution. The new files will not be collected until the next execution. Source File Count is of the long type and is defined as a global MIM context type. Source Pathname This MIM parameter contains the path to the directory where the file currently under processing is located. Source Pathname is of the string type and is defined as a global MIM context type. The path is defined in the Amazon S3 tab. Source Files Left This parameter contains the number of source files that are yet to be collected. This is the number that appears in the Execution Manager backlog. Source Files Left is of the long type and is defined as a header MIM context type. Accesses The agent does not itself access any MIM resources.

---

# Document 164: Creating Tables - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638140
**Categories:** chunks_index.json

To create the necessary tables in the database: In the /mysql/bin directory connect to the database with the following command: $ mysql -uroot -p<password> -h <IP address to the MySQL Server> Show existing databases: mysql> show databases; The current databases should be displayed. Create the PCC database: mysql> create database pcc; The PCC database should now be created. Connect to the database: mysql> use pcc; The database should now be changed to pcc. Create the tables by running the scripts pcc_bucket_mysqlc.sql and pcc_config_mysqlc.sql : mysql> source <path>/pcc_bucket_mysqlc.sql; mysql> source <path>/pcc_config_mysqlc.sql; verify that the creation has been done by either: mysql> show tables; mysql> describe bucket_storage; mysql> describe config_storage; Exit MySQL when you are done: mysql> exit

---

# Document 165: PCC System Administration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816223
**Categories:** chunks_index.json

Search this document: This document describes the different tasks and areas you need to know as a system administrator of a PCC System. Terms and Acronyms This section contains glossaries for all terms and acronyms used throughout the PCC and MediationZone documentation. PCC Terms and Acronyms Term/Acronym Definition Term/Acronym Definition PCC Policy and Charging Control PCRF Policy and Charging Rules Function 3GPP 3rd Generation Partnership Project General Terms and Acronyms For information about general terms and acronyms used in this document, see the Terminology document. Chapters The following chapters and sections are included: System Architecture and Requirements PCC Basic Administration PCC High Availability for PCC Configuration of Data Repository for PCC Backup and Maintenance for PCC

---

# Document 166: MySQL Cluster Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204613336/MySQL+Cluster+Configuration
**Categories:** chunks_index.json

If you want to change the configurations for MySQL Cluster, we recommend that you do it off peak. Even though it is generally safe to increase and decrease parameters, but decreasing a parameter such as DataMemory may have the effect that the memory will not be able to store all data. See the official documentation provided by Oracle, https://docs.oracle.com/cd/E37745_01/html/E38170/toc.html for information about how to change the MySQL Cluster configurations .

---

# Document 167: Command Line Tool Reference Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646337/Command+Line+Tool+Reference+Guide
**Categories:** chunks_index.json

Search this document: This section describes the command line tools available in MediationZone 9.3. Both provide command-line access for system administration and workflow managementbut they are not the same tool. mzsh mzsh is a shell that serves as both a system administration tool and a platform client tool. Its functionality depends on whether the platform is running and whether you are logged in, allowing access to different parts of the system accordingly. mzcli mzcli is a command-line interface (CLI) that allows you to execute system commands independently on multiple hosts, providing flexible remote management capabilities. This section contains the following subsections: mzsh mzcli

---

# Document 168: Batch-Based Real-Time Agents - Example Workflow - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation


---
**End of Part 7** - Continue to next part for more content.
