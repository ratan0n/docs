# RATANON/MZ93-DOCUMENTATION - Part 37/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 37 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.6 KB
---

The Virtual Machine tab contains the information listed below. Open Virtual Machine with the Memory tab displayed Tab Description Tab Description Memory Memory information will be displayed here and Garbage Collection Functions are available. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Virtual Machine with the VM Arguments tab displayed Tab Description Tab Description VM Arguments VM arguments listed. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Virtual Machine with the System Properties t ab displayed Tab Description Tab Description System Properties System Properties. For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Virtual Machine with the MBeans tab displayed Tab Description Tab Description MBeans MBeans . For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Virtual Machine with the Diagnostics tab displayed Tab Description Tab Description Diagnostic Commands For detailed information, see Oracle's Package java.lang.management Description : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html

---

# Document 826: Database Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671569/Database+Profile
**Categories:** chunks_index.json

In a Database profile configuration, you can create database profiles for use in various agents, profiles, and APL functions. These include: Audit Profile Callable Statements (APL) Database Bulk Functions (APL) Database Table Functions (APL) Database Agents Data Masking Profile Event Notifications Prepared Statements (APL) Data Veracity Profile Shared Table Profile SQL Agents SQL Loader Agent Task Workflows Agents (SQL) What a profile can be used for depends on the selected database type. The supported usage for each database type is described in their respective pages. The Database profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Database profile, click the New Configuration button from the Configuration dialog available from Build View , and then select Database Profile from the list of configurations. The profile contains the standard configuration buttons as described in Common Configuration Buttons and one additional button: Button Description Button Description Open Click on this button to Enable External References in an agent profile field. Refer to Enabling External References in an Agent Profile Field in External Reference Profile for further information. The Database profile has two tabs; General and Properties . The General Tab In the General tab, the two radio buttons Default Connection Setup and Advanced Connection Setup make it possible to display different connection options. Default Connection Setup Select the Default Connection Setup radio button to use a preconfigured connection string. Open The Database profile configuration - General tab Setting Description Setting Description Default Connection Setup Select this option to configure a default connection. Advanced Connection Setup Select this option to configure the data source connection using a connection string. For further information, see the section below, Advanced Connection Setup. Database Type Select any of the available database types. You may need to perform some preparations before attempting to connect to the database for the first time. For information about required preparations, see the section below, Database Types. Database Name Enter a name that identifies the database instance. For example, when you configure the profile for an Oracle database, this field should contain the SID. Database Host Enter the hostname or IP address of the host on which the database is running. Type it exactly as when accessing it from any other application within the network. Port Number Enter the database network port. Username Enter the database user name. Password Enter the database password. Enable TLS Truststore Enable TLS/SSL for connection to an SAP HANA Database. This option will be enabled by default. This option applies only to SAP HANA and will only be visible when SAP HANA is selected as the Database Type . For information about advanced connection for SAP HANA with TLS/SSL enabled, you can refer to SAP HANA DB . Note! If the TLS truststore is configured for SAP HANA default connection setup, the following SAP HANA JDBC connection string properties will be added as well: encrypt=true hostNameInCertificate=* For information about SAP HANA connection string properties, you can refer to the SAP HANA documentation at the SAP Help Portal. TLS Truststore Enter the path and filename to the TLS/SSL trust store file that contains the SAP HANA hosting server's certificate. This option applies only for SAP HANA and will only be visible when SAP HANA is selected as the Database Type . TLS Truststore Password Enter the password for the TLS/SSL trust store file. This option applies only to SAP HANA and will only be visible when SAP HANA is selected as the Database Type . Try Connection Click to try the connection to the database, using the configured values. Advanced Connection Setup The Advanced Connection Setup enables you to specify a customized connection string. It can be used for Oracle RAC and Snowflake connections, or when you need to add additional properties to a connection. For more information, see the relevant subsections. Open Database profile configuration - Advanced Connection Setup Setting Description Setting Description Default Connection Setup Select to configure a default connection. For further information, see the section above, Default Connection Setup. Advanced Connection Setup Select to configure the data source connection using a connection string. Database Type Select any of the available database types. You may need to perform some preparations before attempting to connect to the database for the first time, see the respective pages for each database type for information. Connection String Enter a connection string containing information about the database and the means of connecting to it. Notification Service This field is used when the selected Database Type is Oracle. For more information, see Oracle . Username Enter the database user name. Password Enter the database password. Try Connection Click to try the connection to the database, using the configured values. The Properties Tab The Properties tab allows you to configure additional properties for certain database types. Open The Database profile configuration - Properties tab Currently, the following properties can be configured: Database Type Properties Database Type Properties CSV All properties described on the page http://csvjdbc.sourceforge.net/doc.html in the section section "Driver Properties" can be used. MySQL mysql.connectionpool.maxlimit Oracle oracle.pool.connectionwaittimeout oracle.pool.maxlimit oracle.net.encryption_client oracle.net.encryption_types_client oracle.net.crypto_checksum_client oracle.net.crypto_checksum_types_client PostgreSQL postgresql.connectionpool.maxlimit escapeSyntaxCallMode Note! Since PostgreSQL 11, there is support for stored PROCEDURE, prior versions of PostgreSQL support only the stored FUNCTION. Hence, a new connection property called escapeSyntaxCallMode has been introduced by PostgreSQL for users to configure. This property specifies how the driver transforms JDBC escape call syntax into underlying SQL, for invoking procedures or functions. In escapeSyntaxCallMode=select (the PostgreSQL default) mode, the driver always uses a SELECT statement (allowing function invocation only). In escapeSyntaxCallMode=callIfNoReturn mode, the driver uses a CALL statement (allowing procedure invocation) if there is no return parameter specified. Otherwise, the driver uses a SELECT statement. In escapeSyntaxCallMode=call mode, the driver always uses a CALL statement (allowing procedure invocation only) In case you are using Callable Statements with PostgreSQL, you can configure the escapeSyntaxCallMode connection property in the Database Profile. However, if this property is not configured in the Database Profile, it will be using the default escapeSyntaxCallMode =select. If you're utilizing Callable Statements with PostgreSQL, you have the option to set the escapeSyntaxCallMode property either within the Connection String parameters or within the Properties tab of the Database Profile. Should you choose to configure this property in both locations, priority will be given to the setting within the Connection String. SAP HANA sapdb.connectionpool.maxlimit Note! If you have configured any of these properties using topo or the Connection String field when making an Advanced Connection Setup , the properties configured in the Properties tab will override these. Database Types The following table provides information on the database versions supported. Database Version Database Version Cassandra 3.11.3 CSV - DB2 10 Derby 10 MS SQL Server 2008, 2008R2, 2012, 2014, 2016, Azure SQL Database, Azure SQL Data Warehouse/Parallel Data Warehouse MySQL 8 Netezza - Oracle 12cR2/19c PostgreSQL 12/13/14/15/16 Redshift - SAP Hana 2.0 (Please refer to SAP docs on latest supported SPS versions) Snowflake - Sybase IQ The version of Sybase IQ support through the use of the JCONN4 driver Teradata 16.20 TimesTen 11.2 Vertica 8.1 This section includes the following subsections: TimesTen Cassandra CSV Derby IBM DB2 MariaDB MySQL Netezza Oracle PostgreSQL Redshift SAP HANA DB Snowflake SQL Server Sybase IQ Teradata Vertica

---

# Document 827: Legacy Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741656
**Categories:** chunks_index.json

Search this document: This document describes the Legacy Desktop Client used for all versions of MediationZone prior to 9.0.0. The Legacy Desktop is still required for certain functionality that is currently not available in the regular Desktop and includes: ECS Inspector , see the documentation for Error Correction System for more information ECS Statistics , see the documentation for Error Correction System for more information SNMP Collection Agents Configuration Diff Disk Agents in Real-Time Workflows Subfolders, see Legacy Desktop User Interface for more information Chapters The following chapters and sections are included: Installation and Configuration of Legacy Desktop Legacy Desktop Overview Pico Viewer Disk Agents in Real-Time Workflows Configuration Diff Documentation Generator System Monitor

---

# Document 828: Setting Up the SNC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609231
**Categories:** chunks_index.json

The SNC (Secure Network Communcations) is available and enabled by default in the SAP RFC agent to provide additional security layer to the connection. This page contains the additional information and steps to setting up the SNC for SAP RFC Profile. Environment Preparation On all servers, update the .bashrc with the following env variables. The SNC directory is assumed to be /sapmntmz/snc. sudo sed -i -e '$a export SECUDIR=/sapmntmz/snc/sec' /home/mzadmin/.bashrc sudo sed -i -e '$a export SNC_LIB=/sapmntmz/snc/libsapcrypto.so' /home/mzadmin/.bashrc Configuration Follow the steps below to set up the required SNC. Note! You are required to replace the <values> in the following commands with your local values. Prior to setting up the SNC, ensure that the SAP tooling is installed in the /sapmntmz/snc directory. On the CM server, generate a client PSE with the following: ./sapgenpse get_pse -p cmr.pse -x cmr@e1r8 "CN=<cn_name_value>,OU=<ou_value),O=<o_value>,C=<c_value>" Create the credentials and attach them to the OS users. ./sapgenpse seclogin -p cmr.pse -x cmr@e1r8 -O mzadmin Export the certificate. ./sapgenpse export_own_cert -o cmr.crt -p cmr.pse -x cmr@e1r8 Import the certificate to the SAP RFC server. This certificate does not require a signature. The SAP RFC Server provides a signed certificate to be placed in local PSE on the CM server. ./sapgenpse maintain_pk -a E1R_SNC_Certificate.crt -p cmr.pse -x cmr@e1r8 Next, restart the platform. mzsh system stop && mzsh restart platform && mzsh system start SNC Validation Once you have completed the configuration, you can now validate the SNC by creating an RFC Profile with the SNC option selected. For more information, refer to SAP RFC Profile .

---

# Document 829: mzcli - plist - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979623/mzcli+-+plist
**Categories:** chunks_index.json

Usage plist [ -name <package name> ] [ -gen <all|nogenerated|onlygenerated> ] [ -archive [<package archive name>] ] [ -class [<java class name>] ] [ -resource [<java resource name>] ] [ -compact ] [ -inactive ] This command lists packages installed in the system. The list shows an overview of each package's user, version, repository, revision, and date. You can use options to filter packages and view detailed information such as archives, java classes, and resources. Options Option Description Option Description -name <package name> Use this option to show package info for <package name> . -gen <all | nogenerated | onlygenerated> Use this option to show packages that are generated by Ultra and APL configurations. -archive [ <package archive name> ] Use this option to show archive information for <package archive name> . Omit <package archive name> to show information for all archives. -class [ <java class name> ] Use this option together with archive <package archive name> to show class information for an archive. Omit <java class name> to show information for all classes. -resource [ <java resource name> ] Use this option together with archive <package archive name> option to show resource information for an archive. Omit <java resource name> to show information for all resources. -compact Use this option to show the package information in a compact format. -inactive Use this option to show the packages currently not activated by the license. Return Codes Listed below are the different return codes for the plist command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the command was unsuccessful.

---

# Document 830: Parquet Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641970/Parquet+Profile
**Categories:** chunks_index.json

To support the Parquet Encoder and Decoder, create a new Parquet Profile that will be used and shared by Workflow agents. The Parquet Profile allows you to specify a parquet schema as well as parquet encoding options. The Parquet Profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Parquet profile configuration, click the New Configuration and then select Parquet Profile from the Configurations dialog. The contents of the menus in the menu bar may change depending on which configuration type is opened. The Parquet profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The menu in the Parquet profile configuration contains two tabs: Schema and Advanced . Open The Parquet profile's Schema tab is empty when opening a new profile . The section contains the following subsections: Parquet Profile Configuration Schema Parquet Profile Configuration Advanced

---

# Document 831: Pulse Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653518/Pulse+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data that an agent expects and delivers. The Pulse agent produces Pulse UDRs. MIM For information about the MIM and a list of the general MI M par ameters, see Administration and Management in Legacy Desktop . The agent does not publish nor access any MIM parameters.

---

# Document 832: FTPS Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607689/FTPS+Forwarding+Agent+Transaction+Behavior
**Categories:** chunks_index.json

The transaction behavior of the FTPS forwarding agent is presented here. For further information about the general transaction behavior, see Transactions in Workflow Monitor . Emits The agent emits nothing. Retrieves The agent retrieves commands from other agents and generates a state change of the file currently processed based on the commands. Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is created in the target directory, if not already created. Then, a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is closed and the file is moved from the temporary directory to the target directory. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 833: Disk Forwarding Agent Transaction Behavior  - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672773/Disk+Forwarding+Agent+Transaction+Behavior+-+Batch
**Categories:** chunks_index.json

The transaction behavior for the Disk forwarding agent is presented here. For more information about general transaction behavior please refer to the section, Transactions, in Workflow Monitor . Input/Output data This agent does not send anything. The agent acquires commands from other agents and based on them generates a state change of the file currently processed. Command Description Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is first created in the target directory, if not already created. Then a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is first closed and then the Command, if specified in After Treatment, is executed. Finally, the file is moved from the temporary directory to the target directory. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 834: TLS Standard Setup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848693
**Categories:** chunks_index.json

The TLS requires that you set up a keystore to contain certificates and private keys. Follow the steps below to set up a keystore. For instructions to include client authentication (two-way authentication), see Enabling Client Authentication . Example - How to Create a Symmetric Crypto Key keytool -keystore test.ks -storepass password -genseckey -keysize 128 -alias testkey -keyalg AES Example - How to Create a Keystore File with Security Contents The example code below shows how to create a Java keystore file for both the server and client connection. In this example, the file will be generated containing the associated security certificate, public and private key. Code Block keytool -genkey -alias server -keyalg RSA -keystore ./server.jks Note! Remember the password issued for the server.jks file. Example - How to Create a Client-Specific Keystore File To create a client-specific Java Keystore file, you can use the keytool command with the required variables. In this example, the generated file will be for a specific client and contain only their certificate and public key. Code Block keytool -export -alias server -keystore ./server.jks -file ./server.cer ... keytool -import -alias client -file ./server.cer -keystore ./client.jks ... Note! Execution of these commands will present password entry prompts, and you will need to remember the entered passphrase.

---

# Document 835: Ultra Field Plugins - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645263/Ultra+Field+Plugins
**Categories:** chunks_index.json

It is possible to define user defined Ultra field types to use with an external block. A new Ultra field plugin is defined by extending DRUltraFieldPlugin . The code server will locate all such classes and make the field types available within the Ultra format system. The plugin class defines which classes to use for encoding and decoding. Both encoding and decoding are optional operations, but if no decoder class is supplied, it will be impossible to decode the field type (and vice versa). It is important to note that both the field decoder and encoder classes must be stateless to work properly. The reason is that each UDR decoder or encoder only keeps a single instance of the corresponding field decoder or encoder class. DRUltraFieldDecoder A DRUltraFieldDecoder object is instantiated by the UDR decoder to handle all decoding of fields of the corresponding type. DRUltraFieldEncoder A DRUltraFieldEncoder object is instantiated by the UDR encoder to handle all encoding of fields of the corresponding type.

---

# Document 836: Grid Component UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654514/Grid+Component+UDRs
**Categories:** chunks_index.json

The following section explains the UDRs that can be used to create a grid component. 1 Grid UDR 2 GridColumn UDR 3 GridRow UDR Grid UDR The Grid UDR is the first UDR that is required to make a layout with rows and columns. The following fields are included in the Grid UDR : Field Description Field Description attributes (map<string,string>) This field may contain extra attributes to be added. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap . For example, to align all components in the middle text-center can be added. id (string) This field may contain the id of the component rows (list<GridRowUDR>) This field contains a list of GridRow UDRs. GridColumn UDR The GridColumn UDR is used to create a column inside a GridRow . To design the grid there is a width field that can have a value from 1-12. There are 12 template columns available per row, allowing you to create different combinations of elements that span any number of columns. Width indicate the number of template columns to span (example, 4 spans four out of 12, so this column will take 33.3% width of the total number of columns). Actual width on page are set in percentages so you always have the same relative sizing. See example: Open The following fields are included in the GridColumn UDR : Field Description Field Description attributes (map<string,string>) This field may contain extra attributes to be added. breakPoint (string) This field may contain a break point that is widths that determine how your responsive layout behaves across device or viewport sizes. Possible values are: sm (small >= 576px), md (medium >= 768px), lg (large >=992px), xl (extra large >= 1200px) and xxl (extra extra large >=1400px) Please read more on Bootstrap . components (list<ComponentUDR>) This field contain a list of child components, the components that will present in the column. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap . id (string) This field may contain the id of the component width (int) This field may contain a width value. Possible values are 1-12. There are 12 template columns available per row, allowing you to create different combinations of elements that span any number of columns. Width indicate the number of template columns to span (e.g., 4 spans four out of 12, so this column will take 33.3% width of the total number of columns). Actual width on page are set in percentages so you always have the same relative sizing. Default is auto, the column will take as much as it get. GridRow UDR The GridRow UDR is used to create a grid row with one or many columns. The following fields are included in the GridRow UDR : Field Description Field Description columns (list<GridColumnUDR>) This field contain a list of GridColumn UDRs. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap .

---

# Document 837: Preparing and Creating Scripts for KPI Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611452
**Categories:** chunks_index.json

In preparation for using KPI Management in MediationZone, you need to extract the following scripts: The scripts are as follows: flush.sh kpi_params.sh spark_common_param.sh start_master_workers.sh stop.sh submit.sh These scripts will be used for different procedures in the KPI Management - Distributed Processing sections. Preparations before extracting scripts: A Prerequisite is that Spark, ZooKeeper, and Kafka are installed. Zookeeper and Kafka should be up and running as well. For more information about this, see KPI Management - External Software . Before running the command to extract the scripts, these parameters need to be set as environment variables as they will be entered into some scripts: export KAFKA_BROKERS="127.0.0.1:9092" export SPARK_UI_PORT=4040 export MZ_PLATFORM_AUTH="mzadmin:DR-4-1D2E6A059AF8120841E62C87CFDB3FF4" export MZ_KPI_PROFILE_NAME="kpi_common.SalesModel" export MZ_PLATFORM_URL="http://127.0.0.1:9036" export ZOOKEEPER_HOSTS="127.0.0.1:2181" export SPARK_HOME=opt/spark-3.3.2-bin-hadoop3-scala2.13 export KAFKA_HOME=/opt/kafka_2.13-3.3.1 export $PATH=$SPARK_HOME/bin:$KAFKA_HOME/bin:$PATH Extracting scripts and KPI app To extract the scripts and the KPI app: Set up your preferred KPI profile. Find the kpi_spark*.mzp among the installation files and copy it to where you want to keep your KPI application files. To extract the KPI app after building it run the following command. It extracts the software needed by spark for the KPI app as well as the scripts needed for starting and configuring spark. $ cd release/packages $ java -jar kpi_spark_9.1.0.0.mzp install You will find the new directory mz_kpiapp that contains all app software. $ ls -l mz_kpiapp/, will list: app # The MZ kpi app bin # Shell script to handle the app jars # Extra jar files for the app Move the mz_kpiapp folder and add it to the PATH. Example: $ mv mz_kpiapp ~/ $ export PATH=$PATH:/home/user/mz_kpiapp/bin Set the environment variable SPARK_HOME. $ export SPARK_HOME="your spark home" These extracted scripts, kpi_params.sh and spark_common_params.sh , are more of examples than a finished configuration so you need to modify the scripts under the bin folder according to your specifications and requirements. In kpi_params.sh , KAFKA_BROKERS need to be configured with the hosts and ports of the kafka brokers. For example: export KAFKA_BROKERS="192.168.1.100:9092,192.168.1.101:9092,192.168.1.102:9092" The username and password for a user with access to the profile is needed to be entered as the property MZ_PLATFORM_AUTH , unless the default username and password mzadmin/dr is used. The password is encrypted using the mzsh command encryptpassword . The memory settings may need to be altered depending on the expected load, as well as the UI port for the KPI App inside Spark (default 4040). In addition to the addresses and ports of the platform, kafka and zookeeper may need to be updated. In spark_common_params.sh , you may need to change the master host IP and ports if applicable. Edit the kpiapp/bin/spark_common_param.sh , so it has the SPARK_HOME path. Access the conf-folder of Apache Spark, the spark-defaults.conf.template file should be renamed to spark-defaults.conf and the following configuration variables and options added: spark.driver.defaultJavaOptions --add-opens java.base/java.lang=ALL-UNNAMED  --add-opens java.base/java.lang.invoke=ALL-UNNAMED  --add-opens java.base/java.lang.reflect=ALL-UNNAMED  --add-opens java.base/java.util=ALL-UNNAMED  --add-opens java.base/java.util.concurrent=ALL-UNNAMED  --add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED  --add-opens java.base/java.io=ALL-UNNAMED  --add-opens java.base/java.net=ALL-UNNAMED  --add-opens java.base/java.nio=ALL-UNNAMED  --add-opens java.base/sun.nio.ch=ALL-UNNAMED  --add-opens java.base/sun.nio.cs=ALL-UNNAMED  --add-opens java.base/sun.util.calendar=ALL-UNNAMED  --add-opens java.base/sun.security.action=ALL-UNNAMED spark.executor.defaultJavaOptions --add-opens java.base/java.lang=ALL-UNNAMED  --add-opens java.base/java.lang.invoke=ALL-UNNAMED  --add-opens java.base/java.lang.reflect=ALL-UNNAMED  --add-opens java.base/java.util=ALL-UNNAMED  --add-opens java.base/java.util.concurrent=ALL-UNNAMED  --add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED  --add-opens java.base/java.io=ALL-UNNAMED  --add-opens java.base/java.net=ALL-UNNAMED  --add-opens java.base/java.nio=ALL-UNNAMED  --add-opens java.base/sun.nio.ch=ALL-UNNAMED  --add-opens java.base/sun.nio.cs=ALL-UNNAMED  --add-opens java.base/sun.util.calendar=ALL-UNNAMED  --add-opens java.base/sun.security.action=ALL-UNNAMED spark.master.rest.enabled true Add this to the jvmargs section of the execution context definition for the ec that will run the KPI Management workflows. For example: You can open the configuration by running: mzsh mzadmin/<password> topo open kpi_ec jvmargs { args=[ "--add-opens", "java.base/java.lang.invoke=ALL-UNNAMED", "--add-opens", "java.base/java.lang.reflect=ALL-UNNAMED", "--add-opens", "java.base/java.util=ALL-UNNAMED" ] } Starting KPI Note! Prerequisite Before you continue: Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to Spark. Before you can create the topics you must start Kafka and Zookeeper. An example order of topics are the following: kpi-input - For sending data to Spark kpi-output - For spark to write the output to, and thus back to the workflow kpi-alarm - For errors from Spark Startup Spark cluster, here kpiapp is a configurable name: $ start_master_workers.sh $ submit.sh kpiapp Submit the app: $ submit.sh kpiapp ... You should now be able to see workers, and executors: $ jps Will give you something like: pid1 Worker pid2 Worker pid3 CoarseGrainedExecutorBackend pid4 CoarseGrainedExecutorBackend pid5 DriverWrapper pid6 CodeServerMain pid8 Master

---

# Document 838: Diameter Profiles - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999602/Diameter+Profiles
**Categories:** chunks_index.json

The Diameter agents configuration includes a two profile definitions: Diameter Application Profile Diameter Routing Profile

---

# Document 839: Type Comparison Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743602/Type+Comparison+Functions
**Categories:** chunks_index.json

The following functions for Type Comparison described here are: 1 instanceOf instanceOf Returns true if the value is of the specified type. boolean instanceOf ( any myValue , any myType ) Parameter Description Parameter Description myValue The value to evaluate. Can be of any type, even primitive types. myType Any type to evaluate against Returns true or false Example - Using instanceOf consume{ if ( instanceOf( input, module1.type1 ) ) { udrRoute( ( module1.type1 )input, "link_1" ); } else if ( instanceOf( input, module2.type2 ) ) { udrRoute( ( module2.type2 )input, "link_2" ); } else { // handle error case } } You can use variables of the types list and map in myValue , but the the contained types are not tested. In order to avoid confusion, it is recommended to always use any as a contained type in the myType argument. Example - Using instanceOf with a contained type in the myType argument consume { // The contained types are int and string. map<int, string> myMap = mapCreate(int, string); list<string> myList = listCreate(string); // The expressions below are evaluated to true, // since the contained types are not tested. if ( instanceOf( myMap, map<any,any>) ) { debug("This is a map."); } if ( instanceOf( myList, list<any>) ) { debug("This is a list."); } }

---

# Document 840: Real-Time Disk_Deprecated Forwarding Agent MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655382
**Categories:** chunks_index.json

When the Disk_Deprecated forwarding agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the package FNT. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in the APL Reference Guide .

---

# Document 841: FTP Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607565/FTP+Forwarding+Agent+Transaction+Behavior
**Categories:** chunks_index.json

The transa ction behavior for the FTP forwarding agent is presented here. For further information about the general transaction behavior, see Workflow Monitor . Input/Output Data Output Data The agent emits nothing. Input Data The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is first created in the target directory, if not already created. Then, a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is closed and, finally, the file is moved from the temporary directory to the target directory. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 842: LDAP Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000966/LDAP+Agent+Configuration
**Categories:** chunks_index.json

To open the LDAP collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select LDAP in the Processing tab in the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. The LDAP agent configuration has three tabs: Connections , Advanced and Security . Connections Tab LDAP agent configuration - Connections tab The routing logic that you can select in the Connections tab applies to creating new connections. A connection pool is created towards the server nodes listed in the agent configuration. When the pool receives a new request and all of the already established connections are busy, then a new connection is established following the selected routing logic. You can choose between Round Robin and Failover. Setting Description Setting Description Routing Algorithms Settings RoundRobin If you choose Round Robin routing logic, the next server node from the list specified in the agent configuration is used to establish the connection. This is the default logic. Failover If you choose Failover routing logic, the primary server node is used to establish all the connections until the pool realizes that the server node is down. When it fails a second server is used, then the third, etc. Server Nodes Setting Host Enter the host name or IP address for the LDAP server node. Port Enter the port number for the LDAP server node. Note! If you use external references, you must provide hosts and ports in coma separated syntax: <host1>:<port1>,<host2>:<port2>. Advanced Tab LDAP agent configuration - Advanced tab Setting Description Setting Description Connection Settings Max Connections Enter the maximum number of concurrent connections permitted towards the LDAP servers. The default value is 2. Max Connection Age (ms) Enter the maximum amount of time in milliseconds that an LDAP connection can exist before closing it. The default value is 60000 milliseconds. Max Connection Wait Time (ms) Enter the maximum amount of time in milliseconds that you want the agent to wait for a connection to become available from the pool. The default value is 1000 milliseconds. If the maximum amount of time set is exceeded, an exception is thrown which is written to the pico log. No attempt is made to retry the operation. Connection Timeout (ms) Enter the connection timeout in milliseconds before assuming that the initial attempt to connect to the LDAP server is unsuccessful. The default value is 100 milliseconds. If the amount of time set is exceeded, an exception is thrown which is written to the pico log. Pending Answers Limit Enter the maximum number of operations that can be active per connection. When this limit is reached, the connection stops accepting new requests and waits for all of the answers to arrive and it is checked by the pool. The default value is 1000. Pool Check Interval (ms) Enter the time interval, in milliseconds, that you want to wait before checking the pool state after it has been marked as invalid. Invalid means no idle connections are available and all connections that are currently in use are invalid, or the pending answers limit has been exceeded for all of the connections. The default value is 2000 milliseconds. Operation Settings Number of Retries Enter the maximum number of retries permitted for an unsuccessful operation. The default value is 0. To enable retries you must select a value of 1 or more. Retry Interval (ms) Enter the time interval, in milliseconds, that you want to wait before retrying an unsuccessful operation. The default and maximum value is 1000 milliseconds. If the server is overloaded, the delay interval doubles for each retry until the maximum value of 1000 milliseconds is reached. Max Throughput (Ops/sec) Enter the maximum number of operations per second sent to be sent to the LDAP server. If you enter the value of 0, the number of operations per second is unlimited. Operation Timeout (ms) Enter the maximum amount of time in milliseconds permitted after sending a request to the LDAP server without receiving a response, before the request times out. The default value is 5000 milliseconds. If the maximum amount of time set is exceeded, an exception is thrown which is written to the pico log. If you have entered a value for the Number of Retries option, the operation request will be sent again. Otherwise, the operation request will be ignored. Enable Request ID If you want to be able to abandon an operation, you must select this check box. An identifier embedded in an LdapRequestIDUDR is sent by the agent, for every operation request, except abandon. For further information on LDAP Agent UDRs, see LDAP Agent UDRs . Security Tab LDAP agent configuration - Security tab Setting Description Setting Description Security Principal You can enter a security principal to use to bind to the LDAP server. If this field and the Security Credentials field are not populated, the LDAP server is connected to without authentication. Security Credentials You can enter the security credentials for the specified security principal. If this field and the Security Principal field are not populated, the LDAP server is connected to without authentication. Enable TLS Select this check box if you want to enable TLS security. If you select this check box, the Keystore and Keystore Password fields are available. If you enable this check box and leave the Keystore and/or Keystore Password fields empty, a TLS connection is created without a certificate. Keystore Enter the full path to the keystore file that you want to use. The keystore file must be in . jks format. This field is optional. If you leave this field empty, a TLS connection is created without a certificate. Keystore Password Enter the password for the selected keystore file. This field is optional. If you leave this field empty, a TLS connection is created without a certificate. The following command can be used to create a keystore with the Java keytool program: $ keytool -keystore clientkeystore -genkey -alias client Keytool prompts for required information such as identity details and password. Note that the keystore password must be the same as the key password.

---

# Document 843: Python Collection Agent Configuration - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608701/Python+Collection+Agent+Configuration+-+Real-Time
**Categories:** chunks_index.json

The Python collection agent configuration consists of two tabs: General and MIM . General Tab The General tab consists of three different sections: Python code for the execute block, input/output UDR types, and Interpreter configuration. Open The Python collection agent configuration - General tab Setting Description Setting Description Code Area This is the text area where you enter your code, see Python Writer's Guide for further information. The entered code will be color-coded depending on the code type, and for input assistance, a pop-up menu is available. Below the text area, there are line, column, and position indicators. See Python Code Editor Assistance . Input Types Enables selection of UDR Types. One or several UDR Types that the agent expects to receive may be selected. Set To Input Automatically selects the UDR Type distributed by an agent routing back to the Python collection agent. Output Types Select the output types that you have. Output Routes Enter the routes that each output type is sent out on. Interpreter Select which Python Interpreter Profile you want to use. If no selection is made the interpreter that has been set as default in the Python Manager will be used. MIM Tab In the MIM tab, you can set the MIMs that you want the Python collection agent to publish. Open The Python collection agent configuration - MIM tab Parameter Description Parameter Description Assigned Select the type of MIM assigned. Only MIMs of the global type are available. Name Enter the MIM resource name. Type Select the data type of MIM resource.

---

# Document 844: Starting mzsh - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646355/Starting+mzsh
**Categories:** chunks_index.json

The command and its arguments are typed in the same invoking command line, in the Unix command prompt. $ mzsh mzadmin/<password> wfstart MyWF Enter username and password, in a single parameter together with the preferred command. When the command is executed, the command prompt is returned to the Unix shell. The standard input can be passed to mzsh using the standard unix command xargs . This means that commands that are meant to be executed can be included in a script file or simply let other commands produce the commands for it, see the following example. Example - Non-interactive mode $ while read wf ; do echo $wf done < my-wfs.list | xargs mzsh mzadmin/<password> wfstart The command in the example reads the names of the workflows included in the file my-wfs.list and generates start commands for each of them. The output of this action is then fed from standard input as arguments to mzsh .

---

# Document 845: Data Veracity Forwarding Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640586/Data+Veracity+Forwarding+Example
**Categories:** chunks_index.json

This section contains one example for the Data Veracity Forwarding agent. Example Workflow and Data: DV_Forwarding_Example.zip In this workflow example for the Data Veracity Forwarding agent Data Veracity Forwarding workflow example The workflow will collect an input file, decode it and then send it forward into the Analysis_1 agent. The Data Veracity Forwarding agent then receives the erroneous UDR (TT UDR in the example) from the Analysis agent. The Analysis agent contains the following code: Analysis_1 int i = 0; consume { i = i +1; if (i < 20){ udrAddError(input, "my_error_code", "error occured"); }else{ udrAddError(input, "my_second_error_code", "error occured"); } udrRoute(input); } Should there be any existing error code associated to a particular UDR, any new error code added to the UDR will be included along with the existing code. However, the first error code associated with the UDR will be the one to be displayed on the Data Veracity Search result. To clear any existing error code, you can use the udrClearError plugin. With this code, the Analysis agent will: Send 20 UDRs to Data Veracity with the error code my_error_code while using the APL plugin, udrAddError. Send subsequent UDRs will be sent to Data Veracity with the error code my_second_error_code

---

# Document 846: Deactivation Issues - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604952/Deactivation+Issues
**Categories:** chunks_index.json

When a deactivation request is issued from the Execution Manager (or directly from the Workflow Monitor), different dialogs appear depending on the type of workflow to deactivate. Some agents are designed to wait for acknowledgment from sources that they communicate with. Thus, a stop request may take a while before acknowledged. If a network element connected to a collection agent has terminated in a bad state, causing the collection agent to hang, the Execution Context on which the workflow is running must be restarted. UDRs already in the workflow, will be processed if they can be processed within the time interval set in the Execution Context property ec.shutdown.time . The parameter specifies the maximum time in milliseconds that the EC will wait before a real-time workflow stops after a shutdown has been initiated. This is to enable the workflow to stop all input and drain all UDRs in the workflow before stopping. Example - Setting the property ec.shutdown.time $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.ec.shutdown.time <time in milliseconds> Note! The wait time is initially set to 60 seconds (60000 milliseconds). If this value is set to 0 (zero) all draining is ignored and the workflow will stop immediately. The parameter can be changed at any time, but the EC must be restarted before the changes take effect. For further information see the System Administration user guide . If the workflow is unable to drain the data within the specified time, the workflow still stops and any remaining data in the workflow is lost. If this occurs, a log note is added in the System Log. Real-Time Workflows Real-time workflows are deactivated immediately, accepting no more input data. If an Inter Workflow forwarding agent is included in the workflow, the last file may be incomplete. For such cases, the error handling is taken care of by the corresponding Inter Workflow collection agent. Batch Workflows Batch workflows have two termination possibilities, indicating whether to wait for the End Batch or not. If the batches are large, and the batch is being loaded by the workflow, the Immediate option will terminate the workflow without waiting for the current batch to finish. Confirmation dialog when batch workflows are deactivated Stop Type Description Stop Type Description Batch Awaits the next End Batch before unloading the workflow, that is, when the current batch is fully processed. Immediate Deactivates the workflow immediately, causing the current batch to be terminated. This may still take a while, but it is still faster than the Batch termination option.

---

# Document 847: Field Declarations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612829
**Categories:** chunks_index.json

The syntax for fields of a sequential record, record field declarations , are declared as follows: <field_type> <field_name> : <field options> ; Primitive Field Types The following primitive types are supported: Primitive Field Type Description Primitive Field Type Description ascii ASCII encoded string. This type may also be used for other types of string encodings with the character_encoding option. The default encoding used is ISO8859-1 . asn_length This is a special type used to encode a BER encoded size specification. It decodes to the BER length specification as well as the length of the length specification itself. The type makes it possible to decode special cases of BER-encoded data without using ASN.1 format specifications. The special option content_only can also be used to only get the BER length specification. bcd Array of digits encoded in BCD. Nibble order can be specified as bcd(msn_fd) or bcd(lsn_fd) . The msn_fd means that the most significant nibble is the first digit, while lsn_fd uses the least significant nibble as first digit. The msn_fd is default. bytearray The bytearray type is supported. ebcdic A special case of ascii type, equivalent to ascii: character_encoding("Cp1047") . All options available for ascii fields are also available for ebcdic fields. float types ( float , double ) Binary encoded float value. This type supports IEEE754 standard 32-bit and 64-bit data encodings. The only difference between float and double is that the field is automatically mapped to its corresponding internal type. integer types ( byte , short , int , long , bigint ) Binary coded integer value. The first byte is the most significant (that is, big endian order). The field can be used with the field options unsigned or signed to specify if the data is decoded unsigned (which is the default) or two-complement signed. The byte order can also be explicitly specified by, for instance, using int(little_endian) or int(big_endian) as the type name. The only difference between the types, when using an automatic in_map , is that the field is automatically mapped to its corresponding internal type. Note! Since the integer types are handled internally as fixed-length signed integers (except for bigint), there can be overflows in both decoding and encoding. If this occurs the integer values are truncated. msp_length This is a special type used to decode the length field in a Siemens MSP billing event. The length field specifies the event length excluding the length field itself. external MSP_BILLING_EVENT { msp_length l : external_only; ascii v : dynamic_size(l); }; list Type that can be used to decode a list of elements. external MySubUDR { int dataLength : static_size(1); ascii secretData : dynamic_size(dataLength); }; external MyUDR { int elementCount : terminated_by(":"); list<MySubUDR> myList : element_count(elementCount); int userType : terminated_by(0xA); }; The list can have any of the following field size options static_size , dynamic_size , terminated_by , or element_count . For further information, see the section below, Field Size Specifications Primitive Field Options Primitive Field Option Description Primitive Field Option Description character_encoding(<encoding_name>) Specifies that the field is encoded with the encoding named <encoding_name> , using the standard Java encoding support. encode_value(<expr>) Specifies that the encoded value of the field is always <expr> , regardless of whether decoded or set value is chosen during the processing. This is used for encoding only. float , double Informs the decoder that the value is a float value specified as a string and that the automatic mapping of the field is the internal type float or double . Only applicable for ascii fields. int(base10) , int(base16) Informs the decoder that the value is an integer of decimal or hexadecimal base and that the automatic mapping of the field is of the internal type int . The integer types ( byte , short , long , bigint ) can be used instead of int , in which case automatic mapping is done to the specified type. Only applicable for ascii and bcd fields. lsb(<int_constant>) The least significant bit of the field is <int_constant> . Its value range is zero (0) to seven (7) (both inclusive). Only applicable for size one (1) integer type fields. This option is typically used together with the lsb option. msb(<int_constant>) The most significant bit of the field is <int_constant> . Its value range is zero (0) to seven (7) (both inclusive). Only applicable for size one (1) integer type fields. This option is typically used together with the lsb option. native_size(<expr>) Specifies the number of BCD digits for a bcd declared field. This size does not cover field size calculation and dynamic_size must generally also be specified. byte_alignment(<int_constant>) Specifies that a field begins at the next even multiple of an alignment byte size. The value must be an even power of 2 (for example 1, 2, 4, or 8). This field option can also be used in a bit_block or repeat_block . For an example of how this option can be used, see the section below, Bit Blocks. Note! The byte_alignment field option is used for decoding only, and counts from the start of the UDR. Field Size Specifications Field Size Specification Description Field Size Specification Description static_size(<constant>) Used to specify a static size of a field (in bytes). dynamic_size(<expr>) Used to specify a dynamic size of a field (in bytes). element_count(<expr>) Used to specify the size of a list field (in number of elements). terminated_by(<constant>) Used to specify a dynamic field terminated by a specific constant. bit_size(<expr>) Used to specify a size of a field (in bits). This size specification can only be used inside a bit_block . padded_with(<constant>) Used to specify padding character. align(left) Specifies that the field is left-aligned, default. align(right) Specifies that the field is right-aligned. When decoding a field, the size calculation is done in two steps. First the occupying size is calculated. This is the required field size in the record. After that the core size and offset are calculated, which comprise the part of the field actually decoded into the internal field. The occupying size is calculated as follows: If static_size is speci fi ed, this one is used. If dynamic_size is speci fi ed, then this one is used. If element_count is speci fi ed, then this one is used. If terminated_by is speci fi ed, then this one is used. The fi eld size includes the termination character but never takes up more than the total remaining size in the UDR. (The reason that this is not considered as a decoding error is to support the trailing_optional fi eld option). Otherwise (if the fi eld type supports it) the fi eld size is deduced directly from the type. This is supported by constructed types (sub-records) and the asn_length primitive type. The core field data always has the full occupying size for constructed fields (record fields). For primitive fields the size is specified as follows: For a BCD field, with native_size specified, this along with the alignment specification is used. If terminated_by is used to find the occupying size, this terminator char (or nibble for BCD) is removed. Any padding is removed (while considering the alignment specification). The padding is either specified with padded_with or with terminated_by providing the occupying size is not calculated using the terminator (this case is present for historical reasons and in current versions padded_with should be used instead). If the field is an ASCII field, space is used as default padding. Field Options for Optional Fields The following field options are used to specify when a field is present. Field Option Description Field Option Description present if(<condition>) The field is present if the <condition> evaluates to true . trailing_optional The field is present unless the end of the UDR data has been reached. This is a convenient option equivalent to present if(remaining_size >0) . Other Field Options Field Option Description Field Option Description external_only The field is not automatically created in the target_internal when performing automatic mapping. This is useful for fields containing decoding logic and provide no useful information after decoding. Typical examples could be recordLength and recordType fields. udr_size and remaining_size Fields may need to use the size of the containing record in expressions. This is done by using the udr_size keyword. Example - udr_size external SimpleSequential { int recordType : static_size(1); ascii secretData : dynamic_size(udr_size-1); }; In the previous example, the size of SimpleSequential is unknown at declaration time. However, when a size is provided (specified in a parent record type), the secretData field occupies this entire space minus one byte (which is used by the recordType field in this example). Note! If the size is not supplied by a parent record, the record size calculation rules results in an undefined size since the udr_size value is unavailable before the size has been calculated, causing a decoding error. The other special value that depends on the record size is remaining_size , which is the size remaining until the end of the record. The previous example could have been written using remaining_size instead of udr_size , and is shown in the following example. Example - remaining_size external SimpleSequential { int recordType : static_size(1); ascii secretData : dynamic_size(remaining_size); }; Bit Blocks Bit blocks are used when the data record contains fields that are not byte aligned. When declaring fields in bit blocks there are two ways to specify which bits to use for the field content. When using a bit_block of a single byte, it is possible to specify the most and least significant bit of the field using msb and lsb , as previously described. The alternative is to use the bit_size option to specify the number of bits spanned by the field. You can also use the byte_alignment field option if you need to specify from which byte a field begins. This field option can only be used for decoding. For further information on byte_alignment , see the section above, Primitive Field Types. The general syntax of the bit blocks is as follows: bit_block : <size specification> [, present if(<cond>) ] { <bit_block contents> }; Example - bit_block with msb and lsb bit_block : static_size(1) { int LACLength : msb(7), lsb(4); int OwnerIDLength : msb(3), lsb(0); }; Example - bit_block with bit_size bit_block int hour : bit_size(5); int minute: bit_size(6); int second: bit_size(6); int eventId: bit_size(3); }; Example - bit_block with byte_alignment This example shows how the byte_alignment field option can be used in a bit_block , in which the secondBit field begins in the last byte in a bit_block of five bytes: external BitBlock_ByteAlignment { bit_block : static_size(5) { byte firstBit: bit_size(1); byte secondBit: bit_size(1), byte_alignment(4); }; }; Except for simple fields, a bit_block can contain repeat_block constructs in the contents part. For a description of repeat_block see the section below, Repeat Blocks. Repeat Blocks A repeat_block can be used to specify that a group of fields is to be repeated a specified number of times. Currently this construct can only be used inside bit_block structures or another repeat_block structure. However this is restricted to a maximum of two levels of repeat_block. See the example below. You can also use the byte_alignment field option if you need to specify from which byte a field begins. This field option can only be used for decoding. For further information on byte_alignment , see the Primitive Field Types section above. The general syntax of the repeat blocks is as follows: repeat_block ( <repeat count> ) { <repeat_block fields> }; Example - repeat_block external BitBlockTest { bit_block : dynamic_size(remaining_size){ int string_count: bit_size(8); repeat_block(string_count) { int string_length: bit_size(8); repeat_block(string_length) { int character: bit_size(8); }; }; }; }; Note! It is not possible to encode to a structure containing a repeat_block . Constructed Types A sequential field can be a type that is an instance of another external format. Example - Constructed types external MyParentFormat { int field1 : static_size(4); MyEnclosedFormat field1; }; Here MyEnclosedFormat can be any external format. set Construct The set construct is used for decoding formats containing optional blocks of additional data. The syntax of the set Construct is declared as follows: Example - set Construct external MyFormat: dynamic_size(recordSize) { int recordSize: static_size(4); set : dynamic_size( remaining_size ) { MyPackage1 package1: optional; MyPackage2 package2: optional; list<MyPackage3> package3; }; }; All the formats, MyPackage1-3 , must be declared with the identified_by option. The optional packages may appear in any order in the input file, however it is confirmed they do not appear more than once. Currently all fields in a set construct must be declared optional. If the field type in the set is a list type, the set may contain multiple records of the list element type. The list type fields are not optional. Instead, when no matching records are found, the list is empty. If a size is not specified on the set level, Ultra cannot validate that all the data in the UDR has been decoded. The user is therefore recommended to specify the size, unless the set size in advance is unknown (for instance if the record is terminated by a terminator package or the set size calculation is needed for the record size calculation). The dynamic_size(remaining_size) specification used in the previous example is often correct. switched_set Construct The switched_set construct can often be used instead of the set construct. It has advantages (in performance and in ease of usage) especially when the separate sub-packages are simple. The syntax is however more complex compared to the basic set construct. The syntax of the switched_set construct is declared as follows: switched_set( <switch field> ) [: <size specification> ] { <prefix fields> <switch cases> [<default case>] }; The size specification is allowed to contain normal size options. The other parts of the declaration are the prefix fields, decoded for each package in the set and the prefix fields. All the prefixes must have static sizes. The switch field must be one of the prefix fields.The syntax of the switch case is declared as follows: case( <case value> ) [: include_prefix] { <case fields> }; The case fields are normal field specifications with the additional possibility of declaring list fields for the case where a package can be present repeatedly. If include_prefix is specified, then the case body is decoded including the prefix fields. The syntax of the default case is declared as follows: default [: include_prefix] { <case fields> }; The decoding of a switched_set is performed according to the following steps: Decode the prefix fields. Decode the case matching the value of the switch field. If no case matches, decode the default case. If there is no default case, end the switched_set decoding. Repeat steps 1-2 until the switched_set size (or the end of the UDR) is reached. Example - Format with a switched_set: external SwitchedSetExample: terminated_by(0xA) { // Size is remaining_size -1 (minus the terminator linefeed) switched_set( packageId ): dynamic_size( remaining_size - 1 ) { ascii packageId: int(base10), static_size(1); ascii packageLength: static_size(1), int(base10), encode_value( case_size - 2 ); case(1) { list<ascii> list1: dynamic_size( packageLength ); }; case(2): include_prefix { ascii packageId_3: int(base10), static_size(1), encode_value(3), external_only; ascii packageLength_3: int(base10), static_size(1), encode_value(case_size - 2), external_only; ascii body_3: dynamic_size( packageLength_3 ); }; default: include_prefix { list<ascii> defaultContent: dynamic_size( packageLength + 2 ); }; }; }; Encoding Specifications and Expressions To support encoding to binary formats, it is often necessary to explicitly specify which value to be encoded in the external fields. Normally the value is taken from the corresponding internal field, however there are cases when this is not desirable. For instance, if there is no mapped internal field (because the external_only option has been used), or the value must be calculated from information about the encoding (for instance, udr_size ). This is done through the encode_value option and there are several special constructs that may be used in the value expression (see the section above, Primitive Field Types). udr_size - evaluates to the encoded size of the UDR. This is not necessarily the same value as during decoding. field_size(fieldName) - evaluates to the encoded size of the named field. field_present(fieldName) - evaluates to true if the named field is present in the encoding. It is always true for non-optional external fields. case_size - this is only usable within switched_set blocks and evaluates to the encoded size of the current case (including prefix fields). If the size expressions are used, the field encoding has to be postponed until the size is known. To be able to do this, Ultra requires that any such fields are static_size . An example of these concepts is presented next. Example - Encoding specifications and expressions external Ext: dynamic_size( udrSize ) { ascii udrSize: int(base10), static_size(3), align(right), padded_with("0"), encode_value( udr_size ); ascii fieldSize: int(base10), static_size(3), align(right), padded_with("0"), encode_value( field_present( strField ) ? field_size( strField ):0 ); ascii strField: dynamic_size( fieldSize ), present if( fieldSize > 0 ); }; When processing an encode_value instruction, Ultra automatically decides how to convert the value depending on the result type of the expression. When deciding this, Ultra starts with the default internal type of the external field. In this case, the type is called defaultType and the expression type is encodeType , the encoding rules are: If the defaultType is assignable from encodeType , use the default mapping. If the defaultType is string or bytearray and the encodeType is numeric, encode it as a simple ascii value (one byte). If the defaultType is bytearray and the encodeType is string , do standard encoding ( ISO-8859-1 ) of the string. If the encodeType is string and the external base type is ascii (for example using int(base10) ), use direct string encoding. If none of these rules apply, the format will not compile. To understand what this means, consider the following field definitions. Example - Field definitions ascii strField1: static_size(1), encode_value("10"); ascii strField2: static_size(1), encode_value(10); ascii intField1: static_size(1), int(base10), encode_value("10"); ascii intField2: static_size(1), int(base10), encode_value(10); Expected encoded results for these fields: Field Expected encoded result Field Expected encoded result strField1 Both defaultType and encodeType are strings . The normal encoding is used to get the result "1" (since static_size(1) is used, the result "10" is truncated to one byte). strField2 defaultType is string and encodeType is byte (numeric). This means that the second rule is applied, and the result is ascii 10 (newline). intField1 defaultType is int and encodeType is string . There is no mapping between these types, however, since the external base type is ascii , the string is mapped out as for strField1 , and the result is "1". intField2 defaultType is int and encodeType is byte . Since encodeType is directly assignable to defaultType , it is mapped out as normal, and the output is again "1".

---

# Document 848: Increasing the Maximum Number of File Descriptors - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744562/Increasing+the+Maximum+Number+of+File+Descriptors
**Categories:** chunks_index.json

You may need to increase the maximum number of file descriptors on a system with large configurations. The procedure below describes how to increase the limit on a Linux system. Check the current limit: $ cat /proc/sys/fs/file-max Change the limit: $ sysctl -w fs.file-max=<new limit> The limit will be reset to the previous value if the host is rebooted. Add or update the following line in the file /etc/sysctl.conf to make the change permanent: fs.file-max=<new limit>

---

# Document 849: Inter Workflow Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605963/Inter+Workflow+Profile
**Categories:** chunks_index.json



---
**End of Part 37** - Continue to next part for more content.
