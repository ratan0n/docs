# RATANON/MZ93-DOCUMENTATION - Part 8/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 8 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.3 KB
---

**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741536/Batch-Based+Real-Time+Agents+-+Example+Workflow
**Categories:** chunks_index.json

This section provides an example of how you can configure the SFTP collection agent in a real-time workflow. Open Example workflow with SFTP collection agent in a real-time workflow SFTP_1 In this instance, the SFTP collection agent is configured for the workflow to run every 3 seconds. When a decoding error occurs, processing of the current batch is stopped and skips to the next batch, and the workflow is to abort immediately on the first Cancel Batch message from any agent in the workflow. Example SFTP collection agent configuration - Execution tab Python_1 The Python processing agent is configured to handle the incoming UDRs. Example Python processing agent configuration Output The output in the workflow monitor in the image below shows that when an error occurs, the workflow does not abort and the agent retries at the next repeat. The error is also reported in the System Log. Open Example output in Workflow Monitor

---

# Document 169: File System Type -Amazon S3 - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/315195442
**Categories:** chunks_index.json

When selecting Amazon S3 as a file system, you will see two tabs  General and Advanced . Open Amazon S3 File System Type Configuration General tab The following settings are available in the General tab in the File System profile: Setting Description Setting Description File System Type Select which file system type this profile should be applied for. Currently, only Amazon S3 is available. Credentials Settings Credentials from Environment Select this check box in order to pick up the credentials from the environment instead of entering them in this profile. If this check box is selected, the Access Key and Secret Key fields will be disabled. Access Key Enter the access key for the user who owns the Amazon S3 account in this field. Secret Key Enter the secret key for the stated access key in this field. Location Settings Region from Environment Select this check box in order to pick up the region from the environment instead of entering the region in this profile. If this check box is selected, the Region field will be disabled. Region Enter the name of the Amazon S3 region in this field. Bucket Enter the name of the Amazon S3 bucket in this field. Use Amazon Profile Select this check box if you already have an Amazon Profile set up, this will disable the fields above and allow you to utilize the credentials that you have defined in your chosen Amazon Profile. Advanced tab The Advanced tab allows for advanced properties to be configured in the profile. Open File System profile - Amazon S3 - Advanced tab

---

# Document 170: Access Controller - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737438/Access+Controller
**Categories:** chunks_index.json

To be able to operate the system, you need to be defined as a user in the system, and these permissions are configured in the Access Controller. Your access to various applications is defined by the access group that you are assigned to. The Execute permission means that members of an access group can view and read the information in that application. While the Write permission means that the members can perform change or create action in that application. Note! By default, members of the predefined group Administrator have full permissions for the Access Controller. You can enable these permissions for other groups as well. When no members belong in the Administrator group, all users with full permissions for the Access Controller will have Administration access. It is not possible to disable or delete the last active user with full permissions for the Access Controller. This is to prevent system lockout. Members that are not part of the Administrator group will not be able to remove or modify the Administrator group and any of its group members. Only one user may use the Access Controller with write permissions at any given time. It is not possible to delete the last group with members that have full permissions for the Access Controller. This is to prevent system lockout. It is possible to use SCIM via the REST HTTP interface to POST, GET, DELETE, PUT and PATCH user and group configurations. To open the desktop online Access Controller, Go to Manage  Tools & Monitoring and then select Access Controller . System Logging Activity Below are a list of activities that will trigger a system log entry related to Access Controller. Create new user. Update user details. Change user password. Create new SSO user upon first initial login. SSO users group changes from group sync. This section contains the following sub-sections: Users Tab Groups Tab Advanced Tab

---

# Document 171: Distributed Storage Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743566/Distributed+Storage+Functions
**Categories:** chunks_index.json

The Distributed Storage Profile enables you to access a distributed storage solution from APL without having to provide details about its type or implementation. The APL functions described in this section use a key-value model to read, store and remove data via the specified profile. The functions can optionally use sessions that are either committed or rolled back in order to secure transaction consistency. The following functions for Distributed Storage described here are: 1 dsInitStorage 2 dsStore 3 dsStoreUdr 4 dsStoreMany 5 dsStoreManyUdrs 6 dsCommand 7 dsRemove 8 dsRemoveMany 9 dsRead 10 dsReadUdr 11 dsReadMany 12 dsReadManyUdrs 13 dsBeginTransaction 14 dsCommitTransaction 15 dsRollbackTransaction 16 dsCreateKeyIterator 17 dsCreateREKeyIterator 18 dsGetNextKey 19 dsDestroyKeyIterator 20 dsLastErrorMessage Initializing Storage dsInitStorage The dsInitStorage function returns a Distributed Storage Profile object that is used in subsequent APL calls to distributed storage functions. any dsInitStorage(string profid) Parameter Description Parameter Description profid A string containing a reference to a Distributed Storage Profile. Returns A Distributed Storage Profile object. Example - Using dsInitStorage any storage=null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } Note! The dsInitStorage function must be used in the initialize block and not consume block. Storing and Removing Data The following functions enable you to perform changes on data in a distributed storage: dsStore dsStoreUdr dsStoreMany dsStoreManyUdrs dsRemove dsRemoveMany dsStore The dsStore function stores a key-value pair in a distributed storage. void dsStore( any profid, string key, bytearray value, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. key The key of a key-value pair. value The value of a key-value pair. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to store data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsStore any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { bytearray myValue = null; strToBA(myValue, "123"); dsStore(storage, "mykey", myValue, null); if(null != dsLastErrorMessage()) { //Error Handling } } dsStoreUdr The dsStoreUdr function stores a key-value pair in a distributed storage. void dsStoreUdr( any profid, string key, drudr myUDR, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. key The key of a key-value pair. myUDR The UDR to be stored. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to store data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsStoreUDR any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { default.dataUDR udr = udrCreate(default.myultra.dataUDR); dsStoreUdr(storage, "mykey", udr, null); if(null != dsLastErrorMessage()) { //Error Handling } } dsStoreMany The dsStoreMany function stores a set of key-value pairs in a distributed storage. void dsStoreMany( any profid, map<string, bytearray> values, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. values The key-value pairs to store. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to store data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsStoreMany any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { map<string, bytearray> myValues = mapCreate(string, bytearray); bytearray value1; bytearray value2; strToBA(value1, "abc"); mapSet(myValues,"first", value1); strToBA(value2, "def"); mapSet(myValues,"second", value2); dsStoreMany(storage, myValues, null); if(null != dsLastErrorMessage()) { //Error Handling } } dsStoreManyUdrs The dsStoreManyUdrs function stores a set of key-value pairs in a distributed storage. void dsStoreManyUdrs( any profid, map<string, drudr> myUDRs, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. myUDRs The key-value pairs to store. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to store data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsStoreManyUDRs any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { map<string, drudr> myUDRs = mapCreate(string, drudr); default.dataUDR udr1 = udrCreate(default.myultra.dataUDR); mapSet(myUDRs,"first", udr1); default.dataUDR udr2 = udrCreate(default.myultra.dataUDR); mapSet(myUDRs,"second", udr2); dsStoreManyUdrs(storage, myUDRs, null); if(null != dsLastErrorMessage()) { //Error Handling } } dsCommand Note! This function is only valid for Couchbase. The dsCommand function stores data with a Time To Live (TTL) on the entry by updating the expiration field for data in Couchbase, meaning it will be removed when it gets too old. void dsCommand( any profid, string "ttlStore" string key, int ttl, drudr myUDR) You can also use the dsCommand to extend the expiration field of the data in Couchbase using the touch action . void dsCommand( any profid, string "touch" string key, int ttl) Parameter Description Parameter Description profid The Distributed Storage profile. action The action you want to perform, ttlStore for storing UDRs with TTL or touch to extend the TTL. key The key of a key-value pair. ttl Time To Live in seconds. myUDR The UDR to be stored. Returns Nothing . Example - Using dsCommand int ttl = 50; any result = dsCommand(storage, "ttlStore", "3333", ttl,input); debug(result); sleep(10000); result = dsCommand(storage, "touch", "3333", ttl); dsRemove The dsRemove function removes a key-value pair, identified by a key, from a distributed storage. void dsRemove( any profid, string key, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. key The key of the key-value pair to remove. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to remove data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsRemove any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { //It is assumed here that "mykey" is already stored. dsRemove(storage, "mykey", null); if(null != dsLastErrorMessage()) { //Error Handling } } dsRemoveMany The dsRemoveMany function removes a list of key-value pairs, identified by a list of keys, from a distributed storage. void dsRemoveMany( any profid, list<string> keys, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. keys The keys of the key-value pairs to remove. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to remove data in non-transaction mode. If the identifier is set the transaction is completed by calling dsCommitTransaction. Returns Nothing. Example - Using dsRemoveMany any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { list<string> myKeys = listCreate(string); int i = 0; // It is assumed here that "KEY0", "KEY1".."KEY9" // are already stored. while (i < 10) { listAdd(myKeys, "KEY" + (i)); i = i + 1; } dsRemoveMany(storage, myKeys, null); if(null != dsLastErrorMessage()) { //Error Handling } } Reading Data The following functions enable you to read data from a distributed storage. dsRead dsReadUdr dsReadMany dsReadManyUdrs dsRead The dsRead function reads a value, identified by a key, from a distributed storage. bytearray dsRead( any profid, string key, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. key The key of a key-value pair. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to read data in non-transaction mode. If the identifier is set, the read key-value pair is locked for other transactions untildsCommitTransaction or dsRollbackTransaction is called. Returns A bytearray containing the value of a key-value pair. Example - Using dsRead any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { //It is assumed here that "mykey" is already stored. bytearray myValue = dsRead(storage, "mykey", null); if(null != dsLastErrorMessage()) { //Error Handling } } dsReadUdr The dsReadUdr function reads a set of UDRs, identified by a list of keys, from a distributed storage. drudr dsReadUdr( any profid, string key, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. key The key of a key-value pair. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to read data in non-transaction mode. If the identifier is set, the read key-value pair is locked for other transactions untildsCommitTransaction or dsRollbackTransaction is called. Returns A UDR identified by the key parameter. Example - Using dsReadUDR any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { //It is assumed here that "mykey" is already stored. default.myultra.dataUDR udr = (default.myultra.dataUDR) dsReadUdr(storage, "mykey", null); if(null != dsLastErrorMessage()) { //Error Handling } } dsReadMany The dsReadMany function reads a set of key-value pairs, identified by a list of keys, from a distributed storage. map<string, bytearray> dsReadMany( any profid, list<string> keys, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. keys The keys of the key-value pairs to be read. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to read data in non-transaction mode. If the identifier is set, the read key-value pairs are locked for other transactions untildsCommitTransaction or dsRollbackTransaction is called. Returns A map containing the key-value pairs. Example - Using dsReadMany any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { list<string> myKeys = listCreate(string); int i = 0; // It is assumed here that "KEY0", "KEY1".."KEY9" // are already stored. while (i < 10) { listAdd(myKeys, "KEY" + (i)); i = i + 1; } map<string, bytearray> myMap = mapCreate(string, bytearray); myMap = dsReadMany(storage, myKeys, null); if(null != dsLastErrorMessage()) { //Error Handling } } dsReadManyUdrs The dsReadManyUdrs function reads a key-value map, identified by a key, from a distributed storage. map<string, drudr> dsReadManyUdrs( any profid, list< string keys, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. keys The keys of the key-value pairs to be read. transid The transaction identifier that is returned by dsBeginTransaction. Use a null value to read data in non-transaction mode. If the identifier is set, the read key-value pairs are locked for other transactions untildsCommitTransaction or dsRollbackTransaction is called. Returns A map containing the key-value pairs. Example - Using dsReadManyUDRS any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { list<string> myKeys = listCreate(string); int i = 0; // It is assumed here that "KEY0", "KEY1".."KEY9" // are already stored. while (i < 10) { listAdd(myKeys, "KEY" + (i)); i = i + 1; } map<string, default.myultra.dataUDR> myUdrMap = mapCreate(string, default.myultra.dataUDR); myUdrMap = dsReadManyUdrs(storage, myKeys, null); if(null != dsLastErrorMessage()) { //Error Handling } } Transaction Functions The following functions provide transaction safety by performing commit or rollback on a set of calls to a Distributed Storage Profile: dsBeginTransaction dsCommitTransaction dsRollbackTransaction dsBeginTransaction The dsBeginTransaction function begins a new transaction and returns an object that can be used in subsequent APL calls to distributed storage functions. any dsBeginTransaction(any profid) Parameter Description Parameter Description profid The Distributed Storage profile. Returns A transaction identifier. dsCommitTransaction The dsCommitTransaction function ends a transaction and commits changes that has been made using the specified transaction identifier. void dsCommitTransaction( any profid, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. transid The transaction identifier that is returned by dsBeginTransaction. Returns Nothing. Example - Using dsCommitTransaction any storage = null; any transid = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { transid = dsBeginTransaction(storage); bytearray myValue = null; strToBA(myValue, "123"); dsStore(storage, "mykey1", myValue, transid); if(null != dsLastErrorMessage()) { //Error Handling } dsStore(storage, "mykey2", myValue, transid); if(null != dsLastErrorMessage()) { //Error Handling } dsCommitTransaction(storage, transid); if(null != dsLastErrorMessage()) { //Error Handling } } dsRollbackTransaction The dsRollbackTransaction function ends a transaction and reverts changes that has been made using the specified transaction identifier. void dsRollbackTransaction( any profid, any transid) Parameter Description Parameter Description profid The Distributed Storage profile. transid The transaction identifier that is returned by dsBeginTransaction. Returns Nothing. Example - Using dsRollbackTransaction any storage = null; any transid = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { transid = dsBeginTransaction(storage); bytearray myValue = null; strToBA(myValue, "123"); dsStore(storage, "mykey1", myValue, transid); if(null != dsLastErrorMessage()) { //Error Handling } dsStore(storage, "mykey2", myValue, transid); if(null != dsLastErrorMessage()) { //Error Handling } dsRollbackTransaction(storage, transid); //nothing is stored if(null != dsLastErrorMessage()) { //Error Handling } } Iterator Functions The following functions are used for traversing through a set of keys in a distributed storage: dsCreateKeyIterator dsCreateREKeyIterator dsGetNextKey dsDestroyKeyIterator dsCreateKeyIterator Note! This iterator function is only valid for Couchbase. The dsCreateKeyIterator function creates an iterator object that is used in subsequent APL calls to other iterator functions. any dsCreateKeyIterator( any profid, string startkey, string stopkey) Parameter Description Parameter Description profid The Distributed Storage profile. startkey The key to start iterating at. stopkey The key to finish iterating at. Returns An iterator object used in subsequent APL calls to dsGetNextKey and dsDestroyKeyIterator.7.5.2. dsGetNextKey. dsCreateREKeyIterator Note! This iterator function is only valid for Redis. The dsCreateREKeyIterator function creates an iterator object that is used in subsequent APL calls to other iterator functions. any dsCreateREKeyIterator( any profid, string searchpattern) Parameter Description Parameter Description profid The Distributed Storage profile. searchpattern A search pattern which may include wild card '*'. Returns An iterator object used in subsequent APL calls to dsGetNextKey and dsDestroyKeyIterator. dsGetNextKey The dsGetNextKey function gets the next available key from a distributed storage using an iterator object. Each subsequent call to the function returns the next key in the iterator's range. string dsGetNextKey( any profid, any iterator) Parameter Description Parameter Description profid The Distributed Storage Profile. iterator The iterator object returned by dsCreateKeyIterator. Returns The key of a key-value pair. A null value is returned if a key cannot be found. Example - Using dsCreateKeyIterator any storage = null; initialize { storage = dsInitStorage("default.ds_profile"); if(null != dsLastErrorMessage()) { //Error Handling } } consume { string startKey = "mystartkey"; string stopKey = "mystopkey"; any iterator = dsCreateKeyIterator(storage, startKey, stopKey); string key = dsGetNextKey(storage, iterator); if(null != dsLastErrorMessage()) { //Error Handling } // It is assumed here that "mystartkey" and "mystopkey" // are already stored while ( null != key) { debug(dsRead(storage, key, null)); if(null != dsLastErrorMessage()) { //Error Handling } key = dsGetNextKey(storage, iterator); if(null != dsLastErrorMessage()) { //Error Handling } } dsDestroyKeyIterator(storage, iterator); if(null != dsLastErrorMessage()) { //Error Handling } } dsDestroyKeyIterator The dsDestroyKeyIterator function destroys an iterator object created with dsCreateKeyIterator in order to free up memory. It is good practice to always include a call to this function in the APL code though it may not be required for all types of storage. void dsDestroyKeyIterator( any profid, any iterator) Parameter Description Parameter Description profid The Distributed Storage profile. iterator The iterator object returned by dsCreateKeyIterator. Returns Nothing Error Handling dsLastErrorMessage The dsLastErrorMessage function returns the error message from the last call to a distributed storage function. Distributed storage functions that are not used for error handling, do not return error codes and generally do not cause runtime errors. For this reason, The dsLastErrorMessage function must be called after each operation to validate success. string dsLastErrorMessage() Parameter Description Parameter Description Returns An error message, or null if no error has occurred since the last call to this function. The content of message depends on the type of profile that is used with the Distributed Storage profile. For example, with a Couchbase profile, the error message is identical to the error message exposed by the Couchbase API.

---

# Document 172: TextArea UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643785/TextArea+UDR
**Categories:** chunks_index.json

The TextArea UDR is used to create an input for text areas. When this component is used inside a Form UDR the Form UDR can not use encoding PLAIN_TEXT, otherwise the decoding of parameters will fail. You can use this APL code to create a text area with 10 rows TextArea myTextArea = udrCreate(TextArea); myTextArea.rows = 10; myTextArea.placeholder = "Enter your text here:"; The following fields are included in the TextArea UDR : Field Description attributes (map<string,string>) This field may contain extra attributes to be added. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap . disabled (boolean) This field may contain a boolean if the component should be disabled or enabled. id (string) This field may contain the id of the component label (string) This field may contain the label for the text area. labelCssClasses (list<string>) This field may contain a list of extra values added to class attribute of the label. This is typically used to style the component. Please read more on Bootstrap . name (string) This field may contain the name of the component. If the component is present in a Form UDR , the name will be submitted with the form as the key in the Params Map in Request UDR . placeholder (string) This field may contain a placeholder can be used as a help text. readonly (boolean) This field may contain a boolean if the field is readonly. required (boolean) This field may contain a boolean if the component is required. Typically used inside a Form UDR. rows (int) This field may contain a value to specifies how many rows should be visible. value (int) This field may contain a value.

---

# Document 173: mzcli - dumpsyslog - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979489/mzcli+-+dumpsyslog
**Categories:** chunks_index.json

Usage dumpsyslog [-n <N>] [-b] [-m] [-s <I|W|E|D|T> [-d <DATE>] [-h <HOUR>] [-f <FILENAME>] [ -t ] [-q <C|S|D>] Example: dumpsyslog -d 1970-01-01 will give all log entries for the specified date Example: dumpsyslog -d "1970-01-01 2020-06-26" will give all log entries between the two dates. -n enter the (max) number of entries to display (entries are displayed starting from the end of the log, unless -n is used together with -b option) -b entries are shown starting from the beginning of the log (used in conjunction with -n option) -s severity types to display -d filter for a certain date or date interval -h filter for hour or time interval -f direct output to a file -t display stacktraces -m display full log message -q query mode C gives the number of entries S number of information/warning etc D earliest/latest date of entries This command allows you to display and save entries from the System Log. Options Option Description Option Description [-n <N>] Maximum number (N) of entries to dump. By default the most recent entries in the System Log are shown. [-b] Select entries from the beginning of the System Log, that is, the oldest entries instead of the most recent entries. This option is preferably used in conjunction with the -n option. [[ -s ]] Displays the severity type for each entry. The possible severity types are: I - Information W - Warning E - Error D - Disaster [-d <DATE>] Filter on a certain date or time interval. The date must be given in the format specified in the config.xml file. If only one date is given, the display will hold entries for that date only (time 00:00:00 to 23:59:59), unless the -h option is used. MZ>> dumpsyslog -d 1982-01-01 Will give all log entries for the specified date from 00:00:00 to 23:59:59. If an interval is given, the entries must be enclosed within quotation marks "d1 d2": MZ>> dumpsyslog -d "1982-01-01 2007-12-07" Will give all log entries between the two dates, from date1 (time 00:00:00) to date2 (time 23:59:59). [-h <HOUR>] Filter on a certain hour or time interval. Time is given in the format hh:mm:ss and if an interval is given the times have to be enclosed within quotation marks; "h1 h2" . If only one time is given, the display will hold entries for that hour only (time 00:00:00 to 00:59:59). If giving two times, all entries in the interval will be displayed. Note! If both date and time intervals are entered; -d "d1 d2" -h "h1 h2" , the displayed entries will be from date d1 at time h1 to date d2 at time h2 . [-f <FILENAME>] Direct output to a file (including directory path). [-q <C|S|D>] Displays general information about the System Log entries. C - Number of entries S - Number of entries of each severity D - Earliest and latest date of entries [-t] To include stack trace information in the System Log printout. [-m] Sometimes log messages can get truncated, so use this flag to display complete log messages. Return Codes Listed below are the different return codes for the dumpsyslog command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if there was a problem parsing command parameters. 2 Will be returned if there was an error when trying to access the system log database. 3 Will be returned if the output file already exists, or in the event of file write errors.

---

# Document 174: Python Collection Agent Events - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608849/Python+Collection+Agent+Events+-+Batch
**Categories:** chunks_index.json

Agent Message Events

---

# Document 175: Standard Upgrade Procedure - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669969/Standard+Upgrade+Procedure
**Categories:** chunks_index.json

This section describes the standard upgrade procedure that should be used when it is important to minimize downtime and ensure that persisted data is retained. The Standard upgrades must be performed stepwise to the next consecutive main or minor version. Note! Read the Documentation Ensure to read through all the documentation for the Standard Upgrade Procedure and check out the Release Notes for Important information and Known Issues before proceeding with the Upgrade. This section contains the following subsections: Disconnect ECs Shut Down Workflows and Desktops Upgrade Preparations Upgrade Platform Container Upgrade Execution Container Post Upgrade Resume Workflow Execution If you plan to upgrade several versions, repeat the steps on the next page and onwards for each version. Use the setup.sh script provided for each version during the upgrade. Note! There may be changes to certain agents and functions in the new version. Before upgrading, you should take note of any changes or updates that may have happened to your components, as you may have to modify your configuration before it can work with the newer version. See MediationZone Release Information for detailed information about changes in each release. Note! If you have a High Availability setup, you are advised to deactivate the failover functionality in the HA cluster daemon. You must ensure that no HA activities can impede the upgrade procedure. Note! If you have any of the following configurations in your system: Workflow Packages Containing Web Services PCC Extensions (the *.mzp file(s) containing your own custom data model(s) need to be recompiled following the instructions on this page) you will have to perform some additional steps, see the respective page describing the steps for more information.

---

# Document 176: Conditional Trace User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676782
**Categories:** chunks_index.json

Search this document: Conditional Trace is a trouble-shooting function that allows you to trace data in real-time workflows. Conditional Trace templates, defining what you want to trace, are created in the Legacy Desktop, and these templates can then be used in the regular Desktop. The Conditional Trace templates can be configured to match certain workflows, certain UDR types, and fields, as well as using matches for specific parameters, using either specific values or regular expressions. The following sections are included: Conditional Trace Templates Tracing in Desktop Conditional Trace Access Permissions

---

# Document 177: Excel Decoder Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673079/Excel+Decoder+Agent
**Categories:** chunks_index.json

The Excel Decoder agent receives Excel files in bytearray format, convert the data into UDRs and route them forward into the workflow. This section contains the following subsections: Excel Decoder Agent Configuration Excel Decoder Agent Input/Output Data and MIM Excel Decoder Agent Events Excel Decoder Agent Transaction Behavior

---

# Document 178: Workflow Monitoring Framework - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849379
**Categories:** chunks_index.json

MediationZone publishes Meta Information Model (MIM) values from running workflows via JMX, this metadata can be monitored via external supervising tools such as JConsole. Also the platform functionality publishes information for supervision. The image below illustrates a JConsole window receiving runtime metadata (i.e. MIM values) from a workflow and also system parameters such as the event queue. Open Example of the JConsole window showing workflow MIM information In the Console, the MIM values can be monitored both in the form of current value and historic charts as depicted below (charts are presented by double-clicking the value digits). Open Viewing MIM information in a chart

---

# Document 179: An ASN.1 Format Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678369/An+ASN.1+Format+Example
**Categories:** chunks_index.json

This appendix shows how incoming ASN.1 records can be encoded into a sequential external format. An ASN.1 format definition can be pasted directly into an Ultra asn_block . Nested structures are saved as they are in the target_internal type, however the nested fields cannot easily be encoded to a different format without using an APL agent (Analysis or Aggregation). In the example, the incoming ASN.1 record is encoded to a sequential record, mapping all the field values to corresponding fields in the sequential format. Mapping - nested structures to plain, sequential structures - can be accomplished in three ways: By creating a constructed external format definition, that is, where the external record definition consists of sub-externals. By extending the target_internal with first level temporary fields which hold the values of the nested fields to be encoded. By creating a new UDR which is populated with values from the incoming nested UDR. Constructed Internal It is possible to encode to a constructed sequential external, that is, an external sequential definition containing other externals to represent the nested fields. The disadvantage with this approach is that it is not possible to mix different level fields in the produced output record. external Name the fields in the outgoing external exactly as in the incoming ASN.1 structure. This allows the use of automatic mapping. asn_block { exchangeRec DEFINITIONS IMPLICIT TAGS ::= BEGIN main_udr ::= SEQUENCE { duration [ APPLICATION 1 ] INTEGER OPTIONAL, calledNumber [ APPLICATION 2 ] INTEGER OPTIONAL, callingNumber subUDR1 } subUDR1 ::= [ APPLICATION 3 ] SEQUENCE { category [ APPLICATION 4 ] INTEGER OPTIONAL, adressString subUDR2 } subUDR2 ::= [ APPLICATION 5 ] SEQUENCE { number [ APPLICATION 6 ] INTEGER OPTIONAL, ton [ APPLICATION 7 ] INTEGER OPTIONAL, npi [ APPLICATION 8 ] INTEGER OPTIONAL } END }; //------------------------------------------------------- external out { ascii duration : static_size(2); ascii calledNumber : static_size(8); subUDR1 callingNumber : static_size(8); }; external subUDR1 { ascii category : static_size(2); subUDR2 adressString : static_size(6); }; external subUDR2 { ascii number : static_size(2); ascii ton : static_size(2); ascii npi : static_size(2); }; in-map and out_map Automatic mapping considers sub-UDRs as well. in_map inM : external( main_udr ), target_internal( myTI ) { automatic; }; out_map outM : internal( myTI ), external( out ) { automatic; }; decoder and encoder decoder myDec : in_map( inM ); encoder myEnc : out_map( outM ); Extending the target_internal Create an internal which holds the nested fields to be mapped to the sequential format. Define a target_internal holding both the asn_block and the internal . The values of the nested fields of the target_internal are copied to the fields added with the internal format specification, using APL code. Open APL code is necessary to extract the nested values Format Definition (shortened): asn_block { : main_udr : }; Note! All following fields are declared optional. This is to enable differentiation between an absent value and a zero value (default for int type). Thus, if no value is entered it is encoded as empty in output format, as opposed to 0 (zero). internal exchangeRec_Int { int duration_i : optional; string calledNumber_i : optional; int category_i : optional; string number_i : optional; string ton_i : optional; string npi_i : optional; }; Note! Since no automatic mapping specifications are given, no named internal types for SubUDR1 and SubUDR2 are received. This is not a problem as long as referencing the types directly (for instance in APL) is unnecessary. in_map exchangeRecord_MAP_IN: external(MainUdr), internal(exchangeRec_Int), target_internal(exchangeRec_TI){ e:duration and i:duration_i; e:calledNumber and i:calledNumber_i; automatic; }; A structure of sub-UDRs with the same field names as in the internal mapped from is created (that is, the target_internal which has the same structure as the ASN.1 external ). This only produces a line-based comma separated output file. external ConstructedOut: terminated_by("n") { ascii duration : terminated_by(","), int(base10); ascii calledNumber : terminated_by(","); SubOut1 callingNumber; }; external SubOut1: terminated_by("n") { ascii category : terminated_by(","), int(base10); SubOut2 adressString; }; external SubOut2: terminated_by("n") { ascii number : terminated_by(","); ascii ton : terminated_by(","); ascii npi : terminated_by("n"); }; out_map Constructed_Map: external(ConstructedOut), internal(exchangeRec_TI) { e:duration and i:duration_i; e:calledNumber and i:calledNumber_i; automatic; }; encoder ConstructedEnc: out_map(Constructed_Map); APL Code: consume { if ( udrIsPresent( input.callingNumber.adressString ) ) { input.category_i = input.callingNumber.category; input.number_i = input.callingNumber.adressString.number; input.ton_i = input.callingNumber.adressString.ton; input.npi_i = input.callingNumber.adressString.npi; } udrRoute( input ); } Creating a New UDR Create an internal for the sequential format, however do not add it to the incoming ASN.1 structure's target_internal . Instead for each incoming UDR, create a new UDR and copy the field values from the ASN.1 UDR. The ASN.1 UDR can then be discarded, routing the new internal further. Open Internal presentation of the input ASN.1 UDR Open Internal presentation of the output sequential UDR APL Code Definition: exRec.exchangeRec_Int outUDR = udrCreate( exRec.exchangeRec_Int ); consume { outUDR.duration_i = input.duration; outUDR.calledNumber_i = input.calledNumber; if ( udrIsPresent( input.callingNumber.adressString ) ) { outUDR.category_i = input.callingNumber.category; outUDR.number_i = input.callingNumber.adressString.number; outUDR.ton_i = input.callingNumber.adressString.ton; outUDR.npi_i = input.callingNumber.adressString.npi; } udrRoute( outUDR ); } external The ASN.1 definition can be copied directly into an Ultra asn_block definition. An external for the sequential outgoing UDRs is created. Format Definition (shortened): asn_block { : main_udr : }; // The same field names as in the internal format are used // to be able to use automatic mapping. external exchangeRecSEQ ascii duration_i ascii calledNumber_i ascii category_i ascii number_i ascii ton_i ascii npi_i }; internal An internal , containing fields matching the external field names, is created in order to hold the values to be encoded. internal exchangeRec_Int { int duration_i : optional; string calledNumber_i : optional; int category_i : optional; string number_i : optional; string ton_i : optional; string npi_i : optional; }; in_map and out_map The incoming external is turned into a target_internal , without adding any fields. The internal is mapped to the sequential external format. in_map exchangeRecord_MAP_IN_II: external(main_udr), target_internal(exRec_TI){ automatic; }; out_map exchangeRecord_MAP_OUT: external(exchangeRecSEQ), internal(exchangeRec_Int) { automatic; }; decoder and encoder decoder exchangeRec: in_map(exchangeRecord_MAP_IN); encoder exchangeRecSEQ: out_map(exchangeRecord_MAP_OUT);

---

# Document 180: Porting agent configuration from Version 8 DTK - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610905/Porting+agent+configuration+from+Version+8+DTK
**Categories:** chunks_index.json

Currently MediationZone 9 supports the upgrade of MediationZone 8 DTK (agents) plugins by using config contact support. This section describes the porting methods as well as their limitations. This chapter includes the following sections: Creating New Config Class Generating New Config Class using DTK Limitations of porting existing DTK agents from Version 8 to Version 9

---

# Document 181: ECS Insert Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670865/ECS+Insert+Event
**Categories:** chunks_index.json

The ECS Insert event is triggered when data is inserted into ECS, i e: When cancelBatch is called from a batch workflow. When UDRs are sent to ECS via an ECS Forwarding agent. ECS Insert event specific fields ecsMessage - With this field you can configure notifications to be sent only for certain messages associated with the cancelBatch function. If UDRs are inserted, the message will be "None". Use regular expressions to filter on this field. ecsMIM - This field enables you create a regular expression based filter for specific MIM values, i e notifications will only be generated for data containing the specified MIMs. ecsSourceNodeName - This field enables you configure notifications to be sent only for insertions made from specified agents. For batches, this will be the agent issuing the cancelBatch , while for UDRs this will be the ECS Forwarding agent. Use regular expressions to filter on this field. ecsType - For this field you can select if you want notifications to be generated for only batches, only UDRs or both, i e All . ecsUDRCount - This field enables you to configure notifications to be sent only for batches containing a certain amount of UDRs. Use regular expressions to filter on this field. agentName - This field enables you configure notifications to be sent only for events issued from specified agents. Use regular expressions to filter on this field. Fields inherited from the Base event The following fields are inherited from the Base event, and can also be used for filtering, described in more detail in Base Event : category - If you have configured any Event Categories, you can select to only generate notifications for ECS Statistics events with the selected categories. See Event Category for further information about Event Categories. contents - The contents field contains a hard coded string with event specific information. If you want to use this field for filtering you can enter a part of the contents as a hard coded string. However, for ECS Statistics events, everything in the content is available for filtering by using the other event fields, i e eventName, errorCodeCountForNewUDRs, etc. eventName - This field can be used to specify which event types you want to generate notifications for. This may be useful if the selected event type is a parent to other event types. However, since the ECS Statistics event is not a parent to any other event, this field will typically not be used for this event. origin - If you only want to generate notifications for events that are issued from certain Execution Contexts, you can specify the IP addresses of these Execution Contexts in this field. receiveTimeStamp - This field contains the date and time for when the event was inserted into the Platform database. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06.*" for catching all ECS Statistics events from 1st of June, 2012, to 30th of June, 2012. severity - With this field you can determine to only generate notifications for events with a certain severity; Information, Warning, Error or Disaster. However, since ECS Statistics events only have severity Information, this field may not be very useful for filtering. timeStamp This field contains the date and time for when the Execution Context generated the event. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06-15 09:.*" for catching all ECS Statistics events from 9:00 to 9:59 on the 15th of June, 2012. Fields inherited from the Workflow event The following fields are inherited from the Workflow event, and described in more detail in Workflow Event : workflowKey workflowName workflowGroupName

---

# Document 182: Function Blocks for the Python Collection Agent - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642238/Function+Blocks+for+the+Python+Collection+Agent+-+Real-Time
**Categories:** chunks_index.json

When writing code for the Python collection agent, the function blocks in this section apply. For examples and further information on writing code in the Python collection agent, see Python Writer's Guide . The following function blocks are supported by the Python collection agent: Function Block Description def initialize() This function block initializes resources and state. def execute() This function block is the main entry point for collection. def consume(input) This function block consumes and processes UDRs that are routed to the agent . def timeout(obj) This function block is called when using the setTimeout function. UDRs may be routed in a timeout function block. When possible, always route to an asynchronous route from the timeout function block. def stopInput() This function block is called when the workflow does not want the collector to produce any more data. def stop() This function block is called when the workflow is about to stop. def deinitialize() This function block will cleanup resources and state.

---

# Document 183: Internal Formats - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612847/Internal+Formats
**Categories:** chunks_index.json

MediationZone uses internal formats to represent data entities that it can process. All processing agents (for instance, Analysis and Aggregation) work with these internal formats. A syntax for the internal format is declared as follows: internal <name> [: (<class specifications> | <format inheritance>) ] { <field_type> <field_name> [:optional] ; ... }; The field types may be any of the following: Field Type Description Field Type Description any Any type. bigint Big integer. bigdec Big decimal. boolean Boolean. bytearray Byte array. byte Integer type (8-bit signed). char Integer type (16-bit unsigned). short Integer type (16-bit signed). int Integer type (32-bit signed). long Integer type (64-bit signed). float Float type (32-bit). double Float type (64-bit). date Date type, with capability to hold date parts, time parts, or both. bitset A set of bits. ipaddress An IP address. drudr An instance of any other internal (all internal are drudr instances). string String. The field_type can also be any other internal or list type that is defined in either the same ultra file or in another. See the example below. Example - Internal formats Case 1: internal I1 { I2 f1; }; internal I2 { list<int> f1; }; Case 2: In file A internal I1 { <foldername>.<filename>.I2 f1; //When referring an internal from another file that is in the same folder, the folder name can be omitted. }; In file B internal I2 { list<int> f1; }; List types are declared as follows: list< ElementType > Where ElementType can be any of the previous, including an internal format identifier, or another list type. Example - List type internal I1 { list<list<I2> > f1; }; It is also possible to specify a field as optional: Example - Specifying a field as optional internal I1 { drudr f1: optional; }; Similarly, you declare a map field type this way: map< ElementType, ElementType > Example - Declaring a map field internal I1 { map<string, int> f1; }; Internal formats can also be automatically generated from in_map definitions. For further information, see target_internal specification in In-maps . Class Specifications All internal formats are compiled into Java classes. It is possible to specify additional interfaces for the class to implement: Example - Class specifications internal I1 : implements("Interface1"), implements("Interface2") { ... }; However, this requires that Interface1 and Interface2 only declare methods that are later generated by Ultra when it creates the Java class. For further information about methods and types for UDR type methods, see the Development Toolkit user's guide . Format Inheritance You can use alternative base UDR definitions for the generated Ultra classes by using the extends_class or extends option, but all UDR types cannot be used as an extension base. Except for UDR types defined in Ultra, only some specific agent UDRs are extendible, and session UDR types can not be used. extends_class is used by some agents (for instance, the HTTP agent) for better processing support. Example - extends_class internal I1 : extends_class( com.mysite.myDTKUltraFormat ) { ... }; The extends option lets a format inherit fields defined in an ancestor. Example - extends internal A { int a; ... }; internal B : extends ( A ) { int b; ... }; Multiple inheritances is not supported, meaning you can only use the extends or extends_class option once in the definition of an internal format. Event Types It is possible to declare user-defined event types in Ultra by using the event keyword instead of internal . Such an event is a special type of internal format with added event processing support.

---

# Document 184: Upgrade - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736188/Upgrade
**Categories:** chunks_index.json

This section contain separate instructions for a standard upgrade procedure and for a simplified upgrade. The Standard upgrade procedure should be used when it is required to minimize downtime during upgrade and to ensure that configurations, persisted data (in the file system or database), and system properties are properly migrated. The Standard upgrade procedure needs to be done step-wise via every minor and major. The Simplified Upgrade procedure should be used when this is not required, and you only wish to migrate configuration data. The Simplified upgrade procedure can be done over multiple majors and minors in one upgrade. Note! If you have workflow packages containing workflows using Web Services in your system, you need to follow the steps described in Workflow Packages Containing Web Services before the upgrade. Note! In MediationZone 8, Keystore-related fields in an agent configuration can be configured in the Workflow Table. However, in MediationZone 9, these settings are moved to the Security Profile configuration and the Workflow Table does not support configuring the Security Profile fields directly. Standard Upgrade Procedure Simplified Upgrade Procedure Workflow Packages Containing Web Services

---

# Document 185: TimesTen - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605749/TimesTen
**Categories:** chunks_index.json

This section contains information that is specific to the database type TimesTen. Supported Functions The TimesTen database can be used with: Audit Profile Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Event Notifications Prepared Statements (APL) Shared Table Profile SQL Collection/Forwarding Agents Task Workflows Agents (SQL) Note! When storing a date MIM value in TimesTen, do not use the DATE column type. Instead, use the TIMESTAMP type. Preparations The TimesTen Client must be installed on every host (Platform or Execution Context instance) that is connected to a TimesTen data source. Place the ttjdbc11.jar file in the $MZ_HOME/3pp directory . Setting Java library path to include TimesTen library. Include TimesTen library to Java library path vi $MZ_HOME/common/config/cell/default/master/cell.conf # Update existing property value as below java.library.path="$TIMESTEN_HOME/install/lib" Topo activate changes. Topo activate mzsh topo activate --allow-disconnected Restart the Platform and ECs for the change to take effect. Additionally, the TIMESTEN_HOME variable in the shell from which you launch the Platform or Execution Context instance, should include the path to the TimesTen Client native library. Example - $TIMESTEN_HOME is set to the path where TimesTen is installed export TIMESTEN_HOME=/path/to/timesten_home Performance Tuning In order to use the direct driver, you must install TimesTen on the Execution Context hosts. By doing so you improve performance. To decrease re-connection overhead, database connections are saved in a connection pool. To configure the connection pool size, set the Execution Context property timesten.connectionpool.maxlimit in relevant <pico> .conf file. Example - Configuring the connection pool size for TimesTen $ mzsh topo set topo://container:<container>/pico:<pico name>/val:config.properties.timesten.connectionpool.maxlimit 45

---

# Document 186: SFTP Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740560
**Categories:** chunks_index.json

To open the SFTP forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select SFTP in the Forwarding tab in the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. You can configure part of the parameters in the Filename Sequence tab, see Workflow Template for more information. Connection Tab Open The SFTP forwarding agent configuration - Connection tab See the description of the Connection tab in SFTP Collection Agent Configuration . Target Tab Open The SFTP forwarding agent configuration - Target tab The Target tab contains configuration settings related to the remote host, target directories and target files. Setting Description Setting Description Input File Handling Settings Input Type The agent can act on two input types. Depending on which one the agent is configured to work with, the behavior will differ. The default input type is bytearray, that is the agent expects bytearrays. If nothing else is stated, the documentation refer to input of bytearray. If the input type is MultiForwardingUDR , the behavior is different. For further information about the agent's behavior in MultiForwardingUDR input, see SFTP Forwarding Agent MultiForwardingUDR Input . File Information Settings Directory Absolute pathname of the target directory on the remote host, where the forwarded files will be placed. The pathname may also be given relative to the home directory of the user's account. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR in the target directory. When an End Batch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this check box to create the directory, or the directory structure, of the path that you specify in Directory. Note! The directories are created when the workflow is executed. Compression Compression type of the destination files. Determines whether the agent will compress the output files as it writes them. No Compression - the agent will not compress the files. Gzip - the agent will compress the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. Target File Handling Settings Produce Empty Files If you require to create empty files, check this setting. Handling of Already Existing Files Select the behavior of the agent when the file already exists, the alternatives are: Overwrite - The old file will be overwritten and a warning will be logged in the System Log. Add Suffix - If the file already exists, suffix ".1" will be added. If this file also exists, suffix ".2" will be tried instead and so on. Abort - This is the default selection and is the option used for upgraded configurations, that is workflows from an upgraded system. Temporary File Handling Settings Use Temporary Directory If this option is selected, the agent will move the file to a temporary directory before moving it to the target directory. After the whole file has been transferred to the target directory, and the endBatch message has been received, the temporary file is removed from the temporary directory. Use Temporary File If there is no write access to the target directory and, hence, a temporary directory cannot be created, the agent can move the file to a temporary file that is stored directly in the target directory. After the whole file has been transferred, and the endBatch message has been received, the temporary file will be renamed. The temporary filename is unique for every execution of the workflow. It consists of a workflow and agent ID, and a file number. Abort Handling Select how to handle the file in case of cancelBatch or rollback, either Delete Temporary File or Leave Temporary File. Note! When a workflow aborts, the file will not be removed until the next time the workflow is started. Advanced Tab Open The SFTP forwarding agent configuration - Advanced tab The Advanced tabs contain configurations related to more specific use of the SFTP service, which might not be frequently utilized. Setting Description Setting Description Advanced Settings Port The port number the SFTP service will use on the remote host. Timeout (s) The maximum time, in seconds, to wait for response from the server. 0 (zero) means to wait forever. Accept New Host Keys If selected, the agent overwrites the existing host key when the host is represented with a new key. The default behavior is to abort when the key mismatches. Note! Selecting this option causes a security risk since the agent will accept new keys regardless if they might belong to another machine. Enable Key Re-Exchange Used to enable and disable automatic re-exchange of session keys during ongoing connections. This can be useful if you have long lived sessions since you may experience connection problems for some SFTP servers if one of the sides initiates a key re-exchange during the session. Buffered Mode Select this check box to enable buffered mode on the SFTP client. Additional Hosts Settings Additional Hosts List of additional host names or IP-addresses that may be used to establish a connection. These hosts are tried, in sequence from top to bottom, if the agents fail to connect to the remote host set in their Connection tabs. Use the Add, Edit, Remove, Move up and Move down buttons to configure the host list. After Treatment Settings Execute During transfer a temporary file is written, which is then moved to the final file. Select if the script should be executed on the transferred working copy or the final file with the following two options: Before Move: Execute the following command and its arguments on the temporary file. After Move: Execute the following command and its arguments on the final file. Command Enter a command or a script. The script will be executed on the remote system from it's working directory. Argument This field is optional. Each entered parameter value has to be separated from the preceding value with a space. The temporary filename is inserted as the second last parameter, and the final filename is inserted as the last parameter, automatically. This means that if, for instance, no parameter is given in the field, the arguments will be as follows: $1=<temporary_filename> $2=<final_filename> If three parameters are given in the Arguments field, the arguments are set as: $1=<parameter_value_#1> $2=<parameter_value_#2> $3=<parameter_value_#3> $4=<temporary_filename> $5=<final_filename> If After Move has been selected, the argument with <temporary filename> is excluded. Backlog Tab The Backlog tab contains configurations related to backlog functionality. If the backlog is not enabled, the files will be moved directly to their final destination when an endBatch message is received. If the backlog however is enabled, the files will first be moved to a directory called DR_POSTPONED_MOVE_DIR and then to their final destination. For further information about the transaction behavior, see Retrieves in SFTP Forwarding Agent Transaction Behavior . When backlog is initialized, and when backlogged files are transferred, a note is registered in the System Log. Open The SFTP forwarding agent configuration - Backlog tab Setting Description Setting Description Enable Backlog Enables backlog functionality. Directory Base directory in which the agent will create sub directories to handle backlogged files. Absolute or relative path names can be used. Max Size Settings Type If you select the Files option, the Size field below will determine the maximum number of files allowed in the backlog folder. If you select the Bytes option, the Sizefield below will determine the total sum (size) of the files that resides in the backlog folder. If a limit is exceeded the workflow will abort. Size Enter the maximum number of files or bytes that the backlog folder can contain. Processing Order Settings Processing Order Determines the order by which the backlogged data will be processed once connection is reestablished. Select between First In First Out ( FIFO ) or Last In First Out ( LIFO ). Duplicate File Handling Settings Duplicate File Handling Specifies the behavior if a file with the same file name as the one being transferred is detected. The options are Abort or Overwrite and the action is taken both when a file is transferred to the target directory or to the backlog. Security Tab Open The SFTP forwarding agent configuration - Security tab See the description of the Security tab in SFTP Collection Agent Configuration Note! Due to an upgrade of the Maverick library for MediationZone version 8.1.5.0, the default handling of the advanced security has changed. Users should take note of the behaviour change for the Advanced Security Option for the SFTP agents. The Advanced Security Option will be disabled by default. Users will have to enable it on their own accord from the Security Tab in the SFTP agents configuration. With Advanced Security Option disabled, Maverick will manage the connection between the SFTP agent and the server. Maverick will attempt to connect with the STRONG security level. Failing to do so, it will auto downgrade the security level to WEAK and attempt to connect, this behaviour will allow our agents to work well with backwards compatibility for servers with older instances of the Maverick library. Furthermore, having STRONG security level will result in a performance degradation. However, when a user manually enables the Advanced Security Option from the security tab, Maverick will instead assign the WEAK security level, which will not be as strict or resource intensive as the STRONG security level. For more information about security levels, you can refer to this page: https://www.jadaptive.com/managed-security-in-our-java-ssh-apis/

---

# Document 187: Data Veracity Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032329/Data+Veracity+Forwarding+Agent
**Categories:** chunks_index.json

In order to send UDRs to their respective tables in Data Veracity, a workflow must contain a Data Veracity Forwarding agent and the invalid UDRs have to be routed to it. It is recommended to use a preceding Analysis (or Aggregation) agent to associate an Error Code with the UDR. Note! From the Data Veracity Forwarding agent, it is possible to pass on MIM values to be associated with the UDRs in the Data Veracity Search UI. Open An example workflow forwarding UDRs to the Data Veracity tables as defined by the Data Veracity profile The section contains the following subsections: Data Veracity Forwarding Agent Events Data Veracity Forwarding Agent Input/Output Data and MIM Data Veracity Forwarding Agent Configuration Data Veracity Forwarding Agent Transaction Behavior Data Veracity Forwarding Example

---

# Document 188: PostgreSQL Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669899
**Categories:** chunks_index.json

This section describes the preparations necessary when using PostgreSQL as database and includes the following subsections: Extract Database Definition Files for PostgreSQL PostgreSQL Database Creation

---

# Document 189: Radius Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740146
**Categories:** chunks_index.json

This section describes the Radius Server and Radius Client agents. These are collection and processing agents for real-time workflow configurations. Prerequisites The reader of this information should be familiar with: RADIUS (RFC 2865, http://www.ietf.org/rfc/rfc2865.txt ) RADIUS Accounting (RFC 2866, http://www.ietf.org/rfc/rfc2866.txt ) This section includes the following subsections: Radius Server Agent Radius Client Agent Radius Related UDR Types A Radius Example

---

# Document 190: AMQP Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031817/AMQP+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, is generated according to the configuration in the Event Notification Editor. For further information about the agent message event type, see Agent Event . "Disconnected from broker: "+reason- "Reconnected to "+address The message is generated when the agent is disconnected from the broker. "Connected to "+address The message is generated when a connection has been established. "Reconnected to "+address This message is generated when a reconnection has been established. "Failed to connect to broker. Retrying..." This message is generated when an attempt to connect has failed and a new attempt is being made. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 191: Websocket Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741244/Websocket+Agents
**Categories:** chunks_index.json

WebSocket is a protocol that enables bidirectional communication over a single TCP connection. The protocol is designed to be implemented in web browsers and web servers, but it can be used by any client or server application. The WebSocket protocol is an independent TCP-based protocol and the only relation to HTTP is that the handshake is done as an HTTP upgrade request. The WebSocket protocol makes it easier to send real-time data to and from the server by enabling content to be sent to the client without the client having to make a request. Netty version 3 is deprecated from version 9.0.0 of MediationZone. In MediationZone there are both a Websocket Server agent, and a Websocket Client agent. These are collection agents for real-time workflows. Shown below are two workflows using the Websocket Server and Websocket Client. Open Websocket Server workflow Open Websocket Client workflow Supported Protocol Specification The WebSocket protocol has been implemented according to RFC 6455. Prerequisites The reader of this information should be familiar with: TCP protocol HTTP protocol WebSocket protocol, RFC 6455 The section contains the following subsections: Websocket UDRs Websocket Server Agent Websocket Client Agent

---

# Document 192: Email Agent UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738625/Email+Agent+UDRs
**Categories:** chunks_index.json



---
**End of Part 8** - Continue to next part for more content.
