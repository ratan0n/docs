# RATANON/MZ93-DOCUMENTATION - Part 34/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 34 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.3 KB
---

With the System Exporter, you can export data from your system into a Configuration Export (ZIP file) or a Workflow Package Export (MZP file). In Legacy Desktop, you can also export to a directory. The export can contain your system's data, configurations, and runtime data. You can send this export data to another system, using the System Importer to import it and embed its contents locally. Example - How System Exporter can be used You can import a tested and exported ZIP or MZP file of configurations from a test system and use it safely in production. In System Exporter you can select data from the following folder types: Configuration: Workflow configurations, Profiles, Workflow groups, Ultra formats, alarm detectors, etc. Workflow packages: Workflow Packages are compiled versions of configuration, and are created in the Workflow Package mode in the System Exporter. See Workflow Package Export Type When exporting External References profiles of type Properties Database the External Reference Values will also be exported. In System Importer it can be selected if they should be imported or not. This section describes the following: 1 Export Types 2 Configuration Export Type 2.1 Exporting to Local Destination 2.2 Exporting to Remote Destination 3 Workflow Package Export Type 3.1 Exporting to Local Destination 3.2 Exporting to Remote Destination 4 How To Perform an Export Export Types There are two export types. They result in different behaviors when imported later. Select the one that suits your requirements: Configuration : The configurations must be compatible with the software packages of the system to which you import the configurations This is normally not a problem as long as you are using the same release. The export will be downloaded as a ZIP file. See the section below Configuration Export Type. Workflow Package : Export in Workflow Packages format. A Workflow Package is read-only and self-contained, with the same binaries as in the exporting system. A Workflow Package is not affected by, nor will it affect, any existing configurations when imported, since the used binaries are not replaced. When selecting Workflow Package Export Type, you compile a Workflow Package by selecting workflows in the Available Entries table. All dependent configurations will also be selected. See the section below Workflow Package Export Type . Note! See File System Profile to learn how to configure a File System Profile when exporting to Git or AWS S3 Bucket. Configuration Export Type You can export a configuration to your local browser or remote with a File System Profile of type Git or AWS S3. Currently, Git is available in the Desktop interface only. Open Configuration Export type - Local Destination Open Configuration Export type - Remote Destination When the Export Type is Configuration you can select data from the following types: Configuration: Workflow configurations, agent profiles, workflow groups, Ultra formats, or alarm detectors. Run-time: Some data that is generated by the system during workflow run-time. Clear the Option Exclude Runtime Data checkbox. System: Other customized parts such as; Data Veracity, Event Category, Folder (structure), Pico Host, Ultra, User, or Workflow Alarm Value. Workflow packages: Workflow Packages are compiled versions of configuration, and are created in the Export type Workflow Package in the System Exporter. The following options can be found on the toolbar: Option Description Option Description Export This will export the selected configuration(s). The behavior is different depending on your choice of Destination. Described in detail below. Refresh Refreshes the list of shown configurations. Options This will open the Options menu. Each available option can be toggled by the user. See the section below Configurations Export Type Options Expand all Expands all folders in the Available entries section. Collapse all Minimizes all folders in the Available entries section. Configurations Export Type Options Based on the chosen options in the export window, the running system will execute the export operations differently. Open Configurations export type options The following options can be toggled by the users using the Options menu. Export Option Description Export Option Description Abort on error Select this option to abort the export process if an error occurs. If an error occurs and you do not select this option, the export will be completed, but the exported data might contain erroneous components. Invalid Ultra and APL definitions are considered erroneous and result in aborting the export. Select Dependencies Select this option to have dependencies follow along with the entries that you actively select. Encryption Activates encryption on the chosen export. Exclude Runtime Data When enabled it excludes locally stored runtime data from the export. The runtime data that can be included is, Aggregation Session Data and Archive data Export as XML Exporting configuration objects with this option enabled will create compressed XML files instead of JSON. Exporting to Local Destination When the Export to Local Destination is complete, a dialog showing the exact time of the export appears, and the export is automatically downloaded and saved in your default download location. Open Dialog after Successful Export Exporting to Remote Destination When you click the Export button, the following dialog opens and you need to select a File System Profile . You can select the File System Profile either as Git or AWS S3 . Open Select a File System Profile When a profile is selected more fields will appear. When a Git File System Profile is selected it will look like this. Open File System Profile - Git Option Description Option Description Commit message The message will be visible in the Git log. Repository/Branch Read-Only Field, showing the Remote repository name and the branch selected in the profile. Select Target Folder A table showing the folders in the current repository, see GIT Support . New Folder Button Create a new folder on the root level. New Folder Icon on each row Open Create a new folder under the selected row When an AWS S3 File System Profile is selected it will look like this. Open File System Profile - AWS S3 Option Description Option Description File Name The file name of the export. It will automatically become a Zip file. Select Target A table showing the folders in AWS S3 Bucket. Select a folder to export to, when no folder or file is selected the export will be uploaded to the top level of the Bucket. Workflow Package Export Type When this type is selected, Workflow Packages can be created and exported as a MZP file. A Workflow Package is read-only and self-contained, with the same binaries as in the exporting system. A Workflow Package is not affected by, nor will it affect, any existing configurations when imported, since the used binaries are not replaced. When the Export Type is Workflow Package you can select data from the following types: Configuration: Workflow configurations, agent profiles, workflow groups, Ultra formats, or alarm detectors. System: Other customized parts such as; Data Veracity, Event Category, Folder (structure), Pico Host, Ultra, User, or Workflow Alarm Value. Open When one configuration is selected all dependent configurations will also be selected. Export Option Description Export Option Description Export The behavior is different depending on your choice of Destination. Described in detail below. Refresh Refreshes the list of shown configurations. Options This option is not available in workflow package selection mode. Expand all Expands all folders. Collapse all Collapses all folders. Exporting to Local Destination When you select to export to Local Destination, the following dialog box opens. In this dialog box, specify the Package name and Package version . The Output option can also be chosen  the file can either be downloaded to your browser as an MZP file or committed (inserted) to the system. Open Local Workflow Package Export Specify the following information in the Workflow package export options: Package name : The name of the workflow package. Package version : The version of the workflow package. Output option : Specify the output option of the workflow package. Select the Download option if you want to download the file to your browser as an MZP file or select the Commit option to create the Workflow package and import into the current system. Exporting to Remote Destination When you select to export to Remote Destination , the following dialog box appears. Select the File System Profile of type AWS S3. When the profile is selected more fields will show. Open Remote Workflow Package Export Specify the following information in the Workflow package export options: Package name : The name of the workflow package. Package version : The version of the workflow package. Output option : Only Download is valid when the destination is Remote Select Export Folder: A table showing the folders in AWS S3 Bucket. Select a folder to export to, when no folder is selected the export will be uploaded to Bucket top level. How To Perform an Export Note! Since no workflow state, i.e. File Sequence Number, is included in the exported data and only the initial value is exported, you need to take note of information such as file sequence numbers in Collector agents. Select Export Type, Configuration, or Workflow Package . Select Destination, Local or Remote In the System Exporter, select options according to your preferences in the Options dialog. Expand the folders in Available Entries and select the checkboxes in the Include column for the entries you want to export. Click the Export button to start the export process or to fill in the information regarding the Remote destination or Workflow Package name.

---

# Document 751: Aggregation Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646187/Aggregation+Functions
**Categories:** chunks_index.json

This chapter describes the functions, function blocks, and variables that are available for use in the Aggregation Agent APL code. Loading Loading

---

# Document 752: Encryption Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999867/Encryption+Profile
**Categories:** chunks_index.json

In the Encryption Profile you make encryption configurations to be used by the Encryption agent. Configuration To create a new Encryption profile, click the New Configuration button in the upper left part of the Build View , and then select Encryption Profile from the menu. Open Encryption profile Menus The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently displayed tab. The Encryption profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The Edit menu is specific for the Encryption profile configurations. Item Description Item Description External References Select this menu item to enable the use of External References in the Encryption profile configuration. This can be used to configure the following fields: Key KeyStore Path Keystore password Key Name Key Password For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . Settings You can opt to choose a key definition using either a Directly Configured key or to Read Key from Keystore. Select the appropriate setting for the profile. Directly Configured key You can enter a custom key in the Key input box or alternatively click on the Random button to automatically generate an entry. Using the Algorithm setting you can choose either the AES-128 or AES-256 cipher. Read Key from Keystore The keystore must be a JCEKS keystore. Example - How to create a symmetric crypto key $ keytool -keystore test.ks -storepass password -storetype jceks -genseckey -keysize 128 -alias testkey -keyalg AES The following settings are available: Setting Description Setting Description Key Enter the associated directly configured key. Keystore Path Enter the location of the JCEKS-type keystore from which you want to read the key. Keystore Password Enter the relevant keystore password. Key Name If required, enter the key name. Key Password The Key Password fields are optional. You can enter the key password, or if you leave this field empty, the Keystore Password is the default.

---

# Document 753: wfgroupremovewfgroup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657189/wfgroupremovewfgroup
**Categories:** chunks_index.json

usage: wfgroupremovewfgroup <workflow group name> <pattern match expression for workflow group names> This command removes one or more workflow groups from a workflow group. With this command you compare a single pattern match expression, or several, with the full workflow group name, <folder>.<workflowgroupconfigurationname>.<workflowgroupname> , of all the workflow groups. For further information about pattern match expression see Textual Pattern Matches . Note! With wfgroupremovewfgroup you cannot remove all the workflow groups from a workflow group, as this will result in an invalid workflow group configuration. In that case, the command aborts and an error message informs you about the abort cause. As the workflow group should not be emptied, the command enables you to remove all the workflow groups from the parent workflow group only if the workflow group also contains a workflow. This way, after removing all the workflow groups, the parent workflow group is not empty. See the figure below, The wfgroupremovewfgroup Command. The wfgroupremovewfgroup Command Return Codes Listed below are the different return codes for the wfgroupremovewfgroup command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the number of arguments is incorrect. 2 Will be returned if the group is not found. 3 Will be returned if the workflow(s) you want to remove cannot be found. 4 Will be returned if there is no connection to Mgmt_Utils. 5 Will be returned if the group is locked. 6 Will be returned if the updating of group data failed. 7 Will be returned if the configuration lock could not be released. 8 Will be returned if the group is empty (all members have been removed).

---

# Document 754: View, Update, Create, and Delete Picos - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657550/View+Update+Create+and+Delete+Picos
**Categories:** chunks_index.json

This page describes how you can view, update, create, and delete picos in the Pico Management screen in Desktop. View Picos To view the existing picos, open the Pico Management screen by clicking on the Manage screen option in Desktop and then clicking on Pico Management . Open Pico Management All picos in the different containers are displayed with the following information: Column Description Column Description Name At the top level in this column, all the containers in the system are displayed. Click on the arrow to the left of the container name to expand/collapse the list of picos within each container. Each pico is displayed with its defined name, e g platform, ec1, ec2, sc1. Type This column displays the pico type; container, platform, ec, or sc. State This column displays the current state of the pico; Not-started, Starting, Restarting, Stopping, or Running Action This column contains an Edit button that you can use to edit the pico configuration. Only picos of ec and sc types can be edited. Update Picos To edit a pico configuration: In the Pico Management screen, click on the Edit button for the pico you want to update. The Edit Pico dialog will open up. Open Edit Pico dialog Update the pico configuration as described in Updating Pico Configurations . You can click on the View Resolved Config button to exit edit mode and view the full pico configuration. Click on the button again to resume editing. When you are finished, click on the Save button. The Update Pico Configuration dialog will open up, asking you if you are sure you want to update. Open Update Pico Configuration dialog Click OK if you are sure. When the updates have been applied, you will get a message that the update was successful. Open Create a New Pico To create a new pico configuration: In the Pico Management screen, click on the New Pico button. The Create New Pico dialog will open up. Open Create New Pico dialog Select for which container you want to create the pico in the Container drop-down list. Enter a name for the pico in the Name field. If the name already exists you will be notified with a red warning text and will have to enter another name to proceed. Select which template you want to use in the Template drop-down list. The editable parameters in the selected template will be displayed in the coding area beneath. See Managing Picos with Topo for more information about the templates. Edit the parameters as described in Creating Pico Configurations . If you want to view the template you can click on the View Template button. Click on the button again to revert to edit mode. Click on the Create Pico button when you are finished. When the pico has been created, you will get a message that the pico was successfully created. Open Deleting Picos Picos can only be deleted when they are not running. To delete a pico: In the Pico Management screen, stop the pico as described in Starting, Stopping and Restarting Picos . Click on the Delete button for the pico. The Delete Pico dialog will open up asking you if you are sure you want to delete the pico. Open Delete Pico dialog Click OK if you are sure. When the pico has been deleted, you will get a message saying that the pico was successfully deleted. Open

---

# Document 755: Cell Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881399/Cell+Properties
**Categories:** chunks_index.json

This section describes properties that are typically set on a cell level and applicable to the Platform, ECs and SCs. Note! You cannot add JVM arguments or classpaths on the cell level. If you need to add JVM arguments or classpaths that are applied to all pico processes, it is recommended that you do so by using templates. Property Description Property Description mz.apl.use_51_integer_arithmetic_rules Default value: false Set this property to true if you want to use the same integer arithmetic handling as in MediationZone version 5.1. mz.client.dateformat Default value: yyyy-MM-dd This property determines the default format for displaying dates for the client. mz.drdate.lenient Default value: false This property enables lenient interpretation of the date/time in the string parameter in the APL function strToDate . With lenient interpretation, a date such as "January 32, 2016" will be treated as being equivalent to the 31nd day after January 1, 2016. With strict (non-lenient) interpretation, an invalid date will cause the function to leave the submitted date variable unchanged. mz.name Default value: MZ This property specifies the name of the MediationZone system. This will be displayed along with IP address and port to the right in the status bar at the bottom of Desktop. mz.outputstream.use_compact Default value: false This property enables compact serialization of data. Enabling the property can optimize the size of your serialized data. mz.preset.aggregation.storage.path Default value: false This property enables hardcoding of the Aggregation profile directory to the specified path. The Desktop user will still be able to see the set directory, but not change it. mz.preset.dupUDR.storage.path Default value: false This property enables hardcoding of the Duplicate UDR profile directory to the specified path. The Desktop user will still be able to see the set directory, but not change it. mz.preset.interwf.storage.path Default value: false This property enables hardcoding of the Inter Workflow profile directory to the specified path. The Desktop user will still be able to see the set directory, but not change it. mz.httpd.security Default value: false This property enables HTTP communication protected by TLS (i e HTTPS). mz.server.dateformat Default value: yyyy-MM-dd HH:mm:ss This property determines the default format for displaying timestamps. mz.server.datepartformat Default value: yyyy-MM-dd This property determines the default format for displaying dates. mz.server.timepartformat Default value: HH:mm:ss This property determines the default format for displaying times for the server. mz.webserver.xframeoptions This property determines if X-Frame-Options are enabled in the Platform- and EC Web Interface in order to protect from framing. The valid values are: DENY - The X-Frame-Options are enabled so that the Web Interface cannot be displayed in a frame. ALLOW - The X-Frame-Options are disabled so that the Web Interface can be displayed in a frame. ALLOW-FROM <specific origin> - The X-Frame-Options are disabled on a specific origin. This means that the Web Interface can only be displayed in a frame on the specified origin. SAMEORIGIN - The X-Frame-Options are disabled on the same origin as the Web Interface. This means that the Web Interface can only be displayed in a frame on the same origin as the Web Interface itself. pico.cache.basedir Default value: $MZ_HOME/pico-cache This property specifies the directory that should be used for the pico-cache that is cashing information about all running picos, which is used by all servers and clients. pico.rcp.connection.timeout Default value: The value specified for pico.rcp.timeout This property determines the the RCP open connection timeouts in seconds. If you do not specify a value for this property, the value is the same as that set for the property pico.rcp.timeout . pico.rcp.platform.host Default value: "" This property specifies the IP address or the hostname of the Platform, to be used by other pico instances (e g Desktop, Execution Context, and mzsh). When you enter the hostname as the value of this property, if a failover occurs, the hostname is retrieved from the DNS enabling reconnection. If you enter the IP address as the value of this property, if it is a static IP address, reconnection issues may occur if the IP address changes. pico.rcp.platform.port Default value: 6790 This property specifies the port that is used for communicating with the Platform. pico.synchronizer.port Default value: 6791 This property specifies the port that is used for synchronizing files from the Platform to external ECs. pico.rcp.timeout Default value: 30 This property determines the RCP handshake and heartbeat timeouts in seconds. pico.rcp.tls.require_clientauth Default value: false This property specifies if client authentication is required when these are not running on the local host.

---

# Document 756: wfimport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/298647583
**Categories:** chunks_index.json

usage: wfimport [-keepOld [yes|no]] <workflow configuration> <export file> [workflow configuration password] This command updates the specified workflow configuration by importing workflows that are defined in the export file. Example - wfimport The workflow configuration Default.disk_collection workflow configuration is updated with with imported data from the file wf_disk_collection.csv . $ wfimport Default.disk_collection wf_disk_collection.csv Option/Parameter Option/Parameter [ -keepOld [yes|no] ] Set this option to yes to retain the existing workflow table data that is not updated by the export file. Set it to no to remove such data from the workflow table. The default value is no . <workflow configuration> The workflow configuration that you want to be updated by the export file. <export file> The export file name. For further information about the export file see wfexport . Note! wfimport imports the file formats: CSV (Comma Separated Value), SSV (Semicolon Separated Value), and TSV (Tab Separated Value). Text strings within each value are delimited by a quotation mark ( " ). Exported fields that contain profiles, are assigned with a unique string identifier. The ID and Name fields are exported, as well. [workflow configuration password] To import a password protected workflow configuration, specify a password. Return Codes The following is a list of return codes for the wfimport command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the number of arguments is incorrect. 1 Will be returned If login credentials are incorrect. 1 Will be returned if configuration permission is denied. 2 Will be returned if the import file does not exist. 2 Will be returned if the import file directory does not exist. 3 Will be returned if the import file cannot be read (read permission). 5 Will be returned if the import file has an incorrect file suffix. 6 Will be returned if the configuration name is incorrect 7 Will be returned if the configuration does not exist. 8 Will be returned if the configuration is already locked. 9 Will be returned if an encryption passphrase is needed. 10 Will be returned if the user does not have the read and write permission to modify the workflow. 11 Will be returned if the configuration could not be loaded. 12 Will be returned if the import fails (refer to the logs for details).

---

# Document 757: SAP CC Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001750/SAP+CC+Agents
**Categories:** chunks_index.json

This section describes the SAP CC agents. Supported Versions The following versions of SAP CC are currently supported: Version 2023 Note! Update : SAP CC team have announced the following on official support for SAP CC versions For SAP CC 2023 or newer, please use Java 17 based clients such as MZ 9.x For SAP CC 2022 or earlier, please use Java 8 based clients such as MZ 8.3.x Prerequisites SAP Convergent Ch argi ng concepts. Before starting with the SAP CC agents, the following is required: SAP Convergent Charging installation materials that matches the version of your SAP CC core system (i.e the back end servers). Info! Find your desired version of SAP CC installation materials from the SAP CC server provider. For example, here are some links that will direct you to the SAP CC 2023 installation material downloads. SAP CC 2023 Patch Release Note SAP CC 2023 Release Note It is recommended to upgrade your SAP CC server to the latest patch whenever possible. Setting up SAP CC Core SDK jar files Info! For more information on how to set up the SAP CC Core SDK jar files in Execution Container, refer to SAP CC Agent Preparation The section contains the following subsections: SAP CC Online Agent SAP CC Batch Agents SAP CC Notification Agent SAP CC REST Agent SAP CC UDRs SAP CC Agent Preparations SAP CC Secured Connection

---

# Document 758: Java Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881478
**Categories:** chunks_index.json

Out of Memory There are a few parameters for the JVM that might need to be adjusted a little for a given server installation. Depending on the amount of available primary memory and the amount of disk swap space, it might be necessary to inform the JVM how much memory it is allowed to allocate. The Unix process, running an EC, can fork itself depending on configurations in individual agents. The Disk forwarding agent, for instance, can be configured to run an external binary after every forwarded file. The JVM performs a native fork-call to do this, and the forked JVM process will initially have the same memory footprint as the parent process. If there is not enough primary memory and/or swap space available, the EC will abort with the following exception: java.io.IOException: Not enough space at java.lang.UNIXProcess.forkAndExec(Native Method) If this happens, the maximum heap size for the JVM must be lowered, or additional memory must be added to the machine. Lowering the memory can be done by using the JVM argument -Xmx , which is specified for all pico configurations. The following line is an example of how to specify this JVM argument in the STR. mzsh topo set topo://container:<container>/pico:<pico>/obj:config.jvmargs  'xmx:["-Xmx128M"]' Unfortunately, it is difficult to recommend a value. This JVM argument specifies the maximum heap size, meaning that the JVM will probably not reach this limit for a while, depending on how the JVM manages its heap. That, in turn, means that the forking will work for a while, and when the heap size in the JVM has grown large enough, the fork will fail in case there is no free memory pages available in the machine. The only possible recommendation is to lower the maximum heap size value, or to add more system resources (memory or swap disk). If the physical host is running more than one Execution Context, then the memory allocation of these Execution Contexts must be taken into account as well. The JVM also has a kind of memory called direct memory, which is distinct from normal JVM heap memory. You may need to increase the direct buffer memory when Shared Tables have been configured to use off-heap memory. This can be done by either by increasing the maximum heap size, which increases both the maximum heap and the maximum direct memory, or by only increasing the maximum direct memory using the JVM argument XX:MaxDirectMemorySize . The more memory a JVM is given, the better it will perform. However, make sure that the heap never "pages". The sum of all maximum heaps must fit in physical memory. Make sure to adapt these values to better fit the memory in the installed machine. Increasing the heap size for an EC can make a big difference to performance.

---

# Document 759: In-maps - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612864
**Categories:** chunks_index.json

The in_map construct is used to map external formats to internal formats during decoding. The general syntax for an in-map declaration is as follows: in_map <map_name> : <in_map options> { <explicit map specifications> <automatic> : <automatic options> { <automatic mapping specifications> }; <sub-external specifications> }; The general in-map options are: Option Description external(<external_name>) Specifies the external format to map from. This is a mandatory parameter. internal(<internal_name>) The internal format name to map to. This is a mandatory parameter, unless target_internal is specified. target_internal(<target_internal_name>) The target internal format name created when automatically generating a map. This parameter is only valid for automatic mappings. discard_output Specifies that the in_map will produce no output when used in a decoder. This can be useful for uninteresting "filler records" that are not needed for processing. emit_field(field1, field2, ...) Specifies that a decoder using this in_map will not route out the top level UDR. Instead, the content (records or list of records) of the named fields are routed. Specially named options can also be supplied in the in_map depending on the type of the external format. The type specific options are: Option Description ipdr_compact Only applicable for XML based formats (IPDR formats). Specifies that the IPDR "compact" encoding is to be used. PER_aligned Only applicable for ASN.1 based formats. Specifies that PER encoding (ALIGNED version) is to be used. PER_unaligned Only applicable for ASN.1 based formats. Specifies that PER encoding (UNALIGNED version) is to be used. Option Description <explicit map specifications> Describes how the external fields are mapped to the internal fields. These specifications are optional. i:<internal field> and e:<external field> [ using in_map <sub_map> ] ; <automatic> Specifies that all external fields not explicitly mapped in <explicit_map_specifications> will be implicitly mapped according to the external formats implicit type conversions. It is also possible to control the behavior of the automatic mapping through options or a specification block, see the section below, Automatic Maps, for details). <sub-external specifications> This is used to handle the special case where the mapped external is the parent of other externals that must be considered for decoding. This is currently only supported for XML schema based externals where it is used to support, for instance, IPDR decoding. ignore_unknown_tags This option can be used to ignore unknown tags when the external format is ASN.1 BER. Example - ignore_unknown_tags Adding the following for an in_map creates an in_map called inMapIgnoringUnknownTags that simply ignores any unknown tags in the BER encoding: in_map inMapIgnoringUnknownTags: external(Udr), target_internal(Udr), ignore_unknown_tags { automatic; }; Note! This functionality can also be achieved by using the ASN.1 extensibility syntax. For example, entering the following code results in tags of the type TheType to be ignored: TheType ::= SEQUENCE { boolField (1) BOOLEAN OPTIONAL, } Loading

---

# Document 760: mzcli Usage and Options - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979337/mzcli+Usage+and+Options
**Categories:** chunks_index.json

The mzcli command line tool has a number of options to be added for it to know which installation to run the command on. Example - mzcli Usage java -jar mzcli.jar [mzcli] [<username/password>] <command> [<arguments>] [--schema <http|https> : default http] [--user <username> : default mzadmin] [--password <pass>] [--host <host> : default localhost] [--port <port> : default 9000] [--profile <profile-name> : default can be specified in mzcli_configuration.xml with --set-default-profile option] java -jar mzcli.jar [mzcli] --set-default-profile <default-profile-name> java -jar mzcli.jar [mzcli] --get-default-profile java -jar mzcli.jar [mzcli] --help Options The options specified with double-dashes (--) can also be specified via: Environment variables: MZCLI_PORT, MZCLI_HOST, MZCLI_USER, MZCLI_PASSWORD, and MZCLI_SCHEMA. The environment variables need to be set for the machine that is running the mzcli client. A property file called mzcli.properties with these property names and default values: mzcli.user=mzadmin mzcli.schema=http mzcli.port=9000 mzcli.host=localhost mzcli searches for this file in MZ_HOME/etc (if MZ_HOME exists) or in the local folder (if MZ_HOME does not exist). If the mzcli.properties file is not present, it is created with default values. If an option is not specified, its default value is used. A profile containing one or several installations with different options You can also use a simplified user/password syntax by entering the username and password in a single parameter together with the command. When the command is executed, the command prompt is returned to the Unix shell. Priority Since the options can be set in various ways, the following priority applies (ranking lowest to highest): Values from the mzcli.properties file Environment variables Options specified when out of interactive mode Username and password specified using the [<username/password>] syntax Values derived from the default profile. Values derived from the --profile option. Command Line Attributes. Options specified when in interactive mode Lower priority means that it can be overridden by anything with higher priority.

---

# Document 761: KPI Management Service Model Deployment - Distributed Processing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645685/KPI+Management+Service+Model+Deployment+-+Distributed+Processing
**Categories:** chunks_index.json

You can create a KPI model using Creating a Profile and Service Model Using the Model Builder . For more information, see Service Model Definition . To deploy the new service model: Stop all KPI Management workflows that will use the service model. Kill Spark applications and associated drivers that will use the service model. For further information about the Spark UI, see Killing a Spark Application . Remove the checkpoint directory. The default path is SPARK_HOME/spark-checkpoint-dir . Submit the Spark application. For further information, see Submitting a Spark Application . Start the KPI Management workflows that will use the service model. Loading

---

# Document 762: Web Service Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205035086/Web+Service+Profile
**Categories:** chunks_index.json

The Web Services (WS) Profile enables a web service definition. A successfully created profile defines a given web service across the platform. It is used by specifying a WSDL file and/or XSD file or a zipped file of all related contents. The WS profile can include more than one WSDL file reference. The WS profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Saving a WS profile that is assigned with a WSDL file, maps data types that are specified in the WSDL Schema section as UDR types for the workflow. The section contains the following subsections: Web Service Profile Configuration Tab Web Service Profile Security Tab Web Service Profile Advanced Tab Web Service Profile WSDL Content Tab Web Service Profile Saving

---

# Document 763: Azure Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032036
**Categories:** chunks_index.json

The Azure Profile is used for setting up the access credentials and properties to be used to connect to an Azure environment. Currently, the profile can be used with the following agents: ADLS2 File collection agent ADLS2 File forwarding agent Azure Event Hub Consumer agent Azure Event Hub Producer agent and APL functions: kustoTableCreate, for more information, see Database Table Functions . Buttons The contents of the buttons in the button bar may change depending on which configuration type has been opened. The Azure Profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit button is specific to the Azure Profile configurations. Item Description Item Description External References Open Select this menu item to enable the use of External References in the Azure profile configuration. This can be used to configure the following fields: Shared Key Storage Account Name Key Connection String Connection String Secret Key Storage Account Name Namespace Event Hub Name Client ID Tenant ID Client Secret Certificate Storage Account Name Namespace Event Hub Name Client ID Tenant ID Certificate Path Certificate Password For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . General Tab Azure Data Lake Storage Authentication Method - Shared Key The following settings are available in the Shared Key authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Shared Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Key Enter the authorized shared access key used to access the Azure storage account, or use Secret Profile. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Authentication Method - Secret Key The following settings are available in the Secret Key authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Lake Storage. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Lake Storage. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client secret will only be visible when registering the client ID. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Authentication Method - Certificate The following settings are available in the Certificate authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Lake Storage. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Lake Storage. Use Security Profile Click this to use a Keystore from a Security Profile Security Profile Certificate Type Set the certificate format that is used by the Azure AD application. You can set it to either a PEM or PFX formatted certificate. Certificate Path Define the full local path of the certificate. The certificate must be stored in the same location as the EC that will be running the workflows with the ADLS2 file agents. The certificate must be the same one used by the Azure AD application. Certificate Password Enter the password for the PFX certificate, where the password value can also be an empty string. Password locked PEM certificates are not supported. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Note! For the Test Connection button to work while using certificate authentication, the certificate path must point to a certificate located in the Platform. However, when running workflows, the certificate path must point to a certificate located in the EC. Azure Event Hub Authentication Method - Connection String The following settings are available in the Connection String authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Connection String configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Connection String Enter the connection string-primary key of the event hub instance that the profile will be accessing. You can locate the connection string from the shared access policies menu in the target event hub instance. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. Authentication Method - Secret Key The following settings are available in the Secret Key authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Namespace Enter the namespace of the Event Hub that the profile will be accessing. Event Hub Name Enter the name of the Event Hub Instance within the Event Hub Namespace above that the profile will be accessing. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Event Hub. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Event Hub. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client's secret will only be visible when registering the client ID. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. Authentication Method - Certificate The following settings are available in the Certificate authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Namespace Enter the namespace of the Event Hub that the profile will be accessing. Event Hub Name Enter the name of the Event Hub Instance within the Event Hub Namespace above that the profile will be accessing. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Event Hub. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Event Hub. Use Security Profile Click this to use a Keystore from a Security Profile Security Profile Certificate Path Define the full local path of the certificate. The certificate must be stored in the same location as the EC that will be running the workflows with the Event Hub agents. The certificate must be the same one used by the Azure AD application. Certificate Password Enter the password for the PFX certificate, where the password value can also be an empty string. Password-locked PEM certificates are not supported. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. The following settings are available in the Certificate authentication method for the Azure Event Hub application in the Azure profile. Note! For the Test Connection button to work while using certificate authentication, the certificate path must point to a certificate located in the Platform. However, when running workflows, the certificate path must point to a certificate located in the EC. Azure Data Explorer The following settings are available in the Secret Key authentication method for the Azure Data Explorer application in the Azure profile. Open Azure profile - Azure Data Explorer Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. To select Azure Data Explorer, select it from the dropdown menu list. Authentication Method Select the authentication method for accessing Azure Data Explorer. There are 2 choices  Secret Key and Certificate . Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Choosing Secret Key enables this method. Cluster Name Enter the cluster name. Location Enter the associated location. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Explorer. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Explorer. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client's secret will only be visible when registering the client ID. Use Secrets Profile Click this to use stored credentials from a Secrets Profile . Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. The following settings are available in the Certificate authentication method for the Azure Data Explorer application in the Azure profile. Open Azure profile - Azure Data Explorer Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. To select Azure Data Explorer, select it from the dropdown menu list. Authentication Method Select the authentication method for accessing Azure Data Explorer. There are 2 choices  Secret Key and Certificate . Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Choosing Certificate enables this method. Cluster Name Enter the cluster name. Location Enter the associated location. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Explorer. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Explorer. Security Profile Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Advanced Tab Open Advanced Tab The content of this tab changes depending on the selected method in the General Tab . The following fields are available for each option: Field Description Field Description Authority Host Enter the URL to the directory the Microsoft Authentication Library will request tokens. If left empty, the following default values will be used accordingly: Azure Data Lake Storage - https://login.microsoftonline.com Azure Event Hub - https://login.microsoftonline.com API Endpoint Enter the API endpoint in Azure to be used for accessing and managing the services. If left empty is not entered, the following default values will be used accordingly: Azure Data Lake Storage - blob.core.windows.net Azure Event Hub - servicebus.windows.net Azure Data Explorer - kusto.windows.net Additional Information To find out more about the configuration for both authority and endpoints, refer to https://docs.microsoft.com/en-us/azure/active-directory/develop/authentication-national-cloud#azure-ad-authentication-endpoints and https://docs.microsoft.com/en-us/azure/azure-government/compare-azure-government-global-azure .

---

# Document 764: Hash/Database - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/610697500/Hash+Database
**Categories:** chunks_index.json

This tab is activated only when the Hash/Database masking method is selected in the Fields tab. The following settings are available in the Hash/Database masking method in the Data Masking profile. Open Setting Description Setting Description Data Model Database Select the database profile to be used. Table Select the available table in the selected database profile. Unmasked Enter a name for each unmasked storage field. Masked Enter a name for each masked storage field. Key This checkbox is selected for each field by default. If selected, all selected fields will be looked up when unmasking data. Note! If you have a large table or huge amount of lookups, you may consider to only select the necessary fields to be looked up when unmasking data. Hash Salt Enter the key to be appended to the data or click the Random button to generate a random key. Advanced Queue Size Sets the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the maximum number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements for retrieving data.

---

# Document 765: Using Multiple Service Models - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645648/Using+Multiple+Service+Models
**Categories:** chunks_index.json

If you have two or more service models that you want to process in the same cluster simultaneously: Create separate KPI profiles for each kpi model. See KPI Profile . Create separate input, output, and alarm topics in Kafka for each service model. See Starting Clusters and Creating Topics . Create separate Spark application configurations for each service model. This can be done by updating the script kpi_params.sh in the folder mz_kpiapp/bin, making one entry per kpi model. Copy the whole section in the if-statement for "kpiapp", and alter these parameters: if [ "kpiapp" = "$1" ] The name of the profile used export MZ_KPI_PROFILE_NAME="kpisales.SalesModel" Loading

---

# Document 766: Crypto - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/610664826/Crypto
**Categories:** chunks_index.json

This tab is activated only when the Crypto masking method is selected in the Fields tab. The following settings are available in the Crypto masking method in the Data Masking profile. Setting Description Setting Description Derive Key from Passphrase Select this option if you would like to specify a directly configured key. The Passphrase and Algorithm fields will be enabled. Passphrase Enter a passphrase manually or click the Random button to generate a random key. The passphrase is then hashed and it is use as the key. Note! If you use a random passphrase and it has been changed, you will not be able to unmask any of the data that has been masked prior to the change. Algorithm Select one of the following algorithms to be used: AES-128 - uses 128-bit key for data encryption and decryption. AES-256 - uses 256-bit key for data encryption and decryption. Note! To use the AES-256 algorithm, you are required to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files on the EC in order to run the workflow. See JCE Unlimited Strength Jurisdiction Policy Files for JDK/JRE 8 Download for further information. Read Key from Keystore Select this option if you would like a key to be read from a specific keystore. The keystore must be a JCEKS. Example - Creating a symmetric crypto key $ keytool -keystore test.ks -storepass password -storetype jceks -genseckey -keysize 128 -alias testkey -keyalg AES Selecting this option will enable the Keystore Path , Keystore Password , Key Name and Key Password fields. Keystore Path Enter the location of the JCEKS type keystore from which you would like the key to be read. Keystore Password Enter the password to the keystore. Key Name This field is optional. Enter the field name if required. Key Password This field is optional. Enter the password if required, otherwise the Keystore Password is used as the default password.

---

# Document 767: Alarm Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670770/Alarm+Event
**Categories:** chunks_index.json

This event occurs whenever an alarm starts, or once an alarm is closed. The following fields are included: alarmConditionDescription - The contents of the Condition Criteria column on the condition list table. See the figure, The Alarm Detection in Alarm Detection . alarmDescription - The contents of the Description text-box in the Alarm Detection configuration. See the figure, The Alarm Detection in Alarm Detection . alarmDetectionName - The name by which the alarm detection is saved. alarmId - The unique number that the system uses to identify saved configurations. alarmModifier - The user that closes the alarm. alarmModifierComment - The annotation that the user enters when closing the alarm. alarmSeverity - See Severity in the figure, The Alarm Detection in Alarm Detection . alarmSeverityForSNMP - A numeric representation of alarmSeverityForSNMP. alarmState - Open or Closed. alarmSupervisedArea - A configuration that is supervised by the alarm that occurred. Note: Marked with a red check symbol on the Web Interface. alarmSupervisedObject - The specific object within the alarmSupervisedArea that is guarded by the alarm that occurred. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents eventName origin receiveTimeStamp severity timeStamp

---

# Document 768: Disk Collection Agent Configuration  - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672750/Disk+Collection+Agent+Configuration+-+Batch
**Categories:** chunks_index.json

You open the Disk collection agent configuration dialog from a workflow configuration. To open the Disk collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Disk from the Collection tab of the Agent Selection dialog. Part of the configuration may be done in the Filename Sequence or Sort Order service tab described in Workflow Template . Disk Tab The Disk tab contains settings related to the placement and handling of the source files to be collected by the agent. Open Disk collection agent configuration - Disk tab Setting Description Setting Description Collection Strategy If there is more than one collection strategy available in the system a Collection Strategy drop-down list will also be visible. For more information about the nature of the collection strategy, refer to Appendix 4 - Collection Strategies . Directory Enter the absolute pathname of the source directory on the local file system, where the source files reside. The pathname might also be given relative to the $MZ_HOME environment variable. Note! Even if a relative path is defined, for example, input , the value of MIM parameter Source Pathname (see the section, Publishes, in Disk Collection Agent Input/Output Data and MIM - Batch ) includes the whole absolute path; /$MZHOME/input . Include Subfolders Select this check box if you have subfolders in the source directory from which you want files to be collected. Note! Subfolders that are in the form of a link are not supported. If you select Enable Sort Order in the Sort Order tab, the sort order selected will also apply to subfolders. Filename Enter the name of the source files on the local file system. Regular expressions according to Java syntax applies. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILE , type: TTFILE.* Compression Select the compression type of the source files. Determines if the agent will decompress the files before passing them on in the workflow. No Compression - agent does not decompress the files. This is the default setting. Gzip - agent decompresses the files using gzip. Move to Temporary Directory If enabled, the source files will be moved to the automatically created subdirectory DR_TMP_DIR in the source directory, prior to collection. This option supports safe collection of a source file reusing the same name. Append Suffix to Filename Enter the suffix that you want added to the file name prior to collecting it. Important! Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (hours) If the specified value is greater than zero, and if no file has been collected during the specified number of hours, the following message is logged: The source has been idle for more than <n> hours, the last inserted file is <file>. Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ) to the directory specified in the Destination field, after the collection. If the Prefix or Suffix fields are set, the file will be renamed as well. Note! It is possible to move collected files from one file system to another however it causes negative impact on the performance. Also, the workflow will not be transaction safe, because of the nature of the copy plus delete functionality. If it is desired to move files between file systems it is strongly recommended to route the Disk collection agent directly to a Disk forwarding agent, configuring the output agent to store the files in the desired directory. Refer to Disk Forwarding Agent - Batch for information. This is because of the following reasons: It is not always possible to move collected files from one file system to another. Moving files between different file systems usually cause worse performance than having them on the same file system. The workflow will not be transaction safe, because of the nature of the copy plus delete functionality. Rename If enabled, the source files will be renamed after the collection, remaining in the source directory from which they were collected (or moved back from the directory DR_TMP_DIR , if using Move To Temporary Directory). Remove If enabled, the source files will be removed from the source directory (or from the directory DR_TMP_DIR, if using Move To Temporary Directory), after the collection. Ignore If enabled, the source files will remain in the source directory after collection. Destination Enter the absolute pathname of the directory on the local file system of the EC into which the source files will be moved after collection. The pathname might also be given relative to the $MZ_HOME environment variable. This field is only enabled if Move to is selected. Prefix/Suffix Enter the prefix and/or suffix that will be appended to the beginning respectively the end of the name of the source files, after the collection. These fields are only enabled if Move to or Rename is selected. Note! If Rename is enabled, the source files will be renamed in the current directory (source or DR_TMP_DIR ). Be sure not to assign a Prefix or Suffix, giving files new names, still matching the filename regular expression, or else the files will be collected over and over again. Search and Replace To apply Search and Replace , select either Move to or Rename . Search : Enter the part of the filename that you want to replace. Replace : Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note! A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Keep (days) Enter the number of days to keep source files after the collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note, a date tag is added to the filename, determining when the file may be removed. This field is only enabled if Move to or Rename is selected. Use File Reference Select this check box if you want to forward the data to an SQL Loader agent. See SQL Loader Agent for further information.

---

# Document 769: wfgroupremovewf - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612651/wfgroupremovewf
**Categories:** chunks_index.json

usage: wfgroupremovewf <workflow group name> <pattern match expression for workflow names> This command removes one or more workflows from a workflow group. With this command you compare a single pattern match expression, or several, with the full workflow name, <folder>.<workflowconfigurationname>.<workflowname> , of all the workflows. For further information about pattern match expression see Textual Pattern Matches . Note! With wfgroupremovewf you cannot remove all the workflows from a group as this will result in an invalid workflow group configuration. In that case, the command aborts and an error message informs you about the abort cause. As the workflow group should not be emptied, the command enables you to remove all the workflows from the workflow group only if the workflow group also contains a workflow group. This way, after removing all the workflows, the parent workflow group is not empty. See the figure below, The wfgroupremovewf Command. Open The wfgroupremovewf Command Return Codes Listed below are the different return codes for the wfgroupremovewf command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the number of arguments is incorrect. 2 Will be returned if the group is not found. 3 Will be returned if the workflow(s) you want to remove cannot be found. 4 Will be returned if there is no connection to Mgmt_Utils. 5 Will be returned if the group is locked. 6 Will be returned if the updating of group data failed. 7 Will be returned if the configuration lock could not be released. 8 Will be returned if the group is empty (all members have been removed).

---

# Document 770: SAP CC Notification Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653818/SAP+CC+Notification+Agent
**Categories:** chunks_index.json

The SAP CC Notification agent is a collection agent designed to listen for and collect notifications from the SAP Convergent Charging Core Server. The SAP CC Notification agent generates notification UDRs. For more information about the notification UDRs, refer to Notification UDRs . The agent relies on the SAP CC Notification Client (NotificationServiceClient) to handle all communication with the SAP CC instances. For more information regarding this SAP AP I, refer to the NotificationServiceClient SAP documentation . Note! The SAP CC Notification API does not support any acknowledgment that a notification has been received by a client. There is no mechanism in the Convergent Charging Core Server for a retry, and therefore there is no guarantee that a notification has successfully been transmitted. Loading

---

# Document 771: Types of Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848974/Types+of+Workflows
**Categories:** chunks_index.json



---
**End of Part 34** - Continue to next part for more content.
