# RATANON/MZ93-DOCUMENTATION - Part 9/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 9 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.6 KB
---

The Email Agent produces three types of UDRs. EmailUDR , EmailContactUDR and EmailAttachmentUDR . EmailAttachmentUDR This UDR is used for describing a specific attachment. Field Description Field Description FileData (bytearray) A bytearray representation of the attachment. FileName (string) The name of the attachment. EmailContactUDR This UDR is used for describing a specific contact. Field Description Field Description Address (string) The email address of the contact. Name (string) The name of the contact. EmailUDR This is the main UDR for the Email Agent. Contains all information received from the email. Field Description Field Description Attachments (list<MailAttachmentUDR>) A list of MailAttachmentUDRs. BlindCarbonCopies (list<MailContactUDR>) A list of MailContactUDRs. Body (string) The email body. CarbonCopies (list<MailContactUDR>) A list of MailContactUDRs. ContentType (string) Note! The agent support the following content types: text/plain , text/html and multipart/* (for attachments) The email content type. Recipients (list<MailContactUDR>) A list of MailContactUDRs. Sender (MailContactUDR) A MailContactUDR. Subject (string) The email subject.

---

# Document 193: Misc Field - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611786
**Categories:** chunks_index.json

Many of the objects in the PCC Buckets and PCC Products data model includes a field for storing miscellaneous data. The field is called Misc and has the type map<string, any> . Even though the Misc field is stated to support storing values of type any there are some limitations to what type the values can have. The following types can be used: bigint bitset boolean byte bytearray char date float/ double int ipaddress long short string

---

# Document 194: Decoders - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678303/Decoders
**Categories:** chunks_index.json

A decoder specifies how data arrives from a source. There are two basic types of decoders: Simple decoders - Decode one or more external formats. Constructed decoders - Coordinate the decoding between multiple simple or constructed decoders. Simple Decoders The syntax of a simple decoder is declared as follows: decoder <name> : <decoder options>; The decoder options are: Option Description Option Description in_map(<map name>) Specifies what in-maps to use. At least one is required. block_size(<size>) Specifies that this is a blocked format with specified block size. terminated_by(<terminator>) Specifies the block filler used. This option has no effect if block_size has not been specified. The decoder may contain one or several in-maps, depending on whether it manages a single or multiple (mixed) record type. For multiple maps, the corresponding external records except the last one must support identification. The decoder tries each in_map in the specified order. The first one, for which the identification criteria are met, is used. How the record identification is specified depends on the actual external record type. For example, sequential external records must use the identified_by option while BER encoded records support identification by the standard tagging scheme. Simple decoders may be given blocking information through the block_size and terminated_by constructs. The block_size parameter contains the size of a block and the terminated_by parameter specifies the start of a padding character. Example - Simple decoder decoder SimpleDecoder : in_map(Map1), in_map(Map2), block_size(2048), terminated_by(0x00); This decoder starts by reading the next byte and evaluates if it equals 0x00. Should it be 0x00, it jumps to the next even boundary of 2048 and repeats this procedure. If it is not 0x00, it evaluates the identification for both of the externals specified in Map1 and Map2 (in that order). Constructed Decoders A constructed decoder is defined in terms of a number of other simple or constructed decoders. They are used to specify the decoders to be used in sequence, for example to specify separate decoders managing header, trailer, and record information. For instance, three external record types ( Rheader , Rudr , Rtrailer ), with corresponding in_map s ( Mheader , Mudr , and Mtrailer ) and decoder s ( Dheader , Dudr , and Dtrailer ). The following constructed decoder specifies the header record to be decoded first. Then any number of UDR records are decoded, followed by a single trailer record. Example - Constructed decoder decoder TTFile { decoder Dheader; decoder Dudr *; decoder Dtrailer; }; As can be seen, the constructed decoder does not (and cannot) have any decoder options. The asterisk after Dudr indicates that zero or more entries can occur before a terminating trailer record. For such a sub-decoder, the constructed decoder switches to the next decoder when the sub-decoder cannot handle the input (as deduced by the identification criteria of the in_map s in the sub-decoder). In this case it means that the Dudr decoder must support identification logic, or the Dtrailer decoder is never reached (and the decoder aborts with an "Unexpected EOF" error). This example could also have been supported with a simple decoder: Example - Simple decoder decoder TTFile : in_map(Mheader), in_map(Mudr), in_map(Mtrailer); The difference is that the order of header, UDR, and trailer would not be enforced for the simple decoder. The simple decoder also does not work if, for instance, the header does not support identification.

---

# Document 195: Legacy KafkaUDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138683/Legacy+KafkaUDR
**Categories:** chunks_index.json

KafkaUDR is the UDR that is populated via APL and routed to the Kafka Producer agent, which in turn writes the data to the specified partition, and the topic set in the Kafka profile. The Kafka Collection agent consumes the data from the Kafka log, from the specified partition(s). The topic is set in the Kafka profile, and places the data in a KafkaUDR . The following fields are included in the KafkaUDR : Field Description Field Description data (bytearray) Producer: This field holds data to be passed to the Kafka log by the Kafka Forwarding Agent (producer). Collector: This field is populated with the data read from the Kafka log. key (bytearray) This field can be used to set a key for the broker's messages. offset (long) This is a read only field. This field is populated by the Kafka Collection agent and contains the offset in the Kafka log from where the message was consumed. partition (short) Producer: This field holds the partition to which the Kafka Forwarding agent (producer) writes the message. If this field is not populated, the partition is chosen randomly. Collector: This field holds the partition from which the message was consumed by the Kafka Collection agent (consumer). timestamp (long) Producer: This is an optional field. A timestamp can be assigned before the UDR to a Kafka producer. Consumer: If you set a timestamp on the producer it will be shown in the corresponding consumer.

---

# Document 196: Diameter Stack Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032612
**Categories:** chunks_index.json

By including the Diameter Stack agent in a workflow you enable MediationZone to act as a Diameter server for any application that follows the Diameter Base Protocol (RFC 6733). For further information about the agent's operation, see the section, Diameter S tac k in The Diameter Base Protocol . The section contains the following subsections: Diameter Stack Agent Configuration Diameter Stack Agent Events Diameter Stack Agent Input/Output Data and MIM

---

# Document 197: mzcli - systemexport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979774/mzcli+-+systemexport
**Categories:** chunks_index.json

Usage systemexport [ -select <xml-selection file> ] [ -includesysdata ] [ -overwrite ] [ -directory ] <export file|directory> [password] This command exports configuration data from MediationZone to a file or directory. This command's output log information is displayed during the command execution. Although no log file is generated, you can view log information in the shell and save it in a file. Options Option Description Option Description [-select] Specify the name of the xml selection file that you want to use. For information about this parameter, see XML Selection File in mzcli - systemimport . [-includesysdata] Use this option to include system data such as users, pico hosts, and other system data. [-overwrite] Use this option to specify that the export- file or directory should be overwritten. [-directory] Use this option to specify that the export data is sent to a directory, instead of a file. <export file|directory> Specify the full path of the directory or ZIP file that contain the configurations that you want to export. [ password ] To export encrypted configurations, provide a password. Return Codes Listed below are the different return codes for the systemexport command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect or if the export fails. 2 Will be returned if the output directory exists, or if write permission is missing, or if the directory cannot be created for any other reason. 3 Will be returned if the XML selection file cannot be read. 4 Will be returned if any errors were reported during export.

---

# Document 198: ECS Collection Workflow (UDR) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673020
**Categories:** chunks_index.json

In order to send a UDR to the ECS, a workflow must contain an ECS forwarding agent. To perform a table lookup for all UDRs, an Analysis agent is used. If the lookup succeeded, the UDR is sent on the OK route to be saved on disk, while the failing UDRs are sent to the ECS forwarding agent. In the collection workflow the same evaluation is tried again. If it fails, the UDR is sent back to the ECS with the same configuration. To be able to collect ECS data, the UDRs or batches must each belong to an existing reprocessing group, and the reprocessing state must be set to New . Open A workflow collecting and validating ECS data Since we want to redo the processing made in the forwarding workflow, we keep the configurations of the ECS Inspector and ECS forwarding agents the same as in the previous workflow. Workflow Properties The Error tab in the Workflow properties must not be configured to handle cancelBatch behavior, since it will never be valid for ECS collection workflows. No calls to cancelBatch are allowed from any agent since it will cause the workflow to immediately abort. ECS Collection Agent All UDRs conforming to the collection criteria are selected and processed as a batch. Analysis Agent The Analysis agent only needs to validate and route the UDRs. The Error Code and Error Case are already associated with the UDR. Example - Analysis agent udrRoute( input, "error" ); Example - Reassigning to a Different Reprocessing Group Suppose there is a workflow collecting and validating UDRs from the ECS. If the validation fails, the UDRs are sent back to the ECS with an associated Error Code. UDRs assigned to a new or a different Error Code are directed to a new reprocessing group. If you need to associate these UDRs with a different reprocessing group, udrClearErrors must be called prior to udrAddError . The exception is if the new Error Code is associated with the same reprocessing group. Case 1 - Same reprocessing group If the new Error Code belongs to the same reprocessing group: Using udrClearErrors results in a new Error Code and reprocessing group to be associated with the UDR in the ECS. This method avoids several Error Codes pointing at different reprocessing groups, which makes automatic group assignment impossible. Leaving out udrClearErrors results in old and new Error Codes (including the reprocessing group) to be associated with the UDR in the ECS. Case 2 - Different reprocessing group If the new Error Code belongs to a different reprocessing group: Using udrClearErrors results in a new Error Code and reprocessing group to be associated with the UDR in the ECS. Leaving out udrClearErrors does not result in any association to a reprocessing group, however both Error Codes are associated with the UDR in the ECS. Note! All UDRs collected when activating the workflow are processed as one batch. Any call to cancelBatch causes the workflow to abort immediately. For more information, refer to cancelBatch . Note! For more information on the UDR functions mentioned above, see UDR Functions | udrAddError and UDR Functions | udrClearErrors . For details on the cancelBatch workflow function, see Workflow Functions | cancelBatch .

---

# Document 199: Preparing and Creating Scripts for KPI Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611452/Preparing+and+Creating+Scripts+for+KPI+Management
**Categories:** chunks_index.json

In preparation for using KPI Management in MediationZone, you need to extract the following scripts: The scripts are as follows: flush.sh kpi_params.sh spark_common_param.sh start_master_workers.sh stop.sh submit.sh These scripts will be used for different procedures in the KPI Management - Distributed Processing sections. Preparations before extracting scripts: A Prerequisite is that Spark, ZooKeeper, and Kafka are installed. Zookeeper and Kafka should be up and running as well. For more information about this, see https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677074 . Before running the command to extract the scripts, these parameters need to be set as environment variables as they will be entered into some scripts: export KAFKA_BROKERS="127.0.0.1:9092" export SPARK_UI_PORT=4040 export MZ_PLATFORM_AUTH="mzadmin:DR-4-1D2E6A059AF8120841E62C87CFDB3FF4" export MZ_KPI_PROFILE_NAME="kpi_common.SalesModel" export MZ_PLATFORM_URL="http://127.0.0.1:9036" export ZOOKEEPER_HOSTS="127.0.0.1:2181" export SPARK_HOME=opt/spark-3.3.2-bin-hadoop3-scala2.13 export KAFKA_HOME=/opt/kafka_2.13-3.3.1 export $PATH=$SPARK_HOME/bin:$KAFKA_HOME/bin:$PATH Extracting scripts and KPI app To extract the scripts and the KPI app: Set up your preferred KPI profile. Find the kpi_spark*.mzp among the installation files and copy it to where you want to keep your KPI application files. To extract the KPI app after building it run the following command. It extracts the software needed by spark for the KPI app as well as the scripts needed for starting and configuring spark. $ cd release/packages $ java -jar kpi_spark_9.1.0.0.mzp install You will find the new directory mz_kpiapp that contains all app software. $ ls -l mz_kpiapp/, will list: app # The MZ kpi app bin # Shell script to handle the app jars # Extra jar files for the app Move the mz_kpiapp folder and add it to the PATH. Example: $ mv mz_kpiapp ~/ $ export PATH=$PATH:/home/user/mz_kpiapp/bin Set the environment variable SPARK_HOME. $ export SPARK_HOME="your spark home" These extracted scripts, kpi_params.sh and spark_common_params.sh , are more of examples than a finished configuration so you need to modify the scripts under the bin folder according to your specifications and requirements. In kpi_params.sh , KAFKA_BROKERS need to be configured with the hosts and ports of the kafka brokers. For example: export KAFKA_BROKERS="192.168.1.100:9092,192.168.1.101:9092,192.168.1.102:9092" The username and password for a user with access to the profile is needed to be entered as the property MZ_PLATFORM_AUTH , unless the default username and password mzadmin/dr is used. The password is encrypted using the mzsh command encryptpassword . The memory settings may need to be altered depending on the expected load, as well as the UI port for the KPI App inside Spark (default 4040). In addition to the addresses and ports of the platform, kafka and zookeeper may need to be updated. In spark_common_params.sh , you may need to change the master host IP and ports if applicable. Edit the kpiapp/bin/spark_common_param.sh , so it has the SPARK_HOME path. Access the conf-folder of Apache Spark, the spark-defaults.conf.template file should be renamed to spark-defaults.conf and the following configuration variables and options added: spark.driver.defaultJavaOptions --add-opens java.base/java.lang=ALL-UNNAMED  --add-opens java.base/java.lang.invoke=ALL-UNNAMED  --add-opens java.base/java.lang.reflect=ALL-UNNAMED  --add-opens java.base/java.util=ALL-UNNAMED  --add-opens java.base/java.util.concurrent=ALL-UNNAMED  --add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED  --add-opens java.base/java.io=ALL-UNNAMED  --add-opens java.base/java.net=ALL-UNNAMED  --add-opens java.base/java.nio=ALL-UNNAMED  --add-opens java.base/sun.nio.ch=ALL-UNNAMED  --add-opens java.base/sun.nio.cs=ALL-UNNAMED  --add-opens java.base/sun.util.calendar=ALL-UNNAMED  --add-opens java.base/sun.security.action=ALL-UNNAMED spark.executor.defaultJavaOptions --add-opens java.base/java.lang=ALL-UNNAMED  --add-opens java.base/java.lang.invoke=ALL-UNNAMED  --add-opens java.base/java.lang.reflect=ALL-UNNAMED  --add-opens java.base/java.util=ALL-UNNAMED  --add-opens java.base/java.util.concurrent=ALL-UNNAMED  --add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED  --add-opens java.base/java.io=ALL-UNNAMED  --add-opens java.base/java.net=ALL-UNNAMED  --add-opens java.base/java.nio=ALL-UNNAMED  --add-opens java.base/sun.nio.ch=ALL-UNNAMED  --add-opens java.base/sun.nio.cs=ALL-UNNAMED  --add-opens java.base/sun.util.calendar=ALL-UNNAMED  --add-opens java.base/sun.security.action=ALL-UNNAMED spark.master.rest.enabled true Add this to the jvmargs section of the execution context definition for the ec that will run the KPI Management workflows. For example: You can open the configuration by running: mzsh mzadmin/<password> topo open kpi_ec jvmargs { args=[ "--add-opens", "java.base/java.lang.invoke=ALL-UNNAMED", "--add-opens", "java.base/java.lang.reflect=ALL-UNNAMED", "--add-opens", "java.base/java.util=ALL-UNNAMED" ] } Starting KPI Note! Prerequisite Before you continue: Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to Spark. Before you can create the topics you must start Kafka and Zookeeper. An example order of topics are the following: kpi-input - For sending data to Spark kpi-output - For spark to write the output to, and thus back to the workflow kpi-alarm - For errors from Spark Startup Spark cluster, here kpiapp is a configurable name: $ start_master_workers.sh $ submit.sh kpiapp Submit the app: $ submit.sh kpiapp ... You should now be able to see workers, and executors: $ jps Will give you something like: pid1 Worker pid2 Worker pid3 CoarseGrainedExecutorBackend pid4 CoarseGrainedExecutorBackend pid5 DriverWrapper pid6 CodeServerMain pid8 Master

---

# Document 200: ECS Statistics Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605243
**Categories:** chunks_index.json

The ECS Statistics event is triggered when the ECS_Maintenance system task is executed, see Error Correction System for further information about this system task. Filtering In the Event Setup tab, the values for all the event fields are set by default to All in the Match Value(s) column, which will generate event notifications every time the ECS_Maintenance system task is executed. Double-click on the field to open the Match Values dialog where you can click on the Add button to add which values you want to filter on. If there are specific values available, these will appear in a drop-down list. Alternatively, you can enter a hard coded string or a regular expression. The following fields are available for filtering of ECS Statistics events in the Event Setup tab: ECS Statistics event specific fields errorCodeCountNewUDRs - This field enables you create a regular expression based filter for UDRs in state New in order to only generate notifications for UDRs passing the filter. This may be useful for specifying that notifications should only be generated for certain error codes and/or when a certain amount of UDRs have be registered, for example. Specifying error codes errorCodeCountReprocessedUDRs - This field enables you create a regular expression based filter for UDRs in state Reprocessed in order to only generate notifications for UDRs passing the filter. This may be useful for specifying that notifications should only be generated for certain error codes and/or when a certain amount of UDRs have be registered, for example. Fields inherited from the Base event The following fields are inherited from the Base event, and can also be used for filtering, described in more detail in Base Event : category - If you have configured any Event Categories, you can select to only generate notifications for ECS Statistics events with the selected categories. See Event Category for further information about Event Categories. contents - The contents field contains a hard coded string with event specific information. If you want to use this field for filtering you can enter a part of the contents as a hard coded string. However, for ECS Statistics events, everything in the content is available for filtering by using the other event fields, i e eventName, errorCodeCountForNewUDRs, etc. eventName - This field can be used to specify which event types you want to generate notifications for. This may be useful if the selected event type is a parent to other event types. However, since the ECS Statistics event is not a parent to any other event, this field will typically not be used for this event. origin - If you only want to generate notifications for events that are issued from certain Execution Contexts, you can specify the IP addresses of these Execution Contexts in this field. receiveTimeStamp - This field contains the date and time for when the event was inserted into the Platform database. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06.*" for catching all ECS Statistics events from 1st of June, 2012, to 30th of June, 2012. severity - With this field you can determine to only generate notifications for events with a certain severity; Information, Warning, Error or Disaster. However, since ECS Statistics events only have severity Information, this field may not be very useful for filtering. timeStamp This field contains the date and time for when the Execution Context generated the event. If you want to use timeStamp for filtering, it may be a good idea to enter a regular expression, for example, "2012-06-15 09:.*" for catching all ECS Statistics events from 9:00 to 9:59 on the 15th of June, 2012. Note! The values of these fields may also be included in the notifications according to your configurations in the Notifier Setup tab.

---

# Document 201: TCP/IP Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609633/TCP+IP+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The agent expects ConnectionRequestUDR s and RequestUDR s. The agent returns ConnectionStateUDR s, ResponseUDR s and ErrorUDR s. MIM For information about the MIM and a list of the general MIM parameters, see Meta Information Model in Administration and Management in Legacy Desktop . The agent does not publish nor access any MIM parameters.

---

# Document 202: mzcli - derbybackup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979426/mzcli+-+derbybackup
**Categories:** chunks_index.json

Usage derbybackup <directory> This command performs a Derby database online backup, where the target directory, that is the value of <directory>, refers to the remote directory on the platform host. Return Codes The different return codes for the derbybackup command are listed below: Code Description Code Description 0 Will be returned if the command was successful or unsuccessful. 1 Will be returned if the argument count is incorrect.

---

# Document 203: GCP PubSub Publisher Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738992/GCP+PubSub+Publisher+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data This section includes information about the data type that the agent expects and delivers. The agent produces bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see MIM . Publishes The following MIM parameters are published at agent level: MIM Parameter Description MIM Parameter Description messagesSent This MIM parameter states that the messages have been sent but have not been notified by the PubSub service as successfully published. messagesPublished This MIM parameter states that the messages have been successfully published. Accesses The agent does not access any MIM resources.

---

# Document 204: Regenerate Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678610/Regenerate+Configurations
**Categories:** chunks_index.json

When certain areas of the system are upgraded, the existing Ultra formats and other configurations may have to be regenerated or upgraded. This section describes the commands available for doing this. These commands require that the Platform is running. Usage regenerateconfigs [-ultra] Options: -ultra = recompiles Ultra formats The regenerateconfigs command recompiles all configurations. The command should only be used by system administrators with authority to maintain . Options Option Description Option Description [-ultra] This option only recompiles the Ultra formats. In prior releases this command was called regenultra. If the regenerateconfigs command is run without the -ultra option, all configurations are recompiled.

---

# Document 205: Documentation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647950/Documentation
**Categories:** chunks_index.json

The MediationZone Desktop GUI provides on-line user documentation covering operation and configuration of all agents, terminology, installation instructions, system administrators guide, etc. The documentation is in English and is also available in PDF format.

---

# Document 206: Managing Picos with Topo - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205815992/Managing+Picos+with+Topo
**Categories:** chunks_index.json

This section describes how to create, update, remove, and view pico configurations. Each pico configuration consists of a set of attributes, including templates, system properties, JVM arguments, and classpaths. When you change these attributes in a configuration, the corresponding pico instance must be restarted. A configuration may also include "settings" attributes. Unlike e g properties, this type of attribute may contain array values and changes do not require restart of pico instances. Settings At the time of writing, there are three available settings: state - If set to disabled, this pico will not be started by the system command tags - Add tags to group pico instances in the mzsh command system . pico.groups - This attribute is applicable for ECs. Add pico groups to reference multiple pico instances as an entity in a workflow. You can only reference existing EC groups in the pico.groups attribute. For information about how to create an EC group, see Pico Manager and EC Groups for information about how to create an EC group. Classpaths The default classpath attributes that are set in the pico configuration of the Platform. Agents that are installed on the system may require that additional classpaths are set in the pico configuration of ECs. JVM Arguments A number of JVM arguments controlling the memory usage are set in the default template standard , which is inherited by all other default templates. config { ... jvmargs { args=[ "-server" ] maxDirect=[ "-XX:MaxDirectMemorySize=4096M" ] maxMetaspace=[ "-XX:MaxMetaspaceSize=196M" ] xms=[ "-Xms64M" ] xmx=[ "-Xmx256M" ] } ... } You can add additional JVM arguments in a custom template or override the default values in your pico configurations. Each of the JVM arguments in the template has unique label. This makes it possible to edit individual JVM arguments via the mzsh command topo . It is highly recommended that you follow this pattern and consistently use the same labels in all configurations. Omitting or mixing labels may lead to unpredictable results. For instance, the following labels are used in the standard template. Label JVM Argument Label JVM Argument maxDirect MaxDirectMemorySize maxMetaspace maxMetaspace xms Xms xmx Xmx If you set set any of the above JVM arguments in a custom template, but with different labels. Pico instances may start with conflicting JVM arguments. mzsh topo set topo://container:<container>/pico:<pico>/obj:config.jvmargs  '<label>: ["argument1","argument2"]' Example - Setting JVM arguments mzsh topo set topo://container:main1/pico:platform/obj:config.jvmargs  'xmx:["-Xmx256M"] xms:["-Xms64M"] maxMetaspace:["-XX:MaxMetaspaceSize=196M"] maxDirect:["-XX:MaxDirectMemorySize=4096M"] args : ["-server"]' If you have a large block of APL code, e g 300 lines or more, add the JVM argument -XX:DontCompileHugeMethods to pico configurations of ECs. This causes the the JIT compilation to take effect: mzsh topo set topo://container:<container>/pico:<pico>/obj:config.jvmargs  'dontCompileHugeMethods:["-XX:DontCompileHugeMethods"]' System Properties For information about the available system properties, see System Properties . This chapter includes the following sections: Creating Pico Configurations Updating Pico Configurations Removing Pico Configurations Viewing Pico Configurations and Attributes Resetting and Activating Pico Configurations

---

# Document 207: mzcli - packageexport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979575/mzcli+-+packageexport
**Categories:** chunks_index.json

Usage usage: packageexport -select <xml-selection file> [ -overwrite ]<base path> <package name> <package version> [password] or usage: packageexport -packages <xml-package file> [ -overwrite ]<base path> [password] This command exports Workflow Packages from MediationZone to an MZP file located in the specified <base path>. As this command is available only to the mzadmin user, all the entries are exported, regardless of their access permissions. Note! A package is a Workflow Package. <package name> and <package version> refer to the Workflow Package name and version. This command's output log information is displayed during the command execution. Although no log file is generated, you can view log information in the shell and save it to a file. Options Option Description [-select] Specify the name of the Selection file that you want to use. For information about this parameter, see XML Selection File (under the -select <xml-selection file> option) in mzcli - systemimport . You must provide <xml-selection file>, <base path>, <package name> and <package version>. Example: packageexport -select <xml-selection file> [ -overwrite ]<base path> <package name> <package version> packageexport -select /Users/username/Downloads/export.xml /Users/username/mz10-la3-rc6/tmp myPackage 1.0 <base path> The path to the directory where your MZP files will be saved. This parameter is compulsory. <package name> The name of the Workflow Package. When using [-select], <package name> is compulsory. <package version> The version of the Workflow Package. When using [-select], <package version> is compulsory. [-overwrite] Use this option to specify that the export - file or directory - should be overwritten. Example - overwrite export file or directory packageexport -packages /Users/username/Downloads/export.xml -overwrite /Users/username/mz10-la3-rc6/tmp [-packages] Use -packages to specify multiple packages. Specify the name of the Workflow Package and version in the XML file, or select a specific folder. Several items can be selected. See XML Selection File (under the -select <xml-selection file> option) in mzcli - systemimport for how to handle configurations or Workflow Packages. Example - packages and XML file mzcli packageexport -packages packages.xml /tmp/PE --password <password> Where packages.xml looks like this: <pipeline> <packages> <package name="TcpBased" version="1.1"> <configurations> <configuration foldername="TCP_Based"/> </configurations> </package> <package name="HttpDemo" version="1.0"> <configurations> <configuration foldername="HttpDemo"/> </configurations> </package> </packages> </pipeline> Note! The resolveDependencies attribute is always set to true for the packageexport command. [password] To export encrypted configurations, provide a password. Return Codes Listed below are the different return codes for the packageexport command: Code Description Code Description 0 Returned if the command was successful. 1 Returned if the argument count is incorrect or if the export fails. 2 Returned if the output directory exists, if write permission is missing, or if the directory cannot be created for any other reason. 3 Returned if the XML selection file cannot be read. 4 Returned if any errors were reported during export.

---

# Document 208: Error Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638531
**Categories:** chunks_index.json

The Error tab contains configurations related to error handling of the workflow. Since the batch concept does not exist for real-time workflows, batch related options in the tab are only valid for batch workflows. If you decide not to abort after one cancel batch you can choose whether to save errors to 1) the ECS (Error Correction System) or 2) Data Veracity . These two options are both described below. Open Error tab - Data Veracity batch error type Item Description Item Description Abort after one cancel batch If enabled, the workflow immediately aborts on the first Cancel Batch message from any agent in the workflow. The erroneous data batch is kept in its original place and must be moved or deleted manually before the workflow can be started again. Abort after one cancel batch followed by <configured amount> c onsecutive cancel batches If enabled, the value of <configured amount> indicates the number of allowed cancel batch calls, from any agent in a workflow before the workflow is aborted. The counter is reset between each successfully processed data batch. Thus, if 5 is entered, the workflow aborts on the 6th data batch in a row that is reported erroneous. All erroneous files, but the first one, are removed from the stream and placed into Data Veracity or the ECS . Do not abort after cancel batch The workflow will never abort. However, as with the other error handling options, the System Log is always updated for each cancel batch message, and files are sent to Data Veracity or the ECS . Error Batch Type When selecting Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch options above, the Error Batch Type radio buttons are enabled. Choosing Data Veracity will send all erroneous batches to Data Veracity while choosing ECS will send the erroneous batches to the ECS. Data Veracity Batch Error UDR Data Veracity allows for the handling of erroneous batch files received from the input source. You must set up a database table prior to setting up the error batch handling for your workflows. For more information see Data Veracity . The Data Veracity Error UDR sections are grayed out until one of the Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch alternatives is selected and Error Batch Type is set to Data Veracity . Item Description Item Description Error Code Select from a drop-down list of error codes that have been defined from the error code web interface. You can refer to Error Codes for instructions. The Error Codes are shared between Data Veracity and the ECS . Data Veracity Profile The Data Veracity profile to be used. Refer to Data Veracity Profile for instructions to set up the profile for use. Named MIMs MIM values to be associated with the erroneous batch file when sent to Data Veracity. The Named MIMs added in the Data Veracity Profile should be listed in the table. MIM Resource The Mim Resource column is populated with the MIM values that are based on the MIM parameters which are selected from the MIM Browser dialog. ECS Batch Error UDR A UDR that contains information on selected MIMs can be associated with the batch. This is useful when reprocessing a batch from the ECS, as the fields of the Error UDR will appear as MIMs in the collecting workflow. The batch UDR may be populated from Analysis or Aggregation agents as well. This is useful if you want to enter other values than MIMs. The ECS Batch Error UDR section is grayed out until one of the Abort after one cancel batch followed by <configured amount> consecutive cancel batches or Do not abort after cancel batch alternatives is selected and Error Batch Type is set to ECS . Open Error tab - ECS Error Bath Type Item Description Item Description Error Code Select from a drop-down list of error codes that have been defined in the Error Correction System Inspector. The Error Codes are shared between Data Veracity and ECS . Refer to Error Codes for instructions. Error UDR Type The error UDR to be associated with the batch. The appropriate format can be selected from the UDR Internal Format Browser dialog opened by selecting the Browse... button. The columns UDR Field and MIM Resource are populated depending on the selected UDR type. UDR Field A list of all the fields available for the selected Error UDR Type. MIM Resource The Mim Resource column is populated by clicking the MIM button. The preferred MIM to map to the Error UDR Type fields can then be selected from the MIM Browser dialog. Logged MIMs The column Error Mim holds information on what MIM resources to be logged in the System Log when the workflow aborts or sends UDRs and batches to ECS. These values may also be viewed from ECS (the MIM column). The most relevant resources to select are things that identify the data batch, such as the source filename, if available. Note that this is only a short summary of the functionality description. For further information, see Error Correction System .

---

# Document 209: Adding a Launcher Service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741736/Adding+a+Launcher+Service
**Categories:** chunks_index.json

A launcher service is an external web service that returns a list of existing platforms to the desktop launcher. For further information, see Launcher Service Interface . Follow these steps to add a launcher service: Click Add Instance and then Add a launcher service. Adding a launcher service Enter the URL of the launcher service. Enter an arbitrary name for the service. Click OK . The Desktop Launcher downloads a list of instances that are provided by the service. List of instances provided by a launcher service and login dialog

---

# Document 210: Derby - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605768/Derby
**Categories:** chunks_index.json

This section contains information that is specific to the database type Derby. Supported Functions The Derby database can be used with: Audit Profile Database Table Related Functions (APL) Event Notifications Reference Data Management SQL Collection/Forwarding Agents Task Workflows Agents (SQL) Preparations The drivers that are required to use the Derby database are bundled with the MediationZone software and no additional preparations are required. Note! The Database Profile does not support connections to an Embedded Derby Database.

---

# Document 211: Syslog Collection UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609610/Syslog+Collection+UDR+Types
**Categories:** chunks_index.json

This section describes the UDR types that are used with the Syslog Collection agent. SyslogMessageUDR The fields in the SyslogMessageUDR follows the MediationZone naming conventions but are based on the Syslog specification. For further information about the fields described in this section, see RFC5424 and RFC3164. The latter is only applicable if the incoming messages are not compliant with RFC5424. Field Description Field Description AppName (string) This field contains APP-NAME , which identifies the device or application that originated the message. Facility (int) This field contains the numerical code of the facility in the Priority value ( PRIVAL ). HostName (string) This field contains HOSTNAME and identifies the machine that originally sent the Syslog message. Msg (string) This field contains MSG, a free-form message that provides information about the event. MsgId (string) This field contains MSGID and is used to identify the message type. For example, a firewall might use the MSGID "TCPIN" for incoming TCP traffic and "TCPOUT" for outgoing TCP traffic. ProcId (string) This field contains PROCID . The PROCID field is often used to provide the process name or process ID associated with a Syslog system. Severity (int) This field contains the numerical code of the severity in the Priority value ( PRIVAL ). It is used to specify the type of program that is logging the message. StructuredData (map<string,map<string,map>>) This field contains STRUCTURED-DATA. This field is stored in a map that in itself contains maps of SD-ELEMENT . An SD-ELEMENT consists of a key and parameter key-value pairs. The key is referred to as SD-ID . The key-value pairs are referred to as SD-PARAM . SD-ID is case-sensitive and uniquely identify the type and purpose of the SD-ELEMENT . Each SD-PARAM consists of a key, referred to as PARAM-NAME , and a value, referred to as PARAM-VALUE . Open STRUCTURED-DATA Example - Using StructuredData field in APL consume { SyslogMessageUDR myUDR = (SyslogMessageUDR) input; //Note the space between the angle brackets! map<string,map<string,string>> myData = input.StructuredData; //.. } Timestamp (string) This field contains TIMESTAMP . Version (int) This field indicates the compliance level of the incoming messages. 0 - Compliant with RFC3164 1 - Compliant with RFC5424 If the message contains PRI followed by VERSION , the agent will interpret it as compliant with RFC5424. If the message does not contain PRI or VERSION, it will be interpreted as compliant with RFC3164. RFC5424 is more restrictive compared to RFC3164, and deviations from the specification in any of the subsequent message fields will cause decoding errors. Example - SyslogMessageUDR based on RFC5424 compliant message Message <165>1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 [exampleSDID@32473 iut="3" eventSource="Application" eventID="1011"] BOMAn application event log entry... Expected UDR field values AppName: evntslog Facility: 20 Hostname: mymachine.example.com Msg: BOMAn application event log entry.. MsgId: ID47 ProcId: null Severity: 5 StructuredData: {exampleSDID@32473={eventID=1011, eventSource=Application, iut=3}} Timestamp: 2003-10-11T22:14:15.003Z Version: 1 Example - SyslogMessageUDR based on RFC3164 compliant message with PRI Message <34>Oct 11 22:14:15 mymachine su: 'su root' failed for lonvick on /dev/pts/8 Expected UDR field values AppName: null Facility: 4 Msg: su: 'su root' failed for lonvick on /dev/pts/8 Hostname: mymachine MsgId: null ProcId: null Severity: 2 StructuredData: null Timestamp: Oct 11 22:14:15 Version: 0 Example - SyslogMessageUDR based on RFC3164 compliant message without PRI Message "Use the BFG!" Expected UDR field values AppName: null Facility: 0 Hostname: null MsgId: null Msg: Use the BFG! ProcId: null Severity: 0 StructuredData: null Timestamp: null Version: 0

---

# Document 212: SCP Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674641/SCP+Collection+Agent
**Categories:** chunks_index.json

The SCP collection agent collects files from a remote host and inserts them into a workflow, using the SCP protocol over SSH2. Upon activation, the agent establishes an SSH2 connection and an SCP session towards the remote host. If this fails, additional hosts are tried, if configured. On success, the source directory on the remote host is scanned for all files matching the current filter. In addition, the Filename Sequence service may be utilized for further control of the matching files. All files found will be fed one after the other into the workflow. When a file has been successfully processed by the workflow, the agent offers the possibility of moving, renaming, removing or ignoring the original file. The agent can also automatically delete moved or renamed files after a configurable number of days. In addition, the agent offers the possibility of decompressing (gzip) files after they have been collected before they are inserted into the workflow. When all the files have been successfully processed the agent stops, awaiting the next activation, scheduled or manually initiated. If you need t o run the SCP collection agent in a real-time workflow, for further information, see Batch-Based Real-Time Agents . Loading

---

# Document 213: Kafka Real-Time Collection Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138458/Kafka+Real-Time+Collection+Events
**Categories:** chunks_index.json

Agent Events The Kafka real-time collection agent has two agent events: Event Description Event Description Assigned The event is emitted the first time a workflow is started, followed by the partition/partitions it is assigned. Rebalance The event is emitted when a workflow is rebalanced, followed by the partition/partitions it is assigned. Note! Stand-by workflows do not emit any events. A stand-by workflow is a workflow that does not have any partitions assigned, which happens when there are no more available partitions. Debug Events

---

# Document 214: Real-Time Disk_Deprecated Forwarding Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676071/Real-Time+Disk_Deprecated+Forwarding+Agent+Example
**Categories:** chunks_index.json

This section contains information about how to download and run an example workflow configuration (real-time) with a Disk_Deprecated forwarding agent. In this workflow, a Pulse agent routes PulseUDR types to an Analysis agent. The latter extracts the bytearray payload from the UDRs and routes it to a Disk_Deprecated forwarding agent. The Disk_Deprecated forwarding agent is configured to close batches after every 30 UDRs, however, this condition will not be meet since the preceding Analysis agent will close the batch for every 10 UDRs. To do this, it uses the cyclic sequence number in the PulseUDR , which has a range between 1 and 10. The preceding Analysis agent also publishes a MIM parameter that is used to generate a unique filename for each batch. The example includes the following: Example configuration ( disk_forwarding_real_time.zip ): Follow these steps to install and run the example: Copy the downloaded files to /opt/disk_forw_rt_example on an EC host. Open the System Importer and select in /opt/disk_forw_rt_example/disk_forwarding_real_time.zip . Import all configurations. Start the workflow.

---

# Document 215: Ultra Format Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657244
**Categories:** chunks_index.json

The Ultra Format configuration introduces format definitions in the system to be used by agents in workflows. Use the Ultra Format Definition Language (UFDL) to describe format definitions. Apart from selecting Decoder, Encoder, and UDR Type definitions, formats can be imported from APL code, as well as directly from other Ultra definitions: import ultra.folder_name.module_name; // From APL code import folder_name.module_name; // From UFDL code To open an Ultra Format configuration, click Build  New Configuration . Select Ultra Format in the Configurations dialog. Open Ultra Format Editor The configuration contains the standard configuration buttons as described in Common Configuration Buttons . Syntax Highlighting In the code area, the different parts of the code are color coded according to type, for easier identification, and when right-clicking in the code area, a context sensitive popup menu appears, enabling easy access to the most common actions you may want to perform. The text is color-coded according to the following definitions: Blue - Functions Green - Types Orange - Comments Purple - Keywords Yellow/Green - Values Ultra Format Buttons The buttons available in the Ultra Format configuration are the general ones as described in Common Configuration Buttons . Configuration Diff If you want to compare the current version of an Ultra configuration with a previous version of the same configuration, you can use Configuration Diff to compare the two versions side by side. However, this tool is only available in Legacy Desktop. See Configuration Diff for more information.

---

# Document 216: GTP' LGU Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607908/GTP+LGU+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the GTP' LGU Collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Realtime . Click Add Agent and select GTP LGU Collection from the Collection tab of the Agent Selection dialog. Source Tab The Source tab includes connection type settings. Open The GTP' LGU Collection Agent Configuration - Source Tab Setting Description Setting Description Port Default value is 3386. Enter the port through which the agent should await incoming data packages. This port must be located on the host where the Execution Context is running. GSN IP Address This is the IP addresses of the GSN nodes that provide the data. Note! If you must specify to which GSN IP address a redirection request is sent by the GTP' server, you can configure this behavior per workflow instance via Workflow Properties on the Workflow Table tab instead of in the GTP' LGU Collection agent configuration dialog. Configuring this behavior via Workflow Properties overwrites the value defined in the agent. Server Port Enter the server port number for each node. The agent detects GGSN source port changes via the Node Alive Request or Echo Request messages. If a change is detected, it is is registered in the System Log. The agent's internal configuration is updated. Response Hosts Enter the IP address(es) used by the network element to send messages to the GTP' server. The GTP' server matches the associated GSN IP address and returns the response. You can configure more than one IP address by separating each host by a semi-colon. Miscellaneous Tab The Miscellaneous tab includes the format and storage settings of the data that is collected by the agent. Open The GTP' LGU Collection Agent Configuration - Miscellaneous Tab Setting Description Setting Description Format This value must match the Data Record Format in Data Record Packet IE. This is applicable if the Packet Transfer Command is either Send Data Record Packet or Send possibly duplicated Data Record Packet. Perform Format Version Check When this checkbox is selected, the Data Record Format Version in Data Record Packet IE must be identical to the setting in Format Version. Format Version This value should match the Data Record Format version in Data Record Packet IE. This is applicable if the Packet Transfer Command is Send Data Record Packet or Send possibly duplicated Data Record Packet. Directory Enter either the relative pathname to the home directory of the user account, or an absolute pathname of the target directory in the file system on the local host, where the intermediate data for the collection is stored. The intermediate data include: Files that keep track of sequence numbers and restart values. A directory called duplicates, where duplicates are saved. Note! This directory must be attended to manually. Note! When using several Execution Contexts, make sure that the file system that contains the GTP' information is mounted for all Execution Contexts. Acknowledgement from APL Select this checkbox to enable acknowledgement from the APL agent that follows the GTP' LGU Collection agent. This way the GTP' LGU Collection agent will expect a feedback route from the APL module, as well. No GTP' packets will be acknowledged before all the data that is emitted into the workflow is routed back to the collector. Controlling acknowledgement from APL makes sure that complete data is transmitted. Clear the checkbox if you want the agent to acknowledge incoming packets before any data is routed into the workflow. No Private Extension Select this checkbox to remove the private extension from any of the agent's output messages. Use seq num of Cancel/Release req Select this checkbox to change the type of the sequence numbers that are populated in a Data Record Transfer Response to either Release Data Record Packet, or Cancel Data Record Packet, in the Requests Responded field. Otherwise, the agent applies the sequence number of the released, or cancelled, Data Record Packet. Redirection Request Select this checkbox if you want to send Redirection Request messages to all configured GSN nodes when stopping the workflow. The workflow then waits for Redirection Response messages from all the GSN nodes before stopping. However, regardless of this setting, the workflow stops if it exceeds the number of seconds set in the Max Wait for a Response (sec) field, even if it has not received all responses. Advanced Tab Open The GTP' LGU Collection Agent Configuration - Advanced Tab Setting Description Setting Description Max Wait for a Response (sec) Enter the maximum period while the GTP' LGU Collection agent should expect a Node Alive Response message. If both this value and the Max Number of Request Attempts value are exceeded, a message appears in the System Log. For example: If Max Wait for a Response is set to 20 and Max Number of Request Attempts is set to 5, the warning message is logged after 100 seconds. The value also indicates the maximum period during which the GTP' LGU Collection agent awaits a Redirection Response. This period begins right after the agent releases a Redirection Request to the agents from which it is configured to receive data. Max Number of Request Attempts Enter the maximum number of attempts to perform in order to receive a Node Alive Response and Redirection Response. Max Outstanding Numbers Enter the maximum number of packages that you want kept in memory for sequence number checking. Max Drift Between Two Numbers Enter the maximum numbers that can be skipped between two sequence numbers. Disable Sequence Number Validation Select this checkbox to disable validation of Max Outstanding Numbers and Max Drift Between Two Numbers . Clear Checking Select this checkbox to avoid saving the last sequence number when the workflow is stopped. Clear the checkbox to have the agent save the sequence number of the last collected package when the workflow is stopped. This way, as soon as the workflow is restarted, a package with the subsequent number is expected and the workflow continues processing from the where it stopped. Agent Handles Duplicates Select this radio button to store duplicates in a persistent data directory that you specify on the Miscellaneous tab. The packet remains in the directory until the agent receives a request to release or cancel it. Route Duplicates to Select this radio button to route duplicates to a link that you select from the drop-down list. Alternate Node Enter the IP address of a host that runs an alternate Charging Gateway device as a backup. If you enter an IP address, the GTP' LGU Collection agent includes it in the redirection request that it sends to the GSN. Note! The GTP' LGU Collection agent does not backup any of the data that it manages. Make sure that the GSN node takes care of backup. Decoder Tab Open The GTP' LGU Collection Agent Configuration - Decoder Tab

---

# Document 217: Desktop Accessibility Options - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997330/Desktop+Accessibility+Options
**Categories:** chunks_index.json

Mediationzone prov ides accessib ility options for you, making interaction easier and more accommodating for all users. The modern user interface features several important options that cater to users that require assistance when viewing and interacting with the system. Screen Reader Support The Desktop supports screen readers across all elements of the user interface. This allows you to use standard and custom software packages that read out the displayed dialogs, text, and other interactive elements. Keyboard Shortcuts Keyboard shortcuts are enabled across the important interactive sections of the Desktop, including the Configuration screen. Click the Keyboard shortcuts button in the top left corner of the Desktop. This calls a dialog that displays the available shortcuts: Command Keys Command Keys Enable active mode / Focus element options / Execute option Enter Disable active mode / Clear selection and focus graph editor Esc Select next element Tab Select previous element Shift + Tab Remove agents and routes Delete or Backspace Select next agent  Select previous agent  Select next route  Select previous route  Move agent up Shift +  Move agent down Shift +  Move agent left Shift +  Move agent right Shift +  Themes Support There is support for themes that can be changed freely. You can choose which one suits your eyes best from the User Settings menu. In addition a high contrast mode can also be selected for improved reading.

---

# Document 218: User Agent Message Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670846/User+Agent+Message+Event
**Categories:** chunks_index.json

This event holds a value only if the APL function dispatchMessage has been used. The following fields are included: agentMessage - Message issued by the agent. agentName - The name of the agent issuing the event. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents - Workflow: <Workflow name> Agent name: <Agent name>, Message: <message> eventName origin receiveTimeStamp severity timeStamp Fields inherited from the Workflow event The following fields are inherited from the Workflow event, and described in more detail in Workflow Event : workflowKey workflowName workflowGroupName

---

# Document 219: Error Correction - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783756/Error+Correction
**Categories:** chunks_index.json

It is possible to view and modify any field of the UDRs in the ECS storage, and to view the associated error codes and configurable error cases as well, by clicking on the UDR Type and ECS Code columns bringing up UDR File Editor where field modifications can be done. Open UDR File Editor (top) and Error Codes Table (bottom) Any changes to a UDR that is made using the UDR File Editor is logged in the System Log as a user event and includes the user id, date and time of change as well as the ECS ID for the modified UDR. Open Bulk Editor allowing matching and assigning configuration The UDRs in ECS can also be edited in a bulk (i.e. editing multiple records at the same time), allowing the user to use one of the two following methods: The Matcher Configuration of one or more fields to find the records that should be edited, and then the Assignment Configuration to set the values in one or more fields as shown in the figure above. This option will show the results on each record processed by the edit  the change or the indication of no change. Manual APL script that is written into a pop-up window.

---

# Document 220: GTP' LGU Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607886/GTP+LGU+Agents
**Categories:** chunks_index.json

Info! The GTP' LGU agents are software components with so called Limited Availability (LA). Software components with Limited Availability are not available as part of the standard product offering, but instead delivered to fulfill specific customer needs and as such they may require further improvements in terms of scope, development, and documentation. This chapter describes the GTP' LGU Collection and ReCollection agents. These agents are real-time extensions of the original GTP' Agent produced specifically for LGU+ with their specifications. Prerequisites The reader of this information should be familiar with: GPRS Tunneling Protocol (GTP) across the Gn and Gp Interface [ 3GPP TS 29.060 V4.2.0 ] Call and event data for the Packet Switched (PS) domain [ 3GPP TS 32.015 V3.9.0 ] This chapter includes the following sections: GTP' LGU Collection Agent GTP' LGU ReCollection Agent

---

# Document 221: SAP JCo Uploader Agent Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642929/SAP+JCo+Uploader+Agent+Example
**Categories:** chunks_index.json

This section provides an example of a workflow to illustrate how the SAP JCo Uploader agent can be used. Open Example workflow with SAP JCo Uploader agent Decoder Agent An example for the Ultra Format definition for the Decoder can be as follows: Example - Decoder external sap_format_external { ascii value : terminated_by(0xA); }; in_map sap_format_inMap : external (sap_format_external), target_internal(sap_format_external_TI) {automatic;}; decoder sap_format_dec : in_map (sap_format_inMap); This is a simple decoder, decoding by 0xA (line feed) since the file format is flexible. Analysis Agent An example for the Analysis agent can be as follows: Example - Analysis agent import ultra.SAP_JCO; boolean receivedHeader; beginBatch { // Set receivedHeader = false. receivedHeader = false; } consume { // First record is the Header, so we will create HeaderUDR and map the value from input to it if (receivedHeader == false) { HeaderUDR header = udrCreate(HeaderUDR); //Assign Filename to HeaderUDR, this is use as primary key for State Handling header.sourceFilename = (string) mimGet("Disk_1", "Source Filename"); //Translate Input record in HeaderUDR getListOfBitField(header, input.value); //Route HeaderUDR out and set receivedHeader = false udrRoute(header); receivedHeader = true; } else { //For every record after Header, it will be the actual data records //Create RecordUDR and map values from input to it. //This example, input fields are comma separated RecordUDR record = udrCreate(RecordUDR); record.listOfFields = strSplit(input.value, ","); udrRoute(record); } } endBatch { debug("Response Time=" + mimGet("sap_jco_1", "Average Response Time")); debug("NoOfExecutions=" + mimGet("sap_jco_1", "No of Executions")); } /* This example function split the Header String into multiple BitFieldUDRs The example's format has "Version" in Field 1, "RFC Name" in Field 2 and the rest of the format description after that */ list<BitFieldUDR> getListOfBitField(HeaderUDR header, string data) { list<BitFieldUDR> result = listCreate(BitFieldUDR); if (data != null) { //All fields are comma seperated list <string> fields = strSplit(data, ","); if (listSize(fields) > 3) { header.version = listGet(fields,0); header.rfcName = listGet(fields,1); int i = 2; //In this example, every bitFieldUDR has the format "FieldName:FieldType" while (i < listSize(fields)) { BitFieldUDR bitField = udrCreate(BitFieldUDR); string value = listGet(fields,i); list<string> nameAndType = strSplit(value, ":"); if (listSize(nameAndType) >= 2) { bitField.fieldName = listGet(nameAndType, 0); bitField.fieldType = listGet(nameAndType, 1); } else { bitField.fieldName = value; } listAdd(result, bitField); i = i + 1; } //Add the entire bitFieldUDR list into HeaderUDR header.listOfBitFields = result; } else { abort ("Header has less than 3 fields!"); } } return result; }

---

# Document 222: High Availability Setup - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744677/High+Availability+Setup
**Categories:** chunks_index.json

This chapter describes how to enable high availability by configuring the system for automatic failover of pico instances. Storage and CMS Requirements For all high availability deployments, an external cluster management software (CMS) is required to ensure that failovers are executed. Commonly used CMS products in physical hardware deployments include Veritas, or MC Service guard. For batch processing, a shared or clustered storage should be used. For stateful real-time scenarios with session management, session information should be persisted, using a distributed storage such as CouchBase. When MediationZone is installed in an Open Stack cloud you can use Corosync and Pacemaker for monitoring. In Amazon Web Services you can use CloudWatch to perform the monitoring and Auto Scale to dynamically recreate a failed node. Overview MediationZone provides HA monitor scripts that are executed by the cluster management software. After interpreting the result, additional scripts are available to start and stop pico instances. The provided scripts typically requires some amount of modification in order to work with your CMS product. Each pico instance runs a HA Monitor Server that is responsible for reporting its status. This server is configured via a set of system properties. For further information about these properties, see High Availability Properties . The system properties pico.rcp.platform.host and pico.rcp.server.host must be configured to enable failover to a secondary pico instance. Depending on the pico type and the deployment type the specified hostnames must resolve to local, virtual, floating, or elastic IP addresses. For further information about how to set these properties, see Resolving Hostnames . When you setup the topology of the system, it is important to consider the ability to fail over pico instances and containers. For instance, in a deployment based on physical servers, the CMS product may support failover of an individual pico instance that runs in the Platform Container, e g an EC. However, in Amazon Web Services, Auto Scaling is used to start a copy of the failed host. This will impact all pico instances, including the Platform. For simplicity, it is recommended that the Platform, ECs, and SCs runs in separate containers, and in the case of Amazon Web Servicer and Open Stack, on separate hosts. Configuration of Cluster Monitoring Software Configuring the cluster monitoring software is beyond the scope of the MediationZone documentation. See the applicable third party documentation for this purpose. It is important to configure dependencies and availability of the resources within the cluster correctly. The cluster monitoring software must handle network interfaces, storage and database monitoring, and switching, including dependencies. This chapter includes the following sections: HA Monitor Server Recover - Platform Recover - EC and SC Resolving Hostnames

---

# Document 223: mzcli - wfdebug - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547980277/mzcli+-+wfdebug
**Categories:** chunks_index.json

Usage usage: wfdebug <pattern matching expression for workflow names> ... <ON|OFF> This command is used to enable or disable debug information for a workflow. Note that turning on the Debug mode might slow down the workflow due to log filing. Return Codes Listed below are the different return codes for the unregister command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect.

---

# Document 224: REST Client_Deprecated Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642578/REST+Client_Deprecated+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json



---
**End of Part 9** - Continue to next part for more content.
