# RATANON/MZ93-DOCUMENTATION - Part 95/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 95 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.7 KB
---

Agent Message Events An information message from the agent, stated according to the configuration done in Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file : filename Reported, along with the name of the source file, when the file is collected and inserted into the workflow. File cancelled: filename Reported, along with the name of the current file, when a Cancel Batch message is received. This assumes the workflow is not aborted. For further information, refer to Netia FTP Agent Transaction Behavior . Debug Events Debug messages are dispatched when debug is used. During execution, the messages are shown in the Workflow Monitor and can also be stated according to the configuration done in the Event Notification Editor. For further information about the agent debug event type, Debug Event . Command trace A printout of the control channel trace either in the Workflow Monitor or in a file. Loading

---

# Document 2225: SAP CC Notification Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034466/SAP+CC+Notification+Agent+Configuration
**Categories:** chunks_index.json

You open the SAP CC Notification agent configuration dialog from a workflow configuration. To open the SAP CC Online agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select SAP CC Notification from the Agent Selection dialog. Open SAP CC Notification agent configuration dialog Setting Description Setting Description Hosts In this section, add the IP address/hostname and external charging port of at least one SAP Convergent Charging Core server Dispatcher Instance. You can add additional hosts and reorder the hosts list using the Add, Edit, Remove, Up, and Down buttons. Connection Settings Enable API Authentication When enabled, this option will ensure that API Authentication will be used. Enter the associated credentials in the fields to configure this option. Note! API Authentication is only available for SAP CC version 4.1 SP2 and later. User Name Enter the username for API authentication. Password Enter the password for API authentication. Timeout Enter the notification timeout value in ms. Enable Secured Connection Select this to use TLS encrypted communication with Hosts . For more information about setting up Secured Connection, refer to SAP CC Secured Connection . Keystore Path Enter the path to the keystore on an Execution Container host. The path must be the same for all hosts. Note! The keystore format to be used by this particular agent is PKCS12 only. Keystore Password Enter the password of the keystore. Debugging Enable Debug Events When enabled, this option will turn on debug events.

---

# Document 2226: SAP CC Agent Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/391118898/SAP+CC+Agent+Preparations
**Categories:** chunks_index.json

The relevant jar files required for the preparations must be obtained from the SAP CC server provider. The Core SDK jar files can be found in SAP Convergent Charging Installation Materials. SAP CC Core SDK Jar Files for Version 2023 The following jar files are required by SAP CC agents when using with SAP Convergent Charging server version 2023: common_message.jar common_util.jar core_chargingplan.jar core_chargingprocess.jar core_client.jar jaxb-api-2.3.1.jar jaxb-core-2.3.0.1.jar jaxb-impl-2.3.8.jar logging.jar sap.com~tc~logging~java.jar Note! The version of SAP CC Core SDK client JAR files must match the version of your SAP CC core system (i.e. the back-end servers). From the unpacked SAP Convergent Charging installer, the jar files can typically be found by navigating to the directory and unzipping the core_sdk.zip file located in DATA_UNITS/<CC_VERSION>_TOOLS_CONTENT_UC_OSIND . Example - Unzip core_sdk.zip $ unzip DATA_UNITS/CC2023_TOOLS_CONTENT_UC_OSIND/core_sdk.zip -d core_sdk For the SAP Convergent Charging server patches, the JAR files can typically be found by unpacking the patch file for the SAP CC Core SDK software unit. Example - Unzip SAP CC Core SDK patch file $ unzip CORESDK02_0-80008788.ZIP -d core_sdk Example - Listing SAP CC Core SDK jar files $ ls -gG core_sdk/jars/ total 17152 -rw-r--r-- 1 174853 Feb 12 2024 common_message.jar -rw-r--r-- 1 660812 Feb 12 2024 common_util.jar -rw-r--r-- 1 691798 Feb 12 2024 core_chargingplan.jar -rw-r--r-- 1 426602 Feb 12 2024 core_chargingprocess.jar -rw-r--r-- 1 4917429 Feb 12 2024 core_client.jar -rw-r--r-- 1 128076 Feb 12 2024 jaxb-api-2.3.1.jar -rw-r--r-- 1 254858 Feb 12 2024 jaxb-core-2.3.0.1.jar -rw-r--r-- 1 1117712 Feb 12 2024 jaxb-impl-2.3.8.jar -rw-r--r-- 1 89161 Feb 12 2024 logging.jar -rw-r--r-- 1 302450 Feb 12 2024 sap.com~tc~logging~java.jar The classpaths for the jar files are specified for each EC. In the example below, the SAP jar files are located in MZ_HOME/3pp. Note! Ensure that you include existing paths, so that they are not overwritten. Example command to get existing classpaths: $ mzsh topo get topo://container:<container>/pico:<ec>/obj:config.classpath Example - Setting classpath $ mzsh topo set topo://container:<container>/pico:<ec name>/obj:config.classpath.jars ' ["lib/picostart.jar", "3pp/common_message.jar", "3pp/common_util.jar", "3pp/core_chargingplan.jar", "3pp/core_chargingprocess.jar", "3pp/core_client.jar", "3pp/jaxb-api-2.3.1.jar", "3pp/jaxb-core-2.3.0.1.jar", "3pp/jaxb-impl-2.3.8.jar", "3pp/logging.jar", "3pp/sap.com~tc~logging~java.jar"]' After the classpath has been set, you need to manually distribute the jar files so that they are in place when the EC is started. You can then restart the EC to register the changes in the configuration above for the SAP CC agents. Secure Communications via TLS The agent can be configured for secure communication via TLS. This requires a keystore be created for each EC host. For more information about setting up secure communication, refer to SAP CC Secured Connection . For SAP CC Online Agent, it is required to add the hostname information of the SAP Convergent Charging Core server Dispatcher Instance(s). This information is needed by the execution content server . The IP address needs to be explicitly mentioned. Refer to the SAP CC Online Agent Configuration page.

---

# Document 2227: User Privileges Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849334/User+Privileges+Management
**Categories:** chunks_index.json

Access to the MediationZone system is provided through the Access Controller tool within the MediationZone Desktop GUI. Users (including logins and passwords) and groups may be created and permissions may be mapped. Permissions are defined on two levels: Application level  permissions defined per window. A user group may be configured to be denied access to a specific window, or perhaps to have access although with no ability to remove system log entries. Configuration level  permissions defined per configuration. Three levels are available; read, write and execute. Read access allows users of the group to view the configuration. Write access allows users of the group to modify and save the configuration. Execute access allows users of the group to run the workflow. When a user creates a new configuration, the default group for the user will be the group with read, write and possibly execute permission for that configuration. Additionally, this application access to any configuration can be further limited through the permission setting on an individual configuration such as a workflow. Below is an example of the access group application permission set-up using the Access Controller tool. Open Example of application permission setup The following is an example of how a configuration objects permissions are represented in the Configuration Browser. The owners and groups selected below are defined as Users and Access Groups in the Access Controller. Open Example of configuration object's permissions Access to the operating system and the database are controlled through the normal security methods provided within the corresponding software. The MediationZone client security features, such as idle-time log out, additional password configuration and password expiration will be handled in the clients configuration.

---

# Document 2228: mzcli - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979265/mzcli
**Categories:** chunks_index.json

The mzcli client is a downloadable JAR file ( mzcli.jar ), available from the Downloads dialog in the User Settings menu in Desktop. For details, see User Settings , then click on mzcli.jar to download it. You can run mzcli on any computer with Java installed and HTTP connectivity to the platform. For supported Java versions, see System Requirements . This section contains the following subsections: Using mzcli mzcli Commands

---

# Document 2229: KPI Management Quick-Start Guide - Distributed Processing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742794/KPI+Management+Quick-Start+Guide+-+Distributed+Processing
**Categories:** chunks_index.json

The input data in this example use case consists of sales numbers in CSV format. This dataset is from here on, referred to as "sales". The data is collected in real-time from the regions "APAC", "AMERICAS", and "EMEA". We want to calculate the total-, average, and number of sales per minute. These numbers will be our KPIs, broken down per country and region. Example - Input data timestamp region country amount timestamp region country amount 2017-03-08T13:53:52.123 EMEA Sweden 123.50 2017-03-08T13:53:56.123 APAC India 12.12 2017-03-08T13:53:59.123 AMERICAS US 425.23 Note! As a prerequisite, the scripts must be prepared according to Preparing and Creating Scripts for KPI Management . Step-by-Step Instructions Follow these steps to get started: Configure the service model. The service model describes your data, which KPIs (KPI stands for Key Performance Index) to generate, and how to calculate them. A JSON representation is used to describe the model, which includes the following top-level objects: dimension tree metric kpi threshold (optional) Start with the dimensions and tree objects. The dimensions describe the fields of your data used for grouping and the tree the relation between them. The identifying fields in the input data are region and country. A region has one or more countries. The data type is sales. In the dimension object we specify each of our identifying fields as separate objects, with the datatype and field in the body. "dimension": { "Region": { "sales": "region" }, "Country": { "sales": "country" } }, "tree": { "tree1": { "Region": { "Country": { } } } } Define the metrics using the amount field in the input data: totalSales - For total sales, sum up the amount for each record by using the sum function on the expression expr, which contains the amount field. avgSales - For average sales use the avg function instead of sum . numSales - To count the number of records, use the function isSet in the expression. This function evaluates to 1 if there is a value in amount or 0 if there is no value. Use the function sum to sum up the 1s and 0s. Define the KPIs. The expected output is the total sales, average sales, and number of sales per region and country in 60 second periods. Use the property node to describe where in the topology the KPI should be calculated and windowSize to set the period length. Use the name of the metrics defined above in the expr property KPI JSON "kpi": { "Region.TotalSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "totalSales" }, "Region.AvgSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "avgSales" }, "Region.NumberOfSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "numSales" }, "Country.TotalSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "totalSales" }, "Country.AvgSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "avgSales" }, "Country.NumberOfSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "numSales" } } Combine all the objects above for a complete representation of the model. Below is an example containing all types. Full Service Model in JSON { "dimension": { "Region": { "sales": "region" }, "Country": { "sales": "country" } }, "tree": { "tree1": { "Region": { "Country": { } } } }, "metric": { "totalSales" : { "fun": "sum", "expr": { "sales": "amount" } }, "avgSales" : { "fun": "avg", "expr": { "sales": "amount" } }, "numSales" : { "fun": "sum", "expr": { "sales": "isSet(amount)" } } }, "kpi": { "Region.TotalSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "totalSales" }, "Region.AvgSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "avgSales" }, "Region.NumberOfSales": { "node": [ "tree1", "Region" ], "windowSize": 60, "expr": "numSales" }, "Country.TotalSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "totalSales" }, "Country.AvgSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "avgSales" }, "Country.NumberOfSales": { "node": [ "tree1", "Region", "Country" ], "windowSize": 60, "expr": "numSales" } } } Open the Desktop and paste the service model into a KPI profile. Save the profile with the name SalesModel in the folder kpisales . Open Configure Kafka and Zookeper. KPI Management reads and writes its data to and from Kafka. In order for this to work, you need to install and configure both Kafka and Zookeeper. More information about this can be found on the pages Spark, kafka and zookeeper as well as Starting Clusters and Creating Topics . Kafka depends on Zookeeper (which is also included in the Kafka-installation folder) and you need to ensure that Zookeeper is started first. Configure Spark. The Sp ark cluster will be running a so called app for doing the KPI calculations. Install and Configure Spark. The Spark cluster will be running a so called "app" for doing the KPI calculations. First you need to install Spark for Scala ( spark-3.5.0-bin-hadoop3-scala2.13 ). More information about this can be found in the Spark documentation, https://spark.apache.org/docs/3.5.0/ . For further information about properties related to Spark, see Spark, kafka and zookeeper . Please note on the page that the spark-defaults.conf in the spark needs to contain the parameters mentioned on Preparing and Creating Scripts for KPI Management for this to work. The Spark slave node will have one worker that will be assigned four cores. The cores are split between the executors and the Spark driver. This means that we will have three executors running in parallel. The property SPARK_DEFAULT_PARALLELISM in kpi_param.sh is set to match this value. The property MZ_KPI_PROFILE_NAME needs to match the folder- and configuration name of the KPI profile that was created in step 1. Start up Zookeeper, Kafka and Spark. Set up environment variables $ export SPARK_HOME=/opt/spark-3.5.0-bin-hadoop3-scala2.13 $ export KAFKA_HOME=/opt/kafka_2.13-3.3.2 $ export PATH=$KAFKA_HOME/bin:$PATH:/opt/mz_kpiapp/bin And while located in $KAFKA_HOME execute: Start Zookeeper and Kafka $ bin/zookeeper-server-start.sh config/zookeeper.properties & bin/kafka-server-start.sh config/server.properties Run the following command to start spark: Start Spark start_master_worker.sh Create the Kafka topics that are required by the KPI app. Each of the Spark executors needs to read from a separate Kafka partition so each of the topics needs three partitions, i.e the number of partitions for each topic must be identical to the value of the property SPARK_DEFAULT_PARALLELISM in kpi_params.sh . Create Kafka Topics $ bin/kafka-topics.sh --create --topic kpi-input --bootstrap-server localhost:9092 --partitions 2 $ bin/kafka-topics.sh --create --topic kpi-output --bootstrap-server localhost:9092 --partitions 2 $ bin/kafka-topics.sh --create --topic kpi-error --bootstrap-server localhost:9092 --partitions 2 Create the real-time workflow. In this guide we will use Pulse agents to simulate sales data coming from three different sources, EMEA, AMERICAS, and APAC. Add three Pulse agents and an Analysis agent. Open Workflow - Pulse Agents Configure the Pulse agents as follows: AMERICAS will send 1000 TPS - Set Time Unit to MILLISECONDS and Interval to 1 EMEA will send 500 TPS - Set Time Unit to MILLISECONDS and Interval to 2 APAC will send 250 TPS - Set Time Unit to MILLISECONDS and Interval to 4 To be able to identify the data, set the data to the region name. Open Open Pulse agent configuration The pulse agents only sends us a simple event containing the name of the region, the other data that will be used in the KPI calculations are generated in the connected Analysis agent. The APL code below creates the input to KPI Management. APL Code list<string> americas = listCreate(string, "US", "Canada", "Mexico", "Brazil", "Argentina", "Cuba", "Colombia"); list<string> emea = listCreate(string, "Sweden", "UK", "Portugal", "Italy", "France", "Germany", "Norway", "Spain", "Finland", "Denmark"); list<string> apac = listCreate(string, "India", "China", "Japan", "Thailand", "Australia", "Indonesia", "Malaysia","South Korea"); consume { // create KDR - the input for the KPI CLusterIn agent kpimanagement.KDR kdr = udrCreate(kpimanagement.KDR); // The KDR has a type field - we set this to the value we had for our data type in the model kdr.type = "sales"; // It also has a timestamp field - lets populate that from the current time but using seconds. kdr.timestamp = dateCreateNowMilliseconds() / 1000; string region = baToStr(input.Data); // the data in our use case (country, city, amount) we will put in the values field of the kdr. map<string, any> sales = mapCreate(string,any); mapSet(sales, "region", region); // lets create a random amount between 1 and 1000 int amount = randomInt(1000); // set amount and city depending on the region if (region == "AMERICAS") { mapSet(sales, "amount", amount * 1.25d); mapSet(sales, "country", randomCountry(americas)); } else if (region == "EMEA") { mapSet(sales, "amount", amount * 1.0d); mapSet(sales, "country", randomCountry(emea)); } else if (region == "APAC") { mapSet(sales, "amount", amount * 0.65d); mapSet(sales, "country", randomCountry(apac)); } else { mapSet(sales, "amount", 0.0d); mapSet(sales, "country", "UNKNOWN"); debug("Unknown region:" + region); } kdr.values = sales; udrRoute(kdr); } // pick a random country from a list string randomCountry(list<string> countries) { int index = randomInt(listSize(countries)); return listGet(countries, index); } Create a Kafka profile for the KPI Cluster In agent. This agent will write to the kpi-input topic. Open Kafka profile configuration - kpi-input Add a KPI Cluster In agent. Open Workflow - KPI Cluster In agent Open Configure it to use the KPI profile that you created as part of point 'a' above. And add the Kafka Profile that the agent will use to write on the kpi-input topic. This will be read from by the KPI Management Spark application. The Analysis agent is added because the KPI Forwarding agent will send out KafkaExceptionUDR in case of errors in the Kafka communication (if Route On Error option is selected). This example does not cover handling of those errors. Open Create a Kafka Profile for the KPI Cluster Out agent. This agent will read from the kpi-output topic. Open Kafka Profile Configuration - kpi-output Add a KPI Output Agent (on its own, see further down for screenshot) and configure it as follows. This agent will provide the KPI output: Open Add another Analysis agent for debugging of the KPIs. Open Final workflow configuration Add the APL code below to the Analysis agent. APL Code string format = "yyyy-MM-dd'T'HH:mm:ss:S"; consume { // input is KPIAggregatedOutput which contains a list of // KPIOutput list<kpimanagement.KPIOutput> kpis = input.kpiOutputUDRs; // loop the KPIs and debug string dateStr = ""; for (int i = 0; i < listSize(kpis); i++) { kpimanagement.KPIOutput kpi = listGet(kpis, i); dateToString(dateStr, dateCreateFromMilliseconds(kpi.periodStart * 1000), format); debug("Period start: " + dateStr + ", instance: " + kpi.instancePath + ", KPI: " + kpi.kpi + ", Value:" + kpi.value + ", Samples: " + kpi.sampleCount); } } Submit the Spark application to the cluster. Submit the Spark application submit.sh kpiapp Open the Spark UI at http://localhost:8080/ . You should see that kpiapp is running. Open Spark UI Open the workflow configuration in the Workflow Monitor. Enable debugging and select events for the KPI Cluster Out agent and the Analysis agent that produces the debug output. Start the workflow. It may take a minute to display the output data: Open

---

# Document 2230: Event Setup Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638784/Event+Setup+Tab
**Categories:** chunks_index.json

In the Event Setup tab you add the event(s) for which you want notifications to be sent. For each event, the parameters that are populated by using the type Event Field , Formatted (with variables) or SQL , can be assigned different values values from selected event fields. Event Notification - Event Setup tab Setting Description Setting Description Filter Settings Select the event types you want to send notifications for in the Filter table. For each event type you can define that certain values in the event type must be matched in order for a notification to be sent, e g a specific workflow, or certain severities Apart from matching existing values (for instance Workflow name) you can also use regular expressions and hard coded strings. When using regular expressions, Java syntax applies, see http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html Event Field Name The contents of this column varies depending on the selected event type, Event Fields for more information. Note that only fields of string type are visible. Match Value(s) In this column you define the matching criteria for the events you want to send notifications for. Double-click on a cell to open the Edit Match Value dialog box where you can configure matching criteria. The values you can select from are pre-defined suggestions, but you can also use hard coded strings and regular expressions. Example For example, entering: .*idle.* will match any single lines containing "idle". Some fields also contain several lines, so entering: (?s).*idle.* will match any multi-line content containing "idle". The default value for each of the components is All . Note! Some of the Event Fields let you select from four Match Value types: Information , Warning , Error , or Disaster . For the rest of the Event Fields Match Value you use a string. Make sure you enter the exact string format of the relevant Match value. For example: the Event Field timeStamp can be matched against the string format yyyy-mm-dd. Field Map Settings Maps variables against event fields. The Field Map table exists only if any of the parameters for the selected notifier type is set to Formatted , Event Field or SQL . Notifier Field Displays the Notifier parameter available in the Notifier Setup tab. If a specific parameter has more than one variable, it will claim one line per variable. Variable Displays the name of the variable, as entered in the Notifier Setup tab. If the parameter type is Event Field , this field will be empty. Event Field Select the event field from which you want to pick values from in this drop-down list. Add Event... Click on this button to add an event that you can call from APL or Ultra Code. The new event type is added as a separate tab in the Event Setup . Remove Event... Click on this button to remove the event type tab you have selected. Refresh Field Map Click on this button to updates the Field Map table. This is required if the parameter population types or formatting fields have been modified in the Notifier Setup tab.

---

# Document 2231: Filters - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999508
**Categories:** chunks_index.json

Filters allow the user to filter out and locate erroneous UDRs and batch files in Data Veracity. The filter that created in this page, can be used in Data Veracity Search, for more details please refer to Search & Repair . Unsupported Data Type for Search Searching, filtering and repairing of UDRs with list and map data types are currently not supported by Data Veracity. In Filters, you can view and delete the saved filters. Open Data Veracity - Filters Filter Sort Order Sorting by Name column is based on Javas default sorting behavior for Strings(for example, sorting of alphabetical characters in Java is case sensitive). Warning! Existing filters can not be renamed. Deleting a Filter Filters can only be deleted from the Filters web UI. Deleting a filter will remove it completely from Data Veracity. To delete a filter, you can select one or many filters at once and then click on the Delete button. You will be prompted with a message to confirm the deletion of the selected filters. Click OK to proceed with the deletion. Open Data Veracity - Deleting filters

---

# Document 2232: Netia FTP Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031909/Netia+FTP+Agent+Configuration
**Categories:** chunks_index.json

You open the Netia FTP agent configuration dialog from a workflow configuration. To open the Archiving forwarding agent configuration, click Build  New Configuration . Select Workflow in the Configurations list. When prompted to select workflow type, select Batch . Click Add agent and select NetiaFTP from the Collection tab in the Agent Selection list. Connection Tab The Connection tab contains configuration data that is relevant to a remote server. Open The Netia FTP collection agent configuration - Connection tab Field Description Field Description Host Enter the primary host name or IP address of the remote host to be connected. If a connection cannot be established to this host, the Additional Hosts specified in the Advanced tab, are tried. Note! The FTP Agent supports both IPv4 and IPv6 addresses Username Enter the username for an account on the remote host, enabling the FTP session to login. Password Enter the password for the Username entered. Transfer Type Select the data transfer type to be used during file retrieval. Binary - The agent uses the binary transfer type. This is the default setting. ASCII - The agent uses ASCII transfer type. File System Type Select the type of file system on the remote host. Unix - The remote host uses a Unix file system. This is the default setting. Windows NT - The remote host uses a Windows NT file system. VAX/VMS - The remote host uses a VAX/VMS file system. Note! When the File System Type for VAX/VMS is selected, some issues must be considered. If a file is renamed after collection on a VAX/VMS system, the filename might become too long. In that case the following rules apply: A VAX/VMS filename consists of <file name>.<extension>;<version>, where the maximum number of characters for each part is: <file name>: 39 characters <extension>: 39 characters <version>: 5 characters If the new filename turns out to be longer than 39 characters, the agent moves part of the filename to the extension part. If the total sum of the filename and extension part exceeds 78 characters, the last characters are truncated from the extension. Enable Collection Retries Select this check box to enable repetitive attempts to connect and collect data. Retry Interval (s) Enter the time interval in seconds, between retries. Max Retries Enter the maximum number of retries to connect. Note! This number does not include the original connection attempt. Note! If a connection problem occurs the actual time interval will be the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval (s) field. Source Tab The Source tab contains configurations related to the remote host, source directories and source files. Open The Netia FTP collection agent configuration - Source tab Setting Description Setting Description Collection Strategy Only the Default Collection Strategy is available for the Netia FTP collection agent, so this option should always be selected in this drop-down-list. Include Subfolders This is not supported for the Netia FTP collection agent and selecting this checkbox will not have any effect. Directory The directory setting is not used for the Netia FTP collection agent. However, since this field needs to be populated for the agent to validate, simply enter "." or / in this field. Filename This field is used for determining the name of the source file(s) on the remote host. If you have disabled the Filename Sequence settings, you have to enter the complete name of the file to be collected in this field. If you have enabled the Filename Sequence settings, enter the filename up to the point where the sequence number begins in this field. For example if the names of the files are "IX.ICAMA.<sequence number>", enter "IX.ICAMA." in this field. Note! Filename must be one specific filename. Multiple filenames or wildcards cannot be used. For further information, see Filename Sequence Tab below. Compression Compression type of the source files. This setting determines if the agent will decompress the files before passing them on in the workflow. No Compression - The agent does not decompress the files. Gzip - The agent decompresses the files using gzip. Before Collection Move to Temporary Directory If this option is enabled, the source files will be moved to a subdirectory called DR_TMP_DIR in the source directory, before collection. This option supports safe collection when source files repeatedly use the same name. Append Suffix to Filename Enter the suffix that you want to be added to the file name prior to collection. Important! Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (h) If enabled, when the configured number of hours have passed without any file being available for collection, a warning message (event) will appear in the System Log and Event Area: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR if using Move to Temporary Directory ) to the directory specified in the Destination field, after collection. Note! The Directory has to be located in the same file system as the collected files. The absolute path names must be defined. If a file with the same filename, but with different content, already exists in the target directory, the workflow will abort. If a file with the same file name, and the same content, already exists in the target directory, this file will be overwritten and the workflow continue running. Rename Options Rename If this option is enabled, the source files will be renamed after the collection, and remain (or moved back from the directory DR_TMP_DIR if using Move to Temporary Directory ) in the source directory. Note! When the File System Type for VAX/VMS is selected, there are special considerations. If a file is renamed after collection on a VAX/VMS system, the filename might become too long. In that case, the following rules apply: A VAX/VMS filename consists of <file name>.<extension>;<version>, where the maximum number of characters for each part is: <file name>: 39 characters <extension>: 39 characters <version>: 5 characters If the new filename turns out to be longer than 39 characters, the agent will move part of the filename to the extension part. If the total sum of the filename and extension part exceeds 78 characters, the last characters are truncated from the extension. An example: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_ CHARACTERS.DAT;5 will be converted to: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_. CHARACTERSDAT;5 Note! Creating a new file on the FTP server with the same file name as the original file, but with other content, will cause the workflow to abort. Creating a new file with the same file name AND the same content as the original file will overwrite the file. Remove If enabled, the source files will be removed from the directory (or from the directory DR_TMP_DIR , if using the Move to Temporary Directory option, after collection. Ignore If enabled, the source files will remain in the source directory after the collection. This field is not available if the Move to Temporary Directory option is enabled. Destination Enter the full pathname to the directory on the remote host into which the source files will be moved after the collection. This field is only available if Move to is enabled. Prefix and Suffix Prefix and/or suffix that will be appended to the beginning and the end of the name of the source files, respectively, after the collection. These fields are only available if Move to or Rename is enabled. Warning! If Rename is selected, the source files will be renamed in the current (source or DR_TMP_DIR ) directory. Be sure not to assign a Prefix or Suffix , giving files new names still matching the Filename regular expression. That will cause the files to be collected over and over again. Search and Replace Select either the Move or Rename option. Search : Enter the part of the filename that you want to replace. Replace: Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the target agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note! A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text Search and Replace Examples To rename the file file1.new to file1.old , use: Search: .new Replace: .old To rename the file JAN2011_file to file_DONE , use: Search: ([A-Z]*[0-9]*)_([a-z]*) Replace: $2_DONE Note that the search value divides the file name into two parts by using parentheses. The replace value applies to the second part by using the place holder $2 . Keep (days) Enter the number of days to keep moved or renamed source files on the remote host after collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, determining when the file may be removed. This field is only available if the Move to or Rename option is enabled. Advanced Tab The Advanced tab contains configurations related to the use of the FTP service. For example if the FTP server used does not return the file listed in a well defined format, the Disable File Detail Parsing option can be useful, see Disable File Detail Parsing . Open The Netia FTP collection agent configuration - Advanced tab Setting Description Setting Description Command Port The value in this field defines which port number the FTP service will use on the remote host. Timeout (s) The maximum time, in seconds, to wait for a response from the server. 0 (zero) means to wait forever. Passive Mode (PASV) This setting must be enabled if FTP passive mode is used for data connection. In passive mode, the channel for data transfer between client and server is initiated by the client instead of by the server. This is useful when a firewall is situated between the client and the server. Disable File Detail Parsing This setting disables parsing of file detail information received from the FTP server. This enhances the compatibility with unusual FTP servers but disables some functionality. If file detail parsing is disabled, file modification timestamps are unavailable to the collector. The collector does not have the ability to distinguish between directories and simple files. For that reason, subdirectories in the input directory must not match the filename regular expression. The agent assumes that a file named DR_TMP_DIR is a directory because a directory named DR_TMP_DIR is used when Move to Temporary Directory in the Source tab is activated. Therefore, you cannot name a regular file in the collection directory DR_TMP_DIR. Additional Hosts List of additional host names or IP addresses that may be used to access the source directory, from which the source files are collected. These hosts are tried, in sequence from top to bottom, if the agent fails to connect to the remote host, set in the Connection tab. Note! The FTP Agent supports both IPv4 and IPv6 addresses. Use the Add , Edit , Remove , Move up and Move down buttons to configure the host list. Filename Sequence Tab The Filename Sequence settings are used when you want to collect files containing a sequence number in the file name. The sequence number is expected to be found on a specific position in the file name and will have a fixed or dynamic size. Note! When collecting from several sources, the Filename Sequence settings are applied on data arriving from all the sources, as though all the information arrived from a single source. This means that the Filename Sequence Number count is not specific to any of the sources. Open The Netia FTP collection agent configuration - Filename Sequence tab Setting Description Setting Description Enable Filename Sequence This setting determines if the service will be used or not. If the Filename Sequence settings are enabled, a list with possible files ending with the range of numbers defined by the Wrap On and Wrap To values will be generated before collection. Otherwise, a list with only one file with the name stated in the Source tab will be generated. Note! If the Filename Sequence settings are enabled, the workflow will collect files in the sequence until a file cannot be found. The workflow will then stop. The next time the workflow is started, it will continue to collect where it stopped last, that is if the last collected file ended with number 121, the workflow will start to collect the file ending with 122, the next time it is executed. This may result in a slower startup of the workflow. Start Position The offset in the file name where the sequence number starts. The first character has offset 0 (zero). In the example file name TTFILE0001 the start position is 6. Length The length of the sequence number, if the sequence number has a static length (padded with leading zeros). If the sequence number length is dynamic this value is set to 0 (zero). Example - Length setting TTFILE0001-TTFILE9999 Length: 4 TTFILE1-TTFILE9999 Length: 0 Wrap On If the Filename Sequence service is enabled, a value must be specified on which the sequence will wrap. This number must be larger than the value entered for Wrap To . Wrap To Enter the value that the sequence will wrap to. This value must be less than the value entered for Wrap On . Warn On Out of Sequence If you select this check box, the agent will log an informative message to the System Log when detecting an out of sequence file, before deactivating.

---

# Document 2233: Data Hub Overview (To be removed) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658631/Data+Hub+Overview+To+be+removed
**Categories:** chunks_index.json

Data Hub provides the ability to store and query large amounts of data processed by . Typical usage of Data Hub includes: Data tracing Temporary archiving Analytics Integration with external systems Staging data for further processing Data Hub requires access to Cloudera Impala, which provides high-performance, low-latency SQL queries on data stored in an Hadoop filesystem (HDFS). The Data Hub agent bulk loads data in CSV files to HDFS and then inserts it into a Parquet table in the Impala database. The table data is then available for query via the Web UI. In a production environment, it is recommended that the size of the collected files ranges between 1 to 100 MB. Though it is possible to collect and process small batches the overhead of handling a large number of files will have significant impact on performance. You may remove old data from the Impala database with the Data Hub task agent.

---

# Document 2234: Container Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816051
**Categories:** chunks_index.json

This section describes properties that are typically set on a container level and applicable to the Platform, ECs and SCs. Property Description Property Description java.library.path Default value: ${mz.home}/common/lib/native This property must be set to ${mz.home}/common/lib/native . mz.httpd.security Default value: false This property enables HTTP communication protected by TLS (i e HTTPS). mz.httpd.security.hsts.enabled Default value: false This property enables the Strict-Transport-Security flag ( HSTS ) for the desktop. mz.httpd.security.hsts.max_age Default value: 63072000 (2 years) If HSTS is enabled, then this property can be used to control the "max-age" value. mz.httpd.security.keystore Default value: "" This property specifies the path to the keystore that is used for HTTP/TLS mz.httpd.security.keystore.password Default value: "" This property must contain the password to the keystore specified in mz.httpd.security.keystore . mz.httpd.security.key.alias Default value: "" This property specifies which of the keys in the keystore that should used for HTTP/TLS (if there are more than one). HTTP will prefer to use the key with this keystore alias. If it is not set and the keystore contains more than one private key, it is undefined which key is used. mz.httpd.security.key.password Default value: "" This property must contain the password to the key to the key that is used for HTTP/TLS. By default (in keytool ), this is the same as the keystore password. mz.picocache.link.use Default value: false If you set this property to true , a links-based implementation will be used for the pico cache, which will save disk space if you have many picos in the same container. mz.picocache.link.target.dir Default value: $MZ_HOME/pico-cache/lib If you have set the mz.picocache.link.use property to true , you can use this property to specify the directory used for the actual jar files, which may be useful if you want to keep all the jar files for the whole system in one place and not split up for each container. pico.rcp.server.bind_interfaces Default value: "" When you set the property pico.rcp.server.host , pico instances will only bind to the interface associated with that IP address. Due to the network configuration, it may be required by pico instances to bind to additional interfaces. You can specify these by specifying a comma-separated list of IP address or hostnames in the property pico.rcp.server.bind_interfaces. It is also possible to set this property to the value ALL to ensure that the pico instances will bind to all interfaces, even though pico.rcp.server.host has been set. If you have not set pico.rcp.server.host , the property pico.rcp.server.bind_interfaces will have no effect. Example - Using pico.rcp.server.bind_interfaces An EC named ec1 has one external and one internal IP address. Other ECs will have to use the hostname ec1host to be able to connect. The name ec1host maps to either the external or internal IP address depending on the client location in the network. To ensure that all connection attempts will use the hostname, you set the property pico.rcp.server.host to ec1host . This will then cause the ec1 to only bind to ec1host which will map to the internal IP address, since this is the local context. If an other EC on the external network, ec2 in this example, tries to connect to ec1, it will use the hostname ec1host which maps to the external IP. This will fail. To ensure connectivity you need to set pico.rcp.server.bind_interfaces to the external IP address or ALL to ensure that the incoming connection attempt from ec2 will succeed. pico.rcp.server.host Default value: "" This property specifies the IP address or hostname of the pico instances. It will be used to determine the interface that the pico instances must bind to and the IP address/hostname that will be used by connecting processes. When you enter the hostname as the value of this property, if a failover occurs, the hostname is retrieved from the DNS enabling reconnection. If you enter the IP address as the value of this property, if it is a static IP address, reconnection issues may occur if the IP address changes. When the value of this property is left blank, the pico instance will bind to all IP addresses of the host. This means that the pico will listen for inbound network traffic on all network interfaces, and may attempt to use any local IP address for outbound network traffic. Note! If there is more than one IP address for the host, this property has to be set with the correct IP address. Make sure to set the property if you use IPv6, or if a high availability environment is configured. For information about high availability, see High Availability Properties . Note! When pico.rcp.server.host is set in the Platform Container, the value must be identical to pico.rcp.platform.host. pico.rcp.tls.keystore Default value: "" This property specifies the path to a keystore and enables the system to use TLS for all RCP connections that are not from the local host. If this property is not set, TLS will not be used. pico.rcp.tls.keystore.alias Default value: "" This property specifies which of the keys in the keystore that should used for RCP/TLS (if there are more than one). RCP will prefer to use the key with this keystore alias. If it is not set and the keystore contains more than one private key, it is undefined which key is used. pico.rcp.tls.keystore.password Default value: "" This property must contain the password to the keystore specified in pico.rcp.tls.keystore . pico.rcp.tls.key.password Default value: "" This property must contain the password to the key that is used for RCP/TLS. By default (in keytool ), this is the same as the keystore password. pico.rcp.tls.require_clientauth Default value: false This property specifies if client authentication is required when these are not running on the local host. pico.tmpdir Default value: MZ_HOME/tmp This property specifies the temp directory you want to use for your picos. pico.upgrade_history Default value: ${mz.home}/upgrade_history This property specifies the directory where the new and old versions of packages patched into the system are stored. rest.client.max.chunk.size Default value: "8m" This property specifies the maximum chunk size of the HTTP response that the REST Client agent should receive from the server. The agent will reject data with sizes that are larger than the value defined by this property. You can also set this property on the pico level, where the value is only applied to the defined EC. You can refer to Execution Context Properties for more information. rest.client.max.content.length Default value: "64m" This property specifies the maximum length of the HTTP content received by the REST Client agent. The agent will reject content that is longer than the specified value defined by this property. Although it is also possible to set the value of this property to infinite, there will be a possibility where the EC will crash from an out of memory error. So do consider setting the memory size of the EC to be higher than the expected size of the HTTP content that the agent will be receiving. You can also set this property on the pico level, where the value is only applied to the defined EC. You can refer to Execution Context Properties for more information.

---

# Document 2235: Parquet UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608492/Parquet+UDR+Types
**Categories:** chunks_index.json

The Parquet UDR types are designed to enable encoding/decoding Parquet data in workflow agents. The Parquet UDR types can be viewed in the UDR Internal Format Browser in the 'Parquet' folder. To open the browser, first, open an APL Editor, and, in the editing area, right-click and select UDR Assistance . The section contains the following subsections: ParquetDecoderUDR ParquetEncoderUDR ParquetType

---

# Document 2236: Derby Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204996789/Derby+Preparations
**Categories:** chunks_index.json

Derby is installed in the Platform Container. The setup is managed by MediationZone itself which means no extra preparations are needed once the general preparations are finished according to General Preparations Platform . Once the container is installed, that is, after you have performed the software installation described in Platform Software Installation , the database files belonging to Derby are placed in a directory called mzdb under MZ_HOME. Note! Since the Derby database runs within the same process as the Platform, the Platform requires more memory than when using Oracle.

---

# Document 2237: IPDR SP Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000766/IPDR+SP+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The IPDR SP agent expects and produces IPDR UDRs of type: Data TemplateData SAMIS SAMIS_TYPE_1 SAMIS_TYPE_2 See IPDR SP UDRs for further information. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes IPDR SP agent MIM values are formatted into a map of <IPaddress;port;session, long/string> . As the IPDR SP agent may contain multiple connection details at any time, the MIM values will have each connection mapped into it and each will have their own specific MIM value. An example for an IPDR SP agent with 3 connection details would be: Throughput Per Second: {10.60.10.136;4741;0, 2000, 10.60.10.136;4740;0, 2000, 192.168.56.101;4737;0, 0} MIM Parameter Description Average DATA_ACKNOWLEDGE Response Time This MIM parameter contains information about the average time (in seconds) of the DATA_ACKNOWLEDGE response from the exporter. Average DATA_ACKNOWLEDGE Response Time is of the map<string,float> type and is defined as a global MIM context type. CONNECT Messages Sent This MIM parameter contains the number of CONNECT messages sent. CONNECT Messages Sent is of the map<string,long> type and is defined as a global MIM context type. CONNECT_RESPONSE Messages Received This MIM parameter contains the number of CONNECT_RESPONSE messages received. CONNECT_RESPONSE Messages Received is of the map<string,long> type and is defined as a global MIM context type. Connections Status This MIM parameter contains information about the connection status with the exporter. Connections Status is of the map<string,string> type and is defined as a global MIM context type. DATA Messages Discarded This MIM parameter contains the number of DATA messages discarded. DATA Messages Discarded is of the map<string,long> type and is defined as a global MIM context type. DATA Messages Received This MIM parameter contains the number of DATA messages received. DATA Messages Received is of the map<string,long> type and is defined as a global MIM context type. DATA Messages Received Duplicates This MIM parameter contains the number of DATA messages received duplicates. DATA Messages Received Duplicates is of the map<string,long> type and is defined as a global MIM context type. DATA Messages Received out-of-sequence This MIM parameter contains the number of DATA Messages Received out-of-sequence. DATA Messages Received out-of-sequence is of the map<string,long> type and is defined as a global MIM context type. DATA_ACKNOWLEDGE Messages Sent This MIM parameter contains the number of DATA_ACKNOWLEDGE messages sent. DATA_ACKNOWLEDGE Messages Sent is of the map<string,long> type and is defined as a global MIM context type. DISCONNECT Messages Received This MIM parameter contains the number of DISCONNECT messages received. DISCONNECT Messages Received is of the map<string,long> type and is defined as a global MIM context type. DISCONNECT Messages Sent This MIM parameter contains the number of DISCONNECT messages sent. Average DATA_ACKNOWLEDGE Response Time is of the map<string,long> type and is defined as a global MIM context type. DOCSIS 3.0 SAMIS_TYPE_1 Records Received This MIM parameter contains the number of DOCSIS 3.0 SAMIS_TYPE_1 records received. DOCSIS 3.0 SAMIS_TYPE_1 Records Received is of the map<string,long> type and is defined as a global MIM context type. ERROR Messages Received This MIM parameter contains the number of ERROR messages received. ERROR Messages Received is of the map<string,long> type and is defined as a global MIM context type. ERROR Messages Sent This MIM parameter contains the number of ERROR messages sent. ERROR Messages Sent is of the map<string,long> type and is defined as a global MIM context type. FINAL_TEMPLATE_DATA_ACK Messages Sent This MIM parameter contains the number of FINAL_TEMPLATE_DATA_ACK messages sent. FINAL_TEMPLATE_DATA_ACK Messages Sent is of the map<string,long> type and is defined as a global MIM context type. FLOW_START Messages Sent This MIM parameter contains the number of FLOW_START messages sent. FLOW_START Messages Sent is of the map<string,long> type and is defined as a global MIM context type. FLOW_STOP Messages Sent This MIM parameter contains the number of FLOW_STOP messages sent. FLOW_STOP Messages Sent is of the map<string,long> type and is defined as a global MIM context type. GET_SESSIONS Messages Sent This MIM parameter contains the number of GET_SESSIONS messages sent. GET_SESSIONS Messages Sent is of the map<string,long> type and is defined as a global MIM context type. GET_SESSIONS_RESPONSE Messages Received This MIM parameter contains the number of GET_SESSIONS_RESPONSE messages received. GET_SESSIONS_RESPONSE Messages Received is of the map<string,long> type and is defined as a global MIM context type. KEEP_ALIVE Messages Received This MIM parameter contains the number of KEEP_ALIVE messages received. KEEP_ALIVE Messages Received is of the map<string,long> type and is defined as a global MIM context type. KEEP_ALIVE Messages Sent This MIM parameter contains the number of KEEP_ALIVE messages sent. KEEP_ALIVE Messages Sent is of the map<string,long> type and is defined as a global MIM context type. Last DSN Acked This MIM parameter contains information about the last DSN acked. Last DSN Acked is of the map<string,long> type and is defined as a global MIM context type. Last DSN Received This MIM parameter contains information about the last DSN received. Last DSN Received is of the map<string,long> type and is defined as a global MIM context type. MODIFY_TEMPLATE Messages Sent This MIM parameter contains the number of MODIFY_TEMPLATE messages sent. MODIFY_TEMPLATE Messages Sent is of the map<string,long> type and is defined as a global MIM context type. MODIFY_TEMPLATE_RESPONSE Messages Received This MIM parameter contains the number of MODIFY_TEMPLATE_RESPONSE messages received. MODIFY_TEMPLATE_RESPONSE Messages Received is of the map<string,long> type and is defined as a global MIM context type. SESSION_START Messages Received This MIM parameter contains the number of SESSION_START messages received. SESSION_START Messages Received is of the map<string,long> type and is defined as a global MIM context type. SESSION_STOP Messages Received This MIM parameter contains the number of SESSION_STOP messages received. SESSION_STOP Messages Received is of the map<string,long> type and is defined as a global MIM context type. TEMPLATE_DATA Messages Received This MIM parameter contains the number of TEMPLATE_DATA messages received. TEMPLATE_DATA Messages Received is of the map<string,long> type and is defined as a global MIM context type. Throughput Per Second This MIM parameter contains information about the Throughput per second of the agent Throughput Per Second is of the map<string,long> type and is defined as a global MIM context type. Accesses The agent does not itself access any MIM resources.

---

# Document 2238: Log Files - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031026
**Categories:** chunks_index.json

The system can produce the following types of log files during run-time: Pico logs: Logs generated from each Pico Debug logs: Debug logs are generated when workflows have turned on logging to file. See Log and Notification Functions . APL logs: When some log commands are used in APL they are logged to a file, see Log and Notification Functions . See Log Properties for more information. To view the Log Files, go to Manage  Tools & Monitoring and then select Log Files . Open Log Files page Filter Dialog Before anything is visible in the tables a selection needs to be made in the filter dialog. Click on the Get Started button to see the Filter dialog. Open The Pico Log File Filter Field Description Field Description Pico* This dropdown lists all available Picos/ECs from which logs can be retrieved. You must select at least one Pico to view logs, as this field cannot be left blank. It filters capture logs from both platform-level and application-level components, as they all operate within the Pico framework. As a result, applying filters like Platform , EC , or Transport may include logs from related services, such as Derby or Execution Contexts, due to their interaction with the core platform. Note! For each selected Pico, a request will be sent to that Pico to list all files in a specific folder. If you select Picos in bulk, the response time might be slow depending on the network of each Pico. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 File Name Use this field to filter by file name. If you enter multiple file names or parts of names, the filter will show logs that match any of the values you enter. Each Pico lists log files from specific directories based on the following properties: pico.stderr points to the directory containing standard error logs. pico.stdout points to the directory containing standard output logs. log4j.configurationFile specifies the log configuration file. If not defined, it defaults to $MZ_HOME/etc/log4j2.xml , which manages both regular and history log directories. If multiple Picos point to the same log directories, identical files may appear across different Picos. The UI automatically filters duplicates, showing only one file with the same name and timestamp. Debug Log File Tab On each EC it will list files from the debug folder decided by the parameter mz.wf.debugdir . If this parameter is not set it will list files from the debug folder inside the temporary folder decided by the parameter pico.tmpdir . Open The Debug Log File Filter Field Description Field Description EC This dropdown lists all available Execution Contexts (ECs) from which debug logs can be retrieved. You must select at least one EC to view logs, as this field cannot be left blank. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 APL Log File Tab On each EC it will read the log4j configuration described here, log4j APL Logging Configurations , to list the files in the directory specified by log4j. Open The APL Log File Filter Field Description Field Description EC* This dropdown lists all available Execution Contexts (ECs) from which APL logs can be retrieved. You must select at least one EC to view logs, as this field cannot be left blank. Date Range Define a range of dates to filter log entries. If no date range is defined, all entries are displayed. The default date range is set to Today . A few predefined options are available: User-defined All Last hour Today Yesterday This week This month If you select User-defined , enter the Start date and time and End date and time . Hide Empty files Select this to remove files with a size of 0 Log Files Table All three tabs have the same table layout. Open Pico Log Files table with 1 filter applied Table Item Description Table Item Description Filter Button Displays the filter dialog Refresh Button Updates the file list Name Column The name of the log file Last Modified Column The date and time when the file was last modified Pico/EC Column The Pico/EC file is present Size Column Size of the log file Actions Opens the Actions menubar with the option Download and Pre view. Open Preview You can view the first 10 Mb of a file, by clicking Preview in the Actions column. You can also download the log file from the Preview dialog. Open Preview log file dialogue

---

# Document 2239: Diameter Stack Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032612/Diameter+Stack+Agent
**Categories:** chunks_index.json

By including the Diameter Stack agent in a workflow you enable MediationZone to act as a Diameter server for any application that follows the Diameter Base Protocol (RFC 6733). For further information about the agent's operation, see the section, Diameter S tac k in The Diameter Base Protocol . The section contains the following subsections: Diameter Stack Agent Configuration Diameter Stack Agent Events Diameter Stack Agent Input/Output Data and MIM

---

# Document 2240: FTPS Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685648/FTPS+Forwarding+Agent+Configuration
**Categories:** chunks_index.json

To open the FTPS forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select the Forwarding tab in the Agent Selection dialog. Select the Ftps agent. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. Connection Tab Open The FTPS forwarding agent configuration - Connection tab For information of the settings in this tab, see the Connection Tab in section FTPS Collection Agent Configuration . Target Tab Open The FTPS forwarding agent configuration - Target tab Setting Description Setting Description Input Type The agent can act on Input Type bytearray or MultiForwardingUDR . By default, the agent expects bytarrays. If nothing else is stated, the documentation refers to bytearrays. For Input Type MultForwardingUDR , refer to FTPS Forwarding Agent MultiForwardingUDR Input for more information. Directory Enter the absolute path of the remote host's target directory of where you want to place the forwarded files. The path can also be specified relative to the home directory of the user's account. The files are temporarily stored in the automatically created subdirectory DR_TMP_DIR in the target directory. When an endBatch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this checkbox to create the directory, or the directory structure, of the path that you specify in the field Directory . Note! The directories are created when the workflow is executed. Compression Select the compression type to determine if the agent should compress the target files before storage or not: No Compression - The agent does not compress the files. Gzip - The agent compresses the files using gzip. Note! No extra extension is appended to the target filenames even if compression is selected. Produce Empty Files Select this checkbox if you need to create empty files. Handling of Already Existing File Select the behavior of the agent when the file already exists: Overwrite - The old file is overwritten and a warning is logged in the System Log. Add Suffix - If the file already exists, the suffix ".1" is added. If this file also exists, the suffix ".2" is added instead, and so on. Abort - Default. This is the option used for workflows from upgraded configurations / in an upgraded system. Use Temporary Directory If this option is selected, the agent moves the file to a temporary directory before moving it to the target directory. After the whole file has been transferred to the target directory, and the endBatch message has been received, the temporary file is removed from the temporary directory. Use Temporary File If there is no write access to the target directory and, hence, a temporary directory cannot be created, the agent can move the file to a temporary file that is stored directly in the target directory. After the whole file has been transferred, and the endBatch message has been received, the temporary file is renamed. The temporary filename is unique for every execution of the workflow. It consists of a workflow and agent ID, as well as a file number. Abort Handling Select how to handle the file in case of cancelBatch or rollback. Choose between Delete Temporary File or Leave Temporary File . Note! When a workflow aborts, the file is not removed until the next time the workflow is run. Advanced Tab Open The FTPS forwarding agent configuration - Advanced tab Setting Description Setting Description Command Port Enter the port number that the FTPS service should use to connect to the remote host. Timeout(s) Enter the maximum time, in seconds, to wait for a response from the server. Zero (0) means wait forever. Passive Mode (PASV) This checkbox must be selected if FTPS passive mode is used for data connection. In passive mode, the channel for data transfer between client and server is initiated by the client instead of by the server. This is useful when a firewall is situated between the client and the server. Additional Hosts Select Add to enter additional hosts or IP addresses to access the target directory for file storage. If the agent fails to connect to the remote Host specified in the Connection tab, it tries to connect to the hosts listed here, in sequence from top to bottom. Use the Add , Edit , Remove , U p , and Down buttons to configure the host list. Note! Go to the Filename Template tab to configure what to name the created files. Go to the Thread Buffer tab to configure how to use private threads for an agent, to enable multi-threading within a workflow. For further information, see Workflow Template . Backlog Tab The Backlog tab contains configurations related to backlog functionality. Open The FTPS forwarding agent configuration - Backlog tab Setting Description Setting Description Enable Backlog Select this checkbox to enable backlog functionality. If Enable Backlog is cleared, files are moved directly to their final destination when an end batch message is received. If Enable Backlog is selected, files are first moved to a directory called DR_POSTPONED_MOVE_DIR and then to their final destination. Refer to the section Retrieves, in FTPS Forwarding Agent Transaction Behavior for more information about transaction behavior. When a backlog is initialized and when backlogged files are transferred, a note is registered in the System Log. Directory Enter the base directory into which the agent should create subdirectories to handle backlogged files. Absolute or relative path names can be used. Type Select Files to be able to enter the maximum number of files allowed in the backlog folder into the Size field. Selet Bytes to be able to enter the total sum (size) of the files that reside in the backlog folder. If a limit is exceeded the workflow aborts. Size Enter the maximum number of files or bytes that the backlog folder can contain. Processing Order Determine the order by which the backlogged data will be processed once connection is reestablished, select between First In First Out (FIFO) or Last In First Out (LIFO). Duplicate File Handling Select Abort or Overwrite to specify what should happen if a file with the same file name as the one being transferred is detected. This action will apply both when a file is transferred to the target directory and to the backlog.

---

# Document 2241: Excel Encoder Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685419/Excel+Encoder+Agent
**Categories:** chunks_index.json

The Excel Encoder agent receives the UDRs, converts them into a bytearray format to be translated into an Excel file and routes it forward in the workflow. This section contains the following subsections: Excel Encoder Agent Input/Output Data and MIM Excel Encoder Agent Configuration Excel Encoder Agent Transaction Behavior Excel Encoder Agent Events

---

# Document 2242: Duplicate Batch Agent Meta Information Model and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640948/Duplicate+Batch+Agent+Meta+Information+Model+and+Events
**Categories:** chunks_index.json



---
**End of Part 95** - Continue to next part for more content.
