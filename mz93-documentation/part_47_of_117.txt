# RATANON/MZ93-DOCUMENTATION - Part 47/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 47 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.1 KB
---

You open the ADLS2 file forwarding agent configuration dialog from a workflow configuration. To open the ADLS2 file forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Batch . Click Add agent and select ADLS2 from the Forwarding tab of the Agent Selection dialog. To open the ADLS2 file forwarding agent configuration dialog from a workflow configuration, you can do the following: double-click the agent icon, or, select the agent icon and click on the button ADLS2 File Tab The ADLS2 File tab contains the General and Advanced tab. General Tab Open ADLS2 file forwarding agent configuration dialog - General tab Setting Description Setting Description Profile Select the Azure profile you want the agent to use, see Azure Profile for further information about this profile. Container Name Enter the name of the container where the files will be collected from. The container name can be found in the Containers section of the storage account that has been configured in the Azure Pr ofil e. Example An example container name. Open Input Type The agent can act on two input types. Depending on which one the agent is configured to work with, the behavior will differ. The default input type is bytearray, that is the agent expects bytearrays. If nothing else is stated the documentation refer to input of bytearray. If the input type is MultForwardingUDR , the behavior is different. For further information about the agent's behavior in MultiForwardingUDR input, see ADLS2 File Forwarding MultiForwardingUDR Input . Directory Absolute pathname of the target directory on the location stated in the referenced File System profile, where the forwarded files will be stored. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR , in the target directory. When an End Batch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this setting to create the directory, or the directory structure, of the path that you specify in the Directory field. Note! The directories are created when the workflow is executed. Compression Compression type of the target files. Determines if the agent will compress the files or not. - No Compression - agent does not compress the files. Default setting. - Gzip - agent compresses the files using gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. The configuration of the filenames is managed in the Filename Template tab only. Command If a UNIX command is supplied, it will be executed on each successfully closed temporary file, using the parameter values declared in the Arguments field. Note! At this point the temporary file is created and closed, however the final filename has not yet been created. The entered command has to exist in the MediationZone execution environment, either including an absolute path, or to be found in the PATH for the execution environment. Arguments This field is optional. Each entered parameter value has to be separated from the preceding value with a space. The temporary filename is inserted as the second last parameter, and the final filename is inserted as the last parameter, automatically. This means that if, for instance, no parameter is given in the field, the arguments will be as follows: $1=<temporary_filename> $2=<final_filename> If three parameters are given in the field Arguments, the arguments are set as: $1=<parameter_value_#1> $2=<parameter_value_#2> $3=<parameter_value_#3> $4=<temporary_filename> $5=<final_filename> Produce Empty Files If enabled, files will be produced although containing no data. Advanced Tab Open ADLS2 file forwarding agent configuration dialog - Advanced tab Setting Description Setting Description Block Size (MB) The size of each block to be staged. It determines the number of requests that are required. Note! Entering a large block size will ause the upload to make fewer network calls. While each call will send more data, this will cause the call to take longer. Max Concurrency The maximum number of parallel requests to be provided at any given time. The value entered will apply to each API. Max Single Upload Size (MB) The maximum size to upload a single batch of files. If the size of the data entered is less than or equal to this value, the data will be uploaded in a single batch instead of several batches. If the data is uploaded as a single batch, the Block Size will be ignored. Filename Template Tab For a detailed description of the Filename Template tab, see Filename Template Tab in Workflow Template .

---

# Document 1069: Data Aggregation Reports - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783923/Data+Aggregation+Reports
**Categories:** chunks_index.json

Customized data aggregation reports are run either as scheduled or on demand. The kept and unkept records gives an overview of how many UDRs currently exist in aggregation storage and how many UDRs that have been consolidated. The balance should only be zero when no more files have been processed and all aggregated data have been timed out.

---

# Document 1070: Multithreading - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997440/Multithreading
**Categories:** chunks_index.json

Multithreading enables a workflow to operate on more than one UDR at a time. By default, while a batch workflow handles one active thread at a time, a real-time workflow always executes multithreaded. A workflow that is configured to multithreading can only handle data of the UDR type amongst agents that are configured with a thread buffer. Otherwise, during the same period of time, other data types can be processed anywhere else within the workflow. By using asynchronous agents in a workflow that is configured with multithreading, you increase the workflow multithreading capabilities even further. Threads in a Real-Time Workflow In real-time workflows, the collecting agent continuously stores UDRs in a buffer at the beginning of a workflow. UDRs are processed concurrently, and the processing order cannot be guaranteed. This way, an agent might handle as many UDRs as the number of configured threads, simultaneously. Open Real-time workflow multithreading Note! Agents that route bytearray data in a real-time workflow do not use a buffer. Threads in a Batch Workflow To apply multithreading in a batch workflow, a UDR storage buffer has to be configured ahead of an agent with Thread Buffer support. A delivering thread stores a UDR in the buffer and then fetches the next UDR in turn. A processing thread pulls a UDR from the buffer, forwards it to the agent, and then pulls the next UDR in turn. This way, when you add another buffer to the next agent, you also add another thread to the workflow. Batch workflow multithreading - A buffer adds a thread To configure a batch workflow agent with multithreading, use the Thread Buffer tab of the agent configuration. See Thread Buffer Tab in Workflow Template . Thread Buffers The order of processing UDRs is not guaranteed if thread buffers are used when sharing MIM variables between agents!

---

# Document 1071: REST Server_Deprecated Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609014/REST+Server_Deprecated+Agent+Events
**Categories:** chunks_index.json

Agent Events There are no message events for this agent. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Request processed and routed. drRestServerContextID: <number> This message is displayed when the REST Server_Deprecated agent receives a request from the REST client. drRestServerContextID is used to determine the corresponding response by the REST Server_Deprecated agent as well as an identifier for the Cycle(REST) UDR that is being processed in the APL. Response sent. DrRestServerContextID: <number> This message is displayed when the REST Server_Deprecated agent sends out the response to the REST client. DrRestServerContextID is used to identify the preceding REST request that was received by the REST Server_Deprecated agent.

---

# Document 1072: FTP DX200 Agent Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652706
**Categories:** chunks_index.json

Prior to configuring a DX200 agent to use SFTP, consider the following preparation notes: Server Identification Attributes Authentication Server Keys Server Identification The DX200 agent uses a file with known host keys to validate the server identity during connection setup. The location and naming of this file is managed through the property: mz.ssh.known_hosts_file It is set in <pico name>.conf file of the relevant EC to manage where the file is saved. The default value is ${mz.home}/etc/ssh/known_hosts . The SSH implementation uses JCE (Java Cryptography Extension), which means that there may be limitations on key sizes for your Java distribution. This is usually not a problem. However, there may be some cases where the unlimited strength cryptography policy is needed. For instance, if the host RSA keys are larger than 2048 bits (depending on the SSH server configuration). This may require that you update the Java Platform that runs the EC. For unlimited strength cryptography on the Oracle JRE, download the JCE Unlimited Strength Juris- diction Policy Files from http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html . Replace the jar files in $JAVA_HOME/jre/lib/security with the files in this package. The OpenJDK JRE does not require special handling of the JCE policy files for unlimited strength cryptography. Attributes DX200 agent support the following SFTP algorithms: blowfish-cbc, cast128-cbc, twofish192-cbc, twofish256-cbc, twofish128-cbc, aes128-cbc, aes256-cbc, aes192-cbc, 3des-cbc. Authentication The DX200 agent support authentication through either username/password or private key. Private keys can optionally be protected by a Key password. Most commonly used private key files, can be imported into . Typical command line syntax (most systems): ssh-keygen -t <keyType> -f <directoryPath> Argument Description Argument Description keyType The type of key to be generated. Both RSA and DSA key types are supported. directoryPath The directory in which you want to save the generated keys. Example The private key may be created using the following command line: > ssh-keygen -t rsa -f /tmp/keystore Enter passphrase: xxxxxx Enter same passphrase again: xxxxxx Then the following is stated: Your identification key has been saved in /tmp/keystore Your public key has been saved in /tmp/keystore.pub When the keys are created the private key may be imported to the DX200 agent: Open Finally, on the SFTP server host, append /tmp/keystore.pub to $HOME/.ssh/authorized_keys . If the $HOME/.ssh/authorized_keys is not there it must be created. Server Keys The SSH protocol uses host verification as protection against attacks where an attacker manages to reroute the TCP connection from the correct server to another machine. Since the password is sent directly over the encrypted connection, it is critical for security that an incorrect public key is not accepted by the client. The agent uses a file with the known hosts and keys. It will accept the key supplied by the server if either of the following is fulfilled: The host is previously unknown. In this case the public key will be registered in the file. The host is known and the public key matches the old data. The host is known however has a new key and the user has been configured to accept the new key. For further information, see the section Advanced Tab in 9.31.3 FTP DX200 Agent Configuration . If the host key changes for some reason, the file will have to be removed (or edited) in order for the new key to be accepted.

---

# Document 1073: Enabling Authorization Server - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816948/Enabling+Authorization+Server
**Categories:** chunks_index.json

Authorization Server is a service located on the platform and the Access Token generated from the server is sent back to the client from the platform. To enable the Authorization Server, refer to the following steps: Examine the authorization-server.properties file and ensure that all the relevant parameters and values are set correctly. The authorization-server.properties file is located at $MZ_HOME/etc/authorization-server/authorization-server.properties . Please note that if you are using file-based as storage, you do not need to manually create the folder as specified in storage.file-based.storage-location , as it will be automatically created during platform startup. See Authorization Server Properties for an example of the parameters. Startup the platform to enable the Authorization Server. $ mzsh startup platform To enable the https for Authorization Server, see HTTP Encryption .

---

# Document 1074: Workflow Bridge Realtime Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002834/Workflow+Bridge+Realtime+Collection+Agent
**Categories:** chunks_index.json

The Workflow Bridge real-time collection agent collects data that has been sent by a Workflow Bridge forwarding agent. The section contains the following subsections: Workflow Bridge Collection Agent Configuration Workflow Bridge Collection Agent Events Workflow Bridge Collection Agent Load Balancing and Collection Host Definition Workflow Bridge Collection Agent Transaction Behavior, Input/Output Data and MIM

---

# Document 1075: System Importer - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998199/System+Importer
**Categories:** chunks_index.json

System Importer enables you to import data to your system from a ZIP file or a remote source like Git or AWS S3. The import contains data about your system, its configurations, and run-time information. System Importer imports data that has been exported by the System Exporter. Note! In Legacy Desktop, you can also import from a directory. The file exported by the System Exporter can contain data from the following categories: Configuration: Workflow configurations, Profiles, Workflow groups, Ultra formats, Event Notifiers and Alarms etc. Run-Time Data: Data that is produced by the system during workflow execution, for example, ECS UDRs, Aggregation Sessions, etc. System: Other customizable parts such as Error Codes, Reprocessing Groups, Configuration Folders, Pico Host, Ultra, User, or Workflow Alarm Value. Workflow Packages: Workflow Packages are compiled versions of configurations, and are created in the Workflow Package mode in the System Exporter. To open the System Importer, go to Manage  Tools & Monitoring and then select System Importer. Open Source Type: Select the source of the export from either: Local : A locally stored export can be selected or dropped in the selected area. Remote: An export stored in a remote source. Currently available for Git and AWS S3. Git is only available in the Desktop interface . Available entries : Contains a tree layout view of the data you can select to import. Logs : Contains a log of the import process Importing from Local Source To import data/configurations from your local machine: Select the appropriate options according to your preferences by clicking the Options button. Click the Select File... button to select the directory where the data to be imported is located. Alternatively, you can also drag and drop the file in the Drop file here area. Open System Importer with log In the Available Entries field, expand the folders and select the check boxes for the entries you want to import. You can also use the Search field to find the appropriate entry to import. Click on the Import button to start the import process. After the import is completed, the Available entries field is cleared of all directories, and the selected file is also removed. You can perform an initial test import to check for any errors. To do this, click the Dry run button. This does not import the entry into Configurations . When the Dry Run is complete, the Available entries field is not cleared off and the selected file is not removed. You can proceed to click the Import button to import data. Open Dry run logs Upon clicking the Import button, it immediately changes into the Abort button, which enables you to cancel the Import process. After clicking abort, the file is removed and you will need to reimport the file to continue the import. If the directory structure of the imported file is not identical to that of the exported material, the import will fail. Update the dynamic configuration data in the collectors with the file sequence numbers that you had noted down before performing the Export. For further information, see the section, To Export Data, in System Exporter . Enable all the workflows that are configured with Scheduling. Before importing Inter Workflow and Aggregation profiles, empty the Workflow data stream. Otherwise, these agent profiles will be overwritten by the profiles that are included in the imported bundle and might not recognize or reprocess data. Imported Workflow groups are disabled by default. You need to activate all the members, their respective sub-members, and the workflow group itself. When you import a User it is disabled by default. A User with Administrator permissions must enable the user and revise which Access groups the user should be assigned to. Imported Alarms are disabled by default. You enable an Alarm from the Alarm Detection. Imported Event Notifications are disabled by default. You enable an Event notification from the Event Notification Configuration. Importing from Remote Source To import data/configurations from a Git branch or AWS S3 bucket: Select the appropriate options according to your preferences by clicking the Options button. Click the Browse button to choose a File System Profile with type Git or AWS S3. Open Choose File System Profile Click the Select Source to choose the folder to import from. The following image shows the import using Git. Open The following image shows the import using Amazon S3. Open Continue from step 4 in Importing from Local Source . Options Click the Options button to open the Options menu. Open The Options menu The Options menu has multiple settings described in the following table: Setting Description Setting Description Import options Abort On Error Select this option to abort the import process if an error occurs. If an error occurs and you do not select this option, the import will be completed, but the imported data might contain erroneous components. Note! Invalid Ultra and APL definitions are considered erroneous and result in aborting the import. Import External Reference Database Values Select this option if you want to import external reference values. Select Dependencies Select this option to have dependencies follow along with the entries that you actively select. Preserve Permissions Select this option to preserve user permissions in the current system when importing a configuration. Clear this option to accept overwriting of user permissions in the current system. Import Configuration From Package Select this option if you would like to import the Workflow Package (MZP) as a configuration instead of a Workflow Package. New Owner Use this drop-down menu to reassign the ownership of configurations to another user, during an import. Workflow groups executions suppress option No Suppress Select this if you do not want to suppress the workflow group execution. Hold Execution Select this option to prevent scheduled Workflow groups from being executed while importing configurations. Restart For information, see systemimport . Stop and Restart For information, see systemimport . Stop Immediately and Restart For information, see systemimport . Wait for Completion and Restart For information, see systemimport . Beside t he Options button, these buttons exist: Button Description Button Description Expand All Select this option to expand all of the folders to display the folders and all the configurations that they contain. Collapse All Select this option to collapse the folders so only the folders are visible.

---

# Document 1076: encryptpassword - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612469/encryptpassword
**Categories:** chunks_index.json

usage: encryptpassword [[<password>] | [-a|-alias <alias> [<password> | -e <encryptedpassword>]]] Encrypts a password and prints out the result. Use this command to create an encryption of a password. When you run encryptpassword in non-interactive mode, special shell characters must be escaped or the password may become truncated. You can use backslash () to escape a special character. Example - Escaping special character $ mzsh encryptpassword example$password You can escape all special characters in a string by surrounding it with single quotes ('). Example - Escaping all characters in a string $ mzsh encryptpassword '`examplepassword!#$&()|";'<> ' If single quote characters are part of the password, these can be escaped with backslash (). Example - Escaping single quote characters $ mzsh encryptpassword '&example#''password Hint! You can use the encrypted password as a password value in an external reference or in a password cell of a workflow table. Options The command accepts the following options: Option Description Option Description [<password>] The password you want to encrypt. [-a|-alias] Use this option to encrypt a password with an alias. Note! In order to use this option, an alias must have been generated with the Java keytool. When this option is not used, the system default key will be used.If you want to use this option, the path and password to the keystore has to be indicated by setting the Platform properties mz.cryptoservice.keystore.path and mz.cryptoservice.keystore.password . The keystore must also contain keys for all the aliases you want to use. For further information about these properties, see 2.6.4 Platform Properties in the System Administrator's Guide . [-e] Use this option to encrypt a password with another alias. Note! The Platform has to be started, and you have to log in to be able to use the -a and -e options. Return Codes Listed below are the different return codes for the encryptpassword command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect. 3 Will be returned if the encryption went wrong.

---

# Document 1077: AMQP Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031787
**Categories:** chunks_index.json

To open the AMQP agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button The Agent Configuration contains the following settings: Open AMQP Agent Configuration Setting Description Setting Description RPC Answer Timeout (ms) Enter the interval in milliseconds you want the agent to wait for an answer before timing out in this field. Auto Acknowledge Select this checkbox if you want the AMQP agent to automatically send back acknowledgments of receiving messages to the broker. Make Initial Subscription Select this checkbox if you want the AMQP agent to make the initial subscription. Queue Enter the name of of the queue you want to subscribe to and receive messages from in this field. Tag Enter the identifier to be used for the channel. Several clients can use the same tag for the same channel. If this field is empty, the server will generate a unique tag. Authenticate Through Workflow Select this checkbox to provide the authentication credentials for broker connections via LoginInfo UDRs instead of the agent configuration. When you select this check box the Username and Password fields will be disabled. Username Enter the username to be used for broker connections. Password Enter the password associated with the username. Virtual Host Enter the name of the virtual host in this field. Broker Addresses Add the brokers you want the AMQP agent to connect to in this section stating host and port for each broker. You may enter one or several brokers. Use TLS Select this checkbox to have the AMQP agent use TLS. Note! The two-way TLS is not supported in AMQP. Security Profile Note! This field is enabled when the Use TLS checkbox is selected. Only the Java Keystore Type, Keystore Path and Keystore Password fields in the Security profile are used by AMQP agent. A new security profile is automatically generated when importing the AMQP agent workflow configuration of an earlier version prior to MediationZone version 9.

---

# Document 1078: Unit Test Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654926
**Categories:** chunks_index.json

In addition to the Python functions described in Functions, Exceptions, Types, and Constants , the following functions are available for Unit Tests. does not include any native assert functionality, this functionality can be found in the common python libraries. Add the Python Module and use in the unit test. Note! To access the Unit Test functions described below from a Python Module you can import the testkit module. This is useful when you write test-related helper functions. Finalizers addFinalizer This function adds a finalizer function to be called at scope exit. def addFinalizer(func, *args, **kwargs) Parameter Description Parameter Description func A function *args Positional arguments to be passed to the function **kwargs Keyword arguments to be passed to the function Returns An object with a remove() method Example - addFinalizer res = createResource() addFinalizer(res.destroy) finalizers A Python context manager that defines an extra finalization scope. There are two pre-defined finalization scopes; one on the module level and one for each test function. These two scopes should be enough for normal use. Example - finalizers with finalizers: res = createResource() addFinalizer(res.destroy) ... # res.destroy() is called at 'with scope' exit. Conditional testing SkipException Tests can be skipped by raising the SkipException exception in the test function blocks, or the initialize function block. class SkipException(message=None) Parameter Type Parameter Type message str Example - SkipException def initialize(): raise SkipException('All tests are skipped') def test(): raise SkipException('This test is skipped') Log functions logSearch Searches for entries in the System Log matching the given criteria. Returns an iterator with all matching entries. def logSearch(fromDate=None, toDate=None, severities=None, areas=None, wfName=None, wfGroupName=None, agentName=None, userName=None, message=None) Parameter Type Parameter Type fromDate drdate toDate drdate severities list[str] areas list[str] wfName str wfGroupName str agentName str userName str message str Example - logSearch def test(): myit = logSearch(fromDate=drdate('2021-01-26 16:00:00.0 UTC'), toDate=drdate('2021-01-30 23:59:59.0 UTC')) for logEntry in myit: print(logEntry) logRemove Removes the log entry with the specified id. def logRemove(id) Parameter Type Parameter Type id str Workflow functions These functions are event driven, meaning that an event is fired when the function is called. This means that if you use wfStart and immediately call wfIsRunning after it might return false as the workflow has yet to start. It might be necessary to loop over some of these functions while waiting for the status to changed. wfAdd Adds a new workflow to an existing workflow template. def wfAdd(wfName, parameters=None) Parameter Type Parameter Type wfName str parameters dict[str, any] Example - wfAdd def test(): # Add a workflow wfAdd('Default.test.workflow_1') or def test(): # Add a workflow and specify parameter values wfAdd('Default.test.workflow_1', {'Time Unit': 'SECONDS', 'Interval': 4}) wfDelete Deletes a workflow. def wfDelete(wfName) Parameter Type Parameter Type wfName str Example - wfDelete def test(): wfDelete('Default.test.workflow_1') wfExists Returns True if the workflow exists. def wfExists(wfName) Parameter Type Parameter Type wfName str Example - wfExists def test(): wfExists('Default.test.workflow_1') wfStart Starts the workflow. If successful, the workflow has been started but may not yet be running. def wfStart(wfName, ec=None) Parameter Type Parameter Type wfName str ec str Example - wfStart def test(): # Starts the workflow on the default EC wfStart('Default.test.workflow_1') or def test(): # Starts the workflow on ec1 wfStart('Default.test.workflow_1', ec='ec1') wfSetDebugMode Sets the workflow debug mode. Debug mode can be set before starting the workflow or after it has become running. def wfSetDebugMode(wfName, on) Parameter Type Parameter Type wfName str on bool Example - wfSetDebugMode def test(): wfSetDebugMode('Default.test.workflow_1', True) wfStart('Default.test.workflow_1') or def test(): wfStart('Default.test.workflow_1') # Wait until wf is running wfSetDebugMode('Default.test.workflow_1', True) wfIsCompleted Returns True if the workflow completed. def wfIsCompleted(wfName) Parameter Type Parameter Type wfName str Example - wfIsCompleted def test(): wfIsCompleted('Default.test.workflow_1') # False wfStart('Default.test.workflow_1') wfIsCompleted('Default.test.workflow_1') # False wfStop('Default.test.workflow_1') # Wait a sec for the workflow to actually stop wfIsCompleted('Default.test.workflow_1') # True wfIsRunning Returns True if the workflow is running. def wfIsRunning(wfName) Parameter Type Parameter Type wfName str Example - wfIsRunning def test(): wfIsRunning('Default.test.workflow_1') # False wfStart('Default.test.workflow_1') # Wait for the workflow to actually start wfIsRunning('Default.test.workflow_1') # True wfStop('Default.test.workflow_1') wfIsRunning('Default.test.workflow_1') # False wfIsAborted Returns True if the workflow aborted. def wfIsAborted(wfName) Parameter Type Parameter Type wfName str wfAbortMessage Returns the workflow abort message, if any. def wfAbortMessage(wfName) Parameter Type Parameter Type wfName str Example - wfAbortMessage def test(): if wfIsAborted('Default.test.workflow_1'): print(wfAbortMessage('Default.test.workflow_1')) else: pass # do something Event functions Below is a general example for all event functions. Our workflow ('Default.test.workflow_1') used in the example below is a simple workflow with a pulse agent connecting to an analysis agent where we debug the input. For information on how to format the filter see: Event Types(3.0) . Example - Event functions def test(): # Create an event buffer for our test workflow eventBuffer = eventBufferCreate(filter=dict(eventName='Debug Event', workflowName='Default.test.workflow_1'), ttl=60) # Make sure we are debugging as the filter is looking for debug events wfSetDebugMode('Default.test.workflow_1', True) # Start the workflow wfStart('Default.test.workflow_1') # Wait until some events have been produced # and then search the buffer events = eventBufferSearch(eventBuffer) for x in events: # Print only the agent message from the event iterator print(x.agentMessage) # Optionally destroy the buffer # it will be destroyed at the end of scope otherwise eventBufferDestroy(eventBuffer) eventBufferCreate Creates an event buffer where events matching filter are stored in memory for later inspection. Returns the event buffer to be used in calls to other event buffer functions. def eventBufferCreate(filter=None, ttl=None, maxSize=None) Parameter Type Parameter Type filter dict[str, any] ttl float maxSize int eventBufferDestroy Destroys the event buffer and releases any resources associated with it. def eventBufferDestroy(buffer) Parameter Type Parameter Type buffer object eventBufferSearch Searches for events in the event buffer matching filter. Returns an iterator with all currently matching events, with most recent event first. def eventBufferSearch(buffer, filter=None) Parameter Type Parameter Type buffer object filter dict[str, any]

---

# Document 1079: Prometheus Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739851/Prometheus+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. For information about the agent message event type, see Agent Event . Debug Events There are no debug events for this agent.

---

# Document 1080: Salesforce Streaming API UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674480/Salesforce+Streaming+API+UDRs
**Categories:** chunks_index.json

The Salesforce Streaming API agent has two UDR types: SalesForceSubscribeUDR SalesForceResponseUDR SalesForceSubscribeUDR This UDR is used to subscribe to updates about specific objects. Field Description replayFrom (long) This field contains the replayId you want to start from. See the Salesforce documentation for available replayFrom values. topic (string) This field contains the topic you want to subscribe to. SalesForceResponseUDR This UDR is used to send the contents from the received updates into the workflow. Field Description response (map<string,any>) This field contains a mapping of the received response, e.g.: "createdDate" 2018-02-22 "replayId" 28 topicName (string) This field contains the name of the topic the response is coming from.

---

# Document 1081: IPDR SP Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685989/IPDR+SP+Agent+Configuration
**Categories:** chunks_index.json

To open the IPDR SP agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Edit button The IPDR SP Agent Configuration consists of the following tabs: 1 General Tab 2 Template Negotiation Tab 3 Advanced Properties Tab General Tab The General tab contains configurations related to the IPDR SP templates and Connection Details. Users have two options to import or add the exporter information as follows: 1 Exporter Information Provider - CSV File 2 Exporter Information Provider - Table Exporter Information Provider - CSV File Open IPDR SP agent configuration - General tab (via CSV File option) Setting Description Setting Description Exporter Information Provider CSV File Select this option to have the IPDR SP Agent import a CSV file containing the Exporter Address, Exporter Port and Session Id. By selecting this option will enable the CSV Directory and CSV File Regexp fields. CSV Format The format of the CSV file will have to be in the order of: <Exporter Address>,<Port>,<Session Id>. Eg. 10.60.10.100,4737,9 CSV Connection Details The Connection Details listed in the table will not be used when this option is selected. CSV Directory This option is enabled if the CSV File option is selected. Enter the location of the CSV file for the agent to import the exporter connection details from. CSV File Regexp This option is enabled if the CSV File option is selected. Enter the regex value or filename of the CSV file for the agent to import the exporter connections details from. File Collection When there is more than one file that matches the regex value, the agent will collect the file with the latest modified file timestamp. Exporter Information Provider - Table Open IPDR SP agent configuration - General tab (via Table option) Setting Description Setting Description Exporter Information Provider Table Select this option to Add , Edit and Remove the following exporter details: Open IPDR SP agent configuration - Add Exporter Address dialog Exporter Address : Enter the IP address or the hostname of the IPDR exporter. Exporter Port : Enter the port of the IPDR exporter. Session Id : Enter the session id for the IPDR exporter. Template Negotiation Tab Open IPDR SP agent configuration - Template Negotiation tab Setting Description Setting Description Enable Template Negotiation Select this option to have the IPDR SP agent enable the template negotiation. By selecting this option will enable the IPDR SP Template Profile field. IPDR SP Template Profile Click Browse to select a predefined IPDR SP Template Profile. The profile contains details about the IPDR Service Definition and Template Fields. For further information, see IPDR SP Template Profile . Advanced Properties Tab The Advanced tab contains additional properties that can be configured for the IPDR SP agent. Open IPDR SP agent configuration - Advanced Properties tab For further information on other properties that you can configure, refer to the Properties field.

---

# Document 1082: MediationZone 9.3.2 Released! - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/blog/2025/04/11/550863478/MediationZone+9.3.2+Released
**Categories:** chunks_index.json

This release contains the following enhancements: New command line interface Automatic rollback will be triggered when upgrading fails. A rollback also can be manually triggered and a number of bug fixes for: Desktop Devkit MZSH Documentation Installation FTP SCP SFTP SAP RFC SAP CTS+ SQL System Log Workflow UI Events Realtime batch Execution Manager Configuration Browser Conditional Trace Azure Kafka Diameter Liquibase Audit Conditional Trace Pico Parquet JSON Decoder Ultra among others. See Bug Fixes for more details. Information about where the release can be accessed is available here: Release Information . Overall user documentation is available at: MediationZone Documentation 9.3 . Enjoy!

---

# Document 1083: Execution Container Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029670
**Categories:** chunks_index.json

Set the Execution Container related properties described below in the install.xml file. The properties are described in the order they appear in the install.xml file. Common Properties Property Description Property Description pico.rcp.platform.host Default value: "" Example value: 192.168.0.190 This property specifies the IP address or the hostname of the Platform to be used by other pico instances such as Execution Contexts, Service Contexts, or Command Line. If a failover occurs and you have entered a hostname as property value, the hostname is retrieved from the DNS enabling reconnection. If you have entered a static IP address as property value, reconnection issues may occur after a failover, if the IP address has changed. pico.rcp.platform.port Default value: 6790 This property specifies the port for connecting the Execution Contexts to the Platform. pico.rcp.server.host Default value: "" This property specifies the IP address or hostname of the pico instances. It is used to determine the interface that the pico instances must bind to and the IP address/hostname to be used by connecting processes. If a failover occurs and you have entered a hostname as property value, the hostname is retrieved from the DNS enabling reconnection. If you have entered a static IP address as property value, reconnection issues may occur after a failover, if the IP address has changed. When the value of this property is left blank, the pico instance binds to all IP addresses of the host. This means that the pico listens for inbound network traffic on all network interfaces, and may attempt to use any local IP address for outbound network traffic. Note! If the host has more than one IP address, this property has to be set with the correct IP address. Make sure to set the property if you use IPv6, or if a high availability environment is configured. For information about high availability, see High Availability . mz.webserver.port Default value: "9000" This property specifies the port for connecting to the Desktop. Config Properties Property Description Property Description mz.name Default value: DR This property specifies the name of the MediationZone deployment. Execution Context Properties - Not Used in Platform Only Installations Property Description Property Description mz.eclist Default value: ec1 This property specifies the EC(s) to be installed. Only applicable if install.types includes ec. To specify multiple ECs, the values should be delimited by comma: ec1,ec2,ec3 ec.backlog.dir Default value: ${mz.home}/tmp This property specifies the directory where ECs can store their backlogged events. If this parameter is removed, the EC's events are not logged. Only applicable if install.types includes ec. ec.webserver.enabled Default value: true This property specifies if the web server of the Execution Contexts should be active. Only applicable if install.types includes ec. ec.webserver.port Default value: 9090 This property specifies the EC web server port. Only applicable if install.types includes ec. Note! If you specify several ECs, you need to modify this value to use separate port numbers for each EC once the installation is completed. Do this by using the mzsh topo set command: mzsh topo set topo://container:<container>/pico:<name>/val:config.properties.ec.webserver.port <new_port_number>

---

# Document 1084: Azure Event Hub Producer Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606578/Azure+Event+Hub+Producer+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data This agent consumes bytearray type. MIM For information about the MIM and a list of the general MIM parameters, see section MIM .

---

# Document 1085: IBM MQ UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653017
**Categories:** chunks_index.json

The IBM MQ UDRs are designed to handle the connection towards the MQ message queues and the messages that are read and written. If the agent is using dynamic initialization, the connection UDRs are used for setting up the connection. The types MQMessage and MQMessageTopic UDR Types are used for handling the messages. APL commands are used for producing outgoing messages and the UDR types used for this are MQQueueManagerInfo , MQQueue and MQMessage . Connection UDRS If the agent is configured with dynamic initialization, a connection UDR is sent to the Analysis agent at startup. The Analysis agent populates the UDR and routes it back to the IBM MQ Collection agent. The content of the connection UDR will then be used to configure the agent. MQConnectionInfo If the connection mode is set to Queues, MQConnectionInfo will be used as the connection UDR. The following fields are included in the MQConnectionInfo UDR: Field Description Field Description ChannelName (string) The name of the MQ channel. Host (string) The host name of the queue manager host. Port (integer) The port for the queue manager. Properties (map <any,any>)(optional) A map of optional properties to be set, for example, user name. QueueManager (string) The name of the queue manager. Queues (list <string>) A list of queues to listen to. MQConnectionInfoTopic If the connection mode is set to Topics, MQConnectionInfoTopic will be used as the connection UDR. The following fields are included in the MQConnectionInfoTopic UDR: Field Description Field Description ChannelName (string) The name of the MQ channel. Host (string) The host name of the queue manager host. Port (integer) The port for the queue manager. Properties (map <any,any>)(optional) A map of optional properties to be set, for example, user name. QueueManager (string) The name of the queue manager. TopicNames (list <string>) A list of topics to subscribe for. MQConnectionInfoDurableTopic If the connection mode is set to Durable Subscriptions, MQConnectionInfoDurableTopic will be used as the connection UDR. The following fields are included in the MQConnectionInfoDurableTopic UDR: Field Description Field Description ChannelName (string) The name of the MQ channel. DurableSubscriptions (list <string>) A list of subscriptions to listen to. Host (string) The host name of the queue manager host. Port (integer) The port for the queue manager. Properties (map <any,any>)(optional) A map of optional properties to be set, for example, user name. QueueManager (string) The name of the queue manager. MQMessage For each message in the MQ message queue, a UDR is created and sent into the workflow. When the IBM MQ agent receives the MQMessage in return it will remove the message from the queue. The following fields are included in the MQMessage UDR: Field Description Field Description CorrelationID (bytearray) This ID can be used for correlating messages that are related in some way or another, e g requests and answers. The length of this field will always be 24, meaning that fillers will be added to IDs that are shorter, and IDs that are longer will be cut off. Id (bytearray) The message id. Message (bytearray) The message. Persistent (boolean) If set to "true", the message will be sent as a persistent message, otherwise the queue default persistence will be used. ReplyToQueue (string) The name of the queue to reply to. ReplyToQueueManager (string) The name of the queue manager to reply to. SourceQueueName (string) The name of the source queue. MQMessageTopic For each topic message, a UDR is created and sent into the workflow. The following fields are included in the MQMessageTopic UDR: Field Description Field Description DataMessage (bytearray) The message id. MQQueue The MQQueue UDR is a reference to an IBM MQ queue when using APL commands. The UDR is created by the mqConnect function and all fields are read-only. The following fields are included in the MQQueue UDR: Field Description Field Description CurrentDepth (integer) The number of messages currently in the queue. ErrorDescription (string) A textual description of an error. IsError (boolean) Returns true if the UDR contains an error message. IsOpen (boolean) Returns true if the connection was successfully opened. MaxDepth (integer) The maximum number of messages allowed in the queue. MqError (string) The error code provided by IBM MQ when a connection attempt fails or in case of an error related to the mqPut or mqClose commands occurs. QueueManager (string) The name of the queue manager. QueueName (string) The name of the queue to connect to. MQQueueManagerInfo The MQQueueManagerInfo UDR type is used by the APL functions when establishing a connection towards a queue on the Queue Manager for outgoing messages. The following fields are included in the MQQueueManagerInfo UDR: Field Description Field Description ChannelName (string) The name of the MQ channel. Host (string) The hostname of the queue manager host. Port (integer) The port for the queue manager. Properties (map<any,any>) A map of optional properties to be set, for example, user name. QueueManager (string) The name of the queue manager.

---

# Document 1086: Duplicate UDR Locking Mechanism - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672841/Duplicate+UDR+Locking+Mechanism
**Categories:** chunks_index.json

Duplicate UDR maintains storage consistency by putting in place a locking mechanism on the Duplicate UDR storage. Access to the Duplicate UDR storage via Desktop will only be allowed by obtaining a Duplicate UDR storage lock. The Duplicate UDR storage lock corresponds with each: Duplicate UDR profile by default when the Enable Separate Storage Per Workflow setting is disabled Duplicate UDR profile and workflow combination when the Enable Separate Storage Per Workflow setting is enabled. Lock Scenarios and Resolution Certain scenarios may lead to storage locks, impacting access via Duplicate UDR Inspector and Duplicate UDR workflows execution. Here is a highlight of scenarios to assist in managing the locks effectively and to prevent impediment to storage access. Profile Delete and Modifications The following are scenarios which would render the information stored in cache useless for future workflow runs: Deleting a Duplicate UDR profile Changing the UDR settings in the General tab of a Duplicate UDR profile Changing the SQL Storage specific Database Profile setting Changing the File Storage specific Directory setting Changing the following Storage settings common to both File Storage and SQL Storage Based On System Arrival Time Based on Latest Time Stamp in Cache Enable Separate Storage Per Workflow When these scenarios occur, users will be prompted with a confirmation to delete all data corresponding to the previous profile configuration. Upon confirmation, a lock will be obtained for the Duplicate UDR profile before proceeding to delete all data in the storage cache. This operation necessitates a lock that will prevent all other operations from accessing the storage until the data deletion process is completed. Duplicate UDR Workflow Execution Running a workflow with Duplicate UDR will lock the Duplicate UDR storage and prevent all other storage write operations until the workflow run is complete. You will not be able to run another workflow with this locked Duplicate UDR storage. While the workflow is being executed, storage read operations such as doing a Search with Duplicate UDR Inspector is still allowed. Note! If the Enable Separate Storage Per Workflow setting is enabled, multiple different workflows can obtain locks for the same Duplicate UDR profile and will be able to execute in parallel. Deleting Operation via Duplicate UDR Inspector Deleting Storage Data via Duplicate UDR Inspector requires obtaining a lock which would prevent all other storage write operations until the delete operation is complete. Like executing a workflow with Duplicate UDR, other storage read operation will be allowed while the delete operation is ongoing. Note! This is applicable only to the Legacy Desktop. Ensure that the Read Only check box is selected unless you need to delete batches from the cache. If not selected, the profile will be locked and workflows using the profile will not be able to write to the cache.

---

# Document 1087: Clearing the Pico Cache - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657625/Clearing+the+Pico+Cache
**Categories:** chunks_index.json

The pico cache may be cleared manually by removing the cache directory. Make sure that no processes that currently uses the cache is running when clearing the cache. To clear the cache in Unix: $ rm -rf $MZ_HOME/pico-cache To clear the cache in Windows: Delete the directory pico-cache in the MediationZone installation directory. The next time pico started processes are running, the cache directory will be recreated and populated.

---

# Document 1088: ASN.1 Formats - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658059/ASN.1+Formats
**Categories:** chunks_index.json

MediationZone includes direct support for ASN.1 format specifications. Any ASN.1 specification can be directly imported into the system and a corresponding BER or PER decoder/encoder is automatically generated. Using this capability, MediationZone can directly support any ASN.1 based specification. Open Ultra Format Editor - ASN.1 format management

---

# Document 1089: Workflow Bridge Examples - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675568/Workflow+Bridge+Examples
**Categories:** chunks_index.json

This section gives two examples of how to set up Workflow Bridge workflows, in a batch to real-time and a real-time to real-time scenario. The examples are simple and intended to be used as a base for further development. The section contains the following subsections: Workflow Bridge Example Batch to Real-Time Scenario with Action UDR Workflow Bridge Example Real-Time to Real-Time Scenario with Load Balancing Workflow Bridge Example with Static or Dynamic Load Balancing

---

# Document 1090: Functions, Exceptions, Types, and Constants - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034122
**Categories:** chunks_index.json

The following functions, exceptions, types, and constants are available as built-in in the Python agents and Python modules. This section includes the following subsections: Exceptions for Python APL and Ultra Field Types for Python Functions for Python Constants for Python

---

# Document 1091: trace - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743924/trace
**Categories:** chunks_index.json

usage: trace <command> This command can be used for starting a Conditional Trace trace if you have the Jaeger exporter running. Return Codes Listed below are the different return codes for the trace command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the syntax is incorrect. 4 Will be returned if errors occur during execution of the command.

---

# Document 1092: GCP Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652761/GCP+Agents
**Categories:** chunks_index.json

This section describes the GCP agents and associated profiles and it contains the following subsections: GCP Profile GCP PubSub Profile GCP PubSub Agents GCP BigQuery Agent GCP Storage Agents

---

# Document 1093: Creating Client Keystore and Certificate - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002550/Creating+Client+Keystore+and+Certificate
**Categories:** chunks_index.json

After generating the key pair for server, the next step is to generate a key pair for the client. Run the following command: $ keytool -genkey -alias client -keyalg RSA -keystore ./Client.jks -storetype PKCS12 alias = name of the key, for example, client keystore = name of the keystore, for example, Client.jks Note! When prompted for first and last name the hostname where the certificate is valid should be entered, e.g. localhost. Other values can be anything. Generate a Certificate Signing Request (CSR) so that we can get client's certificate signed by a CA. keytool -certreq -alias client -keystore Client.jks -file Client.csr Get the certificate signed by our the CA, Test CA in these example. See Setting Up a Certificate Authority for instructions on how to set up a CA. $ openssl x509 -CA caroot.cer -CAkey cakey.pem -CAserial serial.txt -req -in Client.csr -out Client.cer -days 365 Note! CA , CAkey and CAserial are files generated when setting up the CA. Import the Test CA root self signed certificate in client key store as a trusted certificate. $ keytool -import -alias TestCA -file caroot.cer -keystore Client.jks Import client's certificate signed by Test CA in client key store with the same alias name that was used to generate the key pair during genkey. $ keytool -import -alias client -file Client.cer -keystore Client.jks We also need to import server's public key in the client key store, because client is the first one who need to initiate a conversation with server or the service. And it needs to encrypt the request message (some part of it) using sever's public key. Server does not need client's public in its keystore if the Binary Security Token is used, server is going to get the client public key in the SOAP message itself. $ keytool -import -alias server -file Server.cer -keystore Client.jks

---

# Document 1094: ECS Error Codes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641132
**Categories:** chunks_index.json

ECS Error Codes can be specified in the ECS and associated with both UDRs and batches. A reprocessing group can also be assigned an Error Code, so that when an entity is inserted into the ECS it is automatically available for collection by the ECS Collection agent. There are two predefined Error Codes within the system, AGGR_UNMATCHED_UDR and DUPLICATE_UDR , which are automatically set by the Aggregation and Duplicate UDR Detection agents when the corresponding error condition is detected. All other Error Codes are defined by the user. Apart from being accessible in the ECS Inspector, the error codes are also used in the ECS Statistics, see ECS Statistics , and ECS Statistics Event . Note! Several Error Codes can be attached to the same UDR. This affects the ECS Statistics output. For further information, see Error Code Search in ECS Statistics . To create an Error Code, click the Error Codes button in the ECS Inspector. This displays the Error Codes dialog. Open Error Codes dialog Click Add to open the Add Error Codes dialog. This is where assignments of new Error Codes are made. Open Add Error Codes dialog Setting Description Setting Description Error Code Enter the Error Code to be attached to UDRs or batches. Description Enter a description of the error code. RP Group Enter a reprocessing group to assign the Error Code to. You can send optional information to the ECS from an Analysis or Aggregation agent, as long as an Error Code has been defined. To this Error Code, any information may be appended using APL. See the example below. Example - Error case appended using APL An Error Case can be appended using APL code. udrAddError( input, "CALL ID ERROR", "The callId: " + input.callId + ", Calling number: " + input.anum ); In this example the "CALL ID ERROR" is defined in the Error Codes dialog. Note! To clear a UDR's errors, the udrClearErrors function should be used. For further information, see the example Reassigning to a Different Reprocessing Group, in ECS Collection Workflow (UDR) .

---

# Document 1095: Data Masking Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640487/Data+Masking+Agent+Configuration
**Categories:** chunks_index.json

You open the Data Masking processing agent configuration dialog from a workflow configuration. To open the Database collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Batch . Click Add Agent and select Data Masking from the Processing tab of the Agent Selection dialog. Open Data Masking agent configuration The agent can be set to either mask or unmask data, and if you select to unmask data you can select how you want to handle unmatching data; Ignore, Log a Warning in the System Log, or Route an Error to selected agent. The mapping of which UDR fields is done in the referenced Data Masking profiles. You have the option of referencing one or several Data Masking Profiles. Setting Description Setting Description Operation Select if you want the agent to mask or unmask data by selecting the corresponding radio button. On Unmatched Data These settings are activated when you have selected to unmask data. If matching data is missing when unmasking data, you can select how you want the agent to handle that, either just ignore the data, log a warning to the System Log, or Route an error UDR on the selected route. Data Masking Profiles Add the Data Masking profiles you want the agent to use in this section. You can add several profiles. UDR Types Add the UDR types you want the agent to process in this field. If you only want to process the incoming UDR types, simply click on the Set To Input button.

---

# Document 1096: HOCON Format - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647066/HOCON+Format
**Categories:** chunks_index.json

The data format used for the STR files is HOCON (Human Optimised Config Notation) a relaxed variant of JSON, designed for human/manual editing and supporting comments. For all scripting needs, the topo command must be used, it has many different options to query the contents of the topology registry, to return subsets of information and in strict JSON format if desired. For further information about the HOCON format, visit the official GitHub web site.

---

# Document 1097: Duplicate Filter Collection Strategy - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644345/Duplicate+Filter+Collection+Strategy
**Categories:** chunks_index.json

This section includes a description of the Duplicate Filter Collection Strategy that is applied with the Disk Advanced, FTP, SFTP, and SCP agents. The Duplicate Filter Collection Strategy helps you configure a collection agent to collect files from a directory without having the same files collected again. Mechanism When the agent reads the input folder and collects files, the system will insert the last collected files into a list, known as the File List . The amount of collected files to be kept in this File List at any one time is determined by the File List Size setting. This File List will be used to check whether an input file is a duplicated file. When Duplicate Filter Collection Strategy is turned on, the system collects input files by the file's Modification Timestamp in ascending (ASC) order. Since the Duplicate Filter has its own sorting mechanism, it is important that you do not enable Sort Order in the agent when selecting Duplicate Filter Collection Strategy. There are two ways to determine if a file is a duplicated file during collection: Duplicate Criteria - Filename If the file exists in the File List , the agent is not going to collect it. This process is indicated by the flow chart below: Open Duplicate Criteria - Filename Duplication Criteria - Filename and Timestamp If the file exists in File List and it contains a recent Modification Timestamp than it's previous Modification Timestamp, the agent will re-collect the newer file. This process is indicated by the flow chart below: Open Duplicate Criteria - Filename and Timestamp There is a possibility where the input folder will contain more files than the File List . In the situation where the input file doesn't exist in the File List , the system will check the Modification Timestamp of the oldest file in the list. The system collects it only if the Modification Timestamp is older than the oldest file in the File List as the Duplicate Filter always collects files by it's Modification Timestamp ascending(ASC) order. Example - Duplicate Filter Collection Strategy If the File List Size is set to 5 and there is currently 15 files in the input folder. Add to that the file Modification Timestamp is in ascending (ASC) order just as listed in the picture below. When the workflow is executed, the last 5 collected files will be inserted into the File List. When there are new files coming into the input folder, new files should be newer than the oldest file in the File List, in this case file 11.txt. Every time the workflow executes and collects new files, the older files in File List will be removed and the recently collected files will be inserted. Open Example - when input files > File List Size Note! You should make an estimation on how many files will be going into the input folder per second and set the File List Size with a number that is bigger than this estimation. Consider the case were if you have the total input files > File List Size and all the file's Modification Timestamp are identical, there may be duplicate files being collected. Tip! It is always good practice to have good housekeeping and clean up the source folder from which your workflow collects the input files. This will make the Duplicate Filter run faster during workflow execution instead of repeatedly going through a huge file list to check for duplication. Configuration You configure the Duplicate Filter Collection Strategy from the Source tab in the agent configuration dialog. Open The Duplicate Filter configuration dialog Setting Description Setting Description Collection Strategy From the drop-down list select Duplicate Filter. Directory Absolute pathname of the source directory on the remote host, where the source files reside. The pathname might also be given relative to the home directory of the User Name account. Filename Name of the source files on the remote host. Regular expressions according to Java syntax applies. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example - Matching File Names To match all file names beginning with TTFILE , type: TTFILE.* Compression Compression type of the source files. Determines if the agent will decompress the files before passing them on in the workflow. No Compression - agent does not decompress the files. Gzip - agent decompresses the files using gzip. Duplicate Criteria - Filename Select this option to have only the filename compared for the duplicate check. If the filename is in the list of files which have already been collected once, the file is ignored by the agent. Refer to the Mechanism section above for more information. Duplicate Criteria - Filename and Timestamp Select this option to have both the filename and the time stamp of the last modification, compared when checking for duplicates. If the file has already been collected once, it is collected again only if the duplicate check reveals that the file has been updated since the previous collection. Refer to the Mechanism section above for more information. File List Size Enter a value to specify the maximum size of the list of already collected files. This list of files is compared to the input files in order to detect duplicates and prevent them from being collected by the agent. Refer to the Mechanism section above for more information. When this collection strategy is used with multiple server connection strategy, each host has its own duplicate list. If a server is removed from the multiple server configuration the collection strategy will automatically drop the list of duplicates for that host in the next successful collection. Route FileReferenceUDR Select this checkbox to route File Reference UDR instead of raw data.

---

# Document 1098: JMS Collector Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033709/JMS+Collector+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. Debug Events There are no debug events for this agent.

---

# Document 1099: threshold - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742637/threshold
**Categories:** chunks_index.json

The threshold objects contain a set of level objects that define limits for the KPIs. When a limit is exceeded within a defined period, a threshold object and a level object are referenced in the KPIOutput UDRs. You can define the limit values for a threshold object in ascending or descending order. The following JSON schema describes the data format of the threshold object type: Loading The threshold object type has the following properties: Property Description orderDescending orderDescending controls in which order thresholds are evaluated and if alarms should be triggered when levels are exceeded from below or from above. Set this property to true for descending order and false for ascending order. levels levels must contain a set of level sub-objects, which describe the alert levels that are associated with a threshold object. The level object type contains the following properties: Property Description alarmDescription alarmDescription may contain an arbitrary string. value value must contain a numerical threshold value. A level object must have at least one level. Example - JSON Representation "threshold": { "Region.AD": { "orderDescending": true, "levels": { "1": { "alarmDescription": "", "value": 500 }, "2": { "alarmDescription": "", "value": 400 }, "3": { "alarmDescription": "", "value": 300 }, "4": { "alarmDescription": "", "value": 200 }, "5": { "alarmDescription": "", "value": 100 } } } } Break

---

# Document 1100: Legacy Kafka Batch Collector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138581/Legacy+Kafka+Batch+Collector
**Categories:** chunks_index.json



---
**End of Part 47** - Continue to next part for more content.
