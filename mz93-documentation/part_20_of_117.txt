# RATANON/MZ93-DOCUMENTATION - Part 20/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 20 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~54.9 KB
---

This section describes details of the Diameter Base Protocol implementation that you should take into account when working with the Diameter agents. Limitations While the system provides the capability of a Diameter Server and a Diameter Client, it does not provide all the capabilities of a Diameter Agent as defined in the Diameter Base Protocol RFC 6733, chapter 1.2 Terminology. The following limitations apply: The agent can not act as a relay agent. Cache handling during redirect is not supported. DTLS over SCTP is not supported. Transport security (TLS) is negotiated via the Inband-Security AVP in CER/CEA exchange and not prior to the CER/CEA exchange as recommended in RFC 6733. Number of Decoding Threads You can use the Execution Context property mz.workflow.decoderqueue.max_threads to specify the maximum number of threads used by the Diameter Stack agent for decoding messages. Setting a lower value than default (10) may enhance performance if the EC host has a low number of CPU cores and the active workflows are complex. On the other hand, decoding may constitute a bottleneck when performing simple processing on a host machine with a high number of CPU cores. In this case, setting a higher value may provide better performance. Example - Setting maximum number of decoding threads On a specific EC: $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.workflow.decoderqueue.max_threads <number of threads> On cell level: mzsh topo set topo://cell:default/val:common.config.properties.mz.workflow.decoderqueue.max_threads <number of threads> Failed-AVP The AVP Failed-AVP is populated for the following values in the Result-Code AVP: DIAMETER_INVALID_AVP_VALUE 5004 DIAMETER_MISSING_AVP 5005 DIAMETER_AVP_OCCURS_TOO_MANY_TIMES 5009 DIAMETER_UNABLE_TO_COMPLY 5012 DIAMETER_INVALID_AVP_LENGTH 5014 NAPTR Service Field Format The Diameter Stack agent uses NAPTR records in DNS for dynamic peer discovery. It is case insensitive to the service-parms in the NAPTR service fields that are configured in the DNS server. For more information about NAPTR, see RFC 6408. TWCLOSE Property The optional Execution Context property TWCLOSE should be used when connecting to peers that do not send Diameter Watchdog Requests in the REOPEN state. This property enables a timeout timer that is reset for each received message. The specified time value should exceed Watchdog (ms) in the Advanced tab of the Diameter application profile. Example - Setting TWCLOSE property On a specific EC: $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.diameter.watchdog.twclose <timeout ms> On cell level: mzsh topo set topo://cell:default/val:common.config.properties.mz.diameter.watchdog.twclose <timeout ms>

---

# Document 440: SQS Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/299663361/SQS+Agents
**Categories:** chunks_index.json

The Amazon SQS Agents act as consumers (collection agents) and producers (forwarding agents) of messages in Amazon SQS, which is a fully managed message queueing service. Both standard and FIFO queueing are supported. See What is Amazon Simple Queue Service? - Amazon Simple Queue Service for more information about SQS. Subsections This section has the following subsections: Amazon SQS UDRs Amazon SQS Collection Agent Amazon SQS Forwarding Agent

---

# Document 441: wfexport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/298614810
**Categories:** chunks_index.json

usage: wfexport <workflow configuration> <export file> [-csv|-tsv|-ssv] [workflow configuration password] This command creates a file, (CSV, TSV, or SSV), of the data that is stored in the Workflow Table. This file contains a header row that lists the names of the Workflow Table columns. Option/Parameter Description <workflow configuration> Specify the workflow configuration that you want to export. <export file> Specify the name of the export file [-csv|-tsv|-ssv] Specify one of the wfexport supported formats according to which the export file should be created: CSV (Comma Separated Value) - Default format SSV (Semicolon Separated Value) TSV (Tab Separated Value) Note! Text strings within each value are delimited by a quotation mark ("). In the export file, External References are enclosed in braces ({}) preceded by a dollar symbol ($). For example: ${mywf_abcd} . For further information see External Reference Profile . Similarly, Execution Settings in the export file, are enclosed in braces ({}), but are preceded by a pound symbol (#). For example: #{mywf_exsettings} . For further information see Execution Settings below. To prevent a workflow table column from being updated by the export file data when importing, delete that same column from the export file. [workflow configuration password] For an encrypted export file, provide a password. Example - General use of wfexport Create the file wf_disk_collection.csv under the tmp directory. $ wfexport Default.disk_collection /tmp/wf_disk_collection The wf_disk_collection.csv export file: "ID","Name","[Disk_1]Directory","[Disk_1]Filename" 1,"workflow_1","/tmp/in","in.file" 3,"workflow_3","/tmp/in3","in3.file" 100,"workflow_100","/tmp/in100","in100.file" 109,"workflow_110","/tmp/in101","in101.file" 110,"workflow_110","/tmp/in101","in101.file" 111,"workflow_112","/tmp/in112","${a}" 112,"workflow_113","/tmp/in113","a" Execution Settings An export file of a workflow configuration may include settings for an EC. Setting Description Valid Values Setting Description Valid Values type The configuration type. execsettings enabled Specifies whether or not the configuration is enabled true or false disttype The workflows load balancing method. sequential , wfcount , machineload, or roundrobin . For further information see Workflow Properties . ecgroups A vertical bar (|) delimited string of the configured Execution Context groups. The list is enclosed with brackets. Hint! Multiple ecgroups can be specified at once. [] = an empty ecgroups. Example - Export data of EC configurations With an EC configuration, the export file will include: #{type=execsettings#enabled=true #disttype=wfcount#ecgroups=[ec1|ec2]} With a disabled EC configuration, the export file will include: #{type=execsettings#enabled=false} Return Codes The following is a list of return codes for the wfexport command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the number of arguments is incorrect. 1 Will be returned If login credentials are incorrect. 1 Will be returned if configuration permission is denied. 3 Will be returned if the directory does not exist or has no write access. 5 Will be returned if the export file has an incorrect file suffix. 6 Will be returned if the configuration name is incorrect. 7 Will be returned if the configuration does not exist. 9 Will be returned if an encryption passphrase is needed. 10 Will be returned if the user does not have the read permission to access the workflow. 11 Will be returned if the configuration could not be loaded. 12 Will be returned if the export fails (refer to the logs for details).

---

# Document 442: SCP Forwarding Agent MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674690
**Categories:** chunks_index.json

When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the FNT folder. The declaration follows: internal MultiForwardingUDR { // Entire file content bytearray content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is checked, if not a runtime error will occur. When MultiForwardingUDR s are expected configuration options in the Filename Template referring to bytearray input will be ignored. For information about Filename Template see Workflow Template . Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDRs . import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previous in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the user defined directory. The Create Non-Existing Directories check box under the Filename Template tab in the configuration of the forwarding agent must be checked if the directories do not exist.

---

# Document 443: Appendix 2 - Batch and Real-Time Workflow Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606046
**Categories:** chunks_index.json

This appendix contains descriptions for all the agents that are available in MediationZone: ADLS2 File Agents Aggregation Agent Amazon S3 Agents AMQP Agent Analysis Agent APN Agent Archiving Agents Azure Event Hub Agents Categorized Grouping Agent Compression Agents Database Agents Data Masking Agent Data Veracity Decoder Agent Diameter Agents Disk Agents Duplicate Batch Agent Duplicate UDR Agent Email Agent Encoder Agent Encryption Agent Error Correction System Excel Agents Firebase Agent FTAM EWSD Agent FTAM IOG Agent FTP Agents FTP DX200 Agent FTP EWSD Agent FTP NMSC Agent FTPS Agents GCP Agents GTP' Agent GTP' LGU Agents HDFS Agents HTTP Batch Agent HTTPD_Deprecated Agent HTTP/2 Agents IBM MQ Agent Inter Workflow Agents IPDR SP Agent JMS Agents Kafka Agents LDAP Agent Merge Files Agent MQTT Agent MSMQ Agents Netflow Agent Netia FTP Agent Parquet Agents Prometheus Agent Pulse Agent Python Agents Radius Agents REST_Deprecated Agents Salesforce Streaming API Agent SAP CC Agents SAP JCo Uploader Agent SAP RFC Processor Agent SCP Agents SFTP Agents SMPP Agents SNMP Agents SQL Agents SQL Loader Agent SQS Agents Streaming Telemetry Agent Syslog Collection Agent TCP/IP Agents UDP Agent UI Builder Agent Web Service Agents Websocket Agents Workflow Bridge Agents Batch-Based Real-Time Agents

---

# Document 444: Authorization Server Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816959/Authorization+Server+Overview
**Categories:** chunks_index.json

The Authorization Server is hosted in a . It is used for generating access tokens that are required for accessing REST APIs hosted by the HTTP/2 Server agent when the OAuth 2.0 Authentication feature is enabled in the HTTP/2 Server agent . Client applications that wish to obtain access token from the Authorization Server must first be registered via the Authorization Server's Management API. Refer to Management API for information on the registration process. Once the client application has been registered, it can request for access token from the Authorization Server using the client id and secret that are by provided by the Authorization Server during the registration process. Each access token has an expiry time (in seconds) and can only be used within a limited period of time. Upon expiry, the client application will need to request for another token from the Authorization Server. After obtaining the access token, the client application needs to include it in the HTTP Authorization header fields of the REST API Call request to the HTTP/2 Server agent. If the OAuth 2.0 Authentication feature is enabled in the HTTP/2 Server agent, then it will check the validity of the access token to ensure that it is a JSON Web Token (JWT) and in the format as generated by the Authorization Server and has not expired. Finally, the REST API Response will be generated based on the business logic implemented in the workflow that contains the HTTP/2 Server agent. HTTP/2 Server and Authorization Server architecture

---

# Document 445: FTPS Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652738/FTPS+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the FTPS collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select Ftps from the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. Part of the configuration may be done in the Filename Sequence or Sort Order tabs described in Workflow Template . Connection Tab The Connection tab contains configuration data that is relevant to a remote server. Open The FTPS collection agent configuration - Connection tab Setting Description Setting Description Server Information Provider The standard behavior is Single Server. If your system is installed with the Multi Server File functionality, you can configure the FTPS agent to collect from more than one server. For further information, contact your System Administrator. Host Primary host name or IP address of the remote host to be connected. If a connection cannot be established to this host, the Additional Hosts specified in the Advanced tab, are tried. Username Username for an account on the remote host, enabling the FTPS session to login. Password Password related to the Username. Transfer Type Data transfer type to be used during file retrieval. Binary - The agent uses binary transfer type. Default setting. ASCII - The agent uses ASCII transfer type. Encryption Type You can select an encryption type: Implicit - Full encryption of FTP connection from the start of the session. Explicit - Explicitly define any required security/encryption mechanisms. File System Type Type of file system on the remote host. Unix - Remote host using Unix file system. Default setting. Windows NT - Remote host using Windows NT file system. VAX/VMS - Remote host using VAX/VMS file system. Client Authentication If you select this checkbox, you must enter the keystore file location (JKS format) and password to connect to the FTPS Server. The connection fails if the file is invalid or not found, or if the key in the file is not the same as the file stored in the FTPS server. Keystore Location Enter the keystore file location in JKS format. You must populate this field if you have selected the Client Authentication checkbox. Keystore Password Enter the keystore password. You must populate this field if you have selected the Client Authentication checkbox. Reuse SSL/TTL Session Select this checkbox to reuse the SSL/TTL session. Collection Retries Settings Enable Select this checkbox to enable repetitive attempts to connect and start a file transfer. When this option is selected, the agent attempts to connect to the host as many times as is stated in the Max Retries field. If the connection fails, a new attempt is made after the number of seconds entered in the Retry Interval(s) field. Retry Interval(s) Enter the time interval in seconds, between retries. If a connection problem occurs, the time interval before the first attempt to reconnect is the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval(s) field. For the remaining attempts, the time interval is the number seconds entered in this field. Max Retries Enter the maximum number of retries to connect. If more than one connection attempt is made, this number is reset as soon as a file transfer is completed successfully. Note! This number does not include the original connection attempt. Restart Retries Settings Enable Select this checkbox to enable the agent to send a RESTART command if the connection has been broken during a file transfer. The RESTART command contains information about where in the file you want to resume the file transfer. Before selecting this option, ensure that the FTPS server supports the RESTART command. When this option is selected, the agent attempts to re-establish the connection, and resumes the file transfer from the point in the file stated in the RESTART command, as many times as is entered in the Max Restarts field. When a connection has been re-established, a RESTART command is sent after the number of seconds entered in the Retry Restarts Interval(s) field. Note! The Restart Retries settings will not work if you have selected to decompress the files in the Source tab, see he section Source Tab below. Note! RESTART is not always supported for transfer type ASCII. For further information about the RESTART command, see http://www.w3.org/Protocols/rfc959/ . Retry Restarts Interval(s) Enter the time interval, in seconds, you want to wait before initiating a restart in this field. This time interval are applied for all restart retries. If a connection problem occurs, the time interval before the first attempt to send a RESTART command is the time set in the Timeout field in the Advanced tab plus the time set in the Retry Restarts Interval(s) field. For the remaining attempts, the time interval is the number seconds entered in this field. Max Restarts Enter the maximum number of restarts per file you want to allow. In case more than one attempt to send the RESTART command is made, the number of used retries is reset as soon as a file transfer is completed successfully. Source Tab The Source tab contains configurations related to the remote host, source directories, and source files. The following text describes the configuration options available when no custom strategy has been chosen. Open The FTPS collection agent configuration - Source tab Setting Description Setting Description Collection Strategy If there is more than one collection strategy available in the system, a Collection Strategy drop-down list is also visible. For more information, see Appendix 4 - Collection Strategies . Directory Absolute path of the source directory on the remote host, where the source files reside. If the FTPS server is of UNIX type, the path name can also be given relative to the home directory of the User Name account. Include Subfolders Select this checkbox to include the subfolders of the path specified in the Directory setting. Filename Name of the source files on the remote host. Use regular expressions according to Java syntax. See also http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILE , type: TTFILE.* Note! When collecting files from VAX file systems, the names of the source files include both path and filename, which has to be considered when entering the regular expression. Compression Compression type of the source files: No Compression - The agent does not decompress the files before passing them on in the workflow. Gzip - The agent decompresses the files using gzip before passing them on. Move to Temporary Directory Select this checkbox to move the source files to the automatically created subdirectory DR_TMP_DIR in the source directory, before collection. This option supports safe collection when source files repeatedly use the same name. Append Suffix to Filename Enter a suffix that you want to add to the filename prior to collecting it. Important! Before you execute your workflow, make sure that none of the filenames in the collection directory include this suffix. Inactive Source Warning (h) Enter number of hours after which to display a warning message (event) in the System Log and Event Area if no file has been available for collection for the configured number of hours: The source has been idle for more than <n> hours, the last inserted file is <file>. Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR if using Move to Temporary Directory ), to the directory specified in the Destination field, after collection. Note! The Directory has to be located in the same file system as the collected files at the remote host. Also, absolute pathnames must be defined (relative pathnames cannot be used). If a file with the same filename, but with a different content, already exists in the target directory, the workflow will abort. If a file with the same filename, AND the same content, already exists in the target directory, this file will be overwritten and the workflow will not abort. Destination Enter the full path of the remote host directory into which the source files should be moved after the collection. This field is only available if Move to is enabled. Rename Select this checkbox to rename the source file after the collection, and let them remain (or be moved back from the directory DR_TMP_DIR if using Move to Temporary Directory ) in the source directory from which they were collected. Note! When the File System Type for VAX/VMS is selected, the following must be considered. If a file is renamed after collection on a VAX/VMS system, the filename might become too long. In that case the following rules apply: A VAX/VMS filename consists of <filename>.<extension>;<version>, where the maximum number of characters for each variable is: <filename>: 39 characters <extension>: 39 characters <version>: 5 characters If the new filename turns out to be longer than 39 characters, the agent moves part of the filename to the extension. If the total sum of the filename and extension exceeds 78 characters, the last characters are truncated. Example: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_ CHARACTERS.DAT;5 Is converted to: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_. CHARACTERSDAT;5 Note! If a new file is created on the FTPS server, with the same filename as the original file but with different content, the workflow aborts. If a new file with the same filename AND content is created, the file is overwritten. Enter the prefix and/or suffix to append to the beginnings and/or ends of the source filenames, respectively, after the collection. These fields are only available if Move to or Rename is enabled. Warning! If Rename is enabled, the source files are renamed in the current (source or DR_TMP_DIR ) directory. Be sure not to assign a Prefix or Suffix that gives the files new names that match the Filename regular expression, as this will cause the files to be collected over and over again. Search and Replace Select either Move to or Rename to enable Search and Replace . Search : Enter the section of the filename to replace. Replace : Enter the replacement text. Search and Replace operate on entries in a way similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications: Use a regular expression in the Search field to specify the section of the filename that you want to extract. Enter characters and meta characters that define the pattern and content of the replacement text into the Replace field. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Note that the search value divides the filename into two parts by using parentheses. The replace value applies to the second part by using the placeholder $2 . Remove Select this checkbox to remove the source files from the source directory (or from the directory DR_TMP_DIR , if using Move to Temporary Directory ), after the collection. Keep (days) This field is only available if Move to or Rename is enabled. Enter the number of days to keep moved or renamed source files on the remote host after the collection. To delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, which determines when the file can be removed. Ignore Select this checkbox to let the source files remain in the source directory after the collection. This field is not available if Move to Temporary Directory is enabled. Route FileReferenceUDR Select this checkbox to forward the data to an SQL Loader agent. See SQL Loader Agent for further information. Advanced Tab The Advanced tab contains configurations related to the use of the FTPS service. For example, in case the used FTPS server does not return the file listed in a well-defined format the Disable File Detail Parsing option can be useful. For information refer to that section. The FTPS collection agent configuration - Advanced tab Setting Description Setting Description Command Port Enter the port number that the FTPS service should use to connect to the remote host. Timeout (s) Enter the maximum time, in seconds, to wait for a response from the server. Zero (0) means to wait forever. Passive Mode (PASV) This checkbox must be selected if FTPS passive mode is used for data connection. In passive mode, the channel for data transfer between client and server is initiated by the client instead of by the server. This is useful when a firewall is situated between the client and the server. Disable File Detail Parsing Select this checkbox to disable file detail parsing for information received from the FTPS server. This enhances the compatibility with unusual FTPS servers but disables some functionality. If file detail parsing is disabled, file modification timestamps are not available to the collector. The collector cannot distinguish between directories and simple files. For this reason, sub-directories in the input directory must not match the Filename regular expression. The agent assumes that a file named DR_TMP_DIR is a directory because a directory named DR_TMP_DIR is used when Move to Temporary Directory in the Source tab is selected. Therefore, you cannot name a regular file DR_TMP_DIR in the collection directory. Note! When collecting files from a VAX file system, this checkbox must be selected. Additional Hosts Select Add to enter additional hosts or IP addresses from where to collect the source files. If the agent fails to connect to the remote Host specified in the Connection tab, it tries to connect to the hosts listed here, in sequence from top to bottom. Use the Add , Edit , Remove , U p , and Down buttons to configure the host list.

---

# Document 446: Duplicate UDR Agent Transaction Behavior and Input/Output Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685186/Duplicate+UDR+Agent+Transaction+Behavior+and+Input+Output+Data
**Categories:** chunks_index.json

Transaction Behavior Emits This agent does not emit any commands. Retrieves This agent does not retrieve any commands. Input/Output Data The agent produces and consumes UDR types selected from the UDR Type list.

---

# Document 447: Log and Notification Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743789
**Categories:** chunks_index.json

The following functions are used for debugging APL code, or logging user defined messages and events. The following functions for Log and Notification described here are: 1 debug 2 dispatchAlarmValue 3 dispatchEvent 4 dispatchMessage 5 log* 6 log.* 7 mailNotify 8 mailNotifyHtml debug This function prints the supplied argument to the output target specified in the Execution tab of the Workflow Properties dialog. The valid options are File or Event . If File is selected, the debug is saved in the temporary directory specified in the system property pico.tmpdir . The file must be named debug/<workflow name> . Alternatively, the location of the debug file can also be configured by using the mz.wf.debugdir property. Each time a workflow is activated, a new debug information will be generated and overwrite the existing file. If Event is selected, the output is shown in the Workflow Monitor. For further information, see Workflow Monitor . void debug( arg ) Parameter Description Parameter Description arg Argument to write to debug output. Could be any type. Note that printing a UDR type will dump all the field values, which may be a large amount of data. Similarly, the debug output for a table or list type may be very large. There is a special case if arg is a bytearray. In this case, the output string will be the hex dump returned from the baHexDump built-in function. For all other variable types, the output is the direct string conversion, meaning debug ( arg ) is the same as debug ( (string) arg ). Returns Nothing dispatchAlarmValue The function makes it possible to detect alarm situations based on workflow behavior. It dispatches a user defined <value> with a user defined valueName from the workflow. The valueName used must be defined using Alarm Detection. For further information, see Alarm Detection in the Desktop user's guide. void dispatchAlarmValue(string <"valueName">, long value) Parameter Description Parameter Description "valueName" The workflow alarm value name, as defined in the Alarm Detection Editor value Any value to be associated with the name Returns Nothing Example - Using dispatchAlarmValue The following code example displays a situation and syntax useful for the dispatchAlarmValue . consume { if ( timeToPay ) { udrRoute(chargingUdr, "to_billing"); //Enable for 'amount out of range' Alarm Detection dispatchAlarmValue("chargingAmount", chargingUdr.amount); } } dispatchEvent A user can define a customized event type. This is done using an event UDR, optionally extended with user-defined fields. This event UDR can be populated with any information by using APL code, and then be sent, using the dispatchEvent function, to be caught by the Event Notification. The event will have asynchronous event handling, and if there is an error in the notification, or if dispatching is on the way, this will be logged in either the EC log or the Platform log, depending on what the problem is. For further information about Event Notification, see Event Notifications in the Desktop user's guide. void dispatchEvent( UltraEvent eventUDR ) Parameter Description Parameter Description eventUDR The name of the event UDR Returns Nothing dispatchMessage This method is used to produce user defined messages associated to predefined Event Categories. For further information about the Event Notification editor, see Event Notifications in the Desktop user's guide. For instance, an Event Category could be named 'DISASTER', and be configured to send an email to the System Administrator. Then an APL agent could send a detailed description with the dispatchMessage function whenever this error situation is detected. void dispatchMessage ( string string , string <Event Category> ) Parameter Description Parameter Description string Value/message to append to the Event Category <Event Category> Name of a user defined event as declared in the Event Notification Editor. This event must be defined in the Event Notification Editor in order for the APL code to compile. Returns Nothing log* Logs a message string to the System Log of type error, warning or information. The entry will fall under the Workflow category where workflow name will be the name of the current workflow and agent name will be the name of the agent logging the message. void logError ( string message , string parameterName_n , // Optional string|int parameterValue_n , // Optional ... ) void logInformation ( string message , string parameterName_n , // Optional string|int parameterValue_n , // Optional ... ) void logWarning ( string message , string parameterName_n , // Optional string|int parameterValue_n , // Optional ... ) Parameter Description Parameter Description message A main message appearing in the log parameterName_n Name of an optional parameter. If declared, parameterValue_n must be declared as well. parameterValue_n Value of an optional parameter. If declared, parameterName_n must be declared as well. Returns Nothing Example - Using logWarning The following code example logs a warning message, which when displayed in the System Log will look like the following figure: logWarning( "UDR failed validation", "ANUMBER IS ", input.anum, "BNUMBER IS ", input.bnum, "DURATION IS ", input.duration); Open System Log inspection log.* These functions invokes logging with log4j. For information about how to configure the logging, such as to set the log level, see the System Administrator's Guide . void log.fatal (any message, any tag ) //optional void log.error (any message, any tag ) //optional void log.warn (any message, any tag ) //optional void log.info (any message, any tag ) //optional void log.debug (any message, any tag ) //optional void log.trace (any message, any tag ) //optional Parameter Description Parameter Description message A value that will be appear in the message field in the log output This parameter will be ignored if cannot be typecasted to a primitive data type e g string or int. tag Objects(s) that will appear in the tag field in the log output. Returns Nothing Example - Using log.debug consume { log.debug("In consume."); list<int> rcDebug =listCreate(int); int rc=0; listAdd(rcDebug,rc); rc=1; listAdd(rcDebug,rc); log.debug(rc,rcDebug); } mailNotify Sends an email to a configured recipient. In order to operate, the system must have an email remitter and an SMTP mail server defined. For further information on Platform properties, see Platform in the System Administrator's Guide. Warning! If used within the consume block, make sure that conditional expressions guarantees that this function does not get called for each UDR. void mailNotify ( string address , string subject , string message , string sender , //Optional list<string> attachment ) //Optional Parameter Description Parameter Description address Email address to the recipient on the form: "user@xxx.com" subject Text ending up as the email subject message Message string ending up as the body of the email sender Name or address of the sender of the email Optional This field will remain optional only when the attachment field is not populated. Once attachment is populated, the sender field will be a mandatory field. attachment A list that will contain one or many attachments to be sent with the email. Each list entry will be the full directory path of the attachment. The example path could look like: Example "/home/admin/attachments/word.txt" Returns Nothing mailNotifyHtml Sends an email in HTML format to a configured recipient. In order to operate, the system must have an email remitter and an SMTP mail server defined. For further information on Platform properties, see Platform in the System Administrator's Guide. Warning! If used within the consume block, make sure that conditional expressions guarantees that this function does not get called for each UDR. void mailNotifyHtml ( string address , string subject , string message , string sender , //Optional list<string> attachment ) //Optional Parameter Description Parameter Description address Email address to the recipient on the form: " user@xxx.com " subject Text ending up as the email subject message Message string ending up as the body of the email Note Supports standard HTML content. sender Name or address of the sender of the email Optional This field will remain optional only when the attachment field is not populated. Once attachment is populated, the sender field will be a mandatory field. attachment A list that will contain one or many attachments to be sent with the email. Each list entry will be the full directory path of the attachment. Example "/home/admin/attachments/word.txt" Returns Nothing

---

# Document 448: GCP PubSub Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607700
**Categories:** chunks_index.json

This section describes the GCP PubSub agents. These agents are available in real-time workflow configurations. The GCP PubSub stands for Publisher/Subscriber. The Publisher sends while the Subscriber receives events (messages) of the subscribed topics. Each subscription can represent a group of single or multiple users as illustrated below: Messages published before a subscription was created for a topic will not be delivered for that subscription. Each message is typically delivered once and in the order, as it is published based on the configuration. The section contains the following subsections: GCP PubSub Publisher Agent GCP PubSub Subscriber Agent

---

# Document 449: MSMQ Processing Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686284/MSMQ+Processing+Agent+Configuration
**Categories:** chunks_index.json

To open the MSMQ Processing agent configuration dialog from a workflow configuration, you can do the following: right-click the agent icon and select Configuration... select the agent icon and click the Edit button The Agent Configuration dialog contains the following settings: Open MSMQ processing agent configuration dialog The configuration tab contains the settings required to connect to the MSMQ service. Setting Description Setting Description Host The host name or IP address of the MSMQ server. Domain The domain name that the login user belongs to. Username The account used to login to the MSMQ server to send/receive memory. Password The account password used to login to the MSMQ server. Path Specify the name of the queue you want to send/receive message in this field. Example: .PRIVATE$greeting3

---

# Document 450: Workflow Bridge Batch Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002779/Workflow+Bridge+Batch+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events

---

# Document 451: mzcli Profiles - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979358/mzcli+Profiles
**Categories:** chunks_index.json

If you want to use the mzcli on multiple installations of MediationZone using dedicated installation names, you can use the new --profile option. The --profile option calls a configuration file called mzcli_configuration.xml , in which the required details for connecting to each installation have been stored. This file will automatically be generated the first time you run any mzcli command, and it will be placed in the .mzcli folder of USER_HOME directory. If the . mzcli folder does not exist in the USER_HOME directory, it will be created. Note! Ensure you have write permissions on the USER_HOME directory when running the mzcli command, otherwise, the mzcli_configuration.xml file will fail to be created. Once created the file will look like this: Example - configuration file mzcli_configuration.xml <?xml version="1.0" encoding="UTF-8" standalone="no"?> <installationlist> <installation name="mz_dev"> <host>localhost</host> <port>9000</port> <username>mzadmin</username> <schema>http</schema> </installation> <installation name="mz_prod"> <host>1.2.3.4</host> <port>1234</port> <username>example-user</username> <schema>https</schema> </installation> </installationlist> and you will manually have to update the respective <installation> sections for the different installations you want to be able to run the mzcli command on. The mzcli_configuration.xml file can contain multiple installation entries, each defined within an <installation> section. Each <installation> must include the following four parameters: host port username schema ( http or https ) All of these parameters, along with the name attribute, are required. The name attribute cannot be empty or consist only of spaces. This name is used for the --profile option when running mzcli commands. Caution! The file is considered invalid if any mandatory parameters are missing. Avoid manually editing other parts of the XML file, as this may also invalidate it. If you add comments, be cautiousmissing a closing > could make the file invalid. If the file is invalid, mzcli commands will not execute. Note! If the XML file becomes invalid, you can rename it and run an mzcli command. This will generate a new template file, where you can manually re-enter the previous installation details. Once you have a valid configuration file, you can use the --profile option to state which of the installations you want to run the mzcli command for: mzcli --profile <installation name> <command> You can also define a default installation to be used when running the mzcli command without the --profile flag. Default Profile Setting a Default Profile To set a default profile run the following command: java -jar mzcli.jar [mzcli] --set-default-profile <installation name> If the profile was successfully set, you will get a message saying that the Default Profile has been set to the stated installation, and you will see a new attribute in the <installation-list> tag in the configuration file: <installationlist default-installation="my_installation"> The stated installation must be present in the configuration file. Checking the Default Profile If you want to check if an installation has been set as default from the command line, you can run the following command: java -jar mzcli.jar [mzcli] --get-default-profile You will then get a message stating either that there is no default profile set, or the installation that is currently set along with information about schema, host and port. Overriding the Default Profile If a default profile has been set, you can still override it by stating another installation in your mzcli command: mzcli --profile <installation name> <command>

---

# Document 452: HTTP Server Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656903/HTTP+Server+Example
**Categories:** chunks_index.json

To give an example of how a workflow acting as a HTTP server may be designed, suppose there is an HTML login page requiring login name and password. The login parameters are checked against a user database, opening a new HTML page if authentication succeeded. If not, a new login attempt is requested. Open The login page and the resulting message page, in case of failed authentication An example of how such a workflow may be designed is shown below. Each of the Analysis agents will perform a validation, resulting in either the HTTP UDR being sent back with an updated response field to the collector, or it is sent unchanged to a subsequent agent. The HTTPD agent sends a request to the Query agent which checks that the correct page is requested. If the page does not exist, an error message is sent back to the HTTPD agent on the not_found route. If the page exists, the HTTP UDR is routed on to the Welcome or Login agents, depending on if the login page was addressed. The Login agent checks if the user exists in the database. If authentication fails, the HTTP UDR is routed to the Failure agent which displays a new page stating the username/password is incorrect. If authentication succeeds, a new page is opened. Open A workflow acting as an HTTP Server Note! The data sent to the HTTP agent in the example (the content field) is an HTML page. Since UFDL cannot handle the HTML protocol, the field is defined as a string. If, for instance, XML was used instead (which may be handled with UFDL), this would require an Analysis agent, which turned the field into a bytearray and then decoded it (use the APL functions strToBA and udrDecode ). The Format Definition Create an instance of the built-in HTTP format definition. Additional fields may be entered. This is useful mainly for transport of variable values to subsequent agents. In this case, re-usage of the username as parsed from the login page is desired. internal MYHTTPD: extends_class ("com.digitalroute.wfc.http.HttpdUDR") { string username; }; The Analysis Agents The Analysis agents handle all validation and response handling. Query The query agent checks if the addressed page exists. If it does not, an error message is inserted into the response field and the UDR is returned to the collector. consume { if (input.query == "/") { // Show welcome page. udrRoute(input, "index"); return; } if (input.query == "/login") { // Try to log in. udrRoute(input, "form"); return; } // The request is not found here. input.response = "HTTP ERROR: 404, Not Found"; input.responseStatusCode = "404 Not Found"; input.responseType = "text/plain"; udrRoute(input, "not_found"); } Welcome Populates the response field with an HTML page, and send the UDR back to the collector. final string welcome = "<html > <title >Welcome</title > <body bgcolor=#3b7d73 > <font face=Verdana,Arial > <h2 >HTTPD Doc Example</h2 > Login<p ><form action=/login method=post > <table ><tr ><td align=right > Username</td ><td ><input name=username ></td ></tr > <tr ><td align=right >Password</td > <td ><input name=passwd type=password ></td ></tr > <tr ><td >&nbsp;</td > <td ><input name=Login type=submit ></td ></tr> </table></form ></body ></html >"; consume { input.responseType="text/html"; input.response = welcome; udrRoute(input); } Login Verifies that the user is authenticated, by performing a lookup against a database table. table tmp_tab; initialize { tmp_tab = tableCreate("Default.DatabaseProfile","select user_Name, user_PW from users"); } consume { if (input.requestMethod != "POST") { input.response = "Wrong method."; udrRoute(input, "no"); return; } // Find username. string p = input.content; int i = strIndexOf(p, "username="); p = strSubstring(p, i + 9, strLength(p)); i = strIndexOf(p, "&"); string u = strSubstring(p, 0, i); p = strSubstring(p, i + 1, strLength(p)); i = strIndexOf(p, "passwd="); p = strSubstring(p, i + 7, strLength(p)); i = strIndexOf(p, "&"); p = strSubstring(p, 0, i); // Verify the login. table rowFound_u = tableLookup(tmp_tab, "user_Name", "=", u); int hit_u = tableRowCount(rowFound_u); if (hit_u == 1) { if (p == (string)tableGet(rowFound_u,0,1)) { input.username = (string)tableGet(rowFound_u,0,0); udrRoute( input, "yes" ); } else { input.response = "Wrong password."; udrRoute(input, "no"); } } else { input.response = "Wrong username."; udrRoute(input, "no");; } } Success The response field is populated with an HTML page. final string success1 = " <html > <title >Welcome </title > <body bgcolor=#3b7d73 > <font face=Verdana,Arial > <h2 >HTTPD Doc Example </h2 >Welcome "; final string success2 = "! <p > <a href=/ >Back to start page </a > </body > </html >"; consume { input.responseType = "text/html"; input.response = success1 + input.username + success2; udrRoute(input); } Failure The response field is populated with an HTML page. final string failure1 = "<html><title>Welcome</title> <body bgcolor=#3b7d73><font face=Verdana,Arial> <h2>HTTPD Doc Example</h2>Failed to log in; "; final string failure2 = "<p><a href=/>Back to start page</a> </body></html>"; consume { input.responseType = "text/html"; input.response = failure1 + input.response + failure2; udrRoute(input); }

---

# Document 453: SAP CC REST Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/292618241
**Categories:** chunks_index.json

SAP Convergent Charging provides a rating and charging solution for high-volume processing in service industries. It delivers pricing design capabilities, high-performance rating, and convergent balance management. The SAP CC REST agent provides an easy way to integrate MediationZone with SAP Convergent Charging on SAP Cloud (RISE) environments. The SAP CC REST agent is a real-time agent that can send requests to SAP Convergent Chargings charging and loading services on SAP Cloud. The SAP CC REST agent communicates with the workflow by using a dedicated set of UDRs. The SAP CC REST agent accepts either the SAPCCChargingRequest or the SAPCCLoadingRequest as input, for the Charging service or Loading service, respectively. Either the SAPCCChargingRequest or the SAPCCLoadingRequest UDR, containing both the request and the response will be returned as output. For further information about the SAP CC REST UDR types, see SAP CC REST UDRs . Prerequisites The reader of this information should be familiar with: SAP Convergent Charging Concepts SAP Convergent Charging REST APIs ( https://help.sap.com/docs/Convergent_Charging/b7742287c95f4634bba7057f8a0e60f1/7f9888309f394ac9a59f488ac27f6df2.html?locale=en-US ) The section contains the following subsections: SAP CC REST Agent Configuration SAP CC REST Agent Input/Output Data and MIM SAP CC REST UDRs

---

# Document 454: External Version Control User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611134/External+Version+Control+User+s+Guide
**Categories:** chunks_index.json

Search this document: This document describes how to handle configuration artifacts with an external version control system. Chapters The following chapters and sections are included: Overview External Version Control Usage Scenarios External Version Control Formats and Commands for External Version Control Best Practices for External Version Control

---

# Document 455: Variables - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743620/Variables
**Categories:** chunks_index.json

The following built-in variables are specific to the Aggregation agent. Variable Description Variable Description session - Batch and Real-Time A reference to the current session to be used to access variables defined in the Association tab. A session will remain in the database until manually removed. A route to ECS or an alternative route will not remove it. The variable is available in the consume , sessionInit and timeout function blocks. Example - Using session session.duration = input.duration + session.duration; instruction - Real-Time Only An optionally inserted string, belonging to the currently flushed session. The variable is available in the command function block only. Example - Using instruction input.info = instruction;

---

# Document 456: Starting Clusters and Creating Topics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611497/Starting+Clusters+and+Creating+Topics
**Categories:** chunks_index.json

Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to the Spark cluster. Before you can create the topics you must start Zookeeper and Kafka. Prerequisites: Prepare scripts according to Preparing and Creating Scripts for KPI Management Starting Clusters To start a cluster follow the steps: Start Zookeeper and Kafka To start Zookeeper, run the following: bin/zookeeper-server-start.sh config/zookeeper.properties To start Kafka, run: bin/kafka-server-start.sh config/server.properties Create Kafka topics and partitions using the scripts included in the Kafka installation. The names of the topics must correspond to the Spark application configuration. In order for the Spark KPI Application to work, the required number of partitions for each topic must be equal to the setting of the property spark.default.parallelism in the Spark application configuration. Use a replication factor that is greater than one (1) to make sure that data is replicated between Kafka brokers. This decreases the risk of losing data in case of issues with the brokers. This is how to create topics, assuming the current working directory is the Kafka software folder: $ ./bin/kafka-topics.sh --create --topic <input topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <output topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <alarm topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> Example - Creating Kafka Topics ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 Example - Creating Kafka topics, overriding retention settings ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 --config retention.ms=86400000 Run the following command to start Spark: $ start_master_workers.sh ... To submit the app to the Spark cluster . Submit the app: $ submit.sh kpiapp ... You can now confirm the status of the Spark cluster. Open a browser and go to http://<master host>:8080 . Open Spark UI

---

# Document 457: LDAP Agent UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739565/LDAP+Agent+UDRs
**Categories:** chunks_index.json



---
**End of Part 20** - Continue to next part for more content.
