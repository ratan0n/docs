# RATANON/MZ93-DOCUMENTATION - Part 50/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 50 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.8 KB
---

**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612458/Workflow+Functions
**Categories:** chunks_index.json

Use the functions below to control the workflow state from APL or to retrieve values from the workflow table. The following functions for Workflow described here are: 1 abort 2 getSecret 3 cancelBatch 4 dynamicFieldGet 5 hintEndBatch 6 sleep 7 wfStop abort Stops the workflow, and logs a user-defined message to the System Log. void abort ( string message ) Parameter Description Parameter Description message A message (of type error), which is sent to the System Log when the workflow is aborted Returns Nothing Note! The only APL code to be executed after an abort call is the deinitialize function block. getSecret Retrieves a secret from a Secrets Profile of any type. If the storage type is remote, the boolean "useCached" can be used to tell the command to check the local cache before trying the remote storage. string getSecret(string profileName, string alias, boolean useCache) // Optional Parameter Description Parameter Description profileName The name of the Secrets Profile to use alias The alias for the secret to use (the alias needs to be present in the profile) useCache Whether or not to use the local cache Note! If the cache is used and the secret retrieved doesn't work for some reason, to get a new one you need to call the function again with useCache set to false. Also note that if the workflow in question is configured to be stand-alone, this function can only be used in initialize. cancelBatch Emits a Cancel Batch that aborts the processing of the current batch and possibly continues with the next file (depending on the Workflow Configuration). void cancelBatch ( string message , drudr errorUDR ) //Optional Parameter Description Parameter Description message A message (of type error), which is logged to the System Log when the batch is canceled errorUDR It is possible to send an error UDR containing any useful information to the ECS where the batch is sent (if configured to do so). Note that an error UDR may be defined from the Workflow Properties dialog as well. In this case, the APL code overrules the window configuration. This parameter is optional. Returns Nothing dynamicFieldGet Retrieves the stated dynamic field from the workflow table. The returned value can either be a boolean, an integer, or a string. The fields are configured in the Dynamic Fields tab in the Workflow Properties. See Dynamic Fields Tab in the Desktop User's Guide for further information. Note! The APL code will not validate unless the dynamic field(s) has been configured in Workflow Properties. boolean/int/string dynamicFieldGet ( string category, string name ) Parameter Description Parameter Description category The dynamic field's category name The name of the dynamic field Returns The value of the selected dynamic field hintEndBatch Large input files can be split into smaller output files using hintEndBatch . The function will send an End Batch message to the other agents in the workflow, possibly causing a split of the current batch being processed. How each agent acts upon such a request can be found out in the respective user's guide; in the Transaction Behavior section. If the workflow does not contain any agent capable of acting upon the request, the workflow will abort. Note! The hintEndBatch () function is only supported for workflows containing one of the following: Database Collection agent Disk Collection agent FTP Collection agent void hintEndBatch() Parameter Description Parameter Description Returns Nothing Note! A split batch cannot be sent to ECS. Calling cancelBatch after hintEndBatch , will result in workflow abort. However, the original batch can be sent to ECS to make sure to evaluate if the batch will be canceled before it is split. sleep Puts the current thread to sleep a given number of milliseconds before resuming execution. This is mostly used for simulation, for instance, traffic shaping or limiting the number of calls to an external system. void sleep ( long milliseconds ) Parameter Description Parameter Description milliseconds The number of milliseconds the current thread will sleep before resuming execution. Returns Nothing wfStop Stops a running workflow. wfStop produces a single stop signal and does not wait for the workflow to stop. If it succeeds in stopping the workflow, the stop is registered in the System Log. string wfStop ( string wfName , boolean immediate ) Parameter Description Parameter Description wfName The name of the workflow that should be stopped. Note! This command can be used from within the same workflow that should be stopped. immediate True: Stops currently handled batch. False: Runs through currently handled batch and then stops. Note: In a real-time workflow this immediate flag has no significance. Returns A string. If the command is successful the value is null . Otherwise, a text message is returned.

---

# Document 1164: Installation of Couchbase - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670075/Installation+of+Couchbase
**Categories:** chunks_index.json

Couchbase Software Couchbase Profiles You should now create two Couchbase Profiles in MediationZone; one for the static configurations in the Data Repository, and one for the runtime data in the Data Repository. In Desktop, open the Build screen, click on the New Configuration button, and select Couchbase Profile . The New configuration - Couchbase Profile opens. In this dialog you have three tabs; Connectivity , which contains settings for the connections between the buckets in the cluster(s), Management , which contains the user name, password, size, and monitoring settings for the bucket, and Advanced , which contains the advanced properties. Create one profile for the configurations bucket, where: The Bucket Name is config or something similar. If you have selected Couchbase Release 5.x - 7.x , set the Bucket User and User Password as a user who has access to the bucket and their password. If you have selected Couchbase Release 4.x , it is optional to set a Bucket Password . The Cluster Nodes section contains the IP-address/host name for each of the Couchbase nodes in the cluster. The user name and password for the administrator created during Initial Server setup are added in the Management tab. You can select the Monitoring checkbox if you want to use monitoring functions for detecting unresponsive nodes and performing failover. Open Couchbase Profile - Management tab This profile will create a bucket called config (or similar), which will be used for static data in the Data Repository, such as the different configurations. Create another profile for the usage bucket, where: The bucket name is bucket or something similar. If you have selected Couchbase Release 5.x - 7.x , set the Bucket User and User Password as a user who has access to the bucket and their password. If you have selected Couchbase Release 4.x , it is optional to set a Bucket Password . The Cluster Nodes section contains the IP-address/host name for each of the Couchbase nodes in the cluster. The user name and password for the administrator created during Initial Server setup are added in the Management tab. You can select the Monitoring checkbox if you want to use monitoring functions for detecting unresponding nodes and performing failover. This profile will create a usage bucket called bucket (or similar), which will be used for the runtime data in the Data Repository, i.e the actual usage of the subscribers. In each Execution Container, open the pcc.properties file, located in the MZ_HOME/etc directory, and enter the names of your newly created profiles in the following properties: mz.pcc.storage.couchbase.config.profile= mz.pcc.storage.couchbase.buckets.profile= Example - pcc.properties mz.pcc.storage.couchbase.config.profile=MyFolder.config mz.pcc.storage.couchbase.buckets.profile=MyFolder.bucket In the PCC Config Storage Properties group, ensure that the property mz.pcc.config.storage.class is set to com.digitalroute.pcc.storage.config.couchbase.CouchbaseConfigStorage . In the PCC Bucket Storage Properties group, ensure that the property mz.pcc.bucket.storage.class is set to com.digitalroute.pcc.buckets.storage.couchbase.CouchbaseBucketStorage. Save the pcc.properties file. In order for the ECs to be able to locate the pcc.properties file, you must set the Execution Context property mz.pcc.properties . $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.pcc.properties <path> Example - Setting mz.pcc.properties $ mzsh topo set topo://container:exec1/pico:ec1/val:config.properties.mz.pcc.properties '${mz.home}/"etc/pcc.properties"' $ mzsh topo set topo://container:exec1/pico:ec2/val:config.properties.mz.pcc.properties '${mz.home}/"etc/pcc.properties"' Important! It is important that the pcc.properties property file is located in the stated directory in all Execution Containers. Restart the ECs. For further information about the settings in the Couchbase profile, see Couchbase Profile .

---

# Document 1165: Azure Event Hub Consumer Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999257/Azure+Event+Hub+Consumer+Agent
**Categories:** chunks_index.json

The Azure Event Hub Consumer collects events from the topic and partitions set in the agents' configuration. This section contains the following subsections: Azure Event Hub Consumer Agent Configuration Azure Event Hub Consumer Agent Input/Output Data and MIM Azure Event Hub Consumer Agent Events

---

# Document 1166: mzcli - logger - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979554/mzcli+-+logger
**Categories:** chunks_index.json

Usage logger -pico platform Usage: logger -pico name [-reset] Reset logging to default. This will log to any package/class at the default level. Cannot be used in combination with other optional parameters. [-stacktrace ON | OFF ] Turn stacktrace on/off. Default is on. Cannot be used in combination with other optional parameters. [-level OFF | ERROR | WARN | INFO | CONFIG | DEBUG | TRACE | FINEST | ALL ] [-file filename ] Send logging to additional file. If used, the command applies only to the specified filename. [-package package/class,.. ] Only log specific package/class. Default is any package/class. The command applies only to the specified package. With this command you can edit log settings for the picos and update the logging dynamically. Options Option Description Option Description [-reset] Resets the logging to default settings. This option cannot be used in combination with any other option. [-stacktrace] Turns stacktrace on or off. The default value is on and this option cannot be used in combination with any other option. [-level] Determines for which severity levels events should be logged; OFF, ERROR, WARN, INFO, CONFIG, DEBUG, TRACE, FINEST, or ALL. [-file] Will log to an additional file stated with filename. [-package] Will only log events for the stated package or class. Return Codes Listed below are the different return codes for the logger command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if there are missing arguments: Pico not running Pico name missing Unknown failure 2 Will be returned if the level is missing or invalid. 3 Will be returned if the file is missing. 4 Will be returned if the package is missing or invalid. 7 Will be returned if the stacktrace is missing or invalid. 8 Will be returned if reset has been used incorrectly.

---

# Document 1167: Product Data Model - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656232
**Categories:** chunks_index.json

The UDRs in PCC.Products stores product related data. A product controls the behavior of a bucket. See APL - PCC Runtime Support - Buckets for further information about how to use the PCC.Products UDRs with APL and REST HTTP Interface - Products for information on how to access the PCC.Products UDRs from the REST HTTP interface. The main parts of PCC.Products are: Object Description Object Description Product The product holds information about the capacity of the counters, the enforcements and notifications that should be triggered at specific capacity levels, the interval at which the counters should be reset, how many counters to retain at reset, and, if the product stops, fallthrough in the usage recording process. Since a product is used by buckets it is important not to remove products in use. ProductMapping A flexible mapping table used to map from an external representation to a list of products. The external representation can be, for instance, a rating group, or a combination of rating group and time period classification. The mapping should not include any information related to a specific subscriber. SubscriberProfile Subscriber profiles are used for associating different products to different profiles. The subscribers will then have access to the products associated with their subscriber profile. Overview of the Product Data Model The different UDRs within the Product Data Model are connected as follows: Open Overview of the Product Data Model ProductMapping UDR In the ProductMapping UDR you set up the priority for the product mapping, the rating groups, etc that the product mapping should use, and the products that should be applied. Field Description Field Description ID (int) The unique ID of the product mapping table. Priority (int) The priority of the mapping object, a low value indicates a high priority. Determines the order in which the matched mapping objects are returned. Arguments (list<string>) The arguments of a mapping object, e g the rating group. Targets (list<Product>) The different products used by the mapping object. Below is a screenshot of the UDR Assistance displaying the ProductMapping UDR: Open ProductMapping UDR Example - ProductMapping UDR The following values: ID=101 Priority=1 Arguments=[4] Targets=[100,110] ID=102 Priority=3 Arguments=[8] Targets=[100] will give the following setup: The first product mapping, with ID 101, has a higher priority than the second, with ID 102. The first product mapping refers to rating group 4, while the second product mapping refers to rating group 8. The first product mapping references product 100 and 110, while the second product mapping references product 100. Product UDR The Product UDR holds information about the capacity of the counters, the enforcements and notifications that should be triggered at specific capacity levels, the interval at which the counters should be reset, and how many counters to retain at reset. If StopFallthrough is set to yes , counting for this product should prevent counting in buckets connected to lower priority products. Since a product is used by buckets it is important not to remove products in use. Field Description Field Description Id (int) The unique product ID. Name (string) The name of the product. Capacities (list<Capacity>) Defines the capacities for all counter items relevant for this product. A capacity is used to measure threshold values for enforcements and notifications. For unlimited capacity ignore this field. Price (int) Optional price information. Currency (string) Currency of price, e g USD, EUR, etc. Duration (int) When creating a bucket this field can be used to set the value for the bucket stop time. DurationUnit (int) The unit of the duration, for instance day, week, or month. 1 = minute, 2 = hour, 3 = day, 4 = week, 5 = month and 6 =year. ResetType (int) The type of reset, reset from start (1) or reset from date (2). Enforcements (list<Enforcement(PCC. Product)>) Holds a list of enforcement definitions, see the section below, Enforcement UDR. Notifications (list<Notification(PCC.Product)>) Holds a list of notification definitions, see the section below, Notification UDR. ResetType (int) The type of reset; reset from start (1), which means that if the reset interval is set to "Month", for example, then the buckets connected to the product in question will be reset at the start of the month, even if the day and time in the bucket's StartTime is something else, or reset from date (2), which means that the bucket will be reset at StartTime plus ResetInterval . Note! This is just an example configuration which may be changed in the Workflow template. ResetInterval (int) The interval with which this product is reset. Resetting a product involves clearing the bucket associated with this product. If set to 0 the product is never reset. ResetIntervalUnit (int) The unit of the reset interval, for instance day, week, or month. 1 = minute, 2 = hour, 3 = day, 4 = week, 5 = month and 6 =year. RetainedCounters (int) Controls the number of counters that are retained in the bucket counters list at reset. For example, if the product has an hourly reset interval and RetainedCounters is 24 the bucket will hold discrete usage counter statistics for the last 24 hours. Periods (list<Period (PCC.Periods)>) Determines the list of periods during which the product should be active. The periods in the stated list can contain one or several sub periods, which means that you can combine different time settings in one period. See Periods Data Model Buckets for further information. StartTime (date) The start time for the product. StopTime (date) The stop time for the product. StopFallthrough (boolean) Used in the product selection process. When usage is reported, the first step is to determine the products associated with the reported usage. This results in a product priority list. The next step is to iterate the list and report usage in the associated bucket. If StopFallthrough is true on the product then iteration stops with this. StopAtCapacity (boolean) Determines if the counter for a bucket should continue to count or not when 100% capacity for the bucket has been reached. If set to false, the counter will continue counting. If set to true, the counter will stop once 100% capacity for the bucket is reached. Misc (map<string, any>) See Misc Field for more information. Below is a screenshot of the UDR Assistance displaying the Product UDR: Open Product UDR Example - Products UDR The following values: ID=100 Name='GOLD' Capacitites=[101, 102] Duration=7 DurationUnit=3 Enforcements=[605] Notifications=[1,3,5] ResetType=2 ResetInterval=1 ResetIntervalUnit=3 RetainedCounters=10 Periods=[503] StartTime='2011-06-01 00:00:00' StopTime='2011-09-01 00:00:00' StopFallthrough=true StopAtCapacity=true ID=110 Name='SILVER' Capacitites=[101] Duration=24 DurationUnit=2 Enforcements=[605] Notifications=[2,4] ResetType=2 ResetInterval=1 ResetIntervalUnit=4 RetainedCounters=2 Periods=[504] StartTime='2011-01-01 00:00:00' StopTime='2012-01-01 00:00:00' StopFallthrough=true StopAtCapacity=true will give the following setup: The two products have the IDs 100 and 110, which are used by both PCC Rules and PCC Buckets, and have the names Gold and Silver . The Gold product uses capacities 101 and 102, while the Silver product only uses capacity 101. The Gold product will be available for 7 days from the time you start to use it, while the Silver product will be available for 24 hours from the time you start to use it. Both products use the enforcement 605. The Gold product will receive notifications 1, 3 and 5, while the Silver product will receive notifications 2 and 4. The reset and retained counters settings for the Gold product will reset the product once every day, based on date, and discreet usage counter statistics will be kept for 10 days. The reset and retained counters settings for the Silver product will reset the product once every week, based on date, and discreet usage counter statistics will be kept for 2 weeks. The Gold product will be active during the time periods stated in the Periods list 503, while the Silver product will be active during the time periods stated in the Periods list 504. The Gold product will start on the 1st of June and end on the 1st of September, 2011, while the Silver product will start on the 1st of January, 2011 and end on the 1st of January, 2012. For both products, counting will stop once the bucket for the respective product is full. SubscriberProfile UDR In the SubscriberProfile UDR you can configure different subscriber profiles. The profiles associate subscriber types with certain products, and many subscribers can share the same profile. Field Description Field Description ID (int) The unique id of the subscriber profile. Name (string) The name of the subscriber profile. Products (list<Product>) The products that should be associated with the profile. Misc (map<string, any>) See Misc Field for more information. Below is a screenshot of the UDR Assistance displaying the SubscriberProfile UDR: Open SubscriberProfile UDR Example - SubscriberProfile UDR The following values: ID=10 Name='STAR' Products=[100,110,120] ID=20 Name='MEDIUM' Products=[110] will give the following setup: The two subscriber profiles with IDs 10 and 20 has the names Start and Medium . The subscribers belonging to the Start profile will have access to products 100, 110 and 120, while the subscribers belonging to the Medium profile will only have access to product 110. Capacity UDR In the Capacity UDR you define the capacity for counter items. A capacity is used for measuring threshold values for enforcements and notifications. Field Description Field Description ID (int) The unique capacity ID. Name (string) The name of the capacity. Capacity (long) This parameter can be used for specifying the capacity of the counter type. CapacityUnit (int) This parameter can be used for specifying the unit of the capacity; 0 - bytes, 1 - kB, 2 - MB, 3 - GB, 4 - Event, 5 - Hour, 6 - Minute, or 7 - Other. CounterType (byte) This parameter can be used for specifying the type of counter this capacity applies to; 0 - input, 1 - output, or 2 - total. CounterUnit (int) This parameter can be used for specifying the unit of the counter; 0 - bytes, 1 - kB, 2 - MB, 3 - GB, 4 - Event, 5 - Hour, 6 - Minute, or 7 - Other. QuotaDefault (long) This parameter can be used in the APL code for setting a default quota that will be reserved for a request. If this quota is not available, you can configure the minimum amount of quota, specified by the QuotaMinimum field, to be tried instead. QuotaMinimum (long) This parameter can be used in the APL code for specifying the minimum amount of quota that needs to be available to grant a request. If this quota is not available, you can configure the request to be denied. Misc (map<string, any>) See Misc Field for more information. Below is a screenshot of the UDR Assistance displaying the Capacity UDR: Open Capacity UDR Example - Capacity UDR The following values: ID=101 Capacity=400000 CapacityUnit=0 CounterType=2 ID=102 Capacity=100000 CapacityUnit=1 CounterType=1 will give the following setup: The two capacities have the IDs 101 and 102, which are used by the Product UDR. The first capacity refers to the total amount of bytes, while the second capacity refers to the output bytes. The first capacity has a limit of 400000 bytes (unit=0), while the second capacity has a limit of 100000 kB (unit=1). Enforcement UDR The Enforcement UDR sets a threshold for when an enforcement should be triggered, and is used by both the Product UDR and PCC Rules. A Product can use one or several enforcements. When the usage exceeds enforcement threshold, the enforcement will be applied. When a new enforcement is triggered a new PCC rule should be applied. Fi e ld Description Fi e ld Description ID (int) The unique enforcement ID. CounterType (byte) The type of counter this enforcement refers to; input (0), output (1) or total (2). Level (double) The level at which this enforcement is triggered. Can be a percentage or absolute. Levels that are less than, or equal to 1, will be interpreted as a percentage value. Name (string) The name of this enforcement. Misc (map<string, any>) See Misc Field for more information. Below is a screenshot of the UDR Assistance displaying the Enforcement UDR: Open Enforcement UDR Example - Enforcement UDR The following values: ID=605 CounterType=1 Level=0.75 Name='LEVEL_75' ID=607 CounterType=1 Level=0.5 Name='LEVEL_50' will give the following setup: The two enforcements have the IDs 605 and 607, which are used by the Products UDR. Both enforcements refers to output traffic. The first enforcement is triggered at 75 %, while that second enforcement is triggered at 50 %. The name of the first enforcement is LEVEL_75 , while the name of the second enforcement is LEVEL_50 , and these names can be used in attributes in RulesMapping, for example. Notification UDR With the Notification UDR you can configure notifications to be sent out at different usage levels. Each product can use several notifications. Field Description Field Description ID (int) The unique ID for this notification. Name (string) The name of this notification. CounterType (byte) The type of counter this notification refers to; input (0), output (1) or total (2). Level (double) The usage level at which this notification is triggered. Can be a percentage or absolute. Levels that are less than, or equal to 1, will be interpreted as a percentage value. Required (boolean) Indicates whether the notification should be required or not. If this field is set to false , the notification will not be sent unless it has been stated in the Subscriber UDR, see Subscriber UDR in Buckets Data Model . If this field is set to true , the notification will always be sent regardless of what is stated in the Subscriber UDR. Type (string) The type of notification, for instance SMS or email. Address (string) The address the notification should be sent to. Message (string) The message to be sent as part of the notification. Misc (map<string, any>) This field can contain information about product ID or product name, for instance. See Misc Field for more information. Below is a screenshot of the UDR Assistance displaying the Notification UDR: Open Notification UDR Example - Notification UDR The following values: ID=700 Name='FIRST' CounterType=2 Level=0.60 Required=false Type='SMS' Address='10.10.10.700' Message='PCC Notification: 60% reached' ID=702 Name='SECOND' CounterType=2 Level=0.90 Required=true Type='SMS' Address='10.10.10.700' Message='PCC Notification: 90% reached' ID=703 Name='THIRD' CounterType=2 Level=1.00 Required=true Type='SMS' Address='10.10.10.700' Message='PCC Notification: 100% reached' will give the following setup: The three notifications with IDs 700, 702 and 703 have the names First , Second and Third . All three notifications will trigger based on the total number of bytes. The First notification will be triggered at 60 %, the Second at 90%, and the Third at 100 %. The First notification is not required, while the Second and Third> are required. All three notifications will be sent as SMSs. All three notifications will be sent to IP address 10.10.10.700 .

---

# Document 1168: Oracle Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204637873
**Categories:** chunks_index.json

This section describes the preparations necessary when using Oracle as database and includes the following subsections: Extract Database Definition Files for Oracle Oracle Database Creation

---

# Document 1169: Properties for Oracle - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029572
**Categories:** chunks_index.json

When you are using an Oracle database, set the following properties in install.xml : Property Description Property Description install.ora.owner Default value: mzowner This property specifies the Oracle username of the database owner. The user owns all the data definitions in the Oracle instance to be created. Note! The installation fails if the same username is configured for both the database owner, install.ora.owner , and the mz.jdbc.user . If it is necessary to have the same username for both users, then the oracle_user.sql file must be manually updated prior to creation of the database. install.ora.password Default value: mz This property specifies the password of the database owner, defined with the install.ora.owner variable described above. install.ora.host Default value: localhost This property specifies IP address or hostname for the database instance. install.ora.size Default value: small This property specifies the size of the Oracle database instance. Possible values are small, medium, or large. small is aimed for test and demo installations. medium is aimed for small and medium sized corporates. large is aimed for large operators. install.ora.port Default value: 1521 This property specifies the Oracle database port. install.ora.home Default value: /opt/oracle Example value: /opt/oracle/product/12.1.0 This property specifies the Oracle home directory. This value must be identical to the value assigned to the environment variable ORACLE_HOME previously described. install.ora.sid Default value: MZ This property specifies the Oracle SID for the instance to be created. The value must be identical to value assigned to the environment variable ORACLE_SID previously described. install.ora.data Default value: ${install.ora.home}/oradata This property specifies a directory path to be used in other Oracle directory definitions. The variable is used for convenience, when the same directory path is repeated for any of the following definitions. install.ora.data.tab Default value: ${install.ora.data} Example value: ${install.ora.data}/tables This property specifies the directory where the Oracle table space data files are to be created. install.ora.data.idx Default value: ${install.ora.data} Example value: ${install.ora.data}/indexes This property specifies the directory where the Oracle index data files are to be created. install.ora.data.temp Default value: ${install.ora.data} Example value: ${install.ora.data}/temp This property specifies the directory where the Oracle temporary table space data files are to be created. install.ora.data.log Default value: ${install.ora.data} Example value: ${install.ora.data}/logfiles This property specifies the directory where the Oracle log files are to be created. install.ora.data.roll Default value: ${install.ora.data} Example value: ${install.ora.data}/rollback This property specifies the directory where the Oracle rollback data files are to be created. install.ora.data.ctrl Default value: ${install.ora.data} Example value: ${install.ora.data}/control This property specifies the directory where the Oracle control data files are to be created. install.ora.tb.space.tab Default value: ts_mz_tab This property specifies the name of the tablespace to use to create the table in. install.ora.tb.space.idx Default value: ts_mz_idx This property specifies the name of the tablespace to use for the index. If new directories are specified for the Oracle data, these directories must be created by the Oracle user.

---

# Document 1170: KPI Management User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655872/KPI+Management+User+s+Guide
**Categories:** chunks_index.json

Search this document: Prerequisites The reader of this document should be familiar with: Apache Kafka ( http://kafka.apache.org ) Apache Spark ( http://spark.apache.org ) Terms and Acronyms This section contains glossaries for all terms and acronyms used throughout the KPI Management and MediationZone documentation. KPI Management Terms and Acronyms Term/Acronym Definition Term/Acronym Definition Dimension A dimension represents a category in a data set and is used for grouping of KPIs. KDR A UDR type that holds the source data for KPI calculations. The decoded input to the workflow is represented as key-value pairs in the UDR, which can then be referenced in a service model. KPI A Key Performance Indicator (KPI) is a measurable value that demonstrates how effectively an organization is achieving key business objectives. A KPI is derived and calculated by aggregating metrics in a dimension over a specified period of time. Metric A metric is the result of a calculation based on one or more values in the KDRs. Various functions define how these values should be aggregated in each period. Service Model A service model is set of objects that defines dimensions, metrics, trees, and KPIs and the relation between these entities. Tree A tree is a representation of a set of paths to dimensions in a service model. Period Metrics are aggregated in discrete periods. The length of these periods is determined by properties in the service models. General Terms and Acronyms For information about general Terms and acronyms used in this document, see the Terminology document. Chapters The following chapters are included: KPI Management Overview KPI Management Service Models KPI Profile KPI Management - Non-Distributed Processing KPI Management - Distributed Processing KPI Management UDR Types

---

# Document 1171: GCP BigQuery Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641507/GCP+BigQuery+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section describes the transaction behavior of the GCP BigQuery agent. For more information about general transactions, see, Transactions, in Workflow Monitor . The GCP BigQuery batch forwarding agent uses the streaming insert API that is designed to ensure that data can be loaded at extremely high volumes and also that loaded data is available to queries in real-time. As part of the implementation that does both of these things, newly inserted data is added to a streaming buffer where it is immediately available for queries. However, this data is not moved into standard storage until more than an hour after being loaded. While the data is in the streaming buffer, it can only be queried. It cannot be updated, deleted, or copied. You will need to refer to the GCP documentation for further information on the streaming insert API and the streaming buffer. Due to the restriction on modifying rows in the streaming buffer, the GCP Bigquery agent does not modify the Data table at commit and rollback stage. Instead, the agent adopts a related design utilizing a Transaction ID , unique for each batch. The Data table must have a Transaction ID column. The Batch Status table must be created with a Transaction ID column and a Status column. At commit and rollback stage, the Batch Status is updated with a status code reflecting the current stage that can be used for auditing. Consumers of the loaded data are expected to always access that data through a view. This view should join the two tables on Transaction ID where status = 0, for example: Example - View Joining Data Table and Batch Status Table The following DDL query is used in the BigQuery Query Editor to create a view under the user_analytics Dataset with the table named view1 . CREATE VIEW IF NOT EXISTS user_analytics.view1 AS SELECT * FROM user_analytics.data_tbl1 AS t1 FULL JOIN user_analytics.batch_status_tbl1 AS t2 USING (id) WHERE t2.status = 0;

---

# Document 1172: MariaDB - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605795
**Categories:** chunks_index.json

This section contains information that is specific to the database type MariaDB. Supported Functions The MariaDB database can be used with: Callable Statements (APL) Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Event Notifications Prepared Statements (APL) Reference Data Management SQL Collection/Forwarding Agents SQL Loader Agent Task Workflows Agents (SQL) Preparations A database driver is required to connect to a MariaDB database. This driver must be stored in the Platform Container. Follow the steps below if MariaDB is not set up during the installation of the Platform Container: Download the JDBC driver ( mariadb-java-client - <version>.jar ) for the appropriate MariaDB database version. Copy the downloaded file(s) to the directory MZ_HOME/3pp in the Platform Container. Restart the Platform and ECs for the change to take effect. You should be able to select the MariaDB option from the Database profile after this step.

---

# Document 1173: KPI Management Service Model Deployment - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645541/KPI+Management+Service+Model+Deployment
**Categories:** chunks_index.json

You can create and provision a service model in two different ways: Creating a Profile and Service Model Using the Model Builder , or Creating a Profile and Provisioning via REST . To deploy the new service model, all KPI Management workflows need to be restarted.

---

# Document 1174: Variables - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743620
**Categories:** chunks_index.json

The following built-in variables are specific to the Aggregation agent. Variable Description Variable Description session - Batch and Real-Time A reference to the current session to be used to access variables defined in the Association tab. A session will remain in the database until manually removed. A route to ECS or an alternative route will not remove it. The variable is available in the consume , sessionInit and timeout function blocks. Example - Using session session.duration = input.duration + session.duration; instruction - Real-Time Only An optionally inserted string, belonging to the currently flushed session. The variable is available in the command function block only. Example - Using instruction input.info = instruction;

---

# Document 1175: Runtime APL Support for PCC Buckets - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743206/Runtime+APL+Support+for+PCC+Buckets
**Categories:** chunks_index.json

In runtime you determine how you want the products to be used in the business logic, i.e. how you want to perform a lookup of products from a workflow by using APL. Here you can find descriptions of the APL functions that are used for looking up the products stored in the products database during provisioning. This chapter includes the following sections: APL - PCC Mapper Support - Buckets APL - PCC Runtime Support - Buckets APL - PCC BucketData Support - Buckets APL - PCC BatchData Support

---

# Document 1176: Radius Server Agent with Supervision Service - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686743/Radius+Server+Agent+with+Supervision+Service
**Categories:** chunks_index.json

If you want to reject certain messages when the load gets too heavy, you can use the Supervision Service. With this service you can select one of the following overload protection strategies: Radius_AccessRequest - For rejecting requests of type AccessRequest Radius_AccountingIntermediate - For rejecting AccountingIntermediate requests Radius_AccountingStart - For rejecting AccountingStart requests Radius_AccountingStop - For rejecting AccoutningStop requests For each strategy, you can select if you want to reject 25, 50, or 100 % of the requests. See 3.1.8 Workflow Properties for further information.

---

# Document 1177: Python Processing Agent Events -Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653638/Python+Processing+Agent+Events+-Real-Time
**Categories:** chunks_index.json

Agent Message Events

---

# Document 1178: Firebase Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607448/Firebase+Agent+Configuration
**Categories:** chunks_index.json

You open the Firebase agent configuration dialog from a workflow configuration: you can right-click the agent icon and select Configuration... , or double-click the agent icon. Open Firebase agent configuration dialog Setting Description Setting Description Service Account Path Enter the path and file name of the Service Account credentials *.json file generated by Google Firebase. If no path is stated and only file name is stated, it will be assumed that the file is located in MZ_HOME.

---

# Document 1179: Workflow Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205849019/Workflow+Management
**Categories:** chunks_index.json

Table-Driven Workflow Instancing The Workflow Table can be used to instantiate multiple workflows interfacing similar sources and targets. Although almost every configuration aspect can be parameterized, the start/end-point identifiers (e.g. IP addresses, directories, login, passwords, etc.) are normally the most important. The example in the figure shows the Workflow Table tab in the Workflow Properties, where additional instances of the template workflow can be added. The parameters that are going to be available in the Workflow Table are configured in this tab as follows: Configuration parameters denoted as final are not shown in the Workflow Table and the final values from the template workflow are used in all workflow instances. Configuration parameters denoted as default are shown in the Workflow Table and have the value from the template workflow set as default. Configuration parameters denoted as per workflow are shown in the Workflow Table and are not set and should be set manually for each workflow. Open Using the Workflow Table tab to add workflow instances Workflow Execution Scheduling MediationZone provides support for manual, real-time and scheduled execution of any kind of workflow group holding processing functions (i.e. collection, processing, distribution, or any combination thereof): To perform a manual execution, a group is executed through the Execution Manager. Real-time workflows are always active after they have been activated. If such a workflow terminates, it can either be manually re-activated, it can be re-activated by the scheduler or it can stay in state abort. For scheduled execution of groups, the frequency of collection and delivery of information can be configured based on a period schedule (e.g. collect all files from a certain directory every 15 min). This schedule is based on day plans and can be configured to run every day, a specific weekday or a specific day of the month. On each day the activity can be specified to run only once or periodically at different intervals between different times. Open Execution scheduling GUI Workflow Execution The Execution Manager provides a graphical interface to manage the execution of workflow groups and the monitoring of their status and schedule. It also provides views of running workflows and detailed views of the contents of the workflow groups including throughput and other statistics. Below is an example of the Execution Manager, showing a number of workflow groups and their current status and scheduling. Open Example of workflow group scheduling info in Execution Manager Workflow Execution Suspension A Suspend Execution configuration enables you to apply a restriction that prevents specific workflows and/or workflow groups from running in a specific period of time. In the example below, five workflows are grouped. Note that these are not workflow groups used for defining prerequisites or execution criteria, but rather groups only for the purpose of disabling and enabling at certain points in time. Open Grouped workflows to be suspended The selected workflows are configured to be disabled over a weekend with a scheduled disabling and enabling of groups. This can be useful to disable workflows ahead of time when maintenance windows are planned. Open Execution of workflows has been disabled for period of time Shared In-Memory Table Lookup during Workflow Execution During execution of business logic, many workflows are dependent on retrieving data from a persisted storage, such as a database, by looking up data in own in-memory table. Each workflow instance will load/release its own lookup table during a workflow startup/shutdown. Therefore in the case of execution of many workflow instances, multiple loads/releases of the table will occur with unnecessarily high memory consumption. An alternative is to create a shared in-memory lookup table that is loaded and used by several workflows within same Execution Context as illustrated in the figure below. Open Shared in-memory table lookup Error Handling Many types of error handling can be implemented using the workflow properties. Automated re-transmissions can be executed according to the preferred behavior, e.g. make a specified number of retries before aborting the re-transmission attempts as configured in the example below. All retries will be logged and after the final retry the erroneous file will be sent to the error correction system (ECS) together with the specified file attributes, e.g. timestamp, source file name, etc. Open Configure number of retries before aborting

---

# Document 1180: XML Data Type Mapping - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783065/XML+Data+Type+Mapping
**Categories:** chunks_index.json

XML data types are automatically mapped to data types in Ultra as follows: XML Type Ultra Type <integer types> Use int types , except for long and unsignedLong to map to data types bigint . float Use float to map to data type float. double Use double to map to data type double. dateTime Use dateTime to map to data type date. anyType The AnyType UDR. See AnyType UDR Type . <all other types> Use all other types to map to data type string. The mapping of the data types also depends on the element attribute maxOccurs . When the value of this attribute is greater than 1 (one) or unbounded , the mapped field is of list type. When the element attribute minOccurs is 0 (zero), the field is considered optional and can be omitted in the internal format. Declared attributes are considered mandatory or optional, depending on the use attribute: required - The field is mandatory. optional (default) - The field is optional. prohibited - The use attribute is ignored and the field is optional. Example - Mapping of element- and attribute types element name="REQUEST"> <complexType> <!-- Mapped to string, mandatory --> <attribute name="REQUEST_ID" type="string" use="required"/> <sequence> <!-- Mapped to string, mandatory --> <element name="ITEM" type="string" minOccurs="1" maxOccurs="1"/> <!-- Mapped to list<string>, mandatory --> <element name="A_ITEMS" type="string" minOccurs="1" maxOccurs="2"/> <!-- Mapped to list<string>, mandatory --> <element name="B_ITEMS" type="string" minOccurs="1" maxOccurs="unbounded"/> <!--Mapped to string, optional-> <element name="C_ITEMS" type="string" minOccurs="0" maxOccurs="1"/> </sequence> </complexType> </element>

---

# Document 1181: SCP Prerequisite Preparation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034627/SCP+Prerequisite+Preparation
**Categories:** chunks_index.json

Prior to configuring an SCP agent, it is important to note that the following prerequisites and preparations need to be done. Attributes The SCP collection agent and the SCP forwarding agent share a number of common attributes. They are both supported by a number of algorithms: 3des-cbc, 3des-ctr, blowfish-cbc, aes128-cbc, aes192-cbc, aes256-cbc, aes128-ctr, aes192-ctr, aes256-ctr, arcfour, arcfour128, arcfour256. Authentication The SCP agents support authentication through either username/password or private key. Private keys can optionally be protected by a Key password. Most commonly used private key files, can be imported into the system. Typical command line syntax (most systems): ssh-keygen -t <keyType> -f <directoryPath> Setting Description Setting Description keyType The type of key to be generated. Both RSA and DSA key types are supported. directoryPath Where to save the generated keys. Example The private key may be created using the following command line: > ssh-keygen -t rsa -f /tmp/keystore Enter passphrase: xxxxxx Enter same passphrase again: xxxxxx Then the following is stated: Your identification key has been saved in /tmp/keystore Your public key has been saved in /tmp/keystore.pub When the keys are created the private key may be imported to the SCP agent: Open Finally, on the SCP server host, append /tmp/keystore.pub to $HOME/.ssh/authorized_keys . If the $HOME/.ssh/authorized_keys is not there it must be created. SCP Agents Server Identification The SCP agent uses a file with known host keys to validate the server identity during connection setup. The location and naming of this file is managed through the property: mz.ssh.known_hosts_file It is set in the <pico name> .conf file of the relevant EC to manage where the file is saved. The default value is ${mz.home}/etc/ssh/known_hosts . The SSH implementation uses JCE (Java Cryptography Extension), which means that there may be limitations on key sizes for your Java distribution. This is usually not a problem. However, there may be some cases where the unlimited strength cryptography policy is needed. For instance, if the host RSA keys are larger than 2048 bits (depending on the SSH server con fi guration). This may require that you update the Java Platform that runs the EC. For unlimited strength cryptography on the Oracle JRE, download the JCE Unlimited Strength Jurisdiction Policy Files from http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html . Replace the jar fi les in $JAVA_HOME/jre/lib/security with the fil es in this package. The OpenJDK JRE does not require special handling of the JCE policy fi les for unlimited-strength cryptography. SCP Agents Server Keys The SSH protocol uses host verification to guard against attacks where an attacker manages to reroute the TCP connection from the correct server to another machine. Since the password is sent directly over the encrypted connection, it is critical for security that an incorrect public key is not accepted by the client. The agent uses a file with known hosts and keys. It will accept the key supplied by the server if either of the following is fulfilled: The host is previously unknown. In this case, the public key will be registered in the file. The host is known and the public key matches the old data. The host is known however to have a new key and the user has configured it to accept the new key. For further information, see the description of the Advanced tab. If the host key changes for some reason, the file will have to be removed (or edited) in order for the new key to be accepted.

---

# Document 1182: Periods Data Model Buckets - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677354/Periods+Data+Model+Buckets
**Categories:** chunks_index.json

The UDRs in PCC.Periods contain information about different time periods used for products and their associated buckets. If all the fields (except StartTime and StopTime) are left empty, the product, and its associated buckets, will be active the entire time between the StartTime and StopTime. Usage should only be counted in buckets connected to products that are active, and enforcements should only be checked and applied for products that are active. Periods UDR Field Description Field Description ID (int) The unique ID of the period. Name (string) The name of the period. StartTime (date) Defines the start date and time for this period. This is the overall start time for the period and this field is mandatory for all periods. StopTime (date) Defines the end date and time for this period. This is the overall end time for the period and if it is left empty, the period will have no end. StartTimeOfDay (date) Defines the start time of the day in hours and minutes for the this period. The products using this period will start being active at this time of day. The format should be HH:MM. StopTimeOfDay (date) Defines the end time of the day in hours and minutes for this period. The products using this period will stop being active at this time of day. The format should be HH:MM. Weekdays (list <int>) Determines which weekdays the period should be active. The days are stated with integer values; 0 - Monday, 1 - Tuesday, 2 - Wednesday, 3 - Thursday, 4 - Friday, 5 - Saturday, and 6 - Sunday. Any combination is possible. IncludedPeriods (list <Period (PCC.Periods)>) Contains a list with other periods that should be included in this period. Since the list contains periods, they can in turn include or exclude other periods in their respective IncludedPeriods and ExcludedPeriods settings, which can be useful for creating a more complex inclusion setup. ExcludedPeriods (list<Period (PCC.Periods)>) Contains a list with other periods that should be excluded from this period. Since the list contains periods, they can in turn include or exclude other periods in their respective IncludedPeriods and ExcludedPeriods settings, which can be useful for creating a more complex exclusion setup. Below is a screenshot of the UDR Assistance displaying the Periods UDR: Open Periods UDR Configuring a Period A period configuration can consist of either a single period or a period including or excluding one or several other periods. To configure a period: Configure each period with StartTime and StopTime and any included/excluded periods. For periods that are not supposed to be active 24 h a day, 365 days a year, configure the fields StartTimeOfDay , StopTimeOfDay , and Weekdays . If no Weekdays are defined, the period will be active every day of the week. If no StartTimeOfDay or StopTimeOfDay are defined, the period will be active all the time that the included periods are active. Add periods that you want to include and exclude for the period in the IncludedPeriods and ExcludedPeriods fields. Note! If a period includes other periods, that should all be active during the same hours/minutes and days, you only have to configure the StartTimeOfDay , StopTimeOfDay , and Weekdays fields in the top level period. However, if the included periods should be active during different hours/minutes or days, the StartTimeOfDay , StopTimeOfDay , and Weekdays fields should be configured for each included period and not for the top level period. Using the settings for including and excluding periods will create a tree structure with one period at the top with one or several periods included and excluded beneath. In order for a configured period to be active, all three of the following criteria must met: The current time stamp must be within the set StartTime and StopTime of the top level period. If any periods are included, the current time stamp must be within the set StartTimeOfDay and StopTimeOfDay of at least one included period. If any periods are excluded, the current time stamp cannot be within the StartTimeOfDay or StopTimeOfDay of any of the excluded periods. Example of a Period Configuration In this example, we have a period including and excluding five other periods, configured as follows: "Top Level" Period StartTime 2012-01-01 08:00 StopTime 2015-12-31 08:00 Included periods Weekdays, Weekends Excluded periods Midsummer "Weekdays" Period StartTime 2012-01-01 07:00 StopTime 2013-01-01 06:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Weekdays Monday(0), Tuesday(1), Wednesday(2), Thursday(3), Friday(4) "Weekends" Period StartTime 2012-01-01 07:00 StopTime 2013-01-01 06:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Included periods Ordinary, Christmas "Midsummer" Period StartTime 2012-06-22 12:00 StopTime 2012-06-23 00:00 StartTimeOfDay 12:00 StopTimeOfDay 16:00 Weekdays Friday(4), Saturday(5) "Christmas" Period StartTime 2012-12-24 06:00 StopTime 2012-12-26 00:00 StartTimeOfDay 12:00 StopTimeOfDay 23:00 "Ordinary" Period StartTime 2012-01-01 06:00 StopTime 2013-01-01 00:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Weekdays Saturday(5), Sunday(6) Open Example configuration of Periods If current date and time is 2012-06-08 10:00 ( a Friday) the period will be active. If current date and time is 2012-12-25 23:20 ( a Tuesday )the period will not be active.

---

# Document 1183: ECS Inspector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685310/ECS+Inspector
**Categories:** chunks_index.json

To open the ECS Inspector, click the Inspection button in the upper left part of the Desktop window, and then double-click ECS Inspector in the menu. Initially, the dialog is empty and must be populated with data. Click Search to open the Search ECS dialog. The columns differ depending on whether the search is done for UDRs or batches. For further information, see Searching the ECS . Maximum Number of Displayed UDR Entries To ensure that the GUI does not have to handle too much data, the maximum number of displayed UDR entries is set to 100,000 UDR entries. However, in the configuration file for the Platform instance, you can change the maximum number of displayed UDR entries by setting the property mz.ecs.max.udr.search in the platform.conf file. Example Setting the property mz.ecs.max.udr.search to 10000: mzsh topo set topo://container:<platform container>/pico:platform/val:config.properties.mz.ecs.max.udr.search 10000 If you want quicker and smaller search results you can set this property to a lower value. Note! If the value of the property is set higher than the default value, it may result in poor performance. Note that this property only has effect when UDRs are inspected, not batches. Maximum Number of UDR Entries to Update To ensure that the Platform, using standard 256 Mb heap size, does not run out of memory, the maximum number of UDR entries to update or delete is limited to 15,000,000 UDR entries. However, this limit is easily increased using the Platform property mz.ecs.max.udr.update in the configuration file for the Platform instance. Example Setting the property mz.ecs.max.udr.update to 25000000: mzsh topo set topo://container:<platform container>/pico:platform/val:config.properties.mz.ecs.max.udr.update 25000000 Note! When increasing the default limit, you may have to increase the Platform maximum heap size as well. This is done by changing the -Xmx parameter in the configuration file for the Platform instance. As a general rule, the ECS needs another 10 Mb Platform heap space for every additional one million UDRs. Buttons in the ECS Inspector The following menu items apply for both batches and UDRs. Setting Description Search Displays the Search ECS dialog where search criteria may be defined to limit the entries in the list. For further information, see Searching the ECS . Matching entries are bundled in groups of 500 in the table. The list shows which group, out of how many, that is currently displayed. An operation targeting all matching entries, will have effect on all groups. Select All Selects all entries in the ECS Inspector. Refresh Refreshes the data in the table. Delete Removes selected entries, or if no entries are selected, all entries, provided that the reprocessing state (or RP State , see ECS Inspector Table ) is set to Reprocessed . There is a predefined cleanup task, ECS_Maintenance, if you prefer not to purge the ECS manually. For further information, see ECS Maintenance System Task . Note! If the maximum number of entries that can be displayed in the table has been exceeded. The delete operation will still be applied for all matching entries. Error Codes Displays the Error Codes dialog where error codes in the system may be configured. For further information, see ECS Error Codes . Reprocessing Groups Displays the ECS Reprocessing Group dialog, where reprocessing groups are managed. For further information, see ECS Reprocessing Groups . Searchable Fields Opens the ECS Searchable Fields dialog, where you can define specific UDR fields that you want to add as meta data that can later be used for making searches. See Configuring Searchable Fields in the ECS . Restricted Fields Opens the Restricted Fields dialog, where you can specify certain fields within certain UDR types that should be restricted from being updated in the ECS Inspector. See ECS Restricted Fields Configuration . Right-click Menu Options Right-clicking a row in the ECS Inspector shows row-specific menu options. Some options are batch- or UDR-specific, while other options apply to both batches and UDRs. Right-click menu options Explore UDR... Available for UDRs. Displays the UDR dialog presenting the content of the selected UDR(s). The dialog is also displayed if you double-click on a cell in the UDR Type column in the table of entries. In the dialog, the content of the UDR can be changed, except for the field Original Data . Open The UDR dialog For further information, see the Ultra Reference Guide . Explore Error UDR... Available for batches. Displays the Error UDR Viewer presenting the content of the Error UDR (if any). The viewer is also displayed if you double-click on a cell in the Error UDR column in the table of entries. Open The Error UDR viewer It is not possible to change the content of an Error UDR. Set Tag on UDR(s)... With this option you can set tags on selected UDRs. Select the UDR(s) you want to tag, right-click and select Set Tag on UDR(s)... A dialog opens where you can enter a Tag Name . Entering Tag Name The Tag Name is visible in the Tag column in the ECS Inspector. Clear Tag(s) on UDR(s)... Select this option to remove tags for the selected UDRs. Set State... Defines the state of a selected number of entries or, if no entries are selected, all entries. Possible states are New or Reprocessed . The state Reprocessed indicates that the entry has been collected by an ECS Collection agent and has been reprocessed with errors. Already processed data can be reset to New to enable recollection. When the state is changed, the timestamp in the Last RP State Change column in the ECS Inspector is updated. See ECS Changing State for more information. Note! If the number of matches is larger than the maximum number of UDRs to be displayed the state change is still applied for all matching entries. Assign to RPG... Assigns a selected number of entries or, if no entries are selected, all entries, to a reprocessing group (RPG). Grouped entries can be collected simultaneously by an ECS collection agent. Open Message when collecting grouped entries Note! Assignments to reprocessing groups can be made automatically for each new entry. In order for this to happen, the data must be assigned an Error Code in the workflow, and this Error Code must be mapped to the reprocessing group in the ECS Error Code dialog. Bulk Edit... Available for UDRs. Several UDRs (selected or matched) may be edited simultaneously with the Bulk Editor. The Bulk Editor has a Preview option, which makes it possible to preview the changes prior to approving and saving. When changes have been made you can select to view only the modified entries, the untouched entries, or all entries in the ECS Inspector. Open The ECS Bulk Editor Note! If the number of matches exceed the maximum number of entries that can be displayed, it may be a good idea to set up a workflow for editing the entries using APL instead of performing a bulk edit. Click the Apply Changes button, to display the Bulk Edit Result dialog, which displays modified and untouched entries. Open The Bulk Edit Result dialog Now you can select if you want to view the Entire Result Set , Modified Only , or Untouched Only entries in the ECS Inspector. Click View to make your selection. The selected entry type is displayed in the ECS Inspector. Information about what selection you made is displayed in the top right corner, above the ECS inspector table, e.g. "Modified Entries from Bulk Edit" or "Entire Work Set from Bulk Edit". Hint! If you want to view the changes before applying them, you can click on the Preview button. The Bulk Edit Result - Preview dialog opens, giving you a preview of the changes that are about to be made. If you are satisfied with the preview, click the Apply button, and the Bulk Edit Result dialog opens. For further information about the Bulk Editor, see the Ultra Reference Guide . Delete Open Removes selected, or if no entries are selected, all matched entries, provided that the reprocessing state (or RP State , see ECS Inspector Table ) is set to Reprocessed . The ECS does not have to be purged manually, there is a predefined cleanup task - ECS_Maintenance for automatic purging. For further information, see ECS Maintenance System Task . Note! If the maximum number of entries displayed in the table has been exceeded, the delete operation is still applied for all matching entries. View Batch... Available for batches. Displays the selected batch as raw data. Open View Batch

---

# Document 1184: Map Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646156
**Categories:** chunks_index.json

This section describes functions that enable use of hash maps. The following functions for Map described here are: 1 mapClear 2 mapContains 3 mapCreate 4 mapCreateSync 5 mapCreateOrdered 6 mapGet 7 mapKeys 8 mapRemove 9 mapSet 10 mapSize 11 mapValues mapClear Removes all entries in a map. void mapClear (map<any,any> myLittleMap ); Parameter Description Parameter Description myLittleMap The map to clear Returns Nothing mapContains Evaluates if a key is present in a map. boolean mapContains ( map<any,any> myMap , any myKey ); Parameter Description Parameter Description myMap Name of the map to evaluate myKey The requested key Returns true or false mapCreate Creates a new, empty map. map<any,any> mapCreate ( any myKeyType , any myValueType ); Parameter Description Parameter Description myKeyType Defines the data type of the keys in the map myValueType Defines the data type of the values, associated with the keys in the map Returns An empty map Example - Using mapCreate An example of a map definition, including assignment of the first key/value pair. map<string,int> myMap = mapCreate( string, int); mapSet( myMap, "Failed", 22 ); mapCreateSync Creates a new, empty synchronized map. This function may be useful for global maps in real time workflows, where the same map may be accessed from several threads at the same time. map<any,any> mapCreateSync ( any myKeyType , any myValueType ); Parameter Description Parameter Description myKeyType Defines the data type of the keys in the map myValueType Defines the data type of the values, associated with the keys in the map Returns An empty map Example - Using mapSet An example of a map definition, including assignment of the first key/value pair. map<int,string> mySyncMap = mapCreateSync( int, string); mapSet( mySyncMap, 55, "Successful" ); mapCreateOrdered Creates a new, empty ordered map. This function may be useful when the order of keys is of interest. map<any,any> mapCreateOrdered ( any myKeyType , any myValueType ); Parameter Description Parameter Description myKeyType Defines the data type of the keys in the map myValueType Defines the data type of the values, associated with the keys in the map Returns An empty map Example - Using mapCreate An example of a map definition, including assignment of the first key/value pair. map<string,int> myOrderedMap = mapCreateOrdered( string, int); mapSet( myOrderedMap, "Key1", 22 ); mapSet( myOrderedMap, "Key2", 45 ); mapGet Retrieves the value of a specified key in a map. valueType mapGet ( map<key,value> myMap , keyType key ); Parameter Description Parameter Description myMap The name of the map from which you want to retrieve a value key The name of the key holding the value to you want to retrieve Returns Returns the value associated with the key. The data type of the returned value is the same as the defined value type for the map. If there is no matching key mapGet will return null, false or 0 depending on type. Note - using mapGet If the map is map<int,string> . The return type of "mapGet" will be "string". mapKeys Returns a list of all keys present in a map. Note that the order of the elements in the list is not defined. list<any> mapKeys(map<any,any> myMap ); Parameter Description Parameter Description myMap The map to fetch all keys from Returns An unsorted list of keys according to: [keyA, keyB, keyX...] The data type of the list elements is the same data type as defined for the keys in the map. mapRemove Removes a key and its corresponding value. any mapRemove ( map<any, any> myMap , any myKey ); Parameter Description Parameter Description myMap The map to remove a key from myKey The name of the key to remove Returns The value corresponding to the removed key mapSet Sets or updates a key's associated value. void mapSet ( map<any, any> myMap , any myKey , any myKeyValue ); Parameter Description Parameter Description myMap The map to update myKey The name of the key to set/update myKeyValue The corresponding value to set/update Returns Nothing mapSize Returns the number of keys present in a map. int mapSize( map<any,any> myMap ); Parameter Description Parameter Description myMap The map to examine Returns The size of the map in terms of number of keys mapValues Returns a list of all values present in a map. Note that the order of the elements in the list is not defined. list<any>mapValues(map<any, any> myMap ); Parameter Description Parameter Description myMap The map to fetch all values from Returns A list of all values (unsorted): [valueA, valueB, valueX...]. The data type of the list elements is the same data type as defined for the values in the map.

---

# Document 1185: HTTP Batch Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641686/HTTP+Batch+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json



---
**End of Part 50** - Continue to next part for more content.
