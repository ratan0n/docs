# RATANON/MZ93-DOCUMENTATION - Part 79/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 79 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.7 KB
---

usage: packageexport -select <xml-selection file> [ -overwrite ]<base path> <package name> <package version> [password] or usage: packageexport -packages <xml-package file> [ -overwrite ]<base path> [password] This command exports Workflow Packages from to an MZP file located in the specified <base path>. As this command is available only to the mzadmin user, all the entries are exported, regardless of their access permissions. Note! A package is a Workflow Package. That is, <package name> and <package version> refer to the Workflow Package name and version. This command's output log information is displayed during the command execution. Although no log file is generated, you can view log information in the shell and save it to file. [-select] Specify the name of the Selection file that you want to use. For information about this parameter, see XML Selection File (under the -select <xml-selection file> option) in systemimport . You must provide <xml-selection file>, <base path>, <package name> and <package version>. Example: packageexport -select <xml-selection file> [ -overwrite ]<base path> <package name> <package version> packageexport -select /Users/username/Downloads/export.xml /Users/username/mz10-la3-rc6/tmp myPackage 1.0 <base path> The path to the directory where your MZP files will be saved. This parameter is compulsory. <package name> The name of the Workflow Package. When using [-select], <package name> is compulsory. <package version> The version of the Workflow Package. When using [-select], <package version> is compulsory. [-overwrite] Use this option to specify that the export - file or directory - should be overwritten. Example: packageexport -packages /Users/username/Downloads/export.xml -overwrite /Users/username/mz10-la3-rc6/tmp [-packages] Use -packages to specify multiple packages. Specify the name of the Workflow Package and version in the XML file, or select a specific folder. Several items can be selected. See XML Selection File (under the -select <xml-selection file> option) in systemimport for how to handle configurations or Workflow Packages. Examples - packages and XML file mzsh packageexport -packages packages.xml /tmp/PE --password <password> Where packages.xml looks like this: <pipeline> <packages> <package name="TcpBased" version="1.1"> <configurations> <configuration foldername="TCP_Based"/> </configurations> </package> <package name="HttpDemo" version="1.0"> <configurations> <configuration foldername="HttpDemo"/> </configurations> </package> </packages> </pipeline> Note! The resolveDependencies attribute is always set to true for the packageexport command. [password] To export encrypted configurations, provide a password. Return Codes Listed below are the different return codes for the package export command: Code Description Code Description 0 Returned if the command was successful. 1 Returned if the argument count is incorrect or if the export fails. 2 Returned if the output directory exists, if write permission is missing, or if the directory cannot be created for any other reason. 3 Returned if the XML selection file cannot be read. 4 Returned if any errors were reported during export.

---

# Document 1866: Data Types for Reference Data Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743524/Data+Types+for+Reference+Data+Management
**Categories:** chunks_index.json

Supported data types for Reference Data Management includes a list of Java SQL types retrieved via the JDBC drivers of the supported databases. Reference Data Management currently supports Oracle, PostgreSQL, MariaDB, and SAP HANA. You will need to refer to the respective database JDBC driver documentations for detailed information on the database data type to Java SQL type mapping. For unsupported data types, the following is a list of Reference Data Management features that will be impacted(where unsupported fields will be omitted from SQL statements): Query Expressions Edit Insert Export As a quick guide, the following is a generalized table which maps the supported Java SQL types to some of the commonly used and tested PostgreSQL , Oracle , MariaDB , and SAP HANA data types. Note! This data type is only applicable to JSON Viewer. Category Java SQL Types PostgreSQL Data Types Oracle Data Types MariaDB Data Types SAP HANA Data Types Category Java SQL Types PostgreSQL Data Types Oracle Data Types MariaDB Data Types SAP HANA Data Types STRING java.sql.Types.CHAR java.sql.Types.NCHAR java.sql.Types.VARCHAR java.sql.Types.NVARCHAR character (char) character varying (varchar) CHAR VARCHAR VARCHAR2 CHAR LONGTEXT JSON MEDIUMTEXT TEXT VARCHAR Note! LONGTEXT and JSON data types are only applicable to JSON Viewer. VARCHAR NVARCHAR ALPHANUM SHORTTEXT NUMBER java.sql.Types.SMALLINT java.sql.Types.REAL java.sql.Types.FLOAT java.sql.Types.DOUBLE java.sql.Types.INTEGER java.sql.Types.DECIMAL java.sql.Types.BIGINT java.sql.Types.TINYINT java.sql.Types.NUMERIC numeric (decimal) integer (int, int4) bigint (int8) double precision (float8) NUMBER FLOAT BIGINT DECIMAL DOUBLE FLOAT INT MEDIUMINT SMALLINT TINYINT TINYINT SMALLINT INTEGER BIGINT DECIMAL REAL DOUBLE DATE/TIME java.sql.Types.TIMESTAMP java.sql.Types.DATE java.sql.Types.TIME timestamp date time TIMESTAMP DATE DATE TIME TIMESTAMP DATE TIME TIMESTAMP BOOLEAN java.sql.Types.BOOLEAN boolean (bool) BOOLEAN BOOLEAN LARGE OBJECTS java.sql.Types.CLOB java.sql.Types.OTHER json jsonb Note! These data types are only applicable to JSON Viewer. CLOB Note! This data type is only applicable to JSON Viewer. CLOB Note! This data type is only applicable to JSON Viewer.

---

# Document 1867: SNMP Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643113/SNMP+Agents
**Categories:** chunks_index.json

This section describes the SNMP collection agents. These agents are available in real-time workflow configurations. The SNMP Collection agents package consists of the following: an SNMP Collection profile - This profile is used to import the set of Management Information Base (MIB) files that will be used to build the target UDRs. Usually, the set of MIB files is provided by the equipment vendor. an SNMP OID profile - This profile is used to configure UDR types and fields to poll, as an alternative to configuring this in the SNMP Request agent itself, making the configuration available for multiple agents. and two SNMP real-time agents: SNMP Request agent - This agent is used for polling management data by sending queries to the devices (network elements) on the network periodically. These queries determine the behavior of the devices, for example operational status, or the data in the MIB variables of the devices. A network element is often a router or a switch, but it can be anything that is SNMP-enabled. SNMP Trap agent - This agent is used to receive SNMP notifications (traps and informs), and convert them to UDRs to be processed further by the Platform. Overview The following features are supported for the usage of SNMP collection agents: SNMPv1, v2c, and v3 are supported. Note! For SNMPv3 SHA-1 and MD5 are disabled by default for security reasons and we do not recommend enabling them. If you require them to be enabled, use the property snmp.auth.proto.maxCompatibility . Refer to Execution Context Properties | SNMP Properties for more information. The SNMP Request agent maps all SNMP ASN.1 types to corresponding UDR attribute types including: INTEGER (including integer enumerations defined in MIB files) COUNTER COUNTER64 GAUGE OCTET STRING (including BIT STRING) OPAQUE IPADDRESS OBJECT IDENTIFIER TIMETICKS IPv4 and IPv6 protocols are supported. The SNMP Trap agent supports all types of SNMP notifications: Traps (TRAPv1 and TRAPv2 PDU types) and informs. The SNMP Trap agent also supports enterprise specific traps and generic traps: Cold Start Warm Start Link Up/Down Authentication Failure EGP Neighborless Network Elements The SNMP Request agent uses a CSV file as the input source. This file holds the list of network elements to poll. Extended MIB Support If the target MIBs have any dependent MIBs, ensure that all the dependent MIBs are also loaded to avoid errors. Prerequisites The following lists the relevant specifications for these agents: SNMPv1 RFC RFC 1157 Simple Network Management Protocol. SMIv1 RFCs also apply to all SNMPv1 entities. MIB-II RFCs also apply to all SNMPv1 agent entities. SNMPv2 RFCs RFC 1901 Introduction to Community-based SNMPv2. RFC 1908 Coexistence between Version 1 and Version 2 of the Internet-standard Network Management Framework. RFC 3416 Version 2 of SNMP Protocol Operations. RFC 3417 Transport Mappings. SNMPv3 RFCs RFC 3410 Introduction and Applicability Statements for Internet Standard Management Framework. RFC 3411 An Architecture for Describing SNMP Management Frameworks. RFC 3412 Message Processing and Dispatching. RFC 3413 SNMP Applications. RFC 3414 User-based Security Model. RFC 3415 View-based Access Control Model. RFC 3416 Version 2 of SNMP Protocol Operations. RFC 3417 Transport Mappings. RFC 3584 Coexistence between Version 1, Version 2, and Version 3 of the Internet-standard Network Management Framework. RFC 3826 The Advanced Encryption Standard (AES) Cipher Algorithm in the SNMP User-based Security Model. RFC 5343 Simple Network Management Protocol (SNMP) Context EngineID Discovery. Additional SNMPv3 RFCs including the Datagram Transport Layer Security RFCs (also known as DTLS or (D)TLS) are: RFC 5590 Transport Subsystem for the Simple Network Management Protocol (SNMP). RFC 5591 Transport Security Model for the Simple Network Management Protocol (SNMP). RFC 5953 Transport Layer Security (TLS) Transport Model for the Simple Network Management Protocol (SNMP). SMIv1 RFCs RFC 1155 Structure and Identification of Management Information for TCP/IP-based internets. RFC 1212 Concise MIB Definitions. RFC 1215 Convention for Defining Traps for use with the SNMP. SMIv2 RFCs RFC 2578 Structure of Management Information Version 2 (SMIv2). RFC 2579 Textual Conventions for SMIv2. RFC 2580 Conformance Statements for SMIv2. MIB-II RFCs RFC 1213 Management Information Base for Network Management of TCP/IP-based internets: MIB-II. RFC 2863 The Interfaces Group MIB (IF). RFC 3418 Management Information Base (MIB) for the Simple Network Management Protocol (SNMP). RFC 4001 Textual Conventions for Internet Network Addresses. RFC 4022 Textual Conventions for Internet Network Addresses (TCP). RFC 4113 Management Information Base for the User Datagram Protocol (UDP). RFC 4292 IP Forwarding Table MIB (IP-FORWARD). RFC 4293 Management Information Base for the Internet Protocol (IP). RFC 4898 TCP Extended Statistics MIB (TCP-ESTATS). The section contains the following subsections: SNMP Collection Profile SNMP OID Profile SNMP Request Agent SNMP Trap Agent SNMP Collection UDRs

---

# Document 1868: External - Ericsson IOG/IN Records - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646882/External+-+Ericsson+IOG+IN+Records
**Categories:** chunks_index.json

Record Declaration The syntax of an IN external format is declared as follows: external <format_name> inw { <field declarations> }; No size declarations are necessary for an IN record since it is given by the enclosing sequential record. In fact, an INW record can only be used as the final field in a sequential record. Field Declaration The syntax of an IN field is declared as follows: bcd tag_id(<int_constant>) <field_name> <: options>; The available tag_id values are given by the Ericsson IN documentation. The available options are: Option Description Option Description num_frame Indicates that the field has a number frame. Two fields are generated if this is specified, one with the specified field name and the other suffixed with _NumFrame . gnci Indicates that the field has a GNCI. Two fields are generated if this is specified, one with the specified field name and one suffixed with _GNCI . gnci on (<expr>) As gnci , however, the GNCI is only present if the expression is evaluated to true . terminated_by(<expr>) Identical to sequential definitions. int(base10) Identical to sequential definitions. int(base16) Identical to sequential definitions.

---

# Document 1869: Python Processing Agent Configuration - Batch - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686642/Python+Processing+Agent+Configuration+-+Batch
**Categories:** chunks_index.json

The Python processing agent configuration consists of two tabs: General and MIM . General Tab The General tab consists of three different sections: Python code for the consume block, input/output UDR types, and Interpreter configuration. Open The Python processing agent configuration - General tab Setting Description Setting Description Code Area This is the text area where you enter your code, see Python Writer's Guide for further information. The entered code will be color-coded depending on the code type, and for input assistance, a pop-up menu is available. Below the text area, there are line, column, and position indicators. See Python Code Editor Assistance . Input Types Enables selection of UDR Types. One or several UDR Types that the agent expects to receive may be selected. Set To Input Automatically selects the UDR Type distributed by the previous agent. Output Types Select which output types you want to have. Output Routes Enter the routes you want each output type to be sent out on. Interpreter select which Python Interpreter Profile you want to use. If no selection is made the interpreter that has been set as default in the Python Manager will be used. MIM Tab In the MIM tab, you can set the MIMs that you want the Python processing agent to publish. Open The Python processing agent configuration - MIM tab Column Description Column Description Assigned Select when the MIM resource is assigned a value. Can be any of the following: batch - Value is assigned from the consume block. header - Value is assigned in the beginBatch block. trailer - Value is assigned in the endBatch block. global - Value is assigned from any block. Name Enter the MIM resource name. Type Select the data type of MIM resource.

---

# Document 1870: Event Plugins - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645219/Event+Plugins
**Categories:** chunks_index.json

An event is an object containing information related to an event. As an example, there are Workflow State Events that are emitted when the state of a workflow is changed and there are User Events that are emitted when a user logs in or modifies data in the system. All events are forwarded to a Platform feature, named Event Server. Units interested in events, subscribe to specific events with the Event Server. An example of such units is the Workflow Template, which shows workflow and agent states, the Event Notification configuration, which subscribes and routes events to notifiers, etc. The DTK allows new event types to be introduced and emitted to the Event Server. These events may be collected and parsed in the Event Notification configuration and forwarded to one of the existing notifiers, such as the Log File or a user-defined notifier created with DTK. For further information about notifier plugins, see Notifier Plugins . Open Event For an event plugin example, see com.digitalroute.devkit.examples.event.TAPErrorEvent Event-related examples are located in the package com.digitalroute.devkit.event . Historic Events After an event has been processed by the Event Server, it is disposed of, by default. Thus, a new subscriber will not receive a matching event until the next one arrives. For subscribers such as the Workflow Template, which needs to know the state of each agent when a workflow is loaded, this is not sufficient. To solve this problem, an event can be defined as historic. If such an event arrives at the Event Server, it will cache this event after processing it, instead of throwing it away. Only the last event for each key will be saved. An event defined as clearing history will remove historic events sharing the same group key. Event Suppression The Event Notification configuration (not the Event Server) offers a feature of suppressing duplicate events. For instance, if duplicate suppression is configured to 10 seconds, the first arriving event will be forwarded, while duplicates, arriving within 10 seconds, will be discarded. To handle this, the duplicate key may be defined in the event. This is referred to as the suppress key. DRBaseEvent User-defined events are introduced by extending the DRBaseEvent class. DRBaseEvent itself extends DRAbstractUDR so that it can benefit from the introspection mechanism introduced in the UDR subsystem. The Event Notification configuration utilizes this when allowing automatic filtering on or selecting data from individual event fields. The base event contains some common fields that will be available in all events. These include severity, IP address, and timestamp. The following methods must be overloaded for new events. The method getEventName will return a string representation of the event. This name will be displayed in the list of available events when selecting which events to subscribe to in the Event Notification configuration. The getValue method, called by the toString method, will return an array of name/value pairs that describe the current value of the event. The toString implementation arranges these name/value pairs in a common layout. As previously mentioned, the UDR introspection mechanism automatically displays fields for all methods whose names begin with get_ , i.e. methods such as String get_FirstName() . In the Event Notification configuration, these will allow filtering and value collection from a field named FirstName. Note! Only fields with return type String will show up for filtering in the Event Notification configuration. To serialize the data, writeTo and readFrom methods must be implemented, and if an event will be cached by the Event Server, in order for a new subscriber to immediately obtain a value (without waiting for the next event), then the methods: isHistoryEvent getKey isClearingGroup getGroupKey and must be overloaded. For further information, see Historic Events above.

---

# Document 1871: Web Browsers for Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783422/Web+Browsers+for+Desktop
**Categories:** chunks_index.json

For web based interfaces, it is recommended that you use the latest available versions of the following web browsers: Google Chrome Mozilla Firefox Microsoft Edge

---

# Document 1872: File System Type - HDFS - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/315228209
**Categories:** chunks_index.json

When selecting HDFS as a file system, you will see two tabs  General and Advanced . Open File System profile - HDFS - General tab General tab The following settings are available in the General tab in the File System profile: Setting Description Setting Description General Settings Hadoop Mode Select the Hadoop mode that you want to use, both NON-HA and HA are available. Name Node Settings Host Enter the Hadoop name node host. This option is visible only when the NON-HA Hadoop Mode is selected. Port Enter the Hadoop port number. This option is visible only when the NON-HA Hadoop Mode is selected. Replication Enter the desired number of replications. Advanced tab The Advanced tab allows for advanced properties to be configured in the profile. Open File System profile - HDFS - Advanced tab

---

# Document 1873: Data Veracity - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205651997
**Categories:** chunks_index.json

Data Veracity is used when UDRs fail validation and manual intervention is needed before they can be successfully processed. Erroneous UDRs are sent to their respective database tables that are formatted to the structure of the UDRs, the sending of the data requires the Data Veracity Forwarding agent. To collect data from the respective database tables, the Data Veracity Collection agent is used. Using the Data Veracity Web UI Editor, UDRs may be examined, marked for deletion, force deleted or updated. Data Veracity currently supports the following databases: Oracle Database 19c PostgreSQL 13 PostgreSQL 14 PostgreSQL 15 SAP HANA 2.0 SP 7 Prerequisites The reader of this information should be familiar with: The MediationZone Platform Database Administrations The section contains the following subsections: Data Veracity Profile Data Veracity Collection Agent Data Veracity Maintenance System Task Data Veracity Forwarding Agent Data Veracity Task Agent Data Veracity in Data Management Data Veracity Performance Tuning Managing Data Veracity Access Permission

---

# Document 1874: APL - PCC Provisioning Plugins - Buckets - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656310/APL+-+PCC+Provisioning+Plugins+-+Buckets
**Categories:** chunks_index.json

The provisioning functions include: 1 pccGetData 2 pccListData 3 pccCreateData 4 pccUpdateData 5 pccDeleteData 6 pccLastErrorCode 7 pccLastErrorMessage 8 pccSetProvisioningArea 9 pccGetProvisioningArea 10 pccCopyAreaData 11 pccClearArea Note! These functions can be used to interact with the PCC provisioning interface from APL. pccGetData Retrieves a specific UDR from the provisioning interface based on the stated UDR type and key. drudr pccGetData( string typename, string key ) Parameters Parameter Description Parameter Description typename The fully qualified typename of the requested UDR. key The primary key value of the requested UDR, which is unique. Returns: The matching UDR or null, if no matching UDR was found. Example drudr notification = pccGetData ("PCC.Products.Provisioning.Notification", "70"); will retrieve a UDR of the type PCC.Products.Provisioning.Notification with the key 70 . pccListData Retrieves a list with all the UDRs from the provisioning interface with the stated UDR type. list<drudr> pccListData( string typename Parameters Parameter Description Parameter Description typename The fully qualified typename of the requested UDRs. Returns: A list containing all the UDRs of the requested type. Example list<drudr> notificationsList = pccListData ("PCC.Products.Provisioning.Notification"); will return a list named notificationsList with all the UDRs that have the type PCC.Products.Provisioning.Notification . pccCreateData This function inserts a new UDR through the provisioning interface. drudr pccCreateData( drudr udr ) Parameters Parameter Description Parameter Description udr The new UDR to insert. Returns: The created UDR or null if there was an error. Example drudr created = pccCreateData(input); will insert a new UDR named created through the provisioning interface. pccUpdateData This function updates a UDR through the provisioning interface. drudr pccUpdateData( drudr udr ) Parameters Parameter Description Parameter Description udr The UDR to update. Returns: The updated UDR or null if there was an error. Example drudr udr = pccUpdateData(created); will update the UDR named created through the provisioning interface. pccDeleteData This function deletes a UDR in the provisioning interface. drudr pccDeleteData( drudr udr ) Parameters Parameter Description Parameter Description udr The UDR to delete. Returns: The deleted UDR or null if there was an error. Example drudr udr = pccDeleteData(created); will delete the UDR named created through the provisioning interface. pccLastErrorCode If any of the read, create, delete, update, copy area, or clear area operations should fail, the error code can be retrieved with this method. int pccLastErrorCode() Parameters Return code Description Return code Description 200 Will be returned if the last operation was successful. 400 Will be returned if the object just created/updated is referring to one or more objects that are missing. 401 Will be returned if the delete operation failed because the object contains references from other objects. 402 This error code is only applicable for the pccCreateData function, and will be returned if the object already exists. 403 Will be returned if any of the functions pccGetData, pccUpdateData, or pccDeleteData are referring to a missing object. 500 Will be returned if there was an error trying to read/write to storage. Example If the last error code was 500. int errorCode = pccLastErrorCode(); will return 500. pccLastErrorMessage If any of the read, create, delete, update, copy area, or clear area operations should fail, a more detailed description of the error can be retrieved with this method. string pccLastErrorMessage() Parameters Parameter Description Parameter Description Returns: A description of the last error. Example If the last error code was 500. string errorMsg = pccLastErrorMessage(); will return a description of error code 500. pccSetProvisioningArea Sets a provisioning working area for the workflow. There are two areas available, TEST and PROD. void pccSetProvisioningArea( string area ) Parameters Parameter Description Parameter Description area The name of the area to work against. All delete, update, get and create operations will use this area. Must be one of the either TEST or PROD. Returns: Nothing Example pccSetProvisioningArea("TEST"); will set area TEST as working area for the workflow. pccGetProvisioningArea Retrieves the currently active area for the workflow. string pccGetProvisioningArea() Parameters Parameter Description Parameter Description Returns: The name of the currently active provisioning area. Example If the currently active area for the workflow is TEST, string area = pccGetProvisioningArea(); will return "TEST". pccCopyAreaData Copies configurations from one area to another. Warning! This function will also clear the target area from all previous configurations before the new configurations are inserted. string pccCopyAreaData ( string source , string destination , string configurations ) Parameters Parameter Description Parameter Description source The name of the source area. destination The name of the destination area. configurations The configurations that you want to copy, e g PCC (for all PCC configurations), PCC.Products (for only Product configurations), PCC.Periods (for only Periods configurations), etc. Returns: Null if the command succeeded or the error message if there was an error. Example pccCopyAreaData("TEST","PROD","PCC.Products"); will copy all the provisioned products data from the area TEST to the area PROD. If you want to copy all PCC related data, you can enter ("TEST","PROD","PCC") . pccClearArea Clears an area. All content in the area will be deleted. string pccClearArea( string area ) Parameters Parameter Description Parameter Description area The name of the area to clear. Returns: Null if the command succeeded or the error message if there was an error. Example pccClearArea("TEST", "PCC.Periods"); will clear the area TEST from old PCC configurations for periods. If you want to delete all PCC related data, you can enter ("TEST","PCC") .

---

# Document 1875: GTP' Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033331/GTP+Agent+Configuration
**Categories:** chunks_index.json

You open the GTP' agent configuration dialog from a workflow configuration. To open the GTP' agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Realtime . Click Add Agent and select GTP from the Collection tab of the Agent Selection dialog. Source Tab The Source tab includes connection type settings. Open The GTP' agent configuration dialog - Source tab Setting Description Setting Description Protocol Enter the protocol type that you want the agent to use: Either TCP or UDP. For further information see Limitations - GTP' Transported Over TCP in GTP' Agent MZSH Commands, Events and Limitations . Port Enter the port through which the agent should await incoming data packages. This port must be located on the host where the EC is running. Default value is 3386. Note! Two workflows that are running on the same EC, can subscribe to the same Port and GSNs, if they use different Protocol settings. Accept any This option is used for the dynamic discovery of new Charging Data Functions (CDF). You must use this option when there are multiple senders through different ports that are active on the same CDF and source IP address. A new sequence number handler will be created for each incoming source point (IP:port). When this option is selected, the GTP' agent will not send the following: NODE_ALIVE_REQUEST REDIRECTION_REQUEST Note! If the list below this option is not empty you will receive an error message. GSN IP Address The IP addresses or hostnames of the GSN nodes that provide the data. Server Port Enter the server port number for each node. The agent will detect GGSN source port changes via the Node Alive Request or Echo Request messages. If a change is detected, it is is registered in the System Log. The agent's internal configuration is updated. Note! If more than one source is defined on one IP address, port changes are not supported and only connections from the specified ports are accepted from that host. Miscellaneous Tab Open The GTP' agent configuration dialog - Miscellaneous tab The Miscellaneous tab includes collected format and storage settings of the data that is collected by the agent. Setting Description Setting Description Format Must match the Data Record Format in Data Record Packet IE . This is applicable if the Packet Transfer Command is either Send Data Record Packet or Send possibly duplicated Data Record Packet . Perform Format Version Check When checked, the Data Record Format Version in Data Record Packet IE must be identical to the setting in Format Version . Format Version Must match the Data Record Format Version in Data Record Packet IE . This is applicable if the Packet Transfer Command is Send Data Record Packet or Send possibly duplicated Data Record Packet . Directory Enter either the relative pathname to the home directory of the user account, or an absolute pathname of the target directory in the file system on the local host, where the intermediate data for the collection is stored. The intermediate data includes: Files that keep track of sequence numbers and restart values A directory called duplicates , where duplicates are saved. Note! This directory must be attended to manually. Note! When using several ECs, ensure that the file system that contains the GTP' information is mounted on all ECs. Acknowledgement from APL Check to enable an acknowledgement from the APL module that follows the GTP' agent. This way the GTP' agent will expect a feedback route from the APL module, as well. No GTP packets will be acknowledged before all the data that is emitted into the workflow is routed back to the collector. Controlling acknowledgment from APL enables you to make sure that data is transmitted completely. Clear this check box if you want the agent to acknowledge incoming packets before any data is routed into the workflow. No Private Extension Check to remove the private extension from any of the agent's output messages. Use seq num of Cancel/Release req Check to change the type of the sequence numbers that are populated in a Data Record Transfer Response to either Release Data Record Packet or Cancel Data Record Packet , in the Requests Responded field. Otherwise, the agent applies the sequence number of the released, or cancelled, Data Record Packet . Advanced Tab Open The GTP' agent configuration dialog - Advanced tab Setting Description Setting Description Max Wait for a Response (sec) The maximal period while the GTP' agent should expect a Node Alive Response message. If both this value and the Max Number of Request Attempts value, are exceeded, a message appears on the System Log. For example: If Max Wait for a Response is 20 and Max Number of Request Attempts is 5, the warning message will be logged after 100 seconds. The value also indicates the maximal period during which the GTP' agent awaits a Redirection Response . This period begins right after the agent releases a Redirection Request to the agents that it is configured to receive data from. Max Number of Request Attempts Enter the maximum number of attempts to perform in order to receive a Node Alive Response and Redirection Response . Max Outstanding Numbers Enter the maximum number of packages that you want kept in memory for sequence number checking. Max Drift Between Two Numbers Enter the maximum numbers that can be skipped between two sequence numbers. Clear Checking Check to avoid saving the last sequence number when the workflow has been stopped. Clear to have the agent save the sequence number of the last collected package when the workflow is stopped. This way, as soon as the workflow is restarted, a package with the subsequent number is expected and the workflow continues processing from the where it had stopped. Agent Handles Duplicates Select this option to store duplicates in a persistent data directory that you specify on the Miscellaneous tab. The packet will remain in the directory until the agent receives a request to release or to cancel it. Route Duplicates to Select this option to route duplicates to a link that you select from the drop-down list. Alternate Node Enter the hostname or IP-address of a host that runs an alternate Charging Gateway device as a backup. If you enter an IP address, the GTP' agent will include it in the redirection request that it sends to the GSN. Note! The GTP' agent does not backup any of the data that it manages. Make sure that the GSN node takes care of backup. Decoder Tab Open The GTP' agent configuration dialog - Decoder tab Setting Description Setting Description Decoder Click Browse and select a pre-defined decoder. These decoders are defined in the Ultra Format Editor, and are named according to the following syntax: <decoder> (<module>) The option MZ Format Tagged UDRs indicates that the expected UDRs are stored in the MediationZone specific format. Select this option to make the Tagged UDR Type list accessible for configuration. Tagged UDR Type Click Browse and select a pre-defined Tagged UDR Type. These UDR types are stored in the Ultra and Code servers. The naming format is: <internal>(<module>) If the decoder is intended to reprocess UDRs of an internal format, the Decoder MZ format tagged UDRs has to be selected enabling this list. Once enabled, the internal format must be selected. Full Decode Select this check box to enable full decoding of the UDR, before it enters the workflow. This, in turn, might reduce performance rate. Leave this check box empty to minimize decoding work in the agent. By leaving this check-box empty, you postpone decoding, and discovery of corrupt data to a later phase in the workflow.

---

# Document 1876: KPI Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611397/KPI+Agent
**Categories:** chunks_index.json

The KPI agent sends incoming performs KPI calculations based on the data in the incoming KDR UDRs and the Service Model of the selected KPI profile. The agent routes the output to the workflow as KPIAggregatedOutput UDRs. Open KPI Management workflow Configuration Open KPI agent configuration Setting Description Setting Description KPI Profile Click Browse to select a predefined KPI profile. The profile contains the service model configuration. Delay The delay setting specifies a time period that is added to the upper limit of each KPI output period. If the timestamp of a KDR UDR exceeds the upper limit with the added delay, it is matched with the next period and the current period is closed. For further information about periods, see KPI Management UDR Types . Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The KPI Cluster In agent expects KDR UDRs and delivers KPIAggregatedOutput UDRs. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop in the Desktop User's Guide . Publishes MIM Parameter Description MIM Parameter Description Error Counter This MIM parameter contains the number of errors that have occurred since the workflow was started. Error Counter is of the long type and is defined as a global MIM context type. Discarded KPIs This MIM parameter contains the number of calculated KPIs that were discarded since they belong to a previously closed period. Discarded KPIs is of the long type and is defined as a global MIM context type. Model Config This MIM parameter contains the name of the KPI Profile in the agent configuration. Model Config is of the string type and is defined as a global MIM context type. Model Version This MIM parameter contains the version of the KPI Profile in the agent configuration. Model Version is of the int type and is defined as a global MIM context type. Accesses This agent does not access any MIM parameters. Agent Message Events There are no message events for these agents. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor . Example - Debug events Failed to match dimension: At least one node is missing in /tree1/AMERICAS/country/Country.AvgSales for input Map(region -> AMERICAS, amount -> 563.75, xcountry -> Mexico) Failed metric calculation: Failed to evaluate KPI(Region.AvgSales), EXPR(Some(Expr(amount))) INPUT: Map(xamount -> 505.0) You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see 4.3.23 Debug Event .

---

# Document 1877: SQL Loader Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740849/SQL+Loader+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent receives UDRs of FileReferenceUDR type and emits UDRs of SQLLoaderResultUDR type. MIM The agent does not publish or access any MIM values. For information about the MIM and a list of the general MIM parameters, see MIM .

---

# Document 1878: Decoder Agent Meta Information Model and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606931/Decoder+Agent+Meta+Information+Model+and+Events
**Categories:** chunks_index.json

Meta Information Model For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . The agent does not publish nor access any MIM parameters. Agent Message Events There are no message events for this agent. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Splitting batch Emitted when the Hint End Batch event occurs.

---

# Document 1879: PCC Routing Control - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645897
**Categories:** chunks_index.json

Search this document: PCC Routing Control is an extension of the platform from MediationZone and delivers the capability to route and transform real-time data such as Diameter messages. Prerequisites The reader of this document should be familiar with: Diameter Agents For information about the Terms and Abbreviations used in this document, see the Terminology document. Chapters The following chapters and sections are included: Workflow Packages for Routing Control Routing Control Data Model Provisioning for Routing Control

---

# Document 1880: FTP DX200 Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607603/FTP+DX200+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file: name Reported together with the name of the control (TTTCOFxx.IMG) and data file that have been collected and inserted into the workflow. File cancelled: name Reported together with the name of the current file, each time a Cancel Batch message is received. This assumes the workflow is not aborted; please see, Transaction behavior, Cancel Batch. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Command trace A printout of the control channel trace. This is only valid if FTP command trace in the Advanced tab is selected.

---

# Document 1881: Workflow Bridge Example Batch to Real-Time Scenario with Action UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204741400/Workflow+Bridge+Example+Batch+to+Real-Time+Scenario+with+Action+UDR
**Categories:** chunks_index.json

This section will show an example of a scenario where a batch-forwarding workflow sends data to a real-time collection workflow. The batch workflow will deliver four UDRs to the real-time workflow and the second one will be returned with a user-defined Action UDR connected to it. The following configurations will be created: An Ultra format A Workflow Bridge Profile A Workflow Bridge Batch Forwarding Workflow A Workflow Bridge Real-Time Collection Workflow Example of a batch to real-time scenario Define an Ultra Format A simple Ultra Format needs to be created both for the incoming UDRs as well as for the user-defined WfbActionUDR . For more information about the Ultra Format Editor and the UFDL syntax, r efer to the Ultra Reference Guide . Create an Ultra Format as defined below: internal WFBActionUDR : extends_class( "com.digitalroute.workflowbridge.transport.ultra.WfbActionUDR" ) { int type; ascii action; }; external my_input sequential { // field definitions ascii myId : int(base10),terminated_by(","); ascii text : terminated_by(0xa); }; // Decoder mapping in_map inputMap : external( my_input ), target_internal( my_internal_TI ) { automatic; }; decoder myDecoder : in_map( inputMap ); The input file used in this example should look like: 1,My first UDR 2,My second UDR 3,My third UDR 4,My forth UDR Define a Profile The profile is used to connect the two workflows. See Workflow Bridge Profile for information how to open the Workflow Bridge Profile editor. Open Example of a profile configuration In this dialog, the following settings have been made: Send reply Over Bridge is not selected which means that only responses for WorkflowStateUDR s and UDRs with an Action UDR attached to the response will be returned to the forwarding workflow. Force Serialization is not used since there will be no configuration changes during workflow execution. The Workflow Bridge Real-time Collection agent must always respond to the WorkflowState UDRs. The Response timeout (s) has been set to "60" and this means that the forwarding workflow that is waiting for a WorkflowState UDR reply will timeout and abort (stop) after 60 seconds if no reply has been received from the collection workflow. Bulk Size has been set to "0". This means that the UDRs will be sent from the Workflow Bridge forwarding agent one by one, and not in a bulk. Enter the appropriate bulk size if you wish to use bulk forwarding of UDRs. Bulk Timeout (ms) has been set to "0" since there will be no bulk forwarding. Enter the appropriate bulk timeout if you wish to use bulk forwarding of UDRs. Bulk timeout can only be specified if the bulk functionality has been enabled in the Bulk size setting. Number of Collectors are set to 1 since only one collector since there will be a one-to-one connection in this example. Set the UDR type to my_internal_TI by clicking on the Add button. To remove a UDR type from the UDR Types list, select the UDR type click the Remove button. Create a Batch Forwarding Workflow In this workflow, a Disk agent collects data that is forwarded to an Analysis agent. The data is routed by the Decoder agent to the Workflow Bridge forwarding agent, which in turn forwards the data in a ConsumeCycleUDR to a Workflow Bridge real-time collection agent. Each time the Workflow Bridge batch forwarding workflow changes state, a WorkflowState UDR is sent to the Workflow Bridge real-time collection agent as well. For more information regarding the states a workflow can have, see Workflow Monitor . Since the Send Reply over Bridge option has not been configured, only ConsumeCycleUDR s with an Action UDR attached are returned from the Workflow Bridge real-time collection agent and routed to an Analysis agent in the batch forwarding workflow. Open Example of a batch forwarding workflow The workflow consists of a Disk collection agent named Disk , a Decoder agent named Decoder , a Workflow Bridge batch forwarding agent named Workflow_Bridge_FW, and an Analysis agent named Actions . Disk Disk is a collection agent that collects data from an input file and forwards it to the Decoder agent. Double-click on the Disk agent to display the configuration dialog for the agent: Open Example of a Disk agent co nfigur ation In this dialog, the following settings have been made: The agent is configured to collect data from the /home/trunk/in directory, which is stated in the Directory field. Enter the path to the directory where the file you want to collect is located. The agent will collect all files in the directory. Decoder The Decoder agent receives the input data from the Disk agent, translates it into UDRs and forwards them to the Workflow_Bridge_FW agent. Double-click on the Decoder agent to display the configuration dialog. Example of an Decoder agent configuration In this dialog, choose the Decoder that you defined in your Ultra Format. Workflow_Bridge_FW Workflow_Bridge_FW is the Workflow Bridge batch forwarding agent that sends data to the Workflow Bridge real-time collection agent. Each incoming UDR will be included in the Data field of a ConsumeCycleUDR which is sent to the real-time workflow. Double-click on Workflow_Bridge_FW to display the configuration dialog for the agent. Example of a Workflow Bridge agent configuration In this dialog, the following setting has been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. Analysis The Analysis agent is an analysis agent that receives the responses from the Workflow_Bridge_FW agent. Since the profile does not have the Send Reply Over Bridge option selected, the agent will only receive responses with an Action UDR. Double-click on the Analysis agent to display the configuration dialog. Open Example of an A nalys is agent configuration In this dialog, the APL code for handling input data is written. In the example, there will be a debug printout of the UDRs with an Action UDR connected. Adapt the code according to your requirements You can also see the UDR type used in the UDR Types field, in this example, it is a ConsumeCycleUDR . Create a Real-Time Collection Workflow In this workflow, a Workflow Bridge real-time collection agent collects the data that has been sent in a ConsumeCycleUDR from the Workflow Bridge batch forwarding agent. It also collects the WorkflowState UDRs that inform about state changes in the batch forwarding workflow. An Analysis agent returns all ConsumeCycleUDR s to the Workflow Bridge real-time collection agent, to let the agent know when to send the DrainCycleUDR . The Analysis agent also replies to all WorkflowState UDRs, so that the Workflow Bridge batch forwarding agent will know when to move forward to the next Ag ent Exec ution State. For more information regarding the workflow execution states, see Workflow Monitor . Open Example of a real-time collection workflow Workflow_Bridge_C Workflow_Bridge_C is the Workflow Bridge Real-time Collection agent that receives the data that the Workflow Bridge Batch Forwarding agent has sent over the bridge. Double-click on the Workflow_Bridge_C agent to display the configuration dialog for the agent. Open Example of a Workflow Bridge agent configuration In this dialog, the following settings have been made: The agent has been configured to use the profile that was defined in the section above, Define a Profile. The port that the collector server will listen on for incoming requests has been set to default value "3299". However, if the two workflows will execute on the same execution context, an ephemeral port is used instead. Analysis The Analysis agent is the Analysis agent that receives and analyses the data originally sent from the Workflow Bridge batch forwarding agent in the ConsumeCycleUDR , as well as the workflow state information delivered in the WorkflowState UDRs. This agent will also look for the UDR that has its Id set to 2 and create an Action UDR for this. Double-click on the agent to display the configuration dialog. Open Example of an Analysis agent configuration Example - APL code consume { if (instanceOf(input, wfb.WorkflowStateUDR)) { udrRoute((wfb.WorkflowStateUDR) input); } else if (instanceOf(input, wfb.ConsumeCycleUDR)) { wfb.ConsumeCycleUDR ccUDR = (wfb.ConsumeCycleUDR) input; //validate content of the incoming UDR WFBridge.UltraFormat.my_internal_TI myUDR = (WFBridge.UltraFormat.my_internal_TI) ccUDR.Data; if (myUDR.myId == 2) { //Create an action UDR WFBridge.UltraFormat.WFBActionUDR myAction = udrCreate( WFBridge.UltraFormat.WFBActionUDR); myAction.type = 44; myAction.action = "The second UDR will be returned to the WF"; ccUDR.Action = myAction; } udrRoute((wfb.ConsumeCycleUDR) ccUDR); } else { debug(input); } } } In this example, a reply is sent back to the Workflow_Bridge_C agent, by routing back the WorkflowStateUDR and ConsumeCycleUDR s. Adapt the code according to your requirements. Note! Since WorkflowState UDRs have to be routed back to the Workflow Bridge collection agent in order to be returned to the forwarding workflow, a "response" route have to be added from the Analysis agent to the Workflow Bridge collection agent. You can see the UDR types used in the UDR Types field, i. e. WorkflowStateUDR and ConsumeCycleUDR .

---

# Document 1882: Editing a Bulk of UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612759
**Categories:** chunks_index.json

Other than editing UDRs one at a time, it is also possible to edit a group of UDRs at once. Select the UDRs of interest, or none (this way all UDRs within the file are considered) and click Bulk Edit... Open The Bulk Edit file - data dialog Setting Description Setting Description UDR Type If you have selected UDRs in the UDR File Editor and opened Bulk Edit, the UDR type of the selected UDRs is displayed in this field. If you want to change to another UDR type, click the Browse... button to open the UDR Internal Format Browser where you can select a different UDR type. Filter UDRs and assign UDR fields In this field, you can write custom APL code to edit all selected UDRs. Close Click this button to close the Bulk Edit file - data dialog. Apply Changes Click to apply the changes defined in the APL code. Note! When bulk editing UDRs, the changes of the UDRs are not traceable. All selected UDRs are updated, although their values may not change. Auto Edit Mode This mode allows the editing of bulk of UDRs using matchers and assignments. This mode is available in Legacy Desktop only. Setting Description Setting Description Matchers Defines the UDRs targeted for update. For further information about adding Matchers, see the section below, Add Matcher . Join Style Indicates if All or Any will apply to the conditions in the Matchers list Field Specifies the fields to examine when defining target UDRs Matcher Configuration The expression to apply when matching. Ericsson IOG/IN formats: (see External - Ericsson IOG/IN Records ). This special format type is described through the inw variant of the external block. Assignments Defines the changes to be made to the targeted UDRs. For further information about adding Assignments, see the section below, Add Assignment. Field Fields to edit Assignment Configuration The new value of the field Add Matcher A Matcher is added by selecting Add in the Matchers section. The fields are explained in the table below: Setting Description Setting Description UDR Field List of available fields to use as matchers. Depending on the field type, the Matcher Type list updates to display valid choices. Matcher Type Defines the check criterion for the field. The content of the list varies depending on the field type. Supported types and some of their possible matchers are: Alphanumeric (string, char) - Contains, Begins With, Matches RegExp, etc. Date - Equals, Before, After etc. Numeric - Not Equals, Greater Than, Less Than etc. Bytearray - Is null, Not null, Length equals, Length not equals, Byte at position equals Parameter Some field types can consist of several parts. For instance, date types can contain separate values for year, month, and day. Regular numeric/alphanumeric fields will have Value stated. Is APL Indicates the possibility of entering variable values for the validation field via the use of APL code in the Value column. This feature makes it possible to refer to other fields within the same UDR (as well as call to APL functions) during processing. In the image below, the P155_PackageLength field is of type int . If Is APL is disabled, the Matcher Configuration is interpreted as an int instead of a function. Value The value of the field. By default, a constant is entered. If Is APL is selected, APL syntax can be used to create a variable definition.

---

# Document 1883: REST_Deprecated Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034386/REST_Deprecated+Agents
**Categories:** chunks_index.json

This section describes the REST Client_Deprecated and REST Server_Deprecated agents. The REST Client_Deprecated agent is a processing agent for real-time workflow configurations. The REST Server_Deprecated agent is a collection agent for real-time workflow configurations. Prerequisites The reader of this information should be familiar with: Representational state transfer (REST) RFC 2616 - Hypertext Transfer Protocol -- HTTP/1.1 RFC 6749 - OAuth 2.0 Authorization Framework RFC 7617 - Basic HTTP Authentication Scheme RFC 5246 - Transport Layer Security (TLS) Protocol This section includes the following subsections: REST Server Profile Deprecated REST Client_Deprecated Agent REST Server_Deprecated Agent

---

# Document 1884: Diameter Transport Security - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640789/Diameter+Transport+Security
**Categories:** chunks_index.json

The Diameter protocol communication can be protected by using Transport Layer Security, TLS. TLS Configuration TLS requires a keystore file that is generated by using the Java standard command keytool. For further information about the keytool command, see the JDK product documentation. Example - TLS Configuration To Create a keystore: $ keytool -genkey -keyalg RSA -keystore MZstack.jks Keytool prompts for required information such as identity details and password. Note that the keystore password must be the same as the key password. Generate the certificate: $ keytool -export -keystore MZstack.jks -file ./MZstack.cer The certificate file can now be distributed to the other peers. Install a diameter node certificate in the MZstack keystore: $ keytool -import -alias "peerTLS" -file peerTLS.cer -keystore MZstack.jks Enter the keystore path and the keystore password in the Diameter Stack configuration. From the Peer Table , in the Diameter Routing profile configuration select the TCP/TLS protocol for the peer with which you want to establish a secure connection. TLS Configuration Properties You can control the handling of unrecognized certificates by setting the Execution Context property mz.diameter.tls.accept_all. Example - Handling of unrecognized certificates On a specific EC: $ mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.diameter.tls.accept_all true On cell level: mzsh topo set topo://cell:default/val:common.config.properties.mz.diameter.tls.accept_all true If the property is set to false (default), the Diameter Stack agent does not accept any non-trusted certificates. If it is set to true , the Diameter Stack agent accepts any certificate. In either case any unrecognized certificate will be logged in an entry in the System Log (in PEM format). Check the certificate. If you trust it, import it into the keystore by using the Java standard keytool command. For further information, see the standard Java documentation.

---

# Document 1885: ECS Inspector Table - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672991/ECS+Inspector+Table
**Categories:** chunks_index.json

Once the search is performed and matches are found the table in the ECS Inspector is populated. Each row represents one UDR or one batch. Note! The ECS Inspector caches the result when the user populates a list (for instance the Error Codes). This is done to avoid unnecessary population of workflow names, agent names, and error codes since it is costly in terms of performance. You have to click on the Refresh button in order to repopulate the search window. Open ECS Inspector - UDRs Open ECS Inspector - Batches Columns The following columns are available in the ECS Inspector table: Column Description Column Description # The table sequence number. Db ID A sequence number, automatically assigned to an entry. Date Date and time when the entry was inserted in the ECS. Workflow Name of the workflow from which the data was sent. Agent Name of the agent that sent the entry to ECS. For UDRs, this is the ECS forwarding agent. For batches, this is a collection agent. UDR Type Available for UDRs. This column displays the UDR type. Cancel Agent Available for batches. This column displays the name of the agent that issued the cancelBatch request. Cancel Message Available for batches. This column displays the message sent with the cancel request. The following example shows a user defined request, defined with APL using an Analysis agent: Example - A user defined request, defined using APL cancelBatch("undefined_number_prefixes."); Error Code The Error Code as defined in the ECS. Error UDR Available for batches. This column displays the type of the Error UDR associated with the batch. Double-click on a table cell in this column to show information about the whole Error UDR. The Error UDR is populated with information needed in a workflow when reprocessing a batch. You can populate the Error UDR either from the Workflow properties dialog (see Workflow Properties, Error tab ), or from an agent using APL (see ECS Collection Agent Configuration ). The fields of the Error UDR automatically appear as MIM values in the reprocessing workflow. Open Workflow Properties dialog - Error tab Example - ErrorUDR myErrorUDR eUDR = udrCreate( myErrorUDR ); eUDR.noOfUDRs = (long)mimGet( "IN", "Outbound UDRs" ); udrAddError( eUDR, "nokSOURCE", "Switch not found." ); cancelBatch( "Incorrect source.", eUDR ); The error UDR format is defined as any other format from the Ultra Format Editor. internal myErrorUDR{ long noOfUDRs; }; RP Group Shows the reprocessing group that the entry is assigned to, if any. Assignments can be made both manually and automatically. In the latter case, an Error Code must be mapped to a reprocessing group. RP State Initially, an entry has the reprocessing state New , that is the entry has not been reprocessed. In order for it to be collectable, it has to be assigned to a reprocessing group. When collected by an ECS collection agent, the state is changed to Reprocessed . Note! Only entries in state New may be collected by the ECS collection agent. The state can manually be changed back to New if needed. Only entries set to Reprocessed can be removed. MIM Values Double-clicking this field displays a new dialog, listing the MIM values. MIM values are configured differently for batches and UDRs: Batch - From the Workflow properties dialog. UDR - From the ECS forwarding agent. Note that to open the MIM values dialog, the MIM Values field(s) have to be populated. Tags Available for UDRs. This column displays any tags that have been set for the UDRs. Last RP State Change This column displays the timestamp when the reprocessing state was last changed. The first time a UDR is sent to the ECS, it is set to state New , so in this case the column displays the timestamp when the UDR was inserted into the ECS. When the UDR is collected for reprocessing or if the state is changed manually, this column is updated with the current timestamp. <search field label(s)> Available for UDRs. If you have configured Searchable Fields in the ECS, the search field labels are displayed as individual columns. See Configuring Searchable Fields in the ECS for further information. Tagging UDR After you have searched for UDRs, you can select specific UDRs that are of interest, tag them, and then save a filter based on the tag. This will display only the tagged UDRs whenever the filter is used. Warning! If you remove the tag that you have specified in the filter, the filter will not work properly. To tag UDRs and save as a filter: After populating the ECS Inspector, select the UDRs you want to tag. Right-click in the table and select the Set Tag on UDR(s)... option. A dialog opens asking you to enter a name for the tag. Open Setting tags Enter the tag name in the Tag Name field and click OK . The selected UDRs are now tagged. Open the Search ECS dialog by clicking on the Search button. Select the Tag checkbox and enter the tag you want to search for in the field to the right of the checkbox. Click on the Save As... button beneath the Saved Filters list. A dialog opens asking you to enter a name for the filter. Enter a name in the Saved Filter Name field and click OK . The dialog closes and the new filter appears in the Saved Filters list. Open Tag Filter The next time you want to view the tagged UDRs, select the saved filter when making your search to only display the tagged UDRs in the ECS Inspector.

---

# Document 1886: Batch Answer - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642810
**Categories:** chunks_index.json

The answer field for CCBatchcycleUDR may contain a UDR that is described here or a UDR in Charging Answer UDRs . AcquisitionExceptionUDR This UDR represents the exception thrown when a direct batch acquisition operation of a collection of chargeable items fails in the connected SAP CC Server system. Field Description Field Description message (string) A string description of the error message reason (string) A string description of the reason for the exception REASON_ACCESS_NOT_FOUND (string) This exception is thrown when the service identifier (SID), user service identifier (USID), and consumption date specified in the operation request, do not correspond to existing access data in the customer master data of the service provider. REASON_ACQUISITION_NOT_ALLOWED (string) This exception is thrown when the acquisition operation request is not allowed. REASON_EXPORT_FAILURE (string) This exception is thrown when the chargeable item cannot be exported. REASON_INVALID_ACCESS (string) This exception is thrown when one of the access information subelements specified in the operation request (service identifier (SID), user service identifier (USID), or consumption date) is empty. REASON_INVALID_CHARGEABLE_ITEM (string) This exception is thrown when the chargeable item to acquire is not specified in the operation request or its name is empty. REASON_SERVER_FAILURE (string) This exception is thrown when an unexpected error occurs during the acquisition of the chargeable item. REASON_TARGET_NOT_FOUND (string) This exception is thrown when the provider contract (or subscription) targeted by the access cannot be found in the customer master data of the service provider. REASON_COMMUNICATION_FAILURE (string) This exception is thrown when a communication failure occurs during the acquisition of the chargeable item. AcquisitionResultsUDR This UDR represents the operation results replied by the connected SAP Convergent Charging server in response to a batch acquisition operation request of a chargeable item or a collection of chargeable items. For the field definitions, see the section below, AcquisitionResultUDR. AcquisitionResultUDR This UDR represents the operation result replied by the connected SAP Convergent Charging server in response to a batch acquisition operation request of a chargeable item or a collection of chargeable items. Field Description Field Description exception (AcquisitionExceptionUDR (sapcc.batch)) This field indicates the exception that describes the error encountered during the acquisition of the chargeable item. For more details on the possible values, see the section above, AcquisitionExceptionUDR. isSuccessful (boolean) This field indicates whether the corresponding chargeable item was acquired successfully. operandReference (string) This field is populated with the code or the reference (if no code exists) that identifies the operand of the operation request. If the operation is not auditable, the operand reference is null.

---

# Document 1887: FTP Collection Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685519/FTP+Collection+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes info rmation about the FTP collection agent Input/Output data processing. For information about the general behavior, see Administration and Management . Input/Output Data Input Data The agent transmits commands that change the state of the file currently processed Command Description Begin Batch Transmitted before the first byte of each collected file is fed into a workflow. End Batch Transmitted after the last byte of each collected file has been fed into the system. Output Data Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on the workflow level (set in the workflow properties) is configured to abort the workflow, the agent will never receive the last Cancel Batch message. In this situation, ECS will not be involved, and the file will not be moved. APL code where Hint End Batch is followed by a Cancel Batch will always result in workflow abort. Make sure to design the APL code to first evaluate the Cancel Batch criteria to avoid this sort of behavior. Hint End Batch If a Hint End Batch message is received, the collector splits the batch at the end of the current block processed (32 kB), provided that no UDR is split. If the block end occurs within a UDR, the batch will be split at the end of the preceding UDR. After a batch split, the collector emits an End Batch Message, followed by a Begin Batch message (provided that there is data in the subsequent block).

---

# Document 1888: Usage Data Record (UDR) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647707/Usage+Data+Record+UDR
**Categories:** chunks_index.json

MediationZone is capable of handling various Usage Detail Record (UDR) formats provided that their structure is known. Vendor UDR formats are often complex and hard to work with in agents that will view or manipulate data. Therefore, MediationZone converts the external UDR formats to an internal representation, and vice versa when UDRs are leaving the system, i e decodes and encodes the data. UDR File Editor The UDR File Editor is used to decode batches and view the decoded batch divided into UDR structures without requiring a workflow for processing the data. You can have multiple UDR File Editors opened at the same time, and the UDRs can be opened by double-clicking them and then also edited. If there is an encoder available, it is also possible to encode a new batch. This can be useful in order to test decoder configuration, view key fields of records and generate test data for specific scenarios. The image below shows an MSC binary batch that has been decoded into UDRs and where selected fields configured in a view are presented in individual columns giving an overview of desired fields. Open Example of decoded batch in UDR File Editor The figure below shows the content of one of the selected UDRs. The fields are fully editable and the records can be encoded to generate output files. Open Content of selected UDRs

---

# Document 1889: Categorized Grouping Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738255/Categorized+Grouping+Configuration
**Categories:** chunks_index.json



---
**End of Part 79** - Continue to next part for more content.
