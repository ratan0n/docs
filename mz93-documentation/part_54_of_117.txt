# RATANON/MZ93-DOCUMENTATION - Part 54/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 54 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~66.9 KB
---

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The agent produces bytearray types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Value Description File Modified Timestamp This MIM parameter contains a timestamp, indicating when the file is stored in the collection directory. File Modified Timestamp is of the date type and is defined as a header MIM context type. File Retrieval Timestamp This MIM parameter contains a timestamp, indicating when the file processing starts. File Retrieval Timestamp is of the date type and is defined as a header MIM context type. Source File Size This MIM parameter contains the file size, in bytes, of the source file. Source File Size is of the long type and is defined as a header MIM context type. Source Filename This MIM parameter contains the name of the currently processed file, as defined at the source. Source Filename is of the string type and is defined as a header MIM context type. Source Filenames This MIM parameter contains a list of file names of the files that are about to be collected from the current collection directory. Note! When the agent collects from multiple directories, the MIM value is cleared after the collection of each directory. Then, the MIM value is updated with the listing of the next directory. Source Filenames is of the list<any> type and is defined as a Header MIM context type. Source File Count This MIM parameter contains the number of files, available to this instance for collection at startup. The value is constant throughout the execution of the workflow, even if more files arrive during the execution. The new files will not be collected until the next execution. Source File Count is of the long type and is defined as a global MIM context type. Source Pathname This MIM parameter contains the path to the directory where the file currently under processing is located. Source Pathname is of the string type and is defined as a global MIM context type. The path is defined in the Disk tab. Note! Even if a relative path was defined when configuring the Disk Collection agent (see Disk Tab in Disk Collection Agent Configuration - Batch ), for example, input , the value of this parameter will include the whole absolute path; /$MZHOME/input . Source Files Left This parameter contains the number of source files that are yet to be collected. This is the number that appears in the Execution Manager backlog. Source Files Left is of the long type and is defined as a header MIM context type. Accesses The agent does not access any MIM resources.

---

# Document 1278: Managing a Workflow Group - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204604969
**Categories:** chunks_index.json

This section includes the following topics: Opening a new or saved workflow group Creating a workflow group Removing members Execution: Manual or Scheduled Configuration Opening a Workflow Group Configuration You create a new workflow group configuration from the Build view of the Desktop User Interface. To open the workflow group configuration, click Build  New Configuration . Select Workflow Group from the Configurations dialog. To open an existing workflow group configuration, click Build and select the workflow group configuration in the Configuration Browser, or right-click a workflow configuration and then select View Configuration . Creating a Workflow Group You create a workflow group by adding a workflow or a workflow group, as a member to the group. To create a workflow group: In the Available To Add pane, select a workflow or a workflow group. Note! An invalid workflow member will not affect the validity of the workflow group. Click the button. The member is added in the Group Members list. Note! Batch, task and system task workflow members can be combined in a workflow group, but real-time workflow members can only be combined with other real-time workflow members. However, for real-time workflows, we recommend that only one workflow is included in each workflow group, unless you have selected the Continuous Workflow Execution checkbox in the Execution tab, which will allow you to have multiple real-time workflows in the same group. Click the Save As button and give the new workflow group a name. Removing a Member from a Workflow Group When you remove a member from a workflow group, the member does not cease to exist: A workflow member may still be running according to its configuration, or as a member of a workflow group in the system. A workflow group member may still run as a member of another workflow group in the system. To remove a member from a workflow group: Select the member that you want to remove in the in the Group Members list in the Members tab in workflow group configuration. Click the Remove button. You will get a question if you are sure you want to remove the member. Click Yes if you are sure. The member is removed from the group. Executing a Workflow Group Execute the Workflow group either manually, from the Execution Manager - see Execution Manager - or, schedule an automatic execution - see the section below, Scheduling. Configuring a Workflow Group Configuring a workflow group includes: Planning members' execution order Setting the workflow group execution parameters Setting the workflow group scheduling parameters Members' Execution Order When planning the execution order of the members in your workflow group, use the Prerequisites column in the Group Members table. By doing so you ensure: A linear execution A certain execution order That every member is fully executed before the next member starts running Note! If you use the Continuous Workflow Execution setting, you can override this behavior by allowing scheduled workflows to execute even though all workflow members are not finished. See the section below, Execution, for further information. To configure members' execution order: Select a member in the Group Members pane in the Members tab. Click the Edit button. The Prerequisites dialog box opens. Open The Prerequisites dialog box Select the checkboxes for the members that the current member should follow. Click OK . See the image below for an example of how it may look. Open Workflow Group members' execution prerequisites You can rearrange the members' order of appearance in the Group Members list, by using the Up and Down buttons. When rearranging a list, that is already configured with Prerequisites you notice that the Prerequisites parameter is removed and a yellow warning icon appears instead. Note that this does not affect the workflow group validity. To remove the notification sign, either open the Prerequisites dialog box and click OK , or - to remove all the notification signs - save the workflow group configuration, and reopen it. Execution Click the Execution tab in the workflow group configuration. Open The Workflow Group Execution tab Entry Description Entry Description Max Simultaneous Running Workflows Enter the maximum number of workflows you want to be able to run concurrently. Note! If you do not specify a limit, your specific work environment and equipment will determine the maximum number of workflows that can run simultaneously. This value applies only to the workflow group that you are defining and does not affect members that are workflow groups. Startup Delay If Max Simultaneous Running Workflows is set to a value larger than 1, enter the delay (in seconds) of the execution start for each of the workflows that may run simultaneously. Note! If you do not enter any value, a very short delay will be applied by the system, by default. You can assign a Startup Delay regardless of the members' status. Once the delay is up, if the member in turn is disabled, the Workflow group attempts to execute the next member. Continuous Workflow Execution Select this checkbox if you want to allow members in scheduled workflow groups to execute on schedule even though all workflow members in the group have not finished execution. This may be useful in case one member is delayed for some reason. In that case remaining workflow members will not be prevented from executing on schedule. Note! This feature is not supported for nested group members, only workflow members. For a workflow group with a member that is delayed over the next scheduled time, this setting makes the member execute immediately when it is finished, and the group will be in a continuous Running state, not switching to Idle in between executions as is the default behavior. This needs to be considered when making configurations based on workflow group states. Continue This option activates the default behavior on member abort, which means that the workflow group will run until all its members are fully executed and/or all are aborted. Note! This means that groups with real-time workflow members continue to run until all the members are aborted or stopped manually. Stop Select this option to have the workflow group stop when a member aborts. A batch Workflow will finish the current batch and then stop. Stop Immediately Select this option to have the workflow group stop immediately when a member aborts. A batch workflow will stop even in the middle of processing a batch. Enable Select this checkbox to enable the workflow group execution settings. Note! Execution settings that you configure here, only apply to workflow members for which execution settings have not been enabled in the configurations that they are part of. Workflow groups cannot run as stand-alones, and will be executed on the Platform. For further information about stand-alone, see Execution Context in the Desktop User's Guide . Distribution A workflow executes on an EC group. You can specify these EC groups, or the system can select them automatically. The Distribution rules are applied to all included group members, such as workflows and workflow group configurations. When there are conflicting settings, the members that are lowest in the workflow group hierarchy have precedence. When the Distribution rules of the workflow group configurations are set on the same level in the hierarchy, they do not conflict with each other. Note! If you configure the distribution using EC groups, the selected distribution type is also applied to the ECs within the groups. The following options exist: Sequential - Starts the workflow on the first EC group in the list. If this EC group is not available, it proceeds with the next in line. Workflow Count - Starts the workflow on the EC group running the fewest number of workflows. If the Execution Contexts list contains at least one entry, only this/these EC groups will be considered. Machine Load - Starts the workflow on the EC group with the lowest machine load. If the Execution Contexts list contains at least one entry, only this/these EC groups will be considered. Which EC group to select is based on information from the System Statistics sub-system. Round Robin - Starts the workflow on the available EC groups in turn, but not necessarily in a sequential order. If ecg1, ecg2, and ecg3 are defined, the workflow may first attempt to start on ecg2. The next time it may start on ecg3 and then finally on ecg1. This order is then repeated. If an EC group is not available, the workflow will be started on any other available EC groups. Scheduling The cause of execution for a workflow group can either be a planned time scheme or a specific event. You can configure the cause of execution in the Scheduling tab. Note! Changes to a running workflow group will not apply until the group has finished running, which means that a real-time workflow will have to be stopped manually for changes to apply. Info! The times stated in Scheduling follows the timezone of the server hosting the MediationZone Platform. Open The Workflow Group Scheduling tab Entry Description Entry Description Day Plans Use this table to plan timed triggers that will execute your Workflow group. Note that you can define a list of various plans. The system picks the plan that meets the top priority according to the section below, Day Plans Priority Rule. Event Trigger Use this table to define an event execution trigger for the Workflow group, see the section below, Event Triggers. Day Plans Priority Rule The Day Plans table lets you create a list of different execution schemes of the Workflow group. You configure each Day Plan to any interval between executions. Note! Two Day Plans should not contradict each another. An example of an invalid configuration: Day Plan A is set to Tuesdays Off, while Day Plan B is set to Every 5 minutes between 15:00:00 and 15:05:00 on Tuesdays. The system applies the following priority rule for picking a Day Plan out of the list: Last day of month Day of month (1-31) Weekday (Monday-Sunday) Every day To Configure a Day Plan Schedule: Click the Add button below the Day Plan table in the Scheduling tab. The Add Day Plan dialog opens. Open The Add Day Plan dialog Entry Description Entry Description Day Select the target day. Valid options are: Every day A specific weekday A specific day of the month (1-31) The last day of the month Day Off Select this checkbox to avoid execution on the day specified in the Day list. Start At Enter a start time for the first execution. Stop At Enter the time for when execution should stop. Note! If these fields are left empty, the default stop time, which is 23:59, is applied. Repeat Every Enter the interval between execution start time in seconds, minutes, or hours. Note! If this field is left empty, only one execution session is run at the specified start time. Note! If a member in a group is delayed for some reason, and not finished at the time the execution is set to be repeated, all members in the group have to wait until the next repeat time. To override this behavior, you can use the Continuous Workflow Execution setting in the Execution tab. See the Execution section above. Event Triggers To trigger the execution of a Workflow group you add a row to the Event Trigger table. A row can be either a certain event, or a chain of events, that must occur in order for the Workflow group execution to set off. Note! An Event Trigger that is comprised of a chain of events will take effect only when all the events that it includes have occurred. The events that have occurred are stored in memory. When MediationZone is restarted this information is lost and none of the events on the event chain are considered to have occurred. To Configure an Event Trigger: Click the Add button beneath the Event Triggers table. The Add Event Chain Trigger dialog opens. Open The Add Event Chain Trigger Dialog Click the Add button. The Add Event Selection dialog opens. Open The Add Event Selection dialog Select an Event Type from the drop-down list. See Event Fields in Event Notifications Configuration . Double-click an entry in the Event Filter table. The Edit Match Value dialog opens. Click the Add button. The Add Match Value dialog opens. If you want to filter all the events based on specific values of the selected type, enter the values in the Match Value(s) column. Otherwise, if you leave the default value, all events of the selected event type will trigger the execution of the Workflow group. Click Add/OK/Close to close each of the four dialog boxes, confirming that your entry is included. Note! There are no referential constraints for Event Triggers nor any way to track relations between workflows that are triggered by one another. For example: Workflow A is defined to be activated when Workflow B is activated. Workflow B may be deleted without any warnings, leaving Workflow A, still a valid workflow, without a trigger. This can happen since value matching is based on a regular expression of the workflow name, and not on a precise link match.

---

# Document 1279: Common Data Model - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611983/Common+Data+Model
**Categories:** chunks_index.json

The UDRs in PCF.Common contain general information about different time periods when rules associated with the periods should be active as well as routing information and any miscellaneous information. Open Periods UDR The fields in the Periods are used for defining a period of time during which a rule is active or not active. If all the fields (except StartTime and StopTime) are left empty, the period will always be active. Field Description Field Description ID (int) The unique ID of the period. Name (string) The name of the period, e g used by the Rules UDR. StartTime (date) Defines the start date and time for this period. This is the overall start time for the period and this field is mandatory for all periods. StopTime (date) Defines the end date and time for this period. This is the overall end time for the period and this field is mandatory for all periods. StartTimeOfDay (date) Defines the start time of the day in hours and minutes for this period. The rules using this period will start being active at this time of day. The format should be HH:MM. StopTimeOfDay (date) Defines the end time of the day in hours and minutes for this period. The rules using this period will stop being active at this time of day. The format should be HH:MM. Weekdays (list <int>) Determines which weekdays the period should be active. The days are stated with integer values; 0 - Monday, 1 - Tuesday, 2 - Wednesday, 3 - Thursday, 4 - Friday, 5 - Saturday, and 6 - Sunday. Any combination is possible. IncludedPeriods (list <Period (PCF.Common)>) Contains a list of other periods that should be included in this period. Since the list contains periods, they can in turn include or exclude other periods in their respective IncludedPeriods and ExcludedPeriods settings, which can be useful for creating a more complex inclusion setup. ExcludedPeriods (list<Period (PCF.Common)>) Contains a list of other periods that should be excluded from this period. Since the list contains periods, they can in turn include or exclude other periods in their respective IncludedPeriods and ExcludedPeriods settings, which can be useful for creating a more complex exclusion setup. Below is a screenshot of the UDR Assistance displaying the Periods UDR: Open Periods UDR Configuring a Period A period configuration can consist of either a single period or a period including or excluding one or several other periods. To configure a period: Configure each period with StartTime and StopTime and any included/excluded periods. For periods that are not supposed to be active 24 h a day, 365 days a year, configure the fields StartTimeOfDay , StopTimeOfDay , and Weekdays . If no Weekdays are defined, the period will be active every day of the week. If no StartTimeOfDay or StopTimeOfDay are defined, the period will be active all the time that the included periods are active. Add periods that you want to include and exclude for the period in the IncludedPeriods and ExcludedPeriods fields. Note If a period includes other periods, that should all be active during the same hours/minutes and days, you only have to configure the StartTimeOfDay , StopTimeOfDay , and Weekday fields in the top-level period. However, if the included periods should be active during different hours/minutes or days, the StartTimeOfDay , StopTimeOfDay , and Weekdays fields should be configured for each included period and not for the top-level period. Using the settings for including and excluding periods will create a tree structure with one period at the top with one or several periods included and excluded beneath. In order for a configured period to be active, all three of the following criteria must met: The current timestamp must be within the set StartTime and StopTime of the top-level period. If any periods are included, the current time stamp must be within the set StartTimeOfDay and StopTimeOfDay of at least one included period. If any periods are excluded, the current time stamp cannot be within the StartTimeOfDay or StopTimeOfDay of any of the excluded periods. Example of a Period Configuration In this example, we have a period including and excluding five other periods, configured as follows: "Top Level" Period StartTime 2012-01-01 08:00 StopTime 2015-12-31 08:00 Included periods Weekdays, Weekends Excluded periods Midsummer "Weekdays" Period StartTime 2012-01-01 07:00 StopTime 2013-01-01 06:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Weekdays Monday(0), Tuesday(1), Wednesday(2), Thursday(3), Friday(4) "Weekends" Period StartTime 2012-01-01 07:00 StopTime 2013-01-01 06:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Included periods Ordinary, Christmas "Midsummer" Period StartTime 2012-06-22 12:00 StopTime 2012-06-23 00:00 StartTimeOfDay 12:00 StopTimeOfDay 16:00 Weekdays Friday(4), Saturday(5) "Christmas" Period StartTime 2012-12-24 06:00 StopTime 2012-12-26 00:00 StartTimeOfDay 12:00 StopTimeOfDay 23:00 "Ordinary" Period StartTime 2012-01-01 06:00 StopTime 2013-01-01 00:00 StartTimeOfDay 08:00 StopTimeOfDay 16:00 Weekdays Saturday(5), Sunday(6) Open Example of a Period Configuration If current date and time is 2012-06-08 10:00 ( a Friday ) the period will be active. If current date and time is 2012-12-25 23:20 (a Tuesday ) the period will not be active. RouteToLocation UDR The RouteToLocation UDR defines a traffic route to the application's location. Field Description Field Description ID (int) The unique ID of the RouteToLocation UDR. This field is mandatory. dnai (string) This field contains the Data Network Access Identifier (DNAI), which identifies the location of the application. This field is mandatory. routeInfo (list <RouteToLocation (PCF.Common)>) Contains a list of RouteInformation UDRs which contain the traffic routing information, see the RouteInformation UDR section below. routeProfId (string) The routing profile ID. A RouteToLocation UDR must include either a routeInfo list or a routeProfId . Below is a screenshot of the UDR Assistance displaying the RouteToLocation UDR: Open RouteToLocation UDR RouteInformation UDR The RouteInformation UDR contains explicit routing information that can be used by the RouteToLocation UDR. Field Description Field Description ID (int) The unique ID of the RouteInformation UDR, used by the RouteToLocation UDR. This field is mandatory. Name (string) The name of the RouteInformation UDR. This field is mandatory. ipv4Addr (string) If IPv4 is used, this field specifies the IPv4 address to route to. ipv6Addr (string) If IPv6 is used, this field specifies the IPv6 address to route to. portNumber (int) The port number to route to. This field is mandatory. A RouteInformation UDR must include either an ipv4Addr or an ipv6Addr address. Below is a screenshot of the UDR Assistance displaying the RouteInformation UDR: Open RouteInformation UDR

---

# Document 1280: Performance Tuning with Redis Storage - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606168/Performance+Tuning+with+Redis+Storage
**Categories:** chunks_index.json

This section describes how to tune the Aggregation agent with Redis storage. Unless stated otherwise, the properties referred to in this section are set in the Advanced tab in the Aggregation profile. Connections to Thread Pool There are by default eight threads per workflow in the Redis thread pool that are used for Aggregation storage handling. Each thread corresponds to a Redis connection. You can increase the number of threads by setting mz.redis.agg.nr.of.threads . This may improve throughput performance, depending on the number of available CPUs. Timeouts There are by default two timeout threads per workflow that periodically check the Redis aggregation storage for timed out sessions. You can control how often this check is performed by setting mz.redis.agg.timeoutwait.sec . The default value is 1 second. You can also increase the number of threads that perform this check by setting the property mz.redis.agg.timeout.threads . Setting a higher value than default may speed up detection of timeouts. However, the number of CPUs is a limiting factor. Timeout data is stored in the Aggregation sessions but also in a configurable number of "sorted sets", enabling faster iteration in the timeout threads. When you have a large number of sessions and workflows, you may want to increase the number of sorted sets to reduce resource contention and thereby timeout latency. To do this, increase the default value of mz.redis.agg.timeout.sets . By default, there are 100 sets. By default, the Aggregation agent ensures the consistency of the stored sessions and sorted sets, preventing that timed out sessions are lost. In cases where transaction safety is not critical, you may increase the performance of timeout threads by disabling the consistency checks. To do this, set mz.redis.agg.slow.timeout.enable to false . Hint! You can use the MIM parameter Session Timeout Latency as an indicator of the timeout handling performance. You use the Operation Timeout (ms) setting in the Connectivity tab of the assigned Redis profile to control the timeout of Redis "CRUD" operations, i e create, read, update, and delete. Setting a lower value than the default 1000 ms may have a positive impact on throughput performance. However, if the value is set too low, indicated by a large number of operation timeouts errors in the EC logs, a lower throughput can be expected. Session Storage Format The aggregation sessions are stored in JSON format. However, some of the data within the JSON strings can be stored in binary format instead of plain text (default). You can change the stored format by setting the property json_serializer.format . The valid values are: MZ-BIN - The session data is serialized into JSON strings with binary content. JSON - The session data is serialized into JSON strings with plain text content. Example - Binary Format { "drType": "MZ-BIN", "drFormatVersion": 2, "data": "Af+cAAAAR2NvbS5tZWRpYXRpb256b25lLnVsdHJhLmluLmFnZ3Jl Z2F0aW9uX2NvbW1vbl9zZXNznaW9uX3Nlc3Npb25fMTk1MzI3MTEwAAABT C19yJgAAAAAAAAAAAEAAAAGAQAAAAExAQAAAAExAQAAnAAExAAAABgEAAA AHY29uc3VtZQ==", "SessionTimeout": 1426692360344, "initialized": true }

---

# Document 1281: GCP Storage Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607787/GCP+Storage+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the Database collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to select workflow type, select Batch . Click Add Agent and select GCP Storage from the Collection tab of the Agent Selection dialog. The GCP Storage Collector tab contains settings related to the placement and handling of the source files to be collected by the agent. Open Setting Description Setting Description Profile Select the File System profile you want the agent to use, see File System Profile for further information about this profile. File Information Directory The absolute pathname of the source directory on the location stated in the File System profile, where the source files reside. Include Subfolders Select this check box if you have subfolders in the source directory from which you want files to be collected. If you select Enable Sort Order in the Sort Order tab, the sort order selected will also apply to subfolders. Filename Name of the source files in the location stated in the File System profile. Regular expressions according to Java syntax apply. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all filenames beginning with TTFILE , type: TTFILE.* Compression A compression type of the source file. Determines if the agent will decompress the files before passing them on in the workflow. No Compression - agent does not decompress the files. This is the default setting. Gzip - agent decompresses the files using gzip. Before Collection Move to Temporary Directory If enabled, the source files will be moved to the automatically created subdirectory DR_TMP_DIR in the source directory, prior to collection. This option supports the safe collection of a source file reusing the same name. Append Suffix to Filename Enter the suffix that you want to be added to the file name prior to collecting it. Important! Before you execute your workflow, make sure that none of the file names in the collection directory includes this suffix. Inactive Source Warning (hours) If the specified value is greater than zero, and if no file has been collected during the specified number of hours, the following message is logged: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR , if using Move Before Collecting ) to the directory specified in the >Destination field, after the collection. If the Prefix or Suffix fields are set, the file will be renamed as well. Note! It is only possible to move files within the same bucket. Destination The absolute pathname of the directory on the location specified in the referenced File System profile into which the source files will be moved after collection. This field is only enabled if Move to is selected. Rename If enabled, the source files will be renamed after the collection, remaining in the source directory from which they were collected (or moved back from the directory DR_TMP_DIR , if using the Move To Temporary Directory setting). Prefix/Suffix Prefix and/or suffix that will be appended to the beginning respectively the end of the name of the source files, after the collection. These fields are only enabled if Move to or Rename is selected. Note! If Rename is enabled, the source files will be renamed in the current directory (source or DR_TMP_DIR ). Ensure not to assign a prefix or suffix, give files new names, still matching the filename regular expression, or else the files will be collected over and over again. Search and Replace To apply Search and Replace , select either the Move to or Rename setting. Search : Enter the part of the filename that you want to replace. Replace : Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the following agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note! A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text. Search and Replace Examples To rename the file file1.new to file1.old , use: Search : .new Replace : .old To rename the file JAN2011_file to file_DONE , use: Search : ([A-Z]*[0-9]*)_([a-z]*) Replace : $2_DONE Keep (days) A number of days to keep source files after the collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, determining when the file may be removed. This field is only enabled if Move to or Rename is selected. Remove If enabled, the source files will be removed from the source directory (or from the directory DR_TMP_DIR , if using the Move To Temporary Directory setting), after the collection. Ignore If enabled, the source files will remain in the source directory after collection. UDR Type Route FileReferenceUDR Select this checkbox to route File Reference UDR instead of raw data.

---

# Document 1282: Encoder Agent Configuration Batch Workflow - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640996/Encoder+Agent+Configuration+Batch+Workflow
**Categories:** chunks_index.json

You open the Encoder processing agent configuration dialog from a workflow configuration. To open the Compressor processing agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select Encoder from the Processing tab of the Agent Selection dialog. Open Encoder configuration dialog - Encoder tab Setting Description Setting Description Suppress Encoding If enabled, the agent will not encode the incoming data. It expects a raw byte array as the input type and will pass it through untouched. This mode is used when only a header and/or a trailer is added to a data batch. Encoder List of available encoders introduced via the Ultra Format Editor, as well as the default built-in encoder for the MediationZone internal format (MZ format tagged UDRs). CSV Format The CSV Format selection defines that CSV is to be used. Open CSV Format Selection Options The following options are available when this selection is set: Setting Description Setting Description UDR Type Opens the UDR Type selection box where the desired UDRs can be chosen Format Select the desired format, The available options are Unix, Mac, Windows, Excel , and Custom . Delimiter When the Custom format is chosen, the delimiter input field allows you to set the desired delimiter. Use Quote Enable this checkbox if you want to use a quote. Quote Enter the quote format Line Break When the Custom format is chosen, the line break input field allows you to set the line break. JSON Format The JSON Format selection defines that JSON is to be used. Open JSON Format Selection Options The following options are available when this selection is set: Setting Description Setting Description Indented Output If enabled, indentation will be applied to the output data. Inline Map Field If enabled, this sets inline fields mapped for the designated fields. Select the UDR Type and the target Field using the Browse button. MZ Tagged Format The MZ Tagged Format selection defines that the built-in MZ Tagged Format is to be used. Open MZ Tagged Format Selection Options MZ Tagged Format (Compressed) The MZ Tagged Format (compressed) selection defines that the built-in Compressed MZ Tagged Format is to be used. Open MZ Tagged Format (Compressed) Selection Options Avro Format The Avro Format selection defines that Avro is to be used. Open AVRO Format Selection Options The following options are available when this selection is set: Setting Description Setting Description Schema Register URL Url to a schema register used for obtaining Avro schema used for decoding. Format <http>or<https>//<host>:<port>/<schema_register_endpoint_path> Example: http://localhost:8081/schemas/ids Note! It is possible to use a proxy to contact a schema register. See HTTP Proxy Support for information on how to configure the proxy on the execution context level. Schema Field Field in JSON formated schema register response containing schema Note! The Header and Trailer tabs are described in Encoder Agent Services Batch . The use and setting of private threads for an agent, enabling multi-threading within a workflow, is configured in the Thread Buffer tab. For further information, see Thread Buffer Tab in Workflow Template .

---

# Document 1283: FTP DX200 Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641364/FTP+DX200+Agent+Configuration
**Categories:** chunks_index.json

You open the FTP DX200 collection agent configuration dialog from a workflow configuration. To open the configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Batch . Click Add agent and select FTP DX200 from the Collection tab of the Agent Selection dialog. The dialog contains three tabs; Switch , Advanced, and TTSCOF Settings . Switch Tab The Switch tab includes configuration settings that are related to the remote host and the directory where the control files are located. On this tab you specify from which VDS device the control files are retrieved, and the time zone location of the VDS device. Open FTP DX200 collection agent configuration - Switch tab with FTP Open FTP DX200 collection agent configuration - Switch tab with SFTP Setting Description Setting Description Host Name Enter the name of the Host or the IP address of the switch that is to be connected. Transfer Protocol Choose transfer protocol. Authenticate With Choice of authentication mechanism. Both password and private key authentication are supported. When you select Private Key , a Select... button will appear, which opens a window where the private key may be inserted. If the private key is protected by a passphrase, the passphrase must be provided as well. For further information about private keys, see Authentication in 9.31.2 FTP DX200 Agent Preparations . User Name Enter the name of the user from whose account on the remote Switch the FTP session is created. Password Enter the user password. Root Directory Enter the physical path of the source directory on the remote Host, where the control files are saved. Path To Data Files Enter the path to the directory where the data files are located, if they are not located in the Root Directory, in this field. Switch Time Zone Select the time zone location. Timezone is used when updating the transaction control file. VDS Device No. Enter the network element device from where the control files are retrieved. Advanced Tab The Advanced tab includes configuration settings that are related to more specific use of the FTP service. Open FTP DX200 collection agent configuration - Advanced tab Setting Description Setting Description Prefix Enter the data files name prefix Number Positions Select the length of the number-part in the data file name as follows: 01 for a two digit number 001 for a three digit number 0001 for a four digit number For example: If you select 0001, data file number 99 will include the following four digits: 0099 . Ends With VDS Device No. Select this check box to create data file names that end with VDS device No. Server Port Enter the port number for the server to connect to, on the remote Switch . Note! Make sure to update the Server Port when changing the Transfer Protocol . Number Of Retries Enter the number of attempts to reconnect after temporary communication errors. Retry Interval (ms) Enter the time interval, in milliseconds, between connection attempts. Force Delay (s) Select this option to force a delay (in seconds) between the TTTCOF write and the TTSCOF read. The maximum value is 3600. The default value is 0. Decompress Files Data files are decompressed by default. If you want the data files to be compressed, deselect this check box. Local Data Port Enter the local port number that the agent will listen to for incoming data connections. This port will be used when communication is established in Active Mode. If the default value, zero, is not changed, the FTP server will negotiate about the port the data communication will be established on. Active Mode (PORT) Select this check box to set the FTP connection mode to ACTIVE. Otherwise, the mode is PASSIVE. Transfer Type Select either Binary or ASCII transfer of the data files. Note! Setting Transfer Type to the wrong type might corrupt the transferred data files. FTP Command Trace Select this check box to generate a printout of the FTP commands and responses. This printout is logged in the Event Area of the Workflow Monitor. Use this option only to trace communication problems, as workflow performance might deteriorate. Accept New Host Keys Select this check box if you want the existing host key to be overwritten when the host is represented with a new key. The default behavior is to abort when the key mismatches. Warning! Selecting this option causes a security risk since new keys are accepted regardless of if they belong to another machine. TTSCOF Settings Tab Open FTP DX200 collection agent configuration - TTSCOF Settings tab The TTSCOF Settings tab includes configuration settings that allows you to adjust the default settings for the FTP DX200 agent Setting Description Setting Description Collect when file is present on only one WDU With this setting you can select to allow files to be collected, even though they are only present on one WDU. This may be useful if one of the WDUs cannot be reached for some reason. Default is No, which means that files will only be collected if they are present on both WDUs. Note! WDU is short for Winchester Drive Unit, and each VDS (Virtual Data Storage) has two WDUs. WDU0 Path In this field you can specify the path to WDU0. This setting is optional. WDU1 Path In this field you can specify the path to WDU1. This setting is optional. Select default collection WDU Select the WDU you want to use as default in this list. WDU 1 is default. Collect files with bit 5 In this list you can select if you only want to collect files where bit 5 IS NOT set ( Must not be set ), or where bit 5 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 5 is set or not ( May be set ). Collect files with bit 6 In this list you can select if you only want to collect files where bit 6 IS NOT set ( Must not be set ), or where bit 6 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 6 is set or not ( May be set ). Collect files with bit 7 In this list you can select if you only want to collect files where bit 7 IS NOT set ( Must not be set ), or where bit 7 IS set ( Must be set ), or if you always want to collect files regardless of whether bit 7 is set or not ( May be set ).

---

# Document 1284: Workflow Bridge UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205035349
**Categories:** chunks_index.json

The Workflow Bridge UDR types are designed to exchange data between the workflows. The Workflow Bridge UDR types can be viewed in the UDR Internal Format Browser in the 'wfb' folder. To open the browser, first, open an APL Editor, and, in the editing area, right-click and select UDR Assistance . The section contains the following subsections: ConsumeCycleUDR ErrorCycleUDR User Defined Action UDRs Workflow Execution State UDRs

---

# Document 1285: HTTP Batch Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739202/HTTP+Batch+Agent+Configuration
**Categories:** chunks_index.json

The HTTP Batch agent contains the following tabs: Connection Source Advanced Duplicate Check Conn ec tion Open HTTP batch collection agent configuration dialog - Connection tab Field Description Field Description URL URL to the file that will be collected, the full URL to a file must be given. Note! If collected file contains any links to other pages, these will only be followed if Index Based Collection is checked. Refer to Enable Index Based Collection in the the section below, Source. Username HTTP authorization username used in requests Password HTTP authori zat ion password used in requests Source Open HTTP batch collection agent configuration dialog - Source tab Item Description Item Description Compression Select if the agent should try to decompress the data collected before routing it into the workflow. The options are 'No Compression' and 'Gzip'. Note! If Enable Index Based Collection is selected, only the links in the given URL will be decompressed upon collection. Enable Index Based Collection Select to Enable Index Based Collection. All linked-to URLs found in the HTML-formatted document will be collected. The URL is pointed out in the URL field in the section above, Connection. URL Pattern Either leave empty or enter a regular expression filtering the full URL. If empty all files are collected, otherwise files matching the URL Pattern will be collected. The URL itself will not be routed into the workflow. Enable Control File Based Collection When selected, the agent will only collect files with a control file present. The appearance of the control file is made by defining Position and the appearance of the expected control file. Position The control filename consists of an extension added either before or after the shared filename part. There are two choices: Prefix or Suffix refer to the example below, Control File Extensions, for more information. Control File Extension The Control File Extension is used to define when the data file should be collected. A data file will only be collected if the corresponding control file exists. The text entered in this field is the expected extension to the shared filename. The Control File Extension will be attached to the shared filename depending on the setting made in the Position field, refer to the example below, Control File Extensions, for more information. Data File Extension The Data File Extension is an optional field that is used when a stricter definition of files to be collected is needed. It is only applicable if the Position is set to Suffix . Refer to the example below for more information. Example - Control File Extenstions Consider a directory containing 5 files: FILE1.dat FILE2.dat FILE1.ok ok.FILE1 FILE1 The Position field is set to Prefix and the Control File Extension field is set to ok. . The control file is ok.FILE1 and FILE1 will be the file collected. The Position field is set to Suffix and the Control File Extension field is set to .ok . The control file is FILE1.ok and FILE1 will be the file collected. The Position field is set to Suffix and the Control File Extension field is set to .ok and the Data File Extension field is set to .dat . The control file is FILE1.ok and FILE1.dat will be the file collected. Enable HTTP DELETE Selecting this will issue the web server to delete the file and the control file after the file has been successfully collected. If unchecked the file will be ignored after collection, that is the file will be left in on the webserver. Ad v anced Open HTTP batch collection agent configuration dialog - Advanced tab Item Description Item Description Use Security Profile checkbox Select to enable the use of security profile for HTTPS connection. Security Profile Select the security profile which the Java keystore is attached to. Read Timeout (ms) The timeout value, in milliseconds, to be used to wait for responses from the server. If set to 0, this means to wait forever. Duplicate C he ck Open HTTP batch collection agent configuration dialog - Duplicate Check tab The Duplicate Check feature is only used when Enable Index Based Collection found in the section above, Source, is enabled. Item Description Item Description Enable Duplicate Check When selected, the agent will store every collected URL in a (configurable) number of days. The storage will be checked to make sure that no URL is collected again as long as it remains in the storage. Database Profile Each collected URL will be stored in the database defined in the profile selected. The schema must contain a table called "duplicate_check", for more information about this table refer to HTTP Batch Appendix - Database Requirements for Duplicate Check . Max Cache Age (Days) The number of days to keep collected URLs in the database. When the workflow starts, it will delete entries that are older than this number of days. Note! If a duplicate-check workflow runs on more than one EC on separate servers, and the system clocks are not synchronized, there is a risk that UDR duplicates are prematurely deleted. For example: If two system clocks are 12 hours apart and Max Cashed Age is set to 1 day, duplicate UDRs might be deleted after only 12 hours, instead of 24.

---

# Document 1286: Alarm and Event Handling - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816735/Alarm+and+Event+Handling
**Categories:** chunks_index.json

Alarm management in the MediationZone Platform is designed with plug-and-play to state-of-the-art alarm handling systems in mind, but it also contains basic internal alarm handling. Alarms can be divided into two categories: Event/message based Event based alarms are handled via an internal event broker, the Event Notification configuration. It catches, filters, formats, and dispatches events to different targets. Examples of external targets are SNMP Traps and Alarms, database tables, and Azure Application Insight. Metric based Metric based alarms are handled indirectly via integration to Prometheus, which can be configured to regularly scrape MediationZone for time series data. These can then be used in Prometheus as input data to alarm configurations. The MediationZone time series data is available as automatically populated out-of-the-box MIMs, customized MIMs, and customized fully dynamical PrometheusUDRs. Open Alarm and event handling. The internal alarm management module has basic functionality, like enabling and disabling of conditional alarms of a specific set of types.

---

# Document 1287: HTTP Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656873/HTTP+Functions
**Categories:** chunks_index.json

The HTTP functions are used to exchange data over HTTP or HTTPS as either client or server. This chapter includes the following sections: HTTP Client Functions HTTP Client Helper Functions HTTP Server Functions

---

# Document 1288: SCP Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609296/SCP+Forwarding+Agent
**Categories:** chunks_index.json

The SCP forwarding agent forwards files to a remote host using the SCP protocol over SSH2. Upon activation, the agent establishes an SSH2 connection and an SCP session towards the remote host. On failure, additional hosts are tried, if configured. To ensure that downstream systems will not use the files until they are closed, they are maintained in a temporary directory on the remote host until the endBatch message is received. This behavior is also used for cancelBatch messages. If a Cancel Batch is received, file creation is canceled. The section contains the following sub-sections: SCP Forwarding Agent Configuration SCP Forwarding Agent Events SCP Forwarding Agent Input/Output Data and MIM SCP Forwarding Agent Memory Management SCP Forwarding Agent MultiForwardingUDR Input SCP Forwarding Agent Transaction Behavior

---

# Document 1289: Aggregation Session Inspector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998928
**Categories:** chunks_index.json

The Aggregation Session Inspector enables viewing and editing of existing sessions. The data is displayed in a table where each row represents a session, with the session data ordered in columns. It is also possible to edit the contents of the sessions, that is, to change the timeout and the values of the session variables. Aggregation Session Inspector only inspects sessions stored on disk and SQL. Note! Aggregation Session Inspector is only available in the Desktop interface. Open The Aggregation Session Inspector Note! To use the Aggregation Session Inspector, the EC specified as the Storage Host in the Storage tab of the Aggregation profile configuration must be up and running. Searching for sessions Initially, the table is empty and must be populated with data using the Search Sessions dialog. When you select the Search button, the Search Sessions dialog opens where you can select which group of sessions you want to view. The Search Sessions dialog Setting Description Setting Description Profile Select the Aggregation profile that corresponds to the data of interest. Timeout Period If you select this check box you can select a timeout period from which you want to display data. You can either select the User Defined option in the Period Type drop-down list and then enter date and time in the From and To fields, or you can select one of the predefined time intervals in the drop-down list; Last Hour , Today , Yesterday , This Week , Previous Week , Last 7 Days , This Month or Previous Month . UDR View Allows a more detailed view of the UDRs in the session list. For further information about UDR Views, see the Ultra Reference Guide . Table Toolbar The table toolbar for the Aggregation Session Inspector presents the users with more options to manipulate the session data within the table. The following are the buttons found on the bar: With no sessions selected With no sessions selected Open Option Description Search Displays the Search Sessions a dialog where search criteria may be defined to identify the group of sessions to be displayed, see Searching for sessions for more information. Refresh Refreshes the table. Validate Storage When you select the Validate Storage button, after performing a search, all the aggregation storage session files are validated. This is done by attempting to read the session data to establish what can and cannot be read. If the storage contains references to corrupt sessions, an option to remove them is given. Delete All Deletes all sessions. With one session selected Open Option Description Edit Timeout You can modify the session timeout value here. Changing the value to any time before the current time will have the session timeout during the next execution cycle on its corresponding workflow. Info! This field is only available when you are searching for sessions stored on disk. Sessions stored on SQL will not have this field enabled. Info! The session will acquire a lock from editing when you have opened this field to modify the timeout, preventing other users from modifying the information while you are editing it. The lock is released when you quit the dialog. Open Explore Session Displays a new dialog where the session variables may be viewed and if Read Only was disabled in the Search Session dialog, the session variables may be edited as well. Note! This field will be read-only for SQL Storage sessions. You will be able to explore the session, but you will not be able to edit it. Info! The session will acquire a lock from editing when you have opened this field to modify the session, preventing other users from modifying but not reading the information while you are editing it. The lock is released when you quit the dialog. An example of a UDR Viewer dialog: Open With one or more sessions selected Open Option Description Clear Selection(s) Unselects any highlighted sessions. Delete Permanently removes any selected sessions.

---

# Document 1290: SCIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816113
**Categories:** chunks_index.json

It is possible to use SCIM via the REST HTTP interface to POST, GET, DELETE, PUT and PATCH user and group configurations. This section will cover the schemas used to create, update and remove users and groups, as well as the limitations when using SCIM for MediationZone . For more information regarding the specifications for SCIM, please see RFC: https://tools.ietf.org/html/rfc7643 For information regarding the API endpoints, please see RFC: https://tools.ietf.org/html/rfc7644#section-3.2 Note! When importing the user configurations into MediationZone or when upgrading MediationZone, the users will be disabled after the import operation or the upgrade. To enable the users, you can use PATCH or PUT, a user with attribute active : true. You can also enable the user by ticking the checkbox for the users you want to enable from the User tab in the Access Controller on the MediationZone desktop. When creating a new user from SCIM, the user will be enabled by default. These are the limitations for using SCIM instead of the MediationZone desktop. Only users with write access for application Access Controller should be able to Add, update and delete users or groups. A user can only be created once using the HTTP method POST The password attribute is not mandatory when you create a user with POST, however the user will not be able to login to MediationZone without a password. All user details can be modified except the username. The users assigned group can only be updated using the HTTP method PUT When using PUT to assign a user's group, no default group will be selected. You can only POST an access group with same name one time, the group name can not be changed. It is not possible to set or change the applications connected to the access group using the HTTP methods available via SCIM, this is only possible using the desktop. Custom Schema MediationZone has an additional schema for the "User" resource. The Schema URI for it is: urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser The following attributes are defined: successor : The successor user takes over all configs when the current user is removed. value: The identifier of the successor user. display: A human-readable name, primarily used for display purposes. It is read-only. validityPeriod : The validity period of a user. Format is: yyyy-mm-ddThh:mm:ss from : The "DateTime" the user should be valid from. to: The "DateTime" the user should be valid to. User related APIs This section will cover all the REST HTTP APIs that are used for user related operations. Retrieving Users You can use this to retrieve all users: URL: http://<host>:9000/scim/api/v1/Users Method: GET Header: Accept: application/scim+json Content-Type: application/scim+json You can use this to retrieve a specific user: URL: http://<host>:9000/scim/api/v1/Users/14c257bd-e486-4ec6-b73e-47bb1e9b491b Method: GET Header: Accept: application/scim+json Content-Type: application/scim+json Creating Users You can use this to create a user: Info! The schemas and userName fields as shown below are mandatory. They must be filled in. The rest of the fields are optional URL: http://<host>:9000/scim/api/v1/Users Method: POST Header: Accept: application/scim+json Content-Type: application/scim+json Request Body: { "schemas":["urn:ietf:params:scim:schemas:core:2.0:User", "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser"], "userName":"bjensen", "displayName": "mz80u3", "password": "mz80u3", "active": "true", "emails": [ { "value": "b@b.com", "display": "bbb", "primary": true } ], "externalId":"bjensen", "name": { "formatted":"Ms. Barbara J Jensen III", "familyName":"Jensen", "givenName":"Barbara" }, "groups": [ { "value": "ed309a27-3f34-45d3-ade5-b2f8f798deb5" }, { "value": "86138dad-9742-44a2-a9cb-70347fb884a8" } ], "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser": { "successor": { "value": "71a36bb7-816f-460d-b580-3bd9352b0953" }, "validityPeriod": { "from": "2021-03-19T23:00:00Z", "to": "2021-03-23T22:59:59Z" } } } Updating Users You can use this to update all the values for a user: Info! The schemas and userName fields as shown below are mandatory. They must be filled in. The rest of the fields are optional URL: http://<host>:9000/scim/api/v1/Users/c9706a50-6fd3-44cf-8f8d-7ea00fb05f1c Method: PUT Header: Accept: application/scim+json Content-Type: application/scim+json Request Body: { "schemas": ["urn:ietf:params:scim:schemas:core:2.0:User", "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser"], "userName": "bjensen", "displayName": "mz80u3", "emails": [ { "value": "b@b.com", "display": "mz80u3", "primary": true } ], "groups": [ { "value": "119fe1b7-4b8b-4970-8ea6-b62bdaa11f05" }, { "value": "53aabe0b-715d-4d96-a220-56c6efc11ae9" } ], "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser": { "successor": { "value": "71a36bb7-816f-460d-b580-3bd9352b0953" }, "validityPeriod": { "from": "2021-03-20T23:00:00Z", "to": "2021-03-25T22:59:59Z" } } } You can use this to update specific values for a user: Info! The schemas, Operations, op and value fields as shown below are mandatory. They must be filled in. The rest of the fields are optional URL: http://<host>:9000/scim/api/v1/Users/c9706a50-6fd3-44cf-8f8d-7ea00fb05f1c Method: PATCH Header: Accept: application/scim+json Content-Type: application/scim+json Request Body: { "schemas":["urn:ietf:params:scim:api:messages:2.0:PatchOp", "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser"], "Operations":[ { "op":"add", "value": { "emails":[ { "value":"babs@jensen.org", "type":"home" } ] } }, { "op": "add", "path": "urn:sap:cloud:scim:schemas:extension:custom:2.0:mzuser:validityPeriod", "value": { "from": "2021-03-19T23:00:00Z", "to": "2021-03-23T22:59:59Z" } } ] } Removing Users You can use this to remove a user: URL: http://<host>:9000/scim/api/v1/Users/c9706a50-6fd3-44cf-8f8d-7ea00fb05f1c Method: DELETE Header: Accept: application/scim+json Content-Type: application/scim+json Group related APIs This section will cover all the REST HTTP APIs that are used for group related operations. Retrieving Groups You can use this to retrieve all groups: URL: http://<host>:9000/scim/api/v1/Groups Method: GET Accept: */* Content-Type: */* You can use this to retrieve a specific group: URL: http://<host>:9000/scim/api/v1/Groups/119fe1b7-4b8b-4970-8ea6-b62bdaa11f05 Method: GET Accept: */* Content-Type: */* Creating groups You can use this to create a group: Info! The schemas and userName fields as shown below are mandatory. They must be filled in. The rest of the fields are optional URL: http://<host>:9000/scim/api/v1/Groups Method: POST Accept: */* Content-Type: */* Request body: { "schemas":["urn:ietf:params:scim:schemas:core:2.0:Group"], "displayName":"group2", "members":[ { "value":"a12822ad-a5c0-4f83-9a4e-96733a0d2e1b" }, { "value":"8792b456-860a-499d-aa38-5caf4fe487c3" } ] } Updating Groups You can use this to update a group: Info! The schemas and userName fields as shown below are mandatory. They must be filled in. The rest of the fields are optional URL: http://<host>:9000/scim/api/v1/Groups/a85d8e8c-0b6d-4653-b7c6-33c1fd6c1921 Method: PUT Accept: */* Content-Type: */* Request body: { "schemas":["urn:ietf:params:scim:schemas:core:2.0:Group"], "displayName":"group2", "members":[ { "value":"a12822ad-a5c0-4f83-9a4e-96733a0d2e1b" }, { "value":"8792b456-860a-499d-aa38-5caf4fe487c3" } ] } Deleting Groups You can use this to delete a group: URL: http://<host>:9000/scim/api/v1/Groups/a85d8e8c-0b6d-4653-b7c6-33c1fd6c1921 Method: DELETE Accept: */* Content-Type: */*

---

# Document 1291: MQTT Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653302/MQTT+Example
**Categories:** chunks_index.json

This section contains one example for the MQTT agent. MQTT workflow example In the above example, the MQTT agent sends any of these 3 UDR types: Error, PublishAck or SubscribeResponse to the Analysis agent, which contains the following code: consume { debug(input); if(instanceOf(input, MQTT.SubscribeResponse)){ MQTT.SubscribeResponse resp = (MQTT.SubscribeResponse)input; MQTT.Publish pub = udrCreate(MQTT.Publish); pub.qos = 1; pub.retain = true; pub.topic = "test/"; pub.data = resp.data; udrRoute(pub); } } With this code, the Analysis agent will: Debugs the output of the MQTT agent. If the received UDR is of the SubscriberResponse type, the UDR will be populated into a SubscribeResponse type UDR called resp. Create a UDR of Publish type called pub. Set the qos to 1, to indicate that the message will be published with the At Least once QoS. Set retain to true. Set the topic to test/ Populate the Data with the Data field from the resp UDR. Routes the pub UDR back to the MQTT agent.

---

# Document 1292: Agent Executable - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676543/Agent+Executable
**Categories:** chunks_index.json

The Executable class performs the execution part of an agent. If it is a Collection agent, it routes data into the workflow, and if it is a Processing or Forwarding agent, it consumes data from the previous agent in the workflow. There are two branches for agent executables, one for batch and one for realtime workflows. The different types will be explained separately. They both share the same abstract class DRAgent . Open Executable This section includes the following sections: DRAgent DRBatchAgent DRBatchCollector DRBatchProcessor DRObjectRouter DRRealtimeAgent DRRealtimeAsynchRouter DRRealtimeCollector DRRealtimeProcessor DRRealtimeThreadAgent

---

# Document 1293: Amazon S3 Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639973/Amazon+S3+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The agent consumes bytearray or MultiForwardingUDR types. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Value Description MultiForwardingUDR's FNTUDR This MIM parameter is only set when the agent expects input of MultiForwardingUDR type. The MIM value is a string representing the sub path from the output root directory on the target file system. The path is specified by the fntSpecification field of the last received MultiForwardingUDR . For further information on using input of MultiForwardingUDR type, refer to the section, MultiForwardingUDR Input, in Amazon S3 Forwarding Agent Configuration . This parameter is of the string type and is defined as a batch MIM context type. File Transfer Timestamp This MIM parameter contains a timestamp, indicating when the target file was created in the temporary directory. File Transfer Timestamp is of the date type and is defined as a trailer MIM context type. Target Filename This MIM parameter contains the name of the target filename, as defined in Filename Template . Target Filename is of the string type and is defined as a trailer MIM context type. Target Template Pathname This MIM parameter contains the name of the target pathname, directories and filename, as defined in Filename Template . Target Template Pathname is of the string type and is defined as a trailer MIM context type. Target Pathname This MIM parameter contains the path to the output directory, as defined in the Disk tab. Target Pathname is of the string type and is defined as a global MIM context type. Accesses The agent accesses MIM parameters in the Filename Template configuration to construct the target filename.

---

# Document 1294: HTTPD_Deprecated Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000593/HTTPD_Deprecated+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent consumes and produces UDR types extended with the built-in HTTP format. MIM For information about the MIM and a list of the general MIM parameters, see MIM . The agent does not publish nor access any MIM parameters.

---

# Document 1295: Properties for Oracle - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029572/Properties+for+Oracle
**Categories:** chunks_index.json



---
**End of Part 54** - Continue to next part for more content.
