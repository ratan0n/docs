# RATANON/MZ93-DOCUMENTATION - Part 38/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 38 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~65.4 KB
---

The Inter Workflow profile enables you to configure the storage server that the Inter Workflow forwarding and collection agents use for communication. It is safe to accumulate a lot of data in the storage server directory. When the initial set of directories has been populated with a predefined number of files, new directories are automatically created to avoid problems with file system performance. You can configure the Inter Workflow profile for the following storage types: Database Storage File Storage The Inter Workflow profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Note! Files collected by the Inter Workflow agent depend on and are connected with the Inter Workflow profile in use. If an Inter Workflow profile is imported to the system, files left in the storage connected to the old profile will be unreachable. Configuration To create a new Inter Workflow profile configuration, click the Build  New Configuration in the upper part of the Desktop window, and then select Inter Workflow Profile from the menu. Open InterWorkflow Profile with Database Storage configuration Open InterWorkflow Profile with File Storage configuration The contents of the menus in the menu bar may change depending on which configuration type is opened. The Inter Workflow profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The External References button in the menu bar is specific to Inter Workflow profile configurations. Button Description Open Select this menu item to enable External References in an agent profile field. For further information, see the section Enabling External References in an Agent Profile Field in External Reference Profile The Inter Workflow profile configuration contains the following settings: Setting Description Available when.. Storage Type From the drop-down list, select either File Storage or Database Storage. Using Database Storage means that a database will be used as storage instead of the EC file system. When choosing this option, several new fields will be displayed. Always available Database Profile Click Browse to select the Database Profile you want to use for the Inter Workflow Profile. Note! Currently, the SQL storage only supports PostgreSQL, SAP HANA, and Oracle databases. Database storage is configured Connection Pool Size Specify the number of simultaneous connections for communication to the database. This value determines the size of the connection pool used by Interworkflow agents when accessing the database. Adjust the size based on the expected load and concurrency needs of your solution. Database storage is configured Show SQL Statement When you select Database Profile , this button will print the SQL statements needed to create the tables and indexes required for the Inter Workflow Profile. You can copy and paste this into your database management software. Note! You will have to copy the SQL script generated in the text box to create the PostgreSQL, SAP HANA, or Oracle tables on their own in the database listed in the Database profile. The Inter Workflow profile will not automatically create the tables for you. Database storage is configured Copy to Clipboard This button will copy the text in the text area below, which was generated by the Show SQL Statement button, to the clipboard on your computer. Database Storage is configured Root Directory The directory's absolute pathname on the storage handler where the temporary files will be placed. If this field is greyed out with a stated directory, it means that the directory path has been hard-coded using the mz.present.interwf.storage.path property. This property is set to false by default. Example - Using the mz.preset.interwf.storage.path property To enable the property and state the directory to be used: mzsh topo set val:common.mz.preset.interwf.storage.path '/mydirectory/iwf' To disable the property: mzsh topo unset val:common.mz.preset.interwf.storage.path For further information about all available system properties, see System Properties . File Storage is configured Storage Host From the drop-down list, select either Automatic or an activated EC group. Using Automatic means that the storage will use the EC Group where the first workflow accessing this profile is started. Following workflows using the same profile will use the same EC Group for storage until the first workflow accessing the profile is stopped. The EC Group where the next workflow accessing this profile is started will then be used for storage. The location of the storage will therefore vary depending on the start order of the workflows. Example - Automatic storage host Below is an example of a scenario where Automatic is used as a storage host with the following setup: Workflow 1 is running on EC Group 1 with the Inter Workflow Forwarding agent Workflow 2 is running on EC Group 2 with the Inter Workflow Collection agent Workflow 2 is started. EC Group 2 is used for storage Workflow 1 is started. EC Group 2 is still used for storage. Workflow 1 is stopped. EC Group 2 is still used for storage. Workflow 2 is stopped. No EC Group is used for storage. Workflow 1 is started. EC Group 1 is used for storage. Note! The workflow must be running on the same EC as its storage resides. If the storage is configured to be Automatic, its corresponding directory must be a shared file system between all the EC Groups. Always available Max Bytes An optional parameter stating the limit of the space consumed by the files stored in the Root Directory or Database. If the limit is reached, any Inter Workflow forwarding agent using this profile will abort. Always available Max Batches An optional parameter stating the maximum number of batches stored in the Root Directory or Database. If the limit is reached, any Inter Workflow forwarding agent using this profile will abort. Always available Compress intermediate data Select this checkbox if you want to compress the data sent between the Inter Workflow agents. The data will be compressed into *.gzip format with compression level 5. Always available Named MIMs A list of user-defined MIM names with no values assigned. They are populated with existing MIM values from the Inter Workflow forwarding agent. This way, MIMs from the forwarding workflow can be passed on to the collecting workflow. Always available

---

# Document 850: Amazon S3 Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606297/Amazon+S3+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to configurations done in an Event Notification configuration. For further information about the agent message Ready with file: name Reported along with the name of the target file when it has been successfully stored in the target directory. If an After Treatment Command is specified, the message also indicate that it has been executed. Debug Events There are no debug events for this agent.

---

# Document 851: wfimport - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/298647583/wfimport
**Categories:** chunks_index.json

usage: wfimport [-keepOld [yes|no]] <workflow configuration> <export file> [workflow configuration password] This command updates the specified workflow configuration by importing workflows that are defined in the export file. Example - wfimport The workflow configuration Default.disk_collection workflow configuration is updated with with imported data from the file wf_disk_collection.csv . $ wfimport Default.disk_collection wf_disk_collection.csv Option/Parameter Option/Parameter [ -keepOld [yes|no] ] Set this option to yes to retain the existing workflow table data that is not updated by the export file. Set it to no to remove such data from the workflow table. The default value is no . <workflow configuration> The workflow configuration that you want to be updated by the export file. <export file> The export file name. For further information about the export file see https://infozone.atlassian.net/wiki/spaces/MD93/pages/298614810 . Note! wfimport imports the file formats: CSV (Comma Separated Value), SSV (Semicolon Separated Value), and TSV (Tab Separated Value). Text strings within each value are delimited by a quotation mark ( " ). Exported fields that contain profiles, are assigned with a unique string identifier. The ID and Name fields are exported, as well. [workflow configuration password] To import a password protected workflow configuration, specify a password. Return Codes The following is a list of return codes for the wfimport command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the number of arguments is incorrect. 1 Will be returned If login credentials are incorrect. 1 Will be returned if configuration permission is denied. 2 Will be returned if the import file does not exist. 2 Will be returned if the import file directory does not exist. 3 Will be returned if the import file cannot be read (read permission). 5 Will be returned if the import file has an incorrect file suffix. 6 Will be returned if the configuration name is incorrect 7 Will be returned if the configuration does not exist. 8 Will be returned if the configuration is already locked. 9 Will be returned if an encryption passphrase is needed. 10 Will be returned if the user does not have the read and write permission to modify the workflow. 11 Will be returned if the configuration could not be loaded. 12 Will be returned if the import fails (refer to the logs for details).

---

# Document 852: Oracle Database Creation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029608/Oracle+Database+Creation
**Categories:** chunks_index.json

The creation of the database must be made by the oracle UNIX user, or another UNIX user with the same privileges, on the machine where Oracle is installed. The oracle UNIX user is assumed in this installation. Make sure that all previously defined directories, specified in the install.xml file, exist and are writable for the oracle user, then follow these steps: Make sure the environment variables for Oracle are set. For further information, see Setting Environment Variables for Platform . Make sure the Oracle data directories exist. These directories must be owned by the Oracle user. For further information see Properties for Oracle . Configure and start an Oracle listener for the database instance. The working directory for the listener is $ORACLE_HOME/network/admin . Edit the listener.ora file to match what is set in install.xml . See Appendix A - Oracle Home Settings for an example. When the listener is configured, it needs to be restarted: $ lsnrctl stop $ lsnrctl start Create the Oracle database instance. The working directory is oracle . $ source ./oracle_create_instance.sh Depending on the hardware and the size of the database, this command may take up to one hour to execute. If no error occurs, the database creation is now complete and the rest of this section can be ignored. Hint! After the instance creation script is executed, there will be three log files in the /tmp directory that can be used for troubleshooting. The mz.log file contains Oracle responses from the database creation, the mzuser.log file contains Oracle responses from the user creation, and the mz_db.log file contains Oracle responses from the table creations. If the script fails, cleanup the system before the next try: Start SQL*Plus: $ sqlplus "/ as sysdba" Terminate the bad instance: SQL> shutdown abort SQL> quit Remove any files created by Oracle in any of the previously defined directories for Oracle data. If there is a need to change any installation parameter defined in the file install.xml , the installation must be restarted from the generation step. For further information, see Generation of Oracle Database Definition Files above.

---

# Document 853: Execution Manager - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030703
**Categories:** chunks_index.json

With Execution Manager you can enable, activate, and monitor multiple workflow groups. To open the Execution Manager, go to Manage  Tools & Monitoring and then select Execution Manager . Open The Execution Manager The Execution Manager contains three tabs, Overview , Running Workflows , and Detail Views . You can open a separate Detail View tab for every workflow group or groups, that you want to monitor. Note! The times displayed in the Start time and Next run columns are based on the timezone of the Desktop Client. Overview Tab The Overview tab is comprised of the Status box and the workflow groups table. Status Log In the Status Log, you see a log of events for workflows and workflow groups, which are also displayed in the https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639211 . Workflow Groups Table The Workflow Groups Table displays the following information about all the workflow groups that are configured in the system: Column Description Open Selection Checkbox The selection check boxes can be used for selecting specific entries that you want to perform actions for. When selecting a check box, the table action bar will change and display a number of buttons you can use to perform different actions, see Table Action Buttons and Right-Click Menu . Name This column displays the workflow group name. Mode This column shows whether the workflow group is enabled or disabled. If the workflow group is enabled it can be activated by its scheduling criteria. If it is disabled, the workflow group can only be started manually. State This column displays the current state of the workflow group, see https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605010 . Runtime Info This column displays detailed information about the workflow group state. Started By This column displays how the workflow group was started: Schedule - based on the workflow group scheduling criteria A user The parent workflow group Workflow name if it is auto-started Note! The value will be Unknown if the Platform has been restarted. Start Time This column displays the last time an execution was started. The time will be based on the timezone of the Desktop client. Next Run This column displays the next scheduled execution time. The time will be based on the timezone of the Desktop client. Note! If the workflow is not enabled this space column will be empty. Next Suspend Action This column displays the suspend action which is either the scheduled execution suspension of a configuration (workflow or workflow group) or the removal of such a suspension (activation enabling). Table Action Buttons and Right-click Menu When selecting one or multiple check boxes for workflow groups, the table action bar will change and display a number of buttons you can use to perform different actions. Right-clicking a row in the table opens a menu with the same options although they will only apply for the workflow group you right-click, that is only one single group. The following actions are available: Action Description Clear Clears all selected check boxes. This option is only available in the Table Action Bar since right-clicking a workflow group will always a one-group operation. Detail View/Open in Detail View Opens the workflow group(s) in a separate View tab in the Detail Views tab. Start Triggers execution of the selected workflow group(s). Stop Stops the execution of the selected workflow group(s). Enable Enables the selected workflow group(s). Disable Disables the selected workflow group(s). Note! In the Table Action Bar, this option is available when clicking on the three dots to the right in the panel and selecting it in the menu that is displayed. View Abort Message Opens an error dialog box that specifies the reason for aborting the execution of the particular workflow group or its workflow member. Note! In the Table Action Bar, this option is available when clicking on the three dots to the right in the panel and selecting it in the menu that is displayed. Open Editor Opens the workflow group configuration. Running Workflows Tab The Running Workflows tab displays the workflows that are currently running as well as the ones that are unreachable. Open Note! To stop a workflow, right-click on the workflow and then select Stop . You can also open the Workflow Monitor by selecting Open Monitor . The Running Workflow tab table contains the same columns as the Overview tab, as well as the following: Column Description Name This column displays the name(s) of the running workflow(s). Status This column displays the status of the workflow(s), see Workflow Execution States . EC This column displays the pico name of the EC on which the workflow is running. Debug This column displays whether debug is turned on or off, see Workflow Monitor . Backlog This column displays the number of files that are yet to be processed. The value in the Backlog column is identical to the Source Files Left MIM value. Throughput This column displays the throughput for the workflow's collecting agent. The value shows either number of UDRs or bytes, depending on what the collecting agent produces, and is updated every five seconds as long as the workflow is being executed. Detail Views Tab The Detail Views tab displays the workflow group(s) that you have selected to view details for in the Overview tab. Note! Detail views are saved as part of the user preferences, and therefore enable you to export and import them along with user information. A workflow group that is marked with a yellow warning icon in the Detail View is invalid. The table in this tab contains the same columns as the Running Workflows tab as well as the following: Column Description Prereq This column displays a comma-separated list of the workflow group Prerequisites settings. See Members Execution Order in Managing a Workflow Group . Next Suspend Action The suspend action is either the scheduled execution suspension of a configuration (workflow or workflow group), or the removal of such a suspension (activation enabling). Right-click Menu If you right-click on a row in the table, a menu opens with the following options: Entry Description Debug On/Off This option turns debug on or off for the selected workflow, see Workflow Monitor . Open in Monitor This option opens the selected workflow in the Workflow Monitor. Note! This option is not applicable to workflow groups. Opening a Detail View for a Workflow Group To open a Detail View of a workflow group: In the Overview tab, right-click the workflow group, or select the check boxes for a number of workflow groups. Select Open Detail View option in the right-click menu, or click on the Detail View button in the table action bar. The New Detail View dialog box appears. Open New Detail View dialogue Enter a unique name for the view and click OK. The Detail Views tab opens and displays the selected workflow groups and their members in a separate table. Open Detailed View tab Managing Detail Views Click on the Manage Detail Views button to set the visibility of the Detail Views of workflow groups. To Manage Detail View Tabs: Select Manage Detail Views ; the Manage Detail Views window opens. Open Manage Detail Views Select the Visible checkbox to set the visibility of Detail Views of each workflow group. In the table, you can click the Refresh button to refresh the Detail Views table or click the Delete button to delete the Detail Views table of the workflow group.

---

# Document 854: Encoder Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999815/Encoder+Agent
**Categories:** chunks_index.json

The Encoder agent converts internal UDRs to raw data. It is a processing agent for batch and real-time workflow configurations. The main difference between these two modes of operation is that within a batch workflow, the Encoder agent is capable of adding headers and trailers. For more information on the Ultra format, see Introduction to the Ultra Format Definition Language . Loading

---

# Document 855: SFTP Collection Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653995/SFTP+Collection+Agent+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about the SFTP collection agent transaction behavior. For information about the general transaction behavior, see Transactions in Workflow Monitor . Emits The agent emits commands that changes the state of the file currently processed. Command Description Begin Batch Will be emitted just before the first byte of each collected file is fed into a workflow. End Batch Will be emitted just after the last byte of each collected file has been fed into the system. Retrieves The agent retrieves commands from other agents and, based on them, generates a state change of the file currently processed. Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior defined on workflow level is configured to abort the workflow, the agent will never receive the last Cancel Batch message. In this situation, ECS will not be involved, and the file will not be moved, however left at its current place. APL code where Hint End Batch is followed by a Cancel Batch will always result in workflow abort. Make sure to design the APL code to first evaluate the Cancel Batch criteria to avoid this sort of behavior. Hint End Batch If a Hint End Batch message is received, the collector splits the batch at the end of the current processed block (as received from the server), provided that no UDR is split. If the block end occurs within a UDR, the batch will be split at the end of the preceding UDR. After a batch split, the collector emits an End Batch Message, followed by a Begin Batch message (provided that there is more data in the subsequent block).

---

# Document 856: TCP/IP Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654295/TCP+IP+Collection+Agent
**Categories:** chunks_index.json

The TCP/IP collection agent allows data to be collected and inserted into a workflow, using the standard TCP/IP protocol. Several connections at a time are allowed. It is also possible to send responses back to the source in the form of a bytearray, or in the case of several connections, as a UDR containing a response field. All response handling is done through APL commands. The TCP/IP collection agent supports IPv4 and IPv6 environments. Open A TCP/IP workflow may be configured to send responses to the source Upon activation, the collector binds to the defined port and awaits connections to be accepted. Note the absence of a Decoder in the workflows. The collector has built-in decoding functionality and supports any format as defined in the Ultra Format Editor . The section contains the following subsections: TCP/IP Related UDR Types TCP/IP Collection Agent Configuration The TCP/IP Format TCP/IP Collection Agent Input/Output Data and MIM TCP/IP Collection Agent Events

---

# Document 857: Hash and Checksum Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646277/Hash+and+Checksum+Functions
**Categories:** chunks_index.json

The following hash and checksum functions are available in APL: Checksum functions- Generates a checksum based on the cksum or CRC-32 algorithms for a supplied argument. Hash functions - Uses the algorithms MD2, MD5, SHA-1, SHA-256, SHA-384, or SHA-512 to t urn input of arbitrary length into an output of fixed length This chapter includes the following sections: Checksum Functions Hash Functions

---

# Document 858: GCP BigQuery Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607756/GCP+BigQuery+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, generated according to the configuration in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug event: Caught IOException: error message Indicate that there is error when extracting credentials information from json file configured in GCP profile

---

# Document 859: Hash Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656855/Hash+Functions
**Categories:** chunks_index.json

Use the functions below to t urn bytearray input of arbitrary length into an output of fixed length The following functions for Hash described here are: 1 hashDigest 2 hashReset 3 hashUpdate 4 hashSetAlgorithm 5 hashSetAlgorithmFromProvider hashDigest Completes the hash computation by performing final operations such as padding. After this call is made the digest is reset. bytearray hashDigest(); Parameter Description Parameter Description Returns A hexadecimal hash sum hashReset Resets the digest for further use. void hashReset(); Parameter Description Parameter Description Returns Nothing hashUpdate Updates the digest using the specified array of bytes. void hashUpdate(bytearray); Parameter Description Parameter Description bytearray Any type of bytearray Returns Nothing hashSetAlgorithm Uses a digest that implements the specified digest algorithm. Note! The default setting, that is if the algorithm is not set, the SHA-1 hash method is used. void hashSetAlgorithm(<"hash_method">); Parameter Description Parameter Description <"hash_method"> The name of the algorithm requested. Supported hash algorithms are: MD2, MD5, SHA-1, SHA-256, SHA-384, SHA-512 Returns Nothing. hashSetAlgorithmFromProvider Uses a digest that implements the specified digest algorithm, as supplied from the specified provider. The digest will be reset. void hashSetAlgorithmFromProvider(string,string); Parameter Description Parameter Description <"unique_hash"> The name of the hash algorithm developed by the provider Provider The name of the provider Returns Nothing Hash Example Example - Hash initialize { hashSetAlgorithm("MD2"); } beginBatch { hashReset(); } consume { hashUpdate(input); } drain { bytearray hashsum = hashDigest(); string sHashSum = baToHexString(hashsum); // Optional bytearray result; // Optional strToBA(result, sHashSum); // Optional udrRoute(result); } Note! The optional parts in the example are added to convert the hash sum to a more readable form.

---

# Document 860: Function Blocks - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612136/Function+Blocks
**Categories:** chunks_index.json

APL code is divided into different function blocks that serve as execution entry-points that are applicable to the various workflow states. The following function blocks can be found in this page: An example of how function blocks are used can be found here: initialize The initialize function block is executed once for each invocation of the workflow and enables you to assign, for example, an argument with an initial value. Note! Avoid reading MIM parameters from within initialize . The order by which agents are initialized is undefined and MIM parameters are therefore not necessarily set during the initialize phase. The udrRoute function cannot be used in the initialize block. beginBatch and endBatch Note! beginBatch and endBatch are applicable for batch workflows only. The beginBatch and endBatch function blocks are executed at the beginning and end of each batch respectively. Rules are that the beginBatch block is called when a batch collection agent emits a Begin Batch call. This occurs either at file start, or when a hintEndBatch call is received from any agent capable of utilizing APL code. See hintEndBatch in Workflow Functions for more information. The endBatch block is called every time a batch collection agent emits an End Batch. This occurs either at file end, or when a hintEndBatch call is received from any agent capable of utilizing APL code. Note! The udrRoute function cannot be used in the beginBatch and endBatch blocks. consume The consume function block is executed for each UDR or bytearray passing the agent. Within a consume block, validation, modification and routing can be performed. Each UDR or bytearray is referred to by the special input variable. Built-in Variables Variable Description Example input Read-only variable containing the current UDR. Only available in the consume function block. input.ANumber = 1234567; udrRoute(input); When handling several types of UDRs in the same Analysis agent, the APL code must first determine what type is currently handled, then cast it to the correct type. For an example, see Data types . drain Note! Drain is applicable for batch workflows only. The drain function block is executed right before an endBatch block, and is treated as a final consume block. For instance, if a batch containing ten UDRs is processed by the agent, consume will be executed ten times before the drain function block is called. This is useful, for instance when collecting statistical data for a batch which is to be routed as a new UDR. The advantage with drain is that all consume features (except for the input built-in variable) are accessible, as opposed to the endBatch function block. Example - How to use drain int UDRCounter; int file_count; consume { UDRCounter=UDRCounter+1; udrRoute(input); } drain { myFolder.myUFDLFile.myTrailerFormat myTrailer; myTrailer=udrCreate(myFolder.myUFDLFile.myTrailerFormat); myTrailer.closingdate=dateCreateNow(); myTrailer.numberOfUDRs=UDRCounter; myTrailer.sourceFileName=(string)mimGet("Disk_1", "Source Filename"); udrRoute(myTrailer); } endBatch{ file_count = file_count + 1; debug( "Number of UDRs in file:" + UDR_count ); } cancelBatch Note! cancelBatch is applicable for batch workflows only. The cancelBatch function block is executed if a Cancel Batch is emitted anywhere in the workflow. Note that End Batch and Cancel Batch are mutually exclusive - only one per batch can be executed. If the cancelBatch function block is called and the Cancel Batch behavior is set to Abort Immediately the workflow will immediately abort without the cancelBatch function block being called. The block is only called when the preferences are set to Abort After or Never Abort . For further information about the Abort related configurations, see Workflow Properties in the Desktop user's guide. commit Note! commit is applicable for batch workflows only. The commit function block is executed for each batch when the transaction is successful. In a commit block, actions that concern transaction safety can be performed. The transaction is referred to by the special TransactionDetails UDR that contains the transaction id. The udrRoute function cannot be used in the commit block. Built-in Variables Variable Description Example transaction This is a read-only variable containing the current transaction. The variable is available in the commit and rollback function blocks. debug("commit of txn " + transaction.id); rollback Note! Rollback is applicable for batch workflows only. The rollback function block is executed for each batch when a transaction fails. In a rollback block, actions that concern transaction safety can be performed. The transaction is referred to by the special TransactionDetails UDR that contains the transaction id. Note! If a transaction fails during commit, it will try to commit again, and will not be sent to the rollback block. The udrRoute function cannot be used in the rollback block. deinitialize The deinitialize function block is executed right before the workflow stops. If the deinitialize block is used in a real-time workflow it could be used to clean and close resources, for instance external connections. exceptionHandling The exceptionHandling function block enables you to divert exceptions from the workflow's main processing course to a separate course, where exceptions are processed according to your needs. exceptionHandling in Batch Workflows For example: When a workflow occasionally aborts due to an exception in the APL code, use exceptionHandling to cancel the batch. Example - Using exceptionHandling to cancel a batch consume { int a = 1; int b = 0; debug("The following row will generate an exception"); float c = a/b; } exceptionHandling { debug("Type: "+ exception.type); debug("Message: " + exception.message); debug("Stacktrace: " + exception.stackTrace); cancelBatch("Exception caught", exception); } Note! The exceptionHandling function block for batch workflows is a legacy statement that is retained for backward compatibility. It is recommended that you use try-catch, throw and finally statements instead. These are described in the next section. Exception Handling in Batch and Real-Time Workflows The try-catch , throw and finally statements can be used to handle exceptions in batch and real-time. You use a try-catch block if the statements within the try block might throw an exception. try-catch blocks can be nested. The try block is followed by a catch block, which specifies the type of exception that it handles. You can route any UDR with udrRoute within the catch block. If an exception is thrown, the code jumps to the catch block, which in this case handles all Java class Exceptions. Note that the catch in this case does not catch Throwable Exceptions. A throw block throws an exception to the catch block which handles the exception. A finally block is at the end of a catch block and runs a clean-up. Note! The use of these Exception Handling statements only applies to APL. For example: When an exception is thrown due to division by zero, use the try-catch statement to catch the thrown exception. It is also possible to use the throw statement to throw the exception again. Use finally to ensure a clean-up is done. Example - Using the try-catch statement consume { int b = 0; try { debug("Hello: " + 100/b); } catch (ExceptionDetails exception) { debug("EXCEPTION: " + exception.message); // To throw exception caught, write 'throw exception'. throw testCreate(); // This will cause execution of the (legacy) exceptionHandling if declared (only available in batch workflows), otherwise the workflow aborts. } finally { debug("In finally clause " + b); } } ExceptionDetails testCreate() { ExceptionDetails ed = udrCreate(ExceptionDetails); ed.message = "DONE"; return ed; } ExceptionDetails The ExceptionDetails UDR stores the information for an exception that is caught in a catch block in batch or real-time. Field Description message (string) The message included in the exception stackTrace (string) The function call hierarchy from where the exception occurred type (string) Type of exception OriginalData (bytearray) This field is always empty. Function Blocks Example Example - Function Blocks Example int file_count; int UDR_count; initialize { file_count = 0; } beginBatch { UDR_count = 0; } consume { udrRoute( input ); UDR_count = UDR_count + 1; } endBatch { file_count = file_count + 1; debug( "Number of UDRs in file:" + UDR_count ); } deinitialize { debug(" Number of executed files:" + file_count ); } exceptionHandling { debug(" Exception occurred, stacktrace: " + exception.stackTrace); }

---

# Document 861: System Topology Registry - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647037
**Categories:** chunks_index.json

The STR is used for managing pico configurations, service configurations, and attributes that control the behavior of MediationZone. The STR data is stored in MZ_HOME/common/config/cell and consists of the following: Master registry - The master registry is used for staging changes in the STR. You can update the files in the master registry manually in an editor or using an mzsh command. Active registry - When you are using the mzsh command topo to update the master registry, the changes are automatically validated and propagated to the active registry. The active registry files are used when you start a pico process. These files are hidden and should not be edited directly by users. Backup registry - When the active registry is updated the previous data is stored in the backup registry files. These files are hidden and should not be edited directly by users. Templates - Templates contain pico definitions and default properties. A template have a namespace based on its parent directory. For instance the namespace of the predefined templates is mz . Containers - Containers are used by the STR to reference a MediationZone installation on a host. Container Groups - A container group contains the configuration of picos that can be started on a set of containers. The STR currently supports a single pre-defined container group named default . Pico configurations in this group can be started in any container in the system. Cells - A cell contains a set of Container Groups and can be considered synonymous with "system". At the time of writing, only one predefined cell ( default ) is available. Open STR structure - MZ_HOME/common/config/cell This chapter includes the following sections: HOCON Format STR File Structure STR Replication Working with STR

---

# Document 862: Execution Context Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816064
**Categories:** chunks_index.json

This section describes the different properties that you can use in the STR to configure ECs. Aggregation Properties You can set the properties listed in the Advanced tab of the Aggregation profile in the STR. This will override the values that are set in the profile, including default values. The following properties are applicable when using file storage for aggregation. Property Description Property Description mz.aggregation.storage.maxneedssync Default value: 5000 This property defines when updated aggregation sessions shall be moved from the cache to the file system. When the number of cached aggregation sessions reaches this value, the sessions are written to disk. This property shall be set to a value lower than "Max Cached Sessions", configured in the Aggregation agent. If the property is set to a higher value, the used value will be "Max Cached Sessions" minus 1. For example, if "Max Cached Sessions" is "6000" and this property is set to "6500", the value "5999" will be used instead. For performance reasons, this property should be given a reasonably high value, but consider the risk of a server restart. If this happens, the cached data might be lost. mz.aggregation.storage.profile_session_cache Default value: false This property is used to speed up the start of workflows that run locally (on the EC). Set the property to true to keep the aggregation cache in memory for up to 10 minutes after a workflow has stopped. mz.aggregation.timeout.threads Default value: 0 This property is used to enable Multithreading in Aggregation workflows, that is, to use a thread pool for the timeout function block in the Aggregation agent. Use a value larger than 0, for example, 4 to use four threads in the thread pool. For more information about how to use these properties to tune the performance of the Aggregation agent, see Aggregation Agent in the Desktop User's Guide . EC Web Interface Properties Property Description Property Description ec.webserver.enabled Default value: true This property specifies if the Web Interface for ECs should be active. ec.webserver.host Default value: Taken from the value of the common property pico.rcp.server.host . This property specifies the host used to communicate with an EC Web Interface. Set it to 0.0.0.0 to bind all addresses on the currently used host. This is done to access the Web Interface by both virtual and logical IP addresses or hostnames. ec.webserver.port Default value: 9090 This property specifies the TCP port of the EC Web Interface. Note! Each EC that is located in the same container must have a unique port. mz.wi.cross-origin.allowedOrigins Default value: "*" This property allows you to use RESTful Web Services across different origins. The possible value is a comma-separated list of the hosts:ports that are permitted to use the RESTful Web Services, e.g. 192.168.170.12:6790,10.46.9.26:9000 Couchbase Properties Property Description Property Description mz.cb.statistics.flush.period.in.seconds Default value: 60 If you have enabled the log-to-file functionality in the Advanced tab of the Couchbase profile, you can determine how often in seconds you want to flush this file by modifying the value. Database Properties Property Description Property Description max.cached.prepared.statements Default value: 15 If you want to change the number of preparedStatements that are cached between connections, you can use this property to specify a different number than the default. Note! If you are using SAP HANA as a database type in a Database profile, used by Database agents, this value has to be set to 0. connectionpool.strict.pool.size Default value: false If you enable this property, the database pool enables the Blocking Queue implementation where there is a strict restriction to the maximum number of connections. Note! When the property is set to false , then the database connection pool will create additional temporary connections. When the pool is full, these connections are closed as soon as the workflow thread returns the connection to the pool. This property is general for all supported databases. It is enabled per each EC separately. If enabled the connection pool for the particular EC have a strict maximum limit. The maximum limit can be set in the Database profile agents Properties tab. For more information on this tab, refer to the Database Profile . connectionpool.wait.timeout.milliseconds Default value: 5000 Note! This property can only be used if the connectionpool.strict.pool.size property is set to true . This property is used to configure the amount of time a thread should remain in the blocked queue until it tries for a new connection. If a connection is made available before this time, the thread's waiting time is interrupted. Diameter Properties Property Description Property Description mz.diameter.tls.accept_all Default value: false If the property is set to false (default), the Diameter Stack agent does not accept any non-trusted certificates. If it is set to true, the Diameter Stack agent accepts any certificate. In either case any unrecognized certificate will be logged in an entry in the System Log (in PEM format). mz.workflow.decoderqueue.max_threads Default value: 10 This property specifies the maximum number of threads used by the Diameter Stack agent for decoding messages. Setting a lower value than the default may enhance performance if the host machine has a low number of CPU cores and the active workflows are complex. On the other hand, the decoding may constitute a bottleneck when performing simple processing on a host machine with a high number of CPU cores. In this case, setting a higher value may provide better performance. ECS Properties Property Description Property Description mz.ecs.bulk.transfer.size Default value: 10485760 This property defines the maximum size (in Bytes) of a bulk transfer for the ECS collection agent. FTP/DX200 Properties Property Description Property Description mz.dx200.acceptsequentiallost Default value: "" The default behaviour for the FTP/DX200 collection agent is to skip files if the sequential order of the files has been lost. Add this property and set it to true if you want to continue collecting files even if the sequential order has been lost. mz.dx200.acceptoverwritten Default value: "" The default behaviour for the FTP/DX200 collection agent is to not collect files that have been in FULL state before being set to OPEN, and untransferred data has been overwritten. Add this property and set it to true if you want to collect files that have been overwritten. Proxy Properties Property Description Property Description http.proxyHost The proxy host that routes HTTP traffic http.proxyPort The proxy port for routing HTTP traffic. Default value: 80 http.proxyUser The username, if Basic Authentication is required for proxy handling HTTP traffic http.proxyPassword The password, if Basic authentication is required for proxy handling HTTP traffic. The password can be plain-text or encrypted using the mzsh encryptpassword <password> command. https.proxyHost The proxy host that routes HTTPS traffic https.proxyPort The proxy port for routing HTTPS traffic. Default value: 443 https.proxyUser The username, if Basic Authentication is required for proxy handling HTTPS traffic https.proxyPassword The password, if Basic authentication is required for proxy handling HTTPS traffic. The password can be plain-text or encrypted using the mzsh encryptpassword <password> command. http.nonProxyHosts Indicates the hosts that should be accessed without going through the proxy. Typically, this defines internal hosts. The value of this property is a list of hosts, separated by the '|' character. Additionally, the wildcard character '*' can be used for pattern matching. For example, http.nonProxyHosts="*. foo.com |localhost" will indicate that every host in the http://foo.com domain and the localhost should be accessed directly even when a proxy server is already specified. Rest Client Properties Property Description Property Description rest.client.idleTimeout Default value: "200s" This property specifies the time a connection stays idle before it is eligible to be disconnected. If there is no traffic during the specified time, the REST Client agent will remove the session once the timeout is reached. You can also set this property on the container level, where the value is only applied to all ECs under a particular container. You can refer to Container Properties for more information. rest.client.max.chunk.size Default value: "8m" This property specifies the maximum chunk size of the HTTP response that the REST Client agent should receive from the server. The agent will reject data with sizes that are larger than the value defined by this property. You can also set this property on the container level, where the value is only applied to all ECs under a particular container. You can refer to Container Properties for more information. rest.client.max.content.length Default value: "64m" This property specifies the maximum length of the HTTP content received by the REST Client agent. The agent will reject content that is longer than the specified value defined by this property. Although it is also possible to set the value of this property to infinite, there will be a possibility that the EC will crash from an out-of-memory error. So do consider setting the memory size of the EC to be higher than the expected size of the HTTP content that the agent will be receiving. You can also set this property on the container level, where the value is only applied to all ECs under a particular container. You can refer to Container Properties for more information. SAP RFC Property Property Description Property Description saprfc.bcd.double Default value: false This property determines whether the double data type is used when converting for mapping to SAP JCo ABAP Type P (Binary Coded Decimal). The possible values are: true : Uses the double data type for conversion. false : Uses the BigDecimal data type for conversion. SNMP Properties Property Description Property Description snmp.auth.proto.maxCompatibility Default value: false Due to security risks, SHA-1 and MD5 are by default disabled for SNMPv3. To enable them, set this property to true. This allows compatibility for all authentication algorithms, including SHA-1 and MD5. Note! We do not recommend setting this to true. Only use this property when you have no other options regarding authentication algorithms for your devices. Inter Workflow Properties Property Description Property Description mz.iwf.max_size_block Default value: "" By setting this property (in Bytes) on the EC that runs the Inter Workflow storage, it is possible to restrict memory consumption. If the agent wants to allocate more memory than the given property value during collection, the collection will abort instead of suffering a possible "out of memory". Note that the minimum value is 32000 bytes, and even if a lower value is configured, 32000 will be used. Table Lookup Service Properties Property Description Property Description mz.inmemory_table.table_strategy Default value: Object This property defines how shared tables are kept in memory. The possible values are: Object - The shared tables are stored as Java objects on the JVM heap. If this value is chosen the mz.inmemory_table.index_strategy property will have no effect. On-heap - The shared tables are stored in a compact format on the JVM heap. Unsafe - The shared tables are stored in a compact format outside the JVM heap. The API java.misc.Unsafe is used for the format and is only supported on Oracles JVM. mz.inmemory_table.index_strategy Default value: Object This property defines how the index for shared tables are kept in memory. The possible values are: Object - The index is stored as Java objects on the JVM heap. Pointer - The index is stored as pointers to the table data. mz.inmemory_table.fixed_width Default value: true This property defines if the varchar data fields are stored with a fixed or variable width. The possible values are: true - The varchar data fields will have a fixed width. false - The varchar data fields will have a variable width which will reduce the memory usage for large columns. Additional Properties Property Description Property Description ec.backlog.dir Default value: $MZ_HOME/tmp This property is applicable for ECs and determines whether you want events and error messages occurring when the contact is lost to be logged in a backlog or not. If the property is not present, events and messages will not be logged. The value of this property specifies where the backlog resides. ec.excel.dateformat Default value: "dd/MM/yyyy" This property will define the format of the date to be decoded for the ExcelCellUDR. For the date to be parsed successfully, you will have to provide the format of the date received by the ExcelCellUDR. The values in this property conforms to the standards of the SimpleDateFormat java class. If the date is not parsed successfully, the data type of the date field will be treated as a string. ec.shutdown.time Default value: 60000 This property specifies the maximum time (in milliseconds) the EC will wait before a real-time workflow stops after a shutdown has been initiated. This is to enable the workflow to stop all input and drain all UDRs in the workflow before shutting down. The wait time is initially set to 60 seconds. If this value is set to 0 all draining is ignored and the workflow will stop immediately. pico.bootstrapclass Default value: "" This property specifies the bootstrap classes that are required by some components. pico.ec.do_graceful_shutdown Default value: true This property determines the behavior of the mzsh command shutdown. When you have set the value of this property to false and run the command, it will cause ECs to shut down without forcing the workflows to stop first. pico.rcp.server.port Default value: "" This property specifies the port that is used for EC to EC communication. If no port is set, a dynamic port will be used and the port number will change each time an EC is restarted. mz.use.drdateformat Default value: true The date syntax used in the APL functions dateToString and strToDate conforms to the Java class SimpleDateFormat in Java 6. Even though the syntax conforms to SimpleDateFormat , it is not directly based on this class. You can enable date format handling based on the SimpleDateFormat class in the installed Java version by setting this property to false . This enables use of additional patterns that are available in the installed Java version. For more information about the functions dateToString and strToDate , see the APL Reference Guide. mz.use.date.timezone Default value: false Setting this property to true will instruct the system to use the attached time zone when SQL input originates from a date object. In most cases, the property should be set in the pico configuration of ECs. However, in case of audit processing, the property should also be set in the pico configuration of the Platform. Note that if the mz.use.date.timezone property is used for setting dates with another time zone, there will be no way of keeping track of the actual time zone in the database, and you may have to manually convert the date during selection. To keep track of the time zone in an Oracle database, use the data type TIMESTAMP WITH TIME ZONE. pico.type Default value: "" This property specifies the type of pico instance used for the EC. The possible values are EC and SC.

---

# Document 863: Pulse Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739889/Pulse+Agent+Configuration
**Categories:** chunks_index.json

You open the Pulse collection agent configuration dialog from a workflow configuration. To open the Amazon S3 collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select Pulse from the Collection tab of the Agent Selection dialog. Interval Tab The Interval tab contains configuration settings related to the interval type and the related time parameters that control how often the pulse UDRs will be generated. Open The Pulse agent configuration dialog - Interval tab Setting Description Ti me Unit Select the time unit that is applicable to the time fields on this tab. Interval Type Select an interval type that corresponds with the desired generation of Pulse UDRs. Fixed - UDRs are generated at a fixed time interval Linear - UDRs are generated at a linearly increasing or decreasing time interval. Gaussian - UDRs are generated according to a Gaussian probability distribution. Poisson - UDRs are generated according to a Poisson process. Single - A single UDR is generated at workflow start. Interval Enter the interval between the pulses according to the specified time unit. Interval is available when the selected Interval Type is Fixed . Start Enter the start interval between pulses. Start is available when the selected Interval Type is Linear . Stop Enter the final interval between pulses. Stop is available when the selected Interval Type is Linear . Rate Enter the value that is incremented or decremented from the interval when a Pulse UDR is generated. Use a positive value to increment and a negative value to decrement. Rate is available when the selected Interval Type is Linear . Central Interval When the selected Interval Type is Gaussian , enter the median interval between Pulse UDRs in the probability distribution. When the selected Interval Type is Poisson , enter 1/(expected number of Pulse UDRs per time unit). Example - Poisson interval type Expected number of Pulse UDRs is five (5) per second. 1/5=0.2 seconds between UDRs Select MILLISECONDS in Time Unit and enter 200 in Central Interval . Central Interval is available when the selected Interval Type is Gaussian or Poisson . Width Enter the standard deviation of the probability distribution. When you use a low value, the intervals tend to be close to the value of the Central Interval . Use a high value to spread the intervals over a wider range. Width is available when the selected Interval Type is Gaussian . Note! The real value of the configured interval length is approximated and should not be used for timing-sensitive tasks. To get the most accurate pulse-delay, use fixed time intervals. Data Tab The Data tab includes settings that control the contents of the Pulse UDRs. Open The Pulse agent configuration dialog - Data tab Setting Description Ra nd om With Range Select this radio button to populate the Sequence field in the Pulse UDRs with a random integer. The range of the values are specified in the Range fields. Sequential Select this radio button to populate the Sequence field in the Pulse UDRs with a value from an infinite sequence starting at 0. The sequence is reset when a workflow is restarted. Sequential With Range Select this radio button to populate the Sequence field in the Pulse UDRs with a value from a finite sequence. The range of the values are specified in the Range fields. The sequence is reset when a workflow is restarted or when the end value is reached. Range Enter the start and end value of the sequence. You can use negative values but the end value must be greater than the start value. Random Data Select this radio button to generate random data in the Data field of the Pulse UDRs. The data is generated when the workflow starts and each UDR will contain the same data. Size Enter the size of the random data and select the byte unit from the drop-down list. Fixed Data Select this radio button to have fixed data in each PulseUDR. Enter the data in the text area that is to be populated in the Data field of the Pulse UDRs.

---

# Document 864: Legacy Desktop Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610395/Legacy+Desktop+Overview
**Categories:** chunks_index.json

Legacy Desktop is a user interface application that enables you to manage, navigate, and monitor MediationZone. With the Legacy Desktop you create workflows. A workflow is a set of agents that are connected to each other and represent a flow of data processing. All the agents in a workflow operate on a specific data type, and most agents need to know the structure, the Input/Output data, of the data in order to operate properly. This chapter describes applications, features, and settings used in the Legacy Desktop and contains the following sections: Starting and Managing the Legacy Desktop Legacy Desktop User Interface Administration and Management in Legacy Desktop Legacy Desktop Accessibility Options

---

# Document 865: SAP CC Notification Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740417/SAP+CC+Notification+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The SAP CC Notification agent collects notifications from SAP CC instances and emits notification UDRs of the sapcc type. The SAP CC Notification agent emits the following notification UDRs: NotificationUDR For further information, see SAP CC UDRs . MIM For information about the MIM and a list of the general MIM pa rameter s, see Administration and Management in Legacy Desktop . Publishes The agent does not publish any MIM resources. Accesses The agent does not access any MIM resources.

---

# Document 866: Workflow Bridge Profile Configuration General Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654790/Workflow+Bridge+Profile+Configuration+General+Tab
**Categories:** chunks_index.json

The General tab is displayed by default when creating or opening a Workflow Bridge profile. Open Workflow Bridge profile Setting Description Setting Description Send Reply Over Bridge Select this checkbox if the Collection workflow must send a reply back to the Forwarding workflow each time a ConsumeCycleUDR has been received. If this is not checked only ConsumeCycleUDRs with an wfbActionUDR will be sent back. Note! This only applies for the ConsumeCycleUDR , since the WorkflowState UDRs must always be acknowledged. There is no timeout handling for outstanding ConsumeCycleUDR s requests in the forwarding agent, it must be handled in the Workflow logic if required. Force Serialization This option is enabled by default and applies to situations where the Workflows are running on the same EC. You can disable it to increase performance, if you are certain that no configurations will be changed during the execution of these Workflows. Note! If this option is disabled, it is strongly recommended to NOT perform any configuration changes while the Workflows are running. Response Timeout (s) Enter the time (in seconds) that the Workflow Bridge Forwarding agent will wait for a response for a WorkflowState UDR from the Workflow Bridge Real-time Collection agent. After the specified time, the Workflow Bridge Forwarding agent will time out and abort the workflow. The default value is "60". Bulk Size Configure Bulk Size if you want data to be bulked by the Workflow Bridge Forwarding agent before being sent to the collection side. Enter the number of UDRs that should be bulked. The default value is "0", which means that the bulk functionality will not be used. Bulk Timeout (ms) Enter the time (in milliseconds) that the Workflow Bridge Forwarding agent should wait in case the bulk size criteria is not fulfilled. Default value is "0" which is an infinite timeout. Number of Collectors If you want to configure broadcasting or load balancing, that is, if you want several different Workflow Bridge collecting workflow instances to be able to receive data from the forwarding workflow, you enter the number of collecting workflows you want to use in this field. The number of collecting workflows connected to the workflow bridge must not exceed the limit set by this value. In the case of a batch forwarding workflow, it must be started after at least one collector is running, or it will abort. Collecting workflows that are started after the limit has been reached will also abort. Real-time forwarding workflows do not require any collectors running when started, and Number of Collectors represents an upper limit only. Validation of this configuration will also be performed. Transport Select the transport protocol that you prefer for the best performance, Aeron , TCP , or TCP(netty4) . Note! If you select Aeron, you cannot use Bulk Size and Bulk Timeout. If you select TCP , Netty version 3 is used. If you select TCP(netty4) , netty version 4 is used and you can set the advanced property highThroughput to false (in the Advanced tab) to save CPU usage (at the cost of lower throughput). Default is true. It can be wise to perform some test runs with the sort of traffic you will use later and especially check which one gives the best characteristics in terms of throughput and latency. You can always switch between Aeron or TCP/IP even on a system that is in service, but you must restart the workflow when doing this. Load Balancing Strategy Select the Load Balancing Strategy for preferred scalability and redundancy. The Load Balancing Strategy is used if you set the number of collectors to more than 1. There are two strategies: Static : LoadIDs from the UDRs are used to determine how data should be distributed via the workflows. See Workflow Bridge Example Real-Time to Real-Time Scenario with Load Balancing for how to use this. Dynamic : Select to distribute the load over all the available collectors. The LoadID from the UDR is used virtually and is not connected to any hardware at the collecting side. The LoadID settings of the workflows will be ignored during load balancing. When selecting Dynamic , another field called Virtual Destinations is displayed. The default setting of the Virtual Destination field is 1024. The LoadIDs in the UDR are mapped to virtual destinations distributed evenly on available consumer instances. The LoadIDs set in the Workflow Table are ignored. For instance, if Virtual Destinations is set to 1024, the UDR's LoadIDs must be populated with values from 0-1023. The Workflow Bridge engine will create the amount of virtual destinations that you have specified in the LoadID field. If starting another workflow, the virtual destinations will be redistributed evenly between the available workflows. In most cases the default setting 1024 of Virtual Destinations is sufficient and will work fine. In case you want to change this, it should be set as a number that is the power of two, such as 512, 1024, or 2048. The number of destinations may vary, so make sure the number is set high enough to get a decent distribution. Note! The Virtual Destinations field is only available if you have set Load Balancing Strategy to Dynamic . UDR Types If you enter a UDR type into the list of UDR types then workflows are validated to only allow Workflow Bridge Cycle UDRs, and UDRs of that type, to be sent into the Workflow Bridge Forwarding Agent. Click the Add button to select the UDR types. If you leave this field blank any UDR type can be sent to the Workflow Bridge Forwarding Agent. Click the Remove button to remove a UDR type. Note! After a Workflow Bridge profile has been changed and saved, all running workflows that are connected to this profile must be restarted.

---

# Document 867: SAP CC REST UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/319717377/SAP+CC+REST+UDRs
**Categories:** chunks_index.json



---
**End of Part 38** - Continue to next part for more content.
