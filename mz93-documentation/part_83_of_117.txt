# RATANON/MZ93-DOCUMENTATION - Part 83/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 83 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~68.6 KB
---

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent produces UDRs or bytearray types, depending on the code since UDRs may be dynamically created. It consumes any of the types selected in the UDR Types list. MIM The Aggregation agent publishes different MIM parameters depending on the storage selected in the Aggregation profile. For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes File Storage and SQL Storage The MIM parameters listed in this section are applicable when File Storage or SQL Storage is selected in the Aggregation profile. The values of the MIM parameters below are reset when the EC is started. MIM Parameter Description MIM Parameter Description Created Session Count This MIM parameter contains the number of sessions created. Created Session Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Online Session Count This MIM parameter contains the number of sessions cached in memory. Online Session Count is of the int type and is defined as a global MIM context type. Session Cache Hit Count When an already existing session is read from the cache instead of disk, a cache hit is counted. This MIM parameter contains the number of cache hits. Session Cache Hit Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Session Cache Miss Count When an already existing session is requested and the Aggregation profile cannot read the session information from the cache and instead reads the session information from disk, a cache miss is indicated. If a non-existing session is requested, this will not be counted as a cache miss. This MIM parameter contains the number of cache misses counted by the Aggregation profile. Session Cache Miss Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Session Count This MIM parameter contains the number of sessions in storage on the file system. Session Count is of the int type and is defined as a global MIM context type. Couchbase Storage and Redis Storage The MIM parameters listed in this section are applicable when Couchbase or Redis is selected in the Aggregation profile. The values of the MIM parameters below are reset when the workflow is started. MIM Parameter Description MIM Parameter Description Created Session Count This MIM parameter contains the number of sessions created. Created Session Count is of the long type and is defined as a global MIM context type. Session Remove Count This MIM parameter contains the number of sessions removed. Session Remove Count is of the long type and is defined as a global MIM context type. Mirror Attempt Count This MIM parameter contains the total number of attempts to retrieve a stored mirror session. Mirror Attempt Count is of the long type and is defined as a global MIM context type. Mirror Error Count This MIM parameter contains the number of failed attempts to retrieve a stored mirror session, where the failure was caused by one or more errors. Mirror Error Count is of the long type and is defined as a global MIM context type. This counter is reset each time the workflow is started. Mirror Found Count This MIM parameter contains the number of successful attempts to retrieve a stored mirror session. Mirror Found Count is of the long type and is defined as a global MIM context type. Mirror Not Found Count This MIM parameter contains the number of attempts to retrieve a stored mirror session that did not exist. Mirror Not Found Count is of the long type and is defined as a global MIM context type. Mirror Latency This MIM parameter contains comma separated counters that each contains the number of mirror session retrieval attempts for a specific latency interval. Attempts that failed due to errors are not counted. The parameter contains 20 counters for a series of 100 ms intervals. The first interval is from 0 to 99 ms and the last interval is from 1900 ms and up. Example The value 1000,100,0,0,0,0,0,0,0,0,0,0,0,0,1 should be interpreted as follows: There are 1000 mirror session retrieval attempts with a latency of 99 ms or less. There are 100 mirror session retrieval attempts with a latency of 100 ms to 199 ms. There is one mirror session retrieval attempt with a latency of 1999 ms or more. Mirror Latency is of the String type and is defined as a global MIM context type. Session Timeout Count This MIM parameter contains the number sessions that has timed out. Session Timeout Count is of the long type and is defined as a global MIM context type. Session Timeout Attempt Count Multiple timeout threads may read the same session data, but only one of them will perform an update. If a thread reads a session that has already been updated, it will be counted as an attempt. This MIM parameter contains the number of attempts. Session Timeout Attempt Count is of the long type and is defined as a global MIM context type. Session Timeout Latency This MIM parameter contains comma separated counters that each contains the number of sessions for a specific timeout latency interval i e the difference between the actual timeout time and the expected timeout time. The parameter contains 15 counters for a series of one-minute intervals. The first interval is from 0 to 1 minutes and the last interval is from 14 minutes and up. Example The value 1000,100,0,0,0,0,0,0,0,0,0,0,0,0,1 should be interpreted as follows: There are 1000 sessions with a timeout latency that is less than one minute. There are 100 sessions with a timeout latency of one to two minutes. There is one session with a timeout latency of 14 minutes or more. Session Timeout Latency is of the String type and is defined as a global MIM context type. Slowest Create This MIM parameter contains the duration of the slowest successful create operation in Couchbase/Redis. The duration is counted in milliseconds and the initial value is -1. Slowest Create is of the long type and is defined as a global MIM context type. Slowest Delete This MIM parameter contains the duration of the slowest successful delete operation in Couchbase/Redis. The duration is counted in milliseconds and the initial value is -1. Slowest Delete is of the long type and is defined as a global MIM context type. Slowest Read This MIM parameter contains the duration of the slowest successful read operation in Couchbase/Redis. The duration is counted in milliseconds and the initial value is -1. Slowest Read is of the long type and is defined as a global MIM context type. Slowest Update This MIM parameter contains the duration of the slowest successful update operation in Couchbase/Redis. The duration is counted in milliseconds and the initial value is -1. Slowest Update is of the long type and is defined as a global MIM context type. Total Number Of Create Count This MIM parameter contains the number of successful create operations in Couchbase/Redis. A create operation is considered successful if it can be performed without errors. Total Number Of Create Count is of the long type and is defined as a global MIM context type. Total Number Of Delete Count This MIM parameter contains the number of successful delete operations in Couchbase/Redis. A delete operation is considered successful even if Couchbase returns certain types of errors, i e if data is locked. A delete operation is considered failed if an exception occurs in the internal Couchbase client, if there are concurrent edits (due to optimistic locking), or if a key cannot be found. Total Number Of Delete Count is of the long type and is defined as a global MIM context type. Total Number Of Read Count This MIM parameter contains the number of successful read operations in Couchbase/Redis. A read operation is considered successful even if Couchbase returns certain types of errors, i e if data is locked or a key cannot be found. Total Number Of Read Count is of the long type and is defined as a global MIM context type. Total Number Of Status Locked Read Count This MIM parameter contains the number of successful read operations that resulted in errors related to locked data. Total Number Of Status Locked Read Count is of the long type and is defined as a global MIM context type. Total Number Of Status Read Count This MIM parameter contains the number of successful read operations in Couchbase/Redis that resulted in errors. Errors related to locked data and non-existent keys are not accounted for. Total Number Of Status Read Count is of the long type and is defined as a global MIM context type. Total Number Of Status Update Count This MIM parameter contains the number of successful update operations in Couchbase/Redis that resulted in errors. Errors related to locked data are not accounted for. Total Number Of Update Count is of the long type and is defined as a global MIM context type. Total Number Of Update Count This MIM parameter contains the number of successful update operations in Couchbase/Redis that did not result in errors. Total Number Of Update Count is of the long type and is defined as a global MIM context type. Total Number Of Failed Create Count This MIM parameter contains the number of unsuccessful create operations in Couchbase/Redis. A create operation is considered failed if it cannot be performed without errors. Total Number Of Failed Create Count is of the long type and is defined as a global MIM context type. Total Number Of Failed Delete Count This MIM parameter contains the number of unsuccessful delete operations in Couchbase/Redis. A delete operation is considered failed if an exception occurs in the internal Couchbase client, if there are concurrent edits (due to optimistic locking), or if a key cannot be found. Total Number Of Failed Delete Count is of the long type and is defined as a global MIM context type. Total Number Of Failed Read Count This MIM parameter contains the number of unsuccessful read operations in Couchbase/Redis. A read operation is considered failed if an exception occurs in the internal Couchbase client. Total Number Of Failed Read Count is of the long type and is defined as a global MIM context type. Total Number Of Failed Update Count This MIM parameter contains the number of unsuccessful update operations in Couchbase/Redis. An update operation is considered failed if an exception occurs in the internal Couchbase client, there are concurrent edits (due to optimistic locking), or a key cannot be found. Total Number Of Failed Update Count is of the long type and is defined as a global MIM context type. Elasticsearch Storage The MIM parameters listed in this section are applicable when Elasticsearch is selected in the Aggregation profile. The values of the MIM parameters below are reset when the workflow is started. MIM Parameter Description MIM Parameter Description Created Session Count This MIM parameter contains the number of sessions created. Created Session Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Online Session Count For Elasticsearch, the value for this MIM parameter is always 0. This MIM parameter is retained for Elasticsearch to retain backwards compatitlity for workflows created with File Storage. Online Session Count is of the int type and is defined as a global MIM context type. Session Cache Hit Count When an already existing session is read from the cache instead of disk, a cache hit is counted. For Elasticsearch, the value for this MIM parameter is always 0. This MIM parameter is retained for Elasticsearch to retain backwards compatitlity for workflows created with File Storage. Session Cache Hit Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Session Cache Miss Count When an already existing session is requested and the Aggregation profile cannot read the session information from the cache and instead reads the session information from disk, a cache miss is indicated. If a non-existing session is requested, this will not be counted as a cache miss. For Elasticsearch, the value for this MIM parameter is always 0. This MIM parameter is retained for Elasticsearch to retain backwards compatitlity for workflows created with File Storage. Session Cache Miss Count is of the long type and is defined as a global MIM context type. You can reset the value by using resetCounters via a JMX client. See 7.5 Aggregation Monitoring for further information. Session Count This MIM parameter contains the number of sessions in storage on the file system. Session Count is of the int type and is defined as a global MIM context type. Accesses The agent does not itself access any MIM resources. However, APL offers the possibility of both publishing and accessing MIM resources and values.

---

# Document 1952: Customized Statistics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647896/Customized+Statistics
**Categories:** chunks_index.json

MediationZone further includes comprehensive functionality to provide auditing and statistical information related to workflows and UDR based information, such as record count, field contents etc. related to any sort of processing (association, cloning, manipulation, and so on). The captured statistical data is recorded in user defined database tables. For the creation of the customized reports, we rely on external reporting tools. Alternatively, a workflow can be used to collect the data, process it according to requirements, and forward it as an XML or ASCII file. Since there are different needs for different kind of reports, MediationZone provides for comprehensive support to capture and store statistical information. Statistical data should be exported to a separate server if a lot of data reports are to be generated.

---

# Document 1953: Configuration Contract Tags - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676765/Configuration+Contract+Tags
**Categories:** chunks_index.json

The configuration contract XML document is specified using the tags and attributes described in the following tables and sections. The tables list all the tags and attributes available when writing the configuration contract. It also indicates whether these are optional or required. Name Attribute(s) May occur in Required Name Attribute(s) May occur in Required contract object-version - Yes (exactly 1) class-name - contract Yes (exactly 1) package-name - contract Yes (exactly 1) storable-id - contract Yes (exactly 1) delegate id contract No (exactly 1) upgradable-delegate - delegate No (exactly 1) field active, hidden, id, level, since section Yes (min 1) title - field Yes (exactly 1) name - field Yes (exactly 1) description - field Yes (max 1) type - field Yes (exactly 1) primitive-type name type, list-of, array-of, map-of, set-of No (max 1) object-type name type, list-of, array-of, map-of, set-of No (max 1) enum-type name type, list-of, array-of, map-of, set-of No (max 1) password-type - type, list-of, array-of, map-of, set-of No (max 1) drstorable-type name, profileType type, list-of, array-of, map-of, set-of No (max 1) drdate-type - type, list-of, array-of, map-of, set-of No (max 1) list-of - type No (max 1) array-of - type No (max 1) set-of - type No (max 1) default-value - field Yes (max 1) null-value - field Yes (max 1) allow-null - field Yes (max 1) tooltip - field Yes (max 1) validation - field Yes (max 1) validate minLength, maxLength, minValue, maxValue, isValidRegExp, isValidTZ, notNull, matchRegexp, negateRegexp, matchValue, message validation No (unlimited) These are the available attributes: Name May occur in Required Name May occur in Required object-version contract Yes id field, section, delegate Yes active field No hidden field No level field No since field No minLength validate No maxLength validate No minValue validate No maxValue validate No isValidRegExp validate No isValidTZ validate No notNull validate No matchRegexp validate No negateRegexp validate No matchValue validate No message validate Yes name primitive-type, object-type, enum-type, drstorable-type Yes profileType drstorable-type No contract The configuration contract starts with a contract declaration. The contract tags should enclose all other tags. Syntax: <contract object-version='1.0'> The object-version attribute specifies the configuration version that this contract was designed for. Note! The mz-version attribute for contract is deprecated and has been replaced with the object-version attribute. However, to enable backward compatibility, the mz-version attribute will still work. class-name This tag specifies the name of the Java class that should be generated and is part of the header in the configuration contract. Syntax: <class-name>DiskCollectionConfig</class-name> package-name This tag specifies the package name of the Java class that should be generated and is part of the header in the configuration contract. Syntax: <package-name>com.digitalroute.diskcollection</package-name> The package name will determine in which directory you must be located when creating the *.jar file as described in Creating a DTK Plugin | Creating a User Defined Jar . With the package name stated in the syntax above, you will have to be where the /com/digitalroute/diskcollection/ directory is located when creating the *.jar file. storable-id When specified, the storable-id will be used instead of the classname in exported configurations, and when configurations are persisted to the database. As a result of this, the class can be moved or renamed without breaking existing configurations. This tag is part of the header in the configuration contract. Syntax: <storable-id>Prefix.DiskCollectionConfig</storable-id> Suggested prefix is the company name or similar. delegate The delegate tag is a place holder for various types of sub delegate tags, which are used for triggering specific functions defined in the stated Java class. All sub delegate tags must be enclosed by the main delegate tags, which are part of the header in the configuration contract. Syntax: <delegate id='com.companyname.myagent.AgentDelegate'> <upgradable-delegate/> </delegate> The delegate tag has the following attributes: Attribute Description Attribute Description id The Java class that shall be called when the delegate is triggered. This Java class must implement the java interface for the delegate. upgradable-delegate The upgradable-delegate is used for handling upgrades in configuration contracts If the upgradable-delegate tag is included, the Java class stated as id in the main delegate tag is called when the mzsh upgradeconfig command, or the mzsh systemimport with the -u|-upgrade flag, is used. The upgradeable-delegate tag must be enclosed by delegate tags, and the delegate class must implement java interface UpgradableDelegate . Syntax: <upgradable-delegate> In the class called you can configure the changes to be made, e g if a field has been changed, added, or renamed, or if types have been changed. Example - Upgrade procedure This is an example of how an upgrade procedure may be performed: Open In the configuration contract for Agent Z, some fields have been changed in version Y. A new Java class is needed to implement the UpgradeDelegate interface. This interface is then used as ID in the <delegate>...</delegate> section in the configuration contract, i e agentz.Delegate.java. In the configuration contract for Agent Z a <delegate>...</delegate> section is added with the ID agentzDelegate.java and containing an <upgradable-delegate> tag. The configuration contract is sent to the Contract Tool which generates a new Java file that should be compiled. The resulting package should be committed using pcommit . By running the mzsh upgradeconfig command, the configurations containing this agent will be upgraded with the changes and information about the upgrade will be stored in an upgrader-log. section The section tag specifies a logical grouping of several fields that may be configured in the agent's configuration dialog. Syntax: <section id='Disk Collection'/> The id attribute is the identifier for the section. field The field tag is the heart of the configuration contract. Each field tag defines a piece of data that can be configured for the agent, and encloses all of the field-related tags. Syntax: <field id='directory' level='any' active='true'/> <title>Directory</title> <name>Directory</name> <type> ... </type> </field> The field tag has the following attributes: Attribute Description Attribute Description active The active attribute is optional. It is used if the field should be disabled by default by setting the value to 'false'. The default value is 'true'. hidden The hidden attribute is optional. It is used if the field should not be shown in Workflow Properties to be made available into the Workflow Table. This is done by setting the value to 'true'. The default value is 'false'. id The id attribute is mandatory and must be unique within the configuration contract. level The level attribute is optional. It defines if the field can be configured in the template, workflow or both. The default value is any . The level attribute can have one of the following values: any - The value can be configured on both workflow and template level. instance - The value can only be configured on workflow level. template - The value can only be configured on template level. since The since attribute is optional. It is used to specify at what version of the contract the field was introduced. This makes it possible to manage backward compatibility, so that a new version of the contract can read a previous version. title The title tag identifies the field in the GUI, and should be enclosed by the field tags. Syntax: <title>Directory</title> name The name tag identifies a field when the object is serialized. The value must therefore be unique within the contract document. This tag should be enclosed by the field tags. Syntax: <name>Directory</name> description The description tag defines a descriptive text for the field, and should be enclosed by the field tags. The value is currently not used. Syntax: <description>The source directory</description> type Types are described using a small language which represents the different types. The type tags should be enclosed by the field tags. The following types are supported: primitive-type object-type enum-type password-type drstorable-type drdate-type Syntax: <type>specific type</type> Example - Primitive int type <type> <primitive-type name='int'/> </type> Example - Type indicating a List of String objects <type> <list-of> <object-type name='java.lang.String'/> </list-of> </type> primitive-type The primitive-type tag allows the specification of a field with a primitive Java type. The following types are supported by this tag: boolean byte char double float int long short Syntax: <primitive-type name='int'/> The name attribute is the name of the primitive type. object-type The object-type tag allows the specification of supported Java object fields. The following types are supported by this tag: java.lang.String java.math.BigInteger Syntax: <object-type name='java.lang.String'/> The name attribute must be a fully qualified name. enum-type The enum-type tag allows the specification of enum fields. Any Java Enum type can be specified. Syntax: <enum-type name='enum-type-name'/> The name attribute must be a fully qualified name. password-type The password-type tag allows the specification of encrypted password fields. Syntax: <password-type/> drstorable-type The drstorable-type tag allows the specification of fields implementing the DRStorable interface. Syntax: <drstorable-type name='drstorable-type-name'/> The name attribute must be a fully qualified name. array-of The array-of tag allows the specification of a field containing an array of another type. Syntax: <array-of>contained-type-declaration</array-of> list-of The list-of tag allows the specification of a field containing a List of another type. Syntax: <list-of>contained-type-declaration</list-of> set-of The set-of tag allows the specification of a field containing a Set of another type. Syntax: <set-of>contained-type-declaration</set-of> default-value The default-value tag allows the specification of a default value for a field. Syntax: <default-value>"value"</default-value> tooltip If the tooltip tag is present, the enclosing field should be included in the tooltip for the agent. The value of the tooltip tag usually contains the title of the field. Syntax: <tooltip>"Port"</tooltip> validation The validation tag is the start tag for the specification of validation rules. The validation element can contain several validation rules. Example - Validation of a String <validation> <validate minLength='1' message='The value is too short'> <validate maxLength='10' message='The value is too long'> </validation> validate Specifies a validation rule, and should be enclosed by the validation tags. Syntax: <validate 'rule' = 'value' message='error message' Where rule can be one of the following: Rule Description Rule Description minValue Checks if the value is greater than or equal to the min value. maxValue Checks if the value is less than or equal to the max value. minLength Checks if the value's adjusted length is greater than or equal to the min length. maxLength Checks if the value's length is less than or equal to the max length. isValidRegExp Checks if the value is a valid regular expression. matchRegexp Checks if the value matches the specified regular expression. Example - Validation Rules for a TCP port <validate minValue='0' maxValue='65535' message='Port number is not valid'/>

---

# Document 1954: Real-Time Disk_Deprecated Collection Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655320/Real-Time+Disk_Deprecated+Collection+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data

---

# Document 1955: New Features and Enhancements - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647457
**Categories:** chunks_index.json

Open In this section, you can see information about the new features and functionality in this release. 1 New Features 1.1 New Connectors 1.1.1 New Kafka Agents 1.1.2 Amazon S3 Agents in Real-Time Workflows 1.1.3 Amazon SQS Agents 1.1.4 APN Processing Agent 1.1.5 SAP CC REST Agent 1.2 MariaDB 1.2.1 Supported Agents for MariaDB 1.2.2 Additional APL Functions and Event Notifications Support MariaDB 1.3 Usability 1.3.1 New Command Line Interface 1.3.2 Desktop Search 1.3.3 Log Filter 1.3.4 Support for wfexport and wfimport mzsh Commands 1.3.5 MZSH Command For Shutting Down Desktop and Legacy Desktop 1.4 Integration and Security 1.4.1 User Security Improvements 1.4.2 SAP CTS+ Integration 1.5 Google Secret Manager 1.6 Reference Data Management Supports SAP HANA 1.7 System Statistics 2 Enhancements 2.1 followRedirects Field for RequestCycle UDR in HTTP/2 Agents 2.2 HTTP/2 Client Agent Support Additional OAuth Settings 2.3 Operations REST Interface for Host and Pico 2.4 SAP RFC Processor Agent Supports Execution Time Threshold Configuration 2.5 Configurable Maximum Response Size for HTTP/2 Client 2.6 Data Type Conversion Property for SAP JCo ABAP Type P 2.7 Database Sizing 2.8 Rollback New Features The following new functionality has been added in this release: New Connectors New Kafka Agents Ref: XE-14033 A completely new set of Kafka agents has been designed, which will replace the previous Kafka agents in a later release. The main differences are that the new agents: Include a batch forwarding agent. Store transactions in the Kafka cluster (transactions are only tracked for batch workflows). Store offset in the Kafka cluster only. Enables one consumer to collect from several topics, and one producer to forward to several topics. Include automatic rebalancing for the collection of messages and several workflows can collect messages in parallel from the same topics will keep batch collection workflows in a running state, even when all data has been collected. Use only two UDR types: For more information, see New Kafka Agents . Amazon S3 Agents in Real-Time Workflows Ref: XE-13151, XE-13152 The Amazon S3 collection and forwarding agents can be used for collecting and forwarding files from and to specified buckets and regions in Amazon and they are now also available for real-time workflows. For more information, see Amazon S3 Agents . Amazon SQS Agents Ref : XE-13403 The Amazon SQS Agents act as consumers (collection agents) and producers (forwarding agents) of messages in Amazon Simple Queue Service, which is a fully managed message queueing service. Both standard and FIFO queueing are supported, see SQS Agents for more information. APN Processing Agent Ref: XE-14066 The APN processing agent is now available for real-time workflows. It enables push notifications to be sent to mobile devices using Apple certificates. The APN processing agent includes two distinct UDRs: one for pushing notifications and another for reporting results. For more information, see APN Agent . SAP CC REST Agent Ref: XE-11292 SAP CC REST agent allows you to connect and send charging requests to SAP Convergent Charging on SAP Cloud (RISE). The agent will use API to communicate with the SAP CC server, sending HTTP requests and receiving HTTP responses in return. SAP CC REST agent has its own UDRs separate from other SAP CC agents and it is generated based on YAML files of the REST API version used by the SAP CC server. For more information, see SAP CC REST Agent . MariaDB Supported Agents for MariaDB Ref: XE-13414 MariaDB can now be used with the SQL Collection and Forwarding agents, the SQL Loader agent, and the Task Workflow SQL agent. For more information, see MariaDB . Additional APL Functions and Event Notifications Support MariaDB Ref: XE-13415 You can now select MariaDB when configuring to run the Callable Statements, Database Bulk Lookup Functions, Database Table Related Functions, Event Notifications, and Prepared Statements. For more information, see MariaDB . Usability New Command Line Interface Ref: XE-15667 We have created a new Command Line Interface called mzcli. It does not require connectivity to the platform to execute. See the Command Line Tool Reference Guide for more information. Desktop Search Ref: XE-13735 You can now search for configurations, APL codes, and Ultra formats using the search bar at the top of the Desktop. You can even use the search bar to create new configurations or to search for tools and Data Management applications. For more information, see Search . Log Filter Ref: XE-12917 You can now update the logging settings for picos dynamically, either in the Log Filter in Desktop Online or using mzsh. You can update log level, select to include stacktrace or not, perform logging for a selected package or class, add logging to an additional file and reset to default settings. For more information, see Log Filter and logger . Support for wfexport and wfimport mzsh Commands Ref: XE-13234 The wfexport command generates a file (CSV, TSV, or SSV) containing data from the Workflow Table. This file includes a header row listing the names of the Workflow Table columns. The wfimport command updates the specified workflow configuration by importing workflows defined in the export file. For more information, see wfexport and wfimport . MZSH Command For Shutting Down Desktop and Legacy Desktop Ref: XE-11515 With the, mzsh desktopadmin command you can shut down all Deskops and Legacy Desktops connected to the Platform, see desktopadmin . Integration and Security User Security Improvements Ref : XE-12201 Enhanced user security is now enabled by default upon installation of the Platform. The property mz.security.user.control.enabled is now set to true in the platform.conf file. Furthermore, the enhanced user security property mz.security.user.password.reset.enabled has been removed. With the property removed, by default, any users registered into the system will now be prompted to change their password on their first login. For more information, see Enhanced User Security . SAP CTS+ Integration Ref: XE-13755 SAP CTS+ (Change and Transport System) can now be integrated with MediationZone. It enables you to manage and transport configurations across different environments with greater flexibility and control. For more information see, SAP CTS+ Integration User's Guide . Google Secret Manager Ref: XE-14275 You can use the Google Secret Manager profile to setup up access credentials and properties for connecting to a Google Secret Manager environment. For more information, see Google Secret Manager Profile . Reference Data Management Supports SAP HANA Ref: XE-13153 The Reference Data profile allows you to select which tables are available for querying and editing through the Reference Data Management dashboard via the RESTful interface. You can now select SAP Hana in the Reference Data profile configuration. For more information, see Reference Data Profile . System Statistics Ref: XE-13313 System Statistics can now be viewed from the Desktop. You can access the System Statistics from the Tools and Monitoring section of the Manage View. For more information, see System Statistics . Enhancements followRedirects Field for RequestCycle UDR in HTTP/2 Agents Ref: XE-14079 To accommodate scenarios where a cookie may be included in the 3xx response message, the followRedirects field is introduced in the HTTP/2 RequestCycle UDR to disable automatic redirection if necessary. For more information, see HTTP/2 UDRs . HTTP/2 Client Agent Support Additional OAuth Settings Ref: XE-14080 The following fields have been added to OAuth 2.0 authentication type in HTTP/2 Client agent - Authentication tab: Client Auth Type : Added options for client_secret_basic and client_secret_post Base URL : This field is added to manage base URL settings For more information, see https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739302 . Operations REST Interface for Host and Pico Ref: XE-13029 The Operations REST API is now available for host and pico. Users can access http(s)://<platform server>:<platform port>/ops/mz/host/v1/api-docs for host and http(s)://<platform server>:<platform port>/ops/mz/pico/v1/api-docs for pico on any given running system Platform. For more information, see https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031043 . SAP RFC Processor Agent Supports Execution Time Threshold Configuration Ref: XE-14268 Two additional options; the Enable Logging for Execution Exceeding Time Threshold checkbox and Execution Threshold Time (min) field are added to the SAP RFC Processor agent configuration to allow users to configure the execution time threshold. When an RFC Function takes too long, any attempt to abort the workflow will be logged in the System Log. See https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034531 for more information. Configurable Maximum Response Size for HTTP/2 Client Ref: XE-13740 Users can now set a maximum size limit for responses received by the HTTP/2 Client. To configure the maximum response size, use the Max Response Content Length field introduced in the Client tab of HTTP/2 Client agent configuration. See https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739302 for more information. Data Type Conversion Property for SAP JCo ABAP Type P Ref: XE-14878 A new property is introduced to define the data type used for converting values when mapping to SAP JCo ABAP Type P (Binary Coded Decimal). The saprfc.bcd.double property provides flexibility to users in handling data types. See Execution Container Properties for more information. Database Sizing Ref: XE-17321 We have provided documentation on approach to plan for database sizing. Rollback In previous versions, rollback has not been available, but in this version rollback in case of failed upgrade has been implemented, see https://infozone.atlassian.net/wiki/spaces/MD94/pages/399147509 .

---

# Document 1956: FTP EWSD Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685573/FTP+EWSD+Transaction+Behavior
**Categories:** chunks_index.json

This section includes information about the FTP/EWSD agent transaction behavior. For further information about the general transaction behavior, see Workflow Monitor . Emits The agent emits commands that change the state of the file that is currently being processed. Command Description Begin Batch Invoked right before the first byte of each file is collected by a workflow. End Batch Invoked right after the last byte of each file is collected by a workflow. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Cancel Batch If a Cancel Batch message is received, the agent sends the batch to ECS. Note! If the Cancel Batch behavior, defined for a workflow, is configured to abort the workflow, the agent will not receive the last Cancel Batch message. In such a case, an ECS is not involved, and the established copy is not deleted.

---

# Document 1957: A Constructed Decoder Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646933/A+Constructed+Decoder+Example
**Categories:** chunks_index.json

In some cases, a so-called constructed d ecoder is useful. The main advantage is that it introduces some validation logic, making it possible to evaluate the order of the arriving records. For instance, suppose the incoming files contain one header and one trailer which must be present at the file start and end, in order for the file to be accepted. In between, data records may or may not be present. The data records can be of two types. Headers and trailers are considered to be records as well, so there are actually four record types in this format definition. The source file for which a decoder is exemplified in this appendix: Open An example of the source files discussed in this section This section includes a detailed description of all the code block parts that a constructed decoder might contain: Headers and Trailers (which are included in the external code block) external internal in_map decoder out_map encoder external Both headers and trailers as well as record types should be included in the external code block. Headers and Trailers Since headers and trailers are treated as records, the same syntax as for external records apply. APL syntax can also be used within UFDL code. external FileHeader { ascii header : terminated_by(0xA); }; The first line is always a header. Note! No identified_by is needed, since the decoder does not evaluate the input stream to see if the next record is a leader or not. This compares to whether to read a TypeA or TypeB record, when an identification test is called for. external FileTrailer : identified_by( strStartsWith( trailer, "Date") ){ ascii trailer : terminated_by(0xA); }; The identified_by for the trailer is not crucial, however, it provides additional validation during decoding, since it evaluates that the trailer really starts with "Date". Record Types The definitions of TypeA and TypeB are fairly straightforward. No encode_value for RecordType is set, since this is evaluated from the internal UDR during encoding (see the section below, internal). external TypeA : identified_by( RecordType == "A" ), terminated_by(0xA) { ascii RecordType : static_size(2), terminated_by(":"); ascii A_number : terminated_by(":"); ascii B_number : terminated_by(":"); ascii SequenceNumber : terminated_by(":"); ascii Duration : terminated_by(0xA); }; external TypeB : identified_by( RecordType == "B" ), terminated_by(0xA) { ascii RecordType : static_size(2), terminated_by(","); ascii CallingCountry : terminated_by(","); ascii SequenceNumber : terminated_by(","); ascii LocalAreaCode : terminated_by(","); ascii A_number : terminated_by(","); ascii B_number : terminated_by(","); ascii CauseForOutput : terminated_by(","); ascii CalledCountry : terminated_by(0xA); }; internal Suppose it is desired to output one record type as a replacement for the incoming types A and B the simplest way is to create a mutual internal. internal MyInternal { // These are common fields to TypeA and TypeB string RecordType; string SequenceNumber; string A_number; string B_number; // These may or may not be present depending on // record type string CallingCountry: optional; string LocalAreaCode: optional; string Duration: optional; string CauseForOutput: optional; string CalledCountry: optional; }; Both TypeA and TypeB records are mapped to MyInternal. The common fields are always set. The others are defined as optional, hence, their presence depends on the record type. The RecordType in the internal type is required for encoding, since the encoder needs to evaluate the record type to decide whether to encode as TypeA or TypeB. in_map Both record types A and B are mapped to the same internal. This approach is useful to simplify APL syntax within processing (a lot of if-statements used to determine the record type, can be eliminated), or in case one resulting output type is produced. TypeA and TypeB are both mapped to MyInternal (see the section above, internal). in_map TypeA_in : external( TypeA ), internal( MyInternal ) { automatic; }; in_map TypeB_in : external( TypeB ), internal( MyInternal ) { automatic; }; The headers are not wanted in processing, therefore discard_output is set. However, the target_internal is still useful since it enables you to produce headers for encoding. in_map Header_in : external( FileHeader ), target_internal( Header ), discard_output { automatic; }; // The trailer gets a special record type. in_map Trailer_in : external( FileTrailer ), target_internal( Trailer ) { automatic; }; decoder The following constructed decoder definition expects all batches to start with a header, end with a trailer, and have zero, one, or several A and B records in between. If not, the decoder aborts. // The sub-decoders. decoder Records : in_map( TypeA_in ), in_map( TypeB_in ); decoder Header : in_map( Header_in ); decoder Trailer : in_map( Trailer_in ); // The total (file) decoder. // '*' means zero/one/several records are expected between // one header and one trailer for each file collected. decoder Total { decoder Header; decoder Records *; decoder Trailer; }; out_map Suppose you are required to encode back to the original format. out_map TypeA_out: external(TypeA), internal( MyInternal ) { automatic; }; out_map TypeB_out: external(TypeB), internal( MyInternal ) { automatic; }; out_map Trailer_out: external(FileTrailer), internal(Trailer) { automatic; }; out_map Header_out: external(FileHeader), internal(Header) { automatic; }; The out-maps and encoder are simple. Note! TypeA and TypeB both are encoded from MyInternal. Which type to use depends on the value of the RecordType field. encoder A constructed encoder cannot be created. Hence, the following encoder definition does not care for the order of arriving records, nor that all types must be present in the output file. encoder Total: out_map( TypeA_out ), out_map( TypeB_out ), out_map( Header_out ), out_map( Trailer_out );

---

# Document 1958: Secrets Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737876
**Categories:** chunks_index.json

With the Secrets profile you can use secrets in the software that originate from one central location. It is currently use d with : Azure Profile GCP Profile SAP RFC Profile . To create a new Secrets profile, click the New Configuration button from the Configuration dialog available from Build View , and then select Secrets Profile from the menu. Buttons The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently displayed tab. The Secrets profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . Common Configuration Common Setting Description Common Setting Description Storage Type Select the Secret storage type from the dropdown menu list. The following types are available: Azure KeyVault Secrets - The Secrets are stored in Azure KeyVault Google Secrets Manager - The Secrets are stored in Google Secret Manager Local Storage - The Secrets are stored locally in the configuration. Local Storage Settings Open Secrets Profile - Local Storage Type When the Local Storage type is selected: Setting Description Setting Description Alias This shows the alias of the secret. Secret This shows the associated secret. Azure Keyvault Secrets Settings Open Secrets Profile - Azure KeyVault Storage Type When this option is selected, an Azure KeyVault Profile must be selected. Press the Browse button to open the Select Azure KeyVault Profile dialog box. Open KeyVault Profile Selection Screen Select which profile you want to use and confirm the selection with the OK button. Click the Cancel button to return to the Secrets Profile screen, Clear to remove the current selection, New to open the creation screen for the Azure KeyVault Profiles. When the Azure KeyVault Secrets type is selected: Setting Description Setting Description Alias This shows the alias of the secret. This name must exist as a secret in Azure KeyVault. Google Secrets Manager Settings Open Secrets Profile - Google Secret Manager Storage Type When this option is selected, a Google Secret Manager Profile must be selected. When the Google Secret Manager type is selected you have the following settings: Setting Description Setting Description Name This shows the name of the secret. This name must exist as a secret in Google Secret Manager. Version The version of the Secret. The default is the latest.

---

# Document 1959: APL and Ultra Field Types for Python - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001452/APL+and+Ultra+Field+Types+for+Python
**Categories:** chunks_index.json

The following APL and Ultra field types are supported. Field Type Python Type Description Field Type Python Type Description any drany Can be assigned any value of any supported type. An actual value can never be of type drany , an actual value is always of one of the supported types. An any field may also hold an unknown value, and the value will then be an instance of DRRemoteObject . See the examples below: udr.anyField = 4096 isinstance(udr.anyField, drint) # True udr.anyField = "a string" isinstance(udr.anyField, drstring) # True udr.anyField = drbyte(3) isinstance(udr.anyField, drbyte) # True isinstance(udr.anyField, drany) # Always False bytearray drbytearray Bytearray type. The type drbytearray is the same as the Python built in bytearray type. See the examples below: udr.baField = drbytearray([1,2,3]) udr.baField = bytearray([1,2,3]) boolean drboolean Boolean value ( True or False ). The type drboolean is the same as the Python built in bool type. See the example below: udr.booleanField = True byte drbyte Single byte integer value. See the example below: udr.byteField = 127 short drshort Short integer value. See the example below: udr.shortField = 31000 int drint Integer value. See the example below: udr.intField = 2147483640 long drlong Long integer value. See the example below: udr.longField = 92233720368547 bigint drbigint Storage for any integer value. See the examples below: udr.bigintField = 8823383228338388 udr.bigintField = drbigint(8823383228338388) bigdec drbigdec Storage for any integer value with a decimal point. The type drbigdec is the same as the Python built-in type decimal.Decimal , except that drbigdec only accepts finite values. See the example below: udr.bigdecField = '123.456' udr.bigdecField = drbigdec('123.456') float drfloat Single-precision floating point. Avoid using float , use double instead. See the example below: udr.floatField = 0.5 double drdouble Double-precision floating point. The type drdouble is the same as the Python built in float type. See the example below: udr.doubleField = 3.14 char drchar A single character value. See the example below: udr.charField = 'x' string drstring A string value. The type drstring is the same as the Python built in unicode type on Python 2 and str on Python 3. See the example below: udr.stringField = "my string" date drdate A date value. Dates are represented as UNIX timestamps and hold information on: timestamp - the timestamp as a float in seconds timezone - the timezone as a string as supported by APL hastime - true if the timestamp has time information hasdate - true if the timestamp has date information See the example below: udr.dateField = drdate("2001-10-22 22:11:01.128 UTC", "Europe/London") ts = udr.dateField.timestamp # ts == 1003788661.128 tz = udr.dateField.timezone # tz == "Europe/London" hd = udr.dateField.hasdate # hd == True ht = udr.dateField.hastime # ht == True The string representation of date always presents the timestamp in UTC. See the drdate example below: d = drdate('2001-10-22 22:11:01.128 UTC', 'Europe/London') print(d) 2001-10-22 22:11:01.128 UTC print(repr(d)) drdate(timestamp='2001-10-22 22:11:01.128 UTC', timezone='Europe/London') To create a date representing now, use: current = drdate.now() To create a date with only the hastime or hasdate information use: onlytime = drdate('2001-10-22 22:11:01.128 UTC', 'Europe/London', hasdate=False) onlydate = drdate('2001-10-22 22:11:01.128 UTC', 'Europe/London', hastime=False) To create a date with a timestamp specified in seconds as a float (timezone is optional) use: dt = drdate(timestamp=1003788661.128, timezone='Europe/London') Note! The timestamp must be a float. To compare drdates use their timestamps. A date cannot be modified after construction. ipaddress dripaddress Holds IP address information. Both IPv4 and IPv6 are supported. Use the property ipaddress to get the IP address as a string. See the example below: udr.ipaddressField = dripaddress('127.0.0.1') ip = udr.ipaddressField.ipaddress # ip == '127.0.0.1' bitset drbitset Represents a bit string that grows as needed. The bits are indexed by non-negative integers. See the example below: udr.bitsetField = drbitset('101010100') b0 = udr.bitsetField[0] # b0 == 1 b1 = udr.bitsetField[1] # b1 == 0 udr.bitsetField[2] = 1 size = len(udr.bitsetField) # size == 9 table drtable A special type used to hold table data used by the APL table commands. The drtable does not hold the actual table data but is only a reference to the data. To access the actual rows and columns you must import and use APL functions to extract the information. See the example below: from apl.MyFolder.MyAPLModule import getTableRows rows = getTableRows(udr.tableField) list drlist List of objects of a specified type, implements the Python collections.abc.MutableSequence interface. See the example below: udr.intlistField = [1, 2, 3] udr.intlistField[0] = 7 i = udr.intlistField[1] # i == 2 size = len(udr.intlistField) # size == 3 map drmap Hash map of specified key/value types, implements the Python collections.abc.MutableMapping interface. See the example below: udr.stringintmapField = {'key1': 1, 'key2': 2} udr.stringintmapField['key3'] = 3 key1 = udr.stringintmapField['key1'] # key1 == 1 size = len(udr.stringintmapField) # size == 3 drudr drudr Holds a UDR object. The actual runtime type of the UDR can be checked with isinstance . See the example below: from ultra.MyFolder.MyUltra import MyUDR udr.drudrField = MyUDR() udr.drudrField.myField1 = 100 value = udr.drudrField.myField1 # value == 100 ismyudr = isinstance(udr.drudrField, MyUDR) # ismyudr == True The Python agents are aware of all UDR types and field types that exist. When assigning fields, automatic type conversion is always performed and includes converting numbers to booleans, strings to numbers, numbers to strings, and dictionaries to UDRs. In case types or values mismatch, assigning to a field may result in an exception.

---

# Document 1960: Data Veracity Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205684821/Data+Veracity+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent consumes selected UDR types. MIM For information about the MIM and a list of the general MIM par amete rs, see MIM . Publishes MIM Value Description Agent Name The name of the agent. Inbound UDRs The number of UDRs routed to the agent. Accesses The agent does not itself access any MIM resources.

---

# Document 1961: Aggregation Performance Tuning - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671965
**Categories:** chunks_index.json

This section describes the various settings that are available for performance tuning of the Aggregation agent. The section contains the following subsections: Performance Tuning with Couchbase Storage Performance Tuning with File Storage Performance Tuning with Redis Storage

---

# Document 1962: dimension - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742619
**Categories:** chunks_index.json

The following JSON schema describes the data format of the dimension object type: Loading A dimension represents a category in a data set and is used for grouping KPIs. The dimension object type has one or more sub-objects, consisting of properties that map it to a KDR field. You can map a dimension to different fields based on the expected value in the KDR field type . In the example below, the dimension Region is mapped to the field region_name for the types record_a and record_b. Example - JSON Representation "dimension": { "Region": { "kdr_record_type_a": "region_name", "kdr_record_type_b": "region_name" }, "Country": { "kdr_record_type_a": "country_name", "kdr_record_type_b": "country_name", } } Break

---

# Document 1963: GTP' LGU ReCollection Agent UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000509/GTP+LGU+ReCollection+Agent+UDR+Types
**Categories:** chunks_index.json

The UDR types used by the GTP' LGU ReCollection agent can be viewed in the UDR Internal Format Browser. To open the browser, open an APL Editor, and, in the editing area, right-click and select UDR Assistance . Open UDR types used by the GTP LGU ReCollection agent GTPRecollectionRequestUDR The following fields are included in the GTPRecollectionRequestUDR format: Field Description Field Description Date (string) This field contains the date inserted, based on your APL configuration. HeaderInfo (int) This field contains a header and should be set to 65. MessageType (int) This field contains the message type and should be set to 1. SequenceNumbers (list<long>) This field should contain the sequence numbers of the requested packages. GTPRecollectionResponseUDR The following fields are included in the GTPRecollectionResponseUDR format: Field Description Field Description ResponseState (int) This field contains the state of the FTP transfer after completion: 1 (All requests have been fulfilled.) 2 (A requested sequence number is not available on the network element.) 3 (A request timed out.) 4 (A response from the network element is in an invalid format and cannot be decoded.) SequenceNumbers (list<long>) This field contains the sequence numbers of the requested packages.

---

# Document 1964: General Preparations Execution Context - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204996876
**Categories:** chunks_index.json

Initially, you must create the UNIX user (preferably mzadmin ) that executes the installation and later on runs pico processes in the Execution Container. This is the user that is referenced throughout this document. During the installation procedure, the default application user , mzadmin , is automatically created. Note that this is not related to the OS user or the DB user. You cannot create or use other application users than this default user until after the installation is complete. Note! The mzadmin user must have read/write permissions for all the directories stated in the Environment Variables. It is not recommended to use root as mzadmin , since this makes troubleshooting difficult. MediationZone should always be administrated by users belonging to the access groups created in MediationZone. See the Desktop documentation for further information on how to create users and access groups. The next step is to install the necessary third-party products, that is, Java. For further information, see System Requirements . This section includes the following subsections: Setting Environment Variables for Execution Container Preparing the Installer File for the Execution Container Updating the Installation Properties for Execution Container Extracting Files for Execution Container

---

# Document 1965: Connecting Using SSL - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642362
**Categories:** chunks_index.json

If you require connections to the Python Connector agent from an exploration tool to be made using SSL, you need to provide an SSLContext when connecting, such as in the example below: import mzconnector import ssl ctx = ssl.create_default_context(cafile=<location of CA certificate>) ctx.load_cert_chain(certfile=<location of client certificate>, keyfile=<location of client private key>) agent = mzconnector.connect(<host>, <port>, ctx) Open Example of connecting to the Python Connector agent from a Jupyter Notebook using SSL Hint! If you use keytool to create a self-signed CA certificate, use the command line option -ext BC=ca:true for the certificate to be accepted by Python.

---

# Document 1966: PCC Extensions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677512/PCC+Extensions
**Categories:** chunks_index.json

Apart from the built-in data models for PCC, you can also create your own data models. The data models are defined in xml format, and then packed into an mzp that is committed into the system. Then they will be available from the regular interfaces for provisioning. To create a PCC Extension data model: Create XML files describing your data model and store it in the /mediationzone/packages/pcrf/src/devkit_tools/example/model/ folder. Go to the /mediationzone/packages/pcrf/src/devkit_tools/example/ folder and run the generate.sh script to generate your *.jar file. $./generate.sh A /build directory will be created with all the required classes and resources. A *.jar file will be created and placed in a new sub directory called /build. There is an example of a 5G PCF Rules model included in the Development Toolkit, see Development Toolkit User's Guide . Data Model for PCF Session Management Policy

---

# Document 1967: Execution Container Software Installation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205029687
**Categories:** chunks_index.json

The final step in the Execution Container installation must be executed as the mzadmin UNIX user. The following command session sets up the directory structure and installs all the software. The release content directory is the working directory for the installation. To install the software: Enter the release content directory : $ cd ./<staging directory>/<release content directory> Pre-installation validations are performed to verify the environment and configuration, ensuring a seamless installation experience. To perform validation without starting the installation, run the following command: $ ./setup.sh install -validate-only Run the following command to initiate the installation process: $ ./setup.sh install If validation fails but you choose to proceed, you can run the following command: $ ./setup.sh install -skip-validate Caution! You can choose to skip validation, but be aware that this bypasses checks for potential issues in your environment and configuration. If there are undetected problems, the installation may fail or cause unexpected behavior. Proceed with caution and ensure you have backups before continuing. If the Platform was running during the installation, the Execution Container is registered in STR. You can register the container manually by using the command: $ mzsh topo register The installation of the Execution Container is now complete.

---

# Document 1968: Archive Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640150/Archive+Profile
**Categories:** chunks_index.json

Profiles containing storage, naming scheme and lifetime for targeted files are configured in the Archive profile. You can configure several workflows to use the same profile, however only one of the workflows may be active at a time. The Archive profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Archive profile configuration, click the New Configuration button in the upper left part of the Desktop window, and then select Archive Profile from the me n u. Open Archive profile configuration dialog The contents of the menus in the menu bar may change depending on which configuration type that has been opened. The Archive profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . The Edit button is specific for Archive profile configurations. Item Description Item Description External References Select to Enable External References in an agent profile field. Refer to Enabling External References in an Agent Profile Field in External Reference Profile for further information. See also, the section below, Enabling External References. The full path of each filename to store in the archive is completely dynamic via the Archive File Naming Algorithm. The name is determined by three parameters: AAA/BBB/CCC Where: Name placeholder Description Name placeholder Description AAA Represents one of the base directories specified in the Base Directory list in the Archive profile. If several base directories exist, this value will change according to the frequency selected from the Switch Policy list. The system automatically appends a directory delimiter after this name. BBB This part is constructed from the Directory Template . If the template contains one or several Directory delimiters this part will enclose one or several directory levels itself. For instance, if the template contains Month, Directory delimiter, Day this will yield new directories every day, named 03/01 , 03/02 ... 03/31 , 04/01 , 04/02 ... 04/30 and so on. In this example, files are stored in a directory structure containing all months, which in turn contains directories for all days (which in turn will contain all files from that day). The system automatically appends a directory delimiter after this name. CCC This is the name the file will get. It is defined on each archiving agent using configurations from the Filename Template tab in the Archiving agent configuration dialog. The Archive profile configuration contains the following settings: Setting Description Setting Description Switch Policy If several base directories are configured, the switch policy determines for how long the Archive services will populate each base directory before starting to populate the next one (daily, weekly, or monthly). After the last base directory has been populated, the archiving wraps to the first directory again. Base Directory One or several base directories that can be used for archiving of files. For considerable amounts of data to be archived, several base directories located on different disk partitions might be needed. Directory Template List of tokens that, in run-time, builds subdirectory names appended to one of the base directories. The tokens could be either special tokens or user defined values. Subdirectories on any level, can be constructed by using the special token Directory delimiter. See the section below, Directory Template. Remove Entries (days) If enabled, files older than the entered value will be deleted from the archive. Depending on the agent using the profile, the removal will occur differently. Note! The days that user can define will just be between 1 to 365 days only. Any days more than that will not be supported. For the Local Archiving agent the cleanup of outdated files is mastered by the workflow. It removes the file from its archive directory in the initialize block. For the Archiving agent the cleanup of outdated files is performed by the Archive Cleaner task. It removes the references, as well as the files themselves from its archive directory. Consequently, the data storage is also dependent on the setup of the task scheduling criteria. Keep Files Files will not be deleted. If Keep Files and Remove Entries (days) are combined, only references in the database are removed while the files remain on disk. ( not valid for the Local Archiving agent). Directory Template Open Add Directory Template dialog Setting Description Setting Description Special token Tokens to be used as part of the directory name. Year - Inserts four digits representing the year the file was archived. Month - Inserts two digits representing the month the file was archived. Day - Inserts two digits representing the day of the month the file was archived. Hour - Inserts two digits representing the hour (24) of the day the file was archived. Agent directory name - Inserts the MIM value(s) defined in the Agent Directory Name list in the Archiving agent configuration dialog. Day index - Inserts a day index between zero and the value entered in Remove Entries (days) field. This number is increased by one every day until Remove Entries (days) - 1 is reached. It then wraps back to zero. Day index may not be used in the template if Remove Entries (days) is disabled. Directory delimiter - Inserts the standard directory delimiter for the operating system it distributes files to. This way, a sub-directory is created. Text If enabled, the token is entered from the text field. When disabled, the token is instead selected from the Special token list. Enabling External Referencing Click the External References button to enable external referencing of profile fields. For detailed instructions, see External Reference Profile . When you apply external referencing to profile fields, the following profile parameters are affected: Setting Description Setting Description Base Directory The directory paths that you add to this list are included in the properties file that contains the External References. Example - Directory path myBaseDirectoryKey = /mypath/no1, /mypath/no2 Remove Entries (days) The value with which you set this entry is included in the properties file and interpreted as follows: myRemoveEntriesKey = 1 #! Remove after 1 day myRemoveEntriesKey = 365 #! Remove after 365 days myRemoveEntriesKey = -1 #! Do not remove. #! This value is equal to clearing the #! check-box. Keep Files In the properties file a checked entry is interpreted as true or yes , and a cleared entry as false or no .

---

# Document 1969: SAP JCo Uploader Agent Message Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609194/SAP+JCo+Uploader+Agent+Message+Events
**Categories:** chunks_index.json

An agent message is an information message sent from the agent, stated according to the configurations made in the Event Notification Editor. The agent does not itself publish any Message Events. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. The agent produces the following debug events: Discard record no <record number> as it has been processed Reported when a record is found to have already been processed.

---

# Document 1970: Topology Automation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657941/Topology+Automation
**Categories:** chunks_index.json



---
**End of Part 83** - Continue to next part for more content.
