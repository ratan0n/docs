# RATANON/MZ93-DOCUMENTATION - Part 42/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 42 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~67.9 KB
---

All MediationZone events and user-defined events may be selected, filtered, and routed to notifiers in the Event Notification configuration. The system supplies a few standard notifiers where event information may be directed to a Log File, a database table, by e-mail, by SNMP or to the System Log. If required, new notifiers can be developed and introduced into the system using the DTK. Notifier-related classes are located in the package com.digitalroute.devkit.event where the class DRNotifierTarget and possibly DRNotifierTargetUI and DRAbstractStorable can be extended. Example For a notifier plugin example, see: com.digitalroute.devkit.examples.event.DynamicLogFile* DRNotifierTarget A new notifier plugin is defined by extending DRNotifierTarget . The Code Server will locate all such classes and make them available for configuration in the Event Notification Editor. The configuration of a notifier can be obtained in two ways (or through a combination of both): Static User Interface - This is configuration entered in a user-defined user interface, introduced by extending the DRNotifierTargetUI class. If available, it will appear in the Base Configuration frame of the Notifier Setup in the Event Notification configuration. Dynamic Fields - These are input fields that the Event Notification configuration automatically displays in the Target Field Configuration frame in the Notifier Setup tab. Such fields do not require a user-defined interface and are also possible to populate using formatted input, where values from the incoming events may be picked. The getName method will return that will be displayed in the Notification Type combo box in the Event Notification configuration. If a static user interface is desired, then the getUIClassName method must return the full class name of a class that extends DRNotifierTargetUI . If dynamic fields are desired, then getDynamicFields must return an array of DRUDRField s. The name of the DRUDR field will be displayed as the label and the data type ( DRUDRType ) will determine what input component to use, what validation rules will apply, and how the field will be populated. Valid data types are: DRUDRType.PT_SHORT DRUDRType.PT_INT DRUDRType.PT_LONG DRUDRType.PT_FLOAT DRUDRType.PT_DOUBLE DRUDRType.PT_CHAR DRUDRType.PT_STRING DRUDRType.PT_BOOLEAN DRUDRType.PT_DATE DRUDRType.PT_IPADDRESS For all data types, apart from PT_STRING, the only population options will be manual input and direct mapping from an event field, whose data type must correspond. In addition, the PT_STRING offers formatted input where many event fields may be mapped into the value. The methods isDynamicFieldMandatory will, for each field, determine if it will validate that input is entered, and isDynamicFieldMultiline will determine if the input component will be a single or a multi-line field. When a notifier configuration is saved, the initialize method is called. If the notifier was previously saved, the deinitialize method is called first. When the Event Notification configuration receives an event that the notifier subscribes for, the notify method is called. The method is handed the event itself, and if dynamic fields are used, a hash map is keyed by the dynamic field name and a corresponding object as well. The corresponding object will hold the assigned field value. If the value has been manually entered or assigned directly from an event field, then the object will have the same data type as the dynamic field. If the value used formatted input (PT_STRING only) then this value will contain the formatted string, which means any variables will have been substituted by an event field value. DRNotifierTargetUI If the notifier requires a static user interface for user configuration, the DRNotifierTargetUI class must be extended. The class extends the Java JPanel which is the main container for the configuration components. When a notifier is selected in the user interface, this panel will be displayed in the Base Configuration frame. When the panel is displayed, the method reset is called to reset all components, immediately followed by displayConfig . The latter hands over a previously configured configuration object that has to implement DRStorable , and has to be cast into the appropriate type. Unless this data is null , the data will be displayed. When a notifier is saved in the Event Notification configuration, the validateInput method is called. If the method returns a string, it is displayed as an error message. If it returns null , the collectConfig method is called to create and populate a configuration object. The Platform takes care of storing the configuration.

---

# Document 936: File System Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031291/File+System+Profile
**Categories:** chunks_index.json

The File System Profile is used to configure settings for different file systems. It works with https://infozone.atlassian.net/wiki/x/CQsyD , GCP Storage Agents , https://infozone.atlassian.net/wiki/x/1hEyD , as well as the https://infozone.atlassian.net/wiki/x/NwY4D and https://infozone.atlassian.net/wiki/x/HQgyD . These configurations depend on the specific file system being used. You can find more details about each file system in the subsection links at the bottom of this page. Open New Configuration - File system profile. The default File System Type is Git. External References Button Open Buttons in the File System Profile configuration The External References button is specific to the File System profile configuration. Click this button to enable External References in the File System profile configuration. This can be used to configure the fields shown in the following table File system type Fields available for configuration File system type Fields available for configuration Amazon S3 file systems Access Key Secret Key Bucket Region Advanced Properties HDFS file systems Host Replication Port Advanced Properties GCP file systems Project Id Private Key Id Private Key Client Email Client Id Other Information Bucket For further information on configuring external reference profiles , see https://infozone.atlassian.net/wiki/x/rwc4D and External Reference Profile . Subsections This section includes the following subsections: File System Type - Git File System Type - Amazon S3 File System Type - GCP Storage File System Type - HDFS

---

# Document 937: Upgrade Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669993/Upgrade+Preparations
**Categories:** chunks_index.json

During the upgrade process, you must use the default application user, mzadmin , and correct password whenever you are prompted to enter it. The preparation steps will not affect the running system and can be done in advance. Before proceeding with the upgrade, make the following preparations: Verify that you are on a version from which upgrade is supported. You can check the current version by opening the About window in the Desktop, where the current version is listed in the Pico Version section. See the documentation space for the version you are currently running for more information. Install either Oracle JDK 17 or OpenJDK 17 on the server(s) where the Platform Container and Execution Containers are running. Verify that the System Requirements are met. If the operating system, or database need to be upgraded, this should be done prior to the upgrade. This has to be done for all machines that are hosting the system. Ensure that the environment variables are set correctly: Variable Description MZ_HOME This environment variable specifies where the software is installed. JAVA_HOME This environment variable specifies where the JDK is installed. PATH This environment variable specifies the search path and must contain the following directories: $JAVA_HOME/bin:$MZ_HOME/bin Example - Setting environment variables export MZ_HOME=/opt/mz export JAVA_HOME=/opt/jdk/jdk-17.0.2 export PATH=$JAVA_HOME/bin:$MZ_HOME/bin:$PATH Make an online backup of the database(s). For further information regarding how to perform an online backup, see Backup and Disaster Recovery Procedures . Note! If you are upgrading from 9.x, you can find the database backup instructions in the documentation for 9.x. It is important to make a backup of MZ_HOME for rollback purposes. To make a backup of your MZ_HOME, you can use the following command: Note! Ensure that all processes in your installation are shutdown prior to the backup. You can check which processes are running by using the mzsh status command. cd $MZ_HOME/../ tar -zcvf mzhome_backup.tgz <MZ_HOME directory> mv mzhome_backup.tgz <backup directory> Caution! Use of MZ_HOME backup The MZ_HOME backup is needed in case the upgrade fails. Caution! Use of Filebase persistence When you have the platform property mz.userserver.filebased set to true , to ensure a seamless upgrade process, it is imperative to export all the config before the upgrade and re-import it afterward. Failure to do so may result in the missing configuration data in the upgrade.The MZ_HOME backup is needed in case the upgrade fails. Create a directory to use when unpacking this release and future releases. For the purpose of these instructions, this designated directory is referred to as the staging directory . Important! The staging directory should not be the same directory as the one you created and set up as the MZ_HOME directory. Place the *.tgz file from your release delivery into the staging directory . Use a command line tool, go to the staging directory , and unpack the *.tgz file by running the following command: tar xvzf <filename>.tgz A directory is then created in the staging directory , containing the software to be installed. For the purpose of these instructions, this directory is referred to as the release content directory . Now copy the MZ license file into the release content directory . Note! If you are upgrading from an earlier major or minor version, you need a new license file. Contact Support | DigitalRoute on getting the MZ license file. cp mz.license <release content directory> Enter the release content directory and prepare the install.xml file by running the following command: cd <release content directory> ./setup.sh prepare The *.mzp packages have now been extracted, and the install.xml has been extracted into the release content directory . The install.xml file will automatically be populated with information from your existing installation. Note! Refer Updating the Installation Properties for Platform to know more about these properties and their default values. Important! When upgrading to this release, the install.admin.password property must be set according to your current admin password before proceeding with the ./setup.s h upgrade step. Important! If your existing MZ_HOME platform database is configured to use other database type than the default Derby, you need to check and update the respective database related properties from the install.xml file to match your current setup. Oracle See Properties for Oracle for details. The following properties require manual update: <property name="install.ora.owner" value="mzowner"/> <property name="install.ora.password" value="mz"/> PostgreSQL See Properties for PostgreSQL for details. The following properties require manual update: <property name="install.pg.owner" value="mzowner"/> <property name="install.pg.password" value="mz"/> SAP HANA See Properties for SAP HANA for details. Important! Oracle ojdbc.jars If you are using Oracle as the system database, you need to use Oracle 19c (19.20.0.0) JDBC drivers ( ojdbc8.jar/ojdbc10.jar ). Replace the jar files in $MZ_HOME/3pp and $MZ_HOME/lib folders The jar file can be downloaded from JDBC and UCP Downloads page .

---

# Document 938: help - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743840/help
**Categories:** chunks_index.json

usage: help [command] If help is used without an argument, a list containing available commands and their required arguments will be displayed. Note! If the user is logged in and connected to a running platform the list will include all commands available for the version the platform is running. When an argument is added to the help command, information about the entered command is displayed. Return Codes Listed below are the different return codes for the help command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the command supplied in the argument does not exit.

---

# Document 939: REST Server_Deprecated Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674426/REST+Server_Deprecated+Agent+Configuration
**Categories:** chunks_index.json

The REST Server_Decprated configuration contains three tabs; General , Authentication , and Advanced . General Tab The General tab contains settings related to the location and authentication of the REST Server_Deprecated agent. Open REST Server_Deprecated agent configuration - General tab Setting Description Setting Description Local Address The local address that the server will bind to. This field is mandatory and cannot left empty. Port The port the server will listen to. Default port is 80. REST Server Profile Click Browse to select a predefined REST Server Profile. The profile contains the configuration of the request URI allowed by REST Server. For further information, see REST Server Profile Deprecated . Note! If you have not selected a profile, any request URI will be allowed by the REST Server_Deprecated agent. Use TLS If enabled, the communication channels will be encrypted. You must select this option for the Security Profile , Enable 2-way Authentication as well as the Additional Certificate Validation option to be made available. Security Profile Click Browse to select a security profile with certificate and configuration to use, if you prefer to use a secure connection. Refer to Security Profile for more information. Enable 2-way Authentication Enables two-way authentication for the communication channels. Additional Certificate Validation If enabled, the agent will perform certificate chain validation and revocation status check. You must configure CRL (Certificate Revocation List) file path or CRLDP (Certificate Revocation List Distribution Points) if this is enabled. CRL File Path Path to the Certificate Revocation List (CRL) file that has been downloaded locally. Enable CRLDP will be greyed out when configuring the file path. Enable CRLDP Enable Certificate Revocation List Distribution Points (CRLDP) extension support. CRL File Path will be greyed out when enabling CRLDP. Server Timeout (s) The number of seconds before the server closes a request. If the timeout is set to 0 (zero) no timeout will occur. Default value is 5. Authentication Tab The Authentication tab contains settings related to the OAuth 2.0 Authentication for the REST Server_Deprecated agent. Open REST Server_Deprecated agent configuration - Authentication tab Setting Description Setting Description OAuth 2.0 Authentication If enabled, the REST Server_Deprecated agent will check all incoming HTTP requests for access tokens and validate the access tokens. Only access tokens generated by the Authorization Server will be accepted. If not enabled, the REST Server_Deprecated agent will not check the incoming HTTP requests for access tokens.The Authorization Server is a Service Provider in MediationZone which is used to generate OAuth 2.0 access tokens. For more information on how to setup the Authorization Server, please refer to 2.1 Enabling Authorization Server Note! You must select this option for the OAuth Truststore, OAuth Truststore Password and JWT Public Alias to be made available. Enable Use TLS, under the General tab, is also required if OAuth 2.0 Authentication is enabled. OAuth Truststore Path to the truststore where the public key for access token validation from the OAuth2 Service is stored. Only Java KeyStore (JKS) format is supported. Enter the full path to a truststore file on the local or mounted disk on the execution host. OAuth Truststore Password The password for the truststore. Note! All keys must have the same password as the truststore. JWT Public Key Alias The alias of the key inside the truststore to be used to access token validation. The key referred here is the public key of the RSA key pair defined in the "jwt" configuration of the Authorization Server. Note! The alias password, when configured during the construction of the truststore certificate should be the same as OAuth Truststore password. Advanced Tab The Advanced tab contains additional properties that can be configured for the REST Server_Deprecated agent. Open REST Server_Deprecated agent configuration - Advanced tab See the text in the Properties field for further information about the other properties that you can set.

---

# Document 940: Open API Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639682/Open+API+Profile
**Categories:** chunks_index.json

If you want to use Open API 3.0 with HTTP/2 agents, you require an Open API profile configuration. You select the profile that you configure in the HTTP/2 Server agent configuration. In the Open API profile configuration, you import your OpenAPI specification file and view any other included files defined by the spe cificatio n. All schemas that require a UDR must be named. Due to a limitation in the third-party parsing library used by OpenAPI, unnamed schemas cannot be detected and will not generate a corresponding UDR. Therefore, you must name all schemas that require a UDR. Field names in the yaml specification file containing the following symbols will be replaced with unique string of characters during the UDR generation process as shown below: @ -> _40_ . -> _2E_ - -> _2D_ For example, the field "test-name" will be converted into "test_2D_name" as a UDR. All tab indentations found in the Open API yaml files will be replaced with two spaces. Configuration The Open API profile consists of the following tabs: 1 General Tab 2 Advanced Tab General Tab Open Open API Profile Configuration - General tab Setting Description Setting Description Import Imports the Open API specification file from the local file directory of the desktop or desktop client. Click on this button again after a file has been imported, to import another Open API specification file to overwrite the previously imported specification file. View Opens the selected OPEN API schema file. Referenced Files Lists the yaml files that are already imported into the Open API profile. Clicking on any of the records in the list will allow you to view the yaml file or remove it from the profile. Clicking on the Import button on the Reference Files will allow you to import the reference files for the main Open API specification file as well as any other reference files that are required. Importing Open API Specification Files Users can choose to import or upload the yaml specification files via the following: Browse and select from local directory Drag and drop the file(s) to the Import Open API file dialog box Open Import Open API file dialog box For any files that contain missing dependencies, you can choose to import or upload the referenced files via the following: Browse and select from local directory Drag and drop the file(s) to the Import referenced Open API file(s) dialog box Open Import referenced Open Api file(s) dialog box To view each Referenced file , select the .yaml file from the table and click View Schema . To remove the .yaml file from the table, click Remove . Advanced Tab Open Open API Profile Configuration - Advanced tab Setting Description Setting Description Ignore Read Only Tag Select this option to ignore the readOnly tag in the schema file. Info! When UDRs are generated from the OpenAPI specification file, some UDR fields found in the response body are marked as read-only. This prevents HTPP/2 Server from setting these fields in the APL to generate a proper response. By selecting this option, it allows HTTP/2 Server agents to be able to set the readOnly fields in the APL for use cases that require a response from the HTTP/2 Server agent. Add additionalProperties: false to component schemas Select this option to add the additionalProperties: false to each component in the schema file. Limitations This section lists the limitations that users may encounter when using the OpenAPI profile. OpenAPI specification schema which contains oneOf tag will be decoded as a map instead of a UDR In the following example, the SubscriptionData schema contains the subscrCond property with oneOf tag: Example: SubscriptionData schema contains the subscrCond with oneOf tag SubscriptionData: description: Information of a subscription to notifications to NRF events, included in subscription requests and responses type: object required: - nfStatusNotificationUri - subscriptionId properties: nfStatusNotificationUri: type: string reqNfInstanceId: $ref: 'TS29571_CommonData.yaml#/components/schemas/NfInstanceId' subscrCond: oneOf: - $ref: '#/components/schemas/NfInstanceIdCond' - $ref: '#/components/schemas/NfInstanceIdListCond' - $ref: '#/components/schemas/NfTypeCond' - $ref: '#/components/schemas/ServiceNameCond' - $ref: '#/components/schemas/AmfCond' - $ref: '#/components/schemas/GuamiListCond' - $ref: '#/components/schemas/NetworkSliceCond' - $ref: '#/components/schemas/NfGroupCond' - $ref: '#/components/schemas/NfSetCond' - $ref: '#/components/schemas/NfServiceSetCond' - $ref: '#/components/schemas/UpfCond' - $ref: '#/components/schemas/ScpDomainCond' - $ref: '#/components/schemas/NwdafCond' - $ref: '#/components/schemas/NefCond' The subscrCond is a schema of NfSetCond but it is decoded as a map with key value pair as shown below: [openapi.issue_http.OAPI_NrfMgt.udr.SubscriptionData] nfStatusNotificationUri: http://localhost/dummy subscriptionId: 123456 subscrCond: {nfSetId=MU01} Example: SubscriptionData schema decoded in the APL: To retrieve the value of the map, enter the following code in APL: string ID = mapGet((map<string, any>)subscriptionData.subscrCond, "nfSetId"); debug(ID); The debug output is as follows: MU01 View Included Files Tab Setting Description Setting Description Included Files A list of files that are referenced in the imported OpenAPI specification file will be shown here. Selecting from this list will have its contents be displayed in the box below.

---

# Document 941: Duplicate UDR Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607178
**Categories:** chunks_index.json

A Duplicate UDR agent is configured in two steps. First, a profile has to be defined, then the regular configurations of the agent are made. The Duplicate UDR profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Duplicate UDR profile configuration, click the New Configuration button from the Configuration dialog available from Build View , and then select Duplicate UDR Profile from the menu. To open an existing Duplicate UDR profile configuration, click on the configuration in the Configuration Navigator, or right-click on the configuration and then select View Configuration . The contents of the menus in the menu bar may change depending on which configuration type that has been opened. The Duplicate UDR profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . Item Description Item Description External References Select this menu item to Enable External References in an agent profile field. Refer to Enabling External References in an Agent Profile Field in External Reference Profile for further information. General Tab The General tab is displayed by default. In the General tab, the File Storage is displayed as the default Storage for a New Configuration. Open Duplicate UDR New Configuration The General tab is split into two sections, the Storage settings and the UDR settings. To begin, first select either File Storage or SQL Storage with the Storage selector. When a Storage is selected, only settings relevant to the Storage will be displayed. Storage Settings The Storage settings is the top section of the General Tab. It contains settings to setup the Duplicate UDR cache storage and settings for managing the cache size and data expiration. File Storage Open General Tab Storage settings for File Storage The Duplicate UDR profile configuration contains the following Storage settings specific to File Storage: Setting Description Setting Description Storage Host In the drop-down menu, the preferred storage host, where the duplicate UDRs are to be stored, can be selected. The choice for storage of duplicate repositories is either on a specific EC Group or Automatic . If Automatic is selected, the same EC Group used by the running workflow will be selected. When the Duplicate UDR Inspector is used, the EC Group is automatically selected. Note! The workflow must be running on the same EC Group as its storage resides, otherwise, the Duplicate UDR Agent will refuse to run. If the storage is configured to be Automatic, its corresponding directory must be a file system shared between all the EC Groups. Directory An absolute path to the directory on the selected storage host, in which to store the duplicate cache. If this field is greyed out with a stated directory, it means that the directory path has been hard-coded using the mz.present.dupUDR.storage.path property. This property is set to false by default. Example - Using the mz.preset.dupUDR.storage.path property To enable the property and state the directory to be used: mzsh topo set val:common.mz.preset.dupUDR.storage.path '/mydirectory/dupudr' To disable the property: mzsh topo unset val:common.mz.preset.dupUDR.storage.path For further information about all available system properties, see System Properties . External References specific to File Storage can be used with the following field: Directory SQL Storage Open General Tab Storage settings for SQL Storage The Duplicate UDR profile configuration contains the following Storage settings specific to SQL Storage: Setting Description Setting Description Database Profile This is the database in which to store the Duplicate UDR cache. Click the Browse... button to get a list of all the database profiles that are available. For further information see Database Profile . Duplicate UDR SQL Storage is supported for use with the following database: SAP HANA 2.0 SP 7 PostgreSQL 12 and above Note! If no changes are made to the Duplicate UDR profile, changes to the settings of selected Database Profile will only be detected during Duplicate UDR Agent workflow run. Generate SQL Click this button to bring up a dialog that will contain the SQL statements for the table schema generated for the Duplicate UDR profile. Note! The Duplicate UDR profile Configuration Key is used for generating the names of the Duplicate UDR database tables. You will need to save the profile at least once for the profile to have a Configuration Key, so that proper database table names can be generated. Warning! Users will have to copy the SQL script generated in the dialog and execute the SQL script separately to create the Duplicate UDR tables in the database selected with the Database Profile selector. The Duplicate UDR Profile screen will not automatically create the tables in the database for you. Generate SQL Dialog Box When a user clicks on the SQL Storage Generate SQL button, the associated dialog box will open. The Copy button is a convenient way to copy the whole Create Tables SQL Script. Open SQL Storage Generate SQL Dialog box More Storage Settings The Duplicate UDR profile configuration contains the following Storage settings common to both File Storage and SQL Storage. Setting Description Setting Description Max Cache Age (days) The maximum number of days to keep UDRs in the cache. The age of a UDR stored in the cache is either calculated from the Indexing Field (timestamp) of a UDR in the latest processed batch file, or from the system time, depending on whether Based on System Arrival Time or Based on Latest Time Stamp in Cache is selected. If the Date Field option in the UDR settings section below, is not enabled for the Indexing Field , Max Cache Age setting will be disabled and ignored, and cache size can only be configured using the Max Cache Size settings. If enabled, the default value is 30 days. Note! If enabled and the Duplicate UDR Agent receives UDRs that are too old (exceeded Max Cache Age ), the UDRs will not be processed and will simply be routed to the usual route. Duplicate checking is not performed for these UDRs and a warning will be logged in the System Log. Note! The age calculation cannot be performed if the cache is empty. Based On System Arrival Time When Max Cache Age is enabled, this radio button is selected by default, the calculation of cached UDR's age will be based on the time when a new batch is being processed. In case of a longer system idle time, this setting may have a major impact on which UDRs that are removed from the cache. For more information about the difference between Based on System Arrival Time and Based on Latest Time Stamp in Cache when calculating the UDR age, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Based on Latest Time Stamp in Cache When Max Cache Age is enabled and this radio button is selected, the UDR cache age calculation will be made toward the latest Indexing Field (timestamp) of a UDR that was included in the previously processed batch files. For more information about the difference between Based on System Arrival Time and Based on Latest Time Stamp in Cache when calculating the UDR age, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Max Cache Size (thousands) The maximum number of UDRs to store in the duplicate cache. The value must be in the range of 100-9999999 (thousands), the default is 5000 (thousands). The cache will be made up of containers partitioned by the key from the Indexing Field below. For every incoming UDR, it will be determined in which cache container the UDR will be stored. During the initialization phase of each batch, the agent checks if the cache is full. If the check indicates that there will be less than 10% of the cache available, cache containers will start to be cleared until at least 10% free cache is reached, starting with the oldest container. Note! Depending on how many UDRs are stored in each container, this means that different amounts of UDRs may be cleared depending on the setup. If the Indexing Field of all the UDRs happens to have the same value, then all the UDRs in the cache will be cleared. Note! If you have a very large cache size, it may be a good idea to split the workflows in order to preserve performance. Enable Separate Storage Per Workflow This option enables each workflow to have a separate storage that is checked for duplicates. This allows multiple workflows to run simultaneously using the same Duplicate UDR profile. However, if this checkbox is selected, a UDR in a workflow will not be checked against UDRs in a different workflow. Note! Duplicate UDR Inspector currently does not support Duplicate UDR profiles with Enable Separate Storage Per Workflow enabled For both File Storage and SQL Storage, External References can be used with the fields: Max Cache Age Max Cache Size UDR Settings The UDR settings is the bottom section of the General tab. It contains settings to select which UDR to apply duplicate checks, the UDR field used to segment the Duplicate UDR cache into containers and information to manage the scope and contents in the Duplicate UDR containers. Open General Tab UDR Settings The Duplicate UDR profile configuration contains the following UDR settings common to both File Storage and SQL Storage. Setting Description Setting Description Type The UDR type the agent will process. Indexing Field The UDR field is used as an index in the duplicate comparison. Fields of type long (in milliseconds) and date are valid for selection. The cache will be made up of containers partitioned by the key from this Indexing Field . If Date Field below is disabled, each container will cover an interval of 50 seconds. If Date Field is enabled, each container will cover an interval of 10 minutes. For every incoming UDR, it will be determined in which cache container the UDR will be stored. For performance reasons, this field should preferably be either an increasing sequence number or a timestamp with good locality. This field will always be implicitly evaluated. For further information, see the section, Duplicate UDR Using Indexing Field Instead of System Time . Date Field If selected, the Indexing Field will be treated as a timestamp instead of a sequence number, and this must be selected to enable the Max Cache Age (days) field above to be configured. Note! If the UDR Indexing Field value is a timestamp that is configured to be 24 hours or more ahead of the system time, the workflow will abort. Checked Fields In addition to the Indexing Field , the Checked Fields will be used for the duplication evaluation when deciding if a UDR is a duplicate. Note! If the Checked Fields or Indexing Field are modified after an agent is executed, the already stored information will be considered useless the next time the workflow is activated. Hence, duplicates will never be found amongst the old information since another type of metadata has replaced them. Advanced Tab The Advanced tab is available when you have selected SQL Storage for your Duplicate UDR Storage . It contains properties that can be used for performance tuning. For information about setting up SQL Storage for better performance, see Duplicate UDR SQL Storage Setup Guide . Open Advanced Tab for SQL Storage

---

# Document 942: Data Hub Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676337
**Categories:** chunks_index.json

The Data Hub profile is used by the Data Hub Forwarding Agent and Data Hub Task Agent to connect to an Impala database via Cloudera JDBC, and an HDFS. This profile is also used to map the input UDRs to the Data Hub forwarding and an Impala database table. Data Hub Query also uses the profile to access the data stored in the Impala database. The Data Hub profile is loaded when you start a workflow that relies on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Data Hub profile configuration, click the New Configuration in the Build view and select Data Hub Profile from the Configurations browser. The contents of the menus in the menu bar may change depending on which configuration type that has been opened in the currently active tab. The Data Hub profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Build View . Impala Tab The Impala tab contains connection settings for the Impala database. Open Data Hub profile configuration - Impala tab Setting Description Setting Description Host Enter the hostname or the IP address of the Impala database. Port Enter the port number that is configured for the connection into the Impala database. Enable TLS Allows the user to enable the TLS functionality for the connection to the Impala database. Allow Self Signed Cert Checkbox to enable the usage of Self Signed Certificates. If this checkbox is selected, both Trust Store File Path and Trust Store Password will be disabled. Trust Store File Path Enter the location of the trust store file. This path is used to store certificates from other Certified Authorities. The setup is required to establish a successful connection at the client side. Note! This field is enabled when the Enable TLS checkbox is selected. However, when the Allow Self Signed Cert checkbox is selected, this field is disabled. Trust Store Password Enter the passphrase of the trust store file. This password is used to access the certificates stored in the trust store. Note! This field is enabled when the Enable TLS checkbox is selected. However, when the Allow Self Signed Cert checkbox is selected, this field is disabled. Key Store File Path Enter the location of the key store file. This path is used to store your credential. This is required when setting up the server side on the SSL. Note! This field is enabled when the Enable TLS checkbox is selected. Key Store Password Enter the passphrase of the key store file. This password is used to access the credentials stored in the key store. Note! This field is enabled when the Enable TLS checkbox is selected. Database Name Click the Refresh button next to Database Name to retrieve a list of available databases and then select a database from the drop-down menu. The Tables Mapping tab will appear. Refresh Click this to retrieve a list of available databases. Test Connection Click to test the JDBC connection to the Impala database. HDFS Tab The HDFS tab contains the properties required for the connection to HDFS as well as properties for staging paths. Open Data Hub profile configuration - HDFS tab Setting Description Setting Description HDFS URI Enter the URI of the HDFS NameNode. Staging Path Enter the absolute path to an existing directory on the HDFS. This directory will be used as a staging directory for the data. MZ Temp Path Enter the path for temporarily storing the files locally before it is inserted into the HDFS staging directory. Create Directory Select this to create the MZ Temp Path directory if it does not exist. Advanced Tab The Advanced tab contains the properties needed for Data Hub agent to connect to any Cloudera configurations that has LDAP and Kerberos enabled. Open Data Hub profile configuration - Advanced tab See the text in the Properties field for further information about the other properties that you can set. Kerberos JVM Due to the behavior of the Kerberos JVM, Data Hub profiles and agents that will interface with a Kerberos-enabled Cloudera must be configured to run on the same EC. Kerberos Keytab You will need to ensure the .keytab file is located in the same host as the EC that will be running the workflow with the Data Hub agent. Tables Mapping Tab Open Data Hub profile configuration - Tables Mapping tab Setting Description Setting Description Table Select a database table from the drop-down list. The name of the table columns and their data type will appear. Hint! You can map more than one table to a UDR type. This makes it possible to use the same profile for multiple Data Hub agents. Note! If you have created a new table that appears to be missing in the drop-down list, click the Refresh button in Impala tab and then reselect the database. The table should now be listed. UDR Type Click the Browse button and then select a UDR type that will be routed to a Data Hub agent. For information about Ultra types that can be mapped to Impala types, see Compatible Types below. Auto Map Click this button to automatically map UDR fields and database columns with identical names. The automatic mapping is not case-sensitive. If a field cannot be mapped, the current value in the UDR Field column remains unchanged. Column The name of the columns in the selected table. Type This displays a list of valid Impala types based on the Column in the selected table. Each column must be mapped against a type. For information about Ultra types that can be mapped to Impala types, see Compatible Types below. UDR Field Select a UDR field from the drop-down list. This represents selectable fields available based on the selected UDR Type above. Date Hint Select the date format to be stored in the table. This is required for a partition column. A date in the Ultra format can be stored as INT , BIGINT and SMALLINT types. When the column is a partition, you must select the corresponding date format from this drop-down list: yyyyMMddHH - e g 2018123013 yyyyMMdd - e g 20181230 yyyyMM - 201812 yyyy - 2018 The selected format determines the granularity of date pickers in the Web UI. When you change the Date Hint value of a partition column for an existing profile, make sure to review the settings of Data Hub task workflows that depend on the updated profile and table. If the table already contains data when you change the Date Hint of a partition column, the Data Hub task agent will fail to delete any of the partitions. To resolve this issue you must manually delete the partitions that contain the old format, e g using Cloudera Hue. When the UDR fields do not have names that match the database columns, you can map these manually by clicking the corresponding cell and selecting the field from a drop-down list. Compatible Types The following table shows allowed mappings of Impala types to Ultra types. Impala Type Impala Range Ultra Type Ultra Range Impala Type Impala Range Ultra Type Ultra Range STRING Maximum of 32,767 bytes string Unlimited INT -2147483648 to +2147483647 int -2,147,483,648 to +2,147,483,647 short -32,768 to +32,767 byte -128 to +127 FLOAT 1.40129846432481707e-45 to 3.40282346638528860e+38 (positive or negative) float 1.40129846432481707e-45 to 3.40282346638528860e+38 (positive or negative) DOUBLE 4.94065645841246544e-324d to 1.79769313486231570e+308 (positive or negative) double 4.94065645841246544e-324d to 1.79769313486231570e+308d (positive or negative) float 1.40129846432481707e-45 to 3.40282346638528860e+38 (positive or negative) BOOLEAN TRUE or FALSE boolean true or false BIGINT -9223372036854775808 to +9223372036854775807 long -9223372036854775808 to +9223372036854775807 int -2,147,483,648 to +2,147,483,647 short -32,768 to +32,767 byte -128 to +127 REAL 4.94065645841246544e-324d to +1.79769313486231570e+308 (positive or negative) double 4.94065645841246544e-324d to 1.79769313486231570e+308d (positive or negative) SMALLINT -32768 to +32767 short -32,768 to +32,767 byte -128 to +127 TINYINT -128 to +127 byte -128 to +127 TIMESTAMP YYYYMMDDHH date yyyyMMddHH

---

# Document 943: Configuration Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638352/Configuration+Types
**Categories:** chunks_index.json

Overview The Configurations Browser is used to access the different configurations available in MediationZone. Each individual configuration has its own set of properties and can differ visually from others. The standard menu buttons are shared between each of them, but depending on the complexity and available fields some configurations can be presented to the users in a different way. Common Configurations Common configurations refer to functions that are not dependent on specific profiles or agents. They usually serve to configure a specific process that modifies the way configurations are executed. For all configurations, except Workflow, a dialog will open up where you create your configuration. For Workflows, the Workflow Editor will open up. The following table shows a list of all common configurations: Configuration Description Configuration Description External References Profile The External Reference profile enables you to use configuration values that originate from property files or exported environment variables from the Platform's startup shell. KPI Profile The KPI profile contains a KPI Management service model and is used by the KPI Cluster, KPI Cluster Out, and KPI agents. Reference Data Profile In a Reference Data profile configuration, you can select the tables that should be available for query and editing via the Reference Data Management Web UI, or RESTful interface. Unit Test The Unit Test configuration enables the creation of tests used in continuous deployment scenarios. Workflow The Workflow configuration enables the creation and management of three different types of workflows: batch, real-time, and task. See the Working with Workflows section to learn more. W orkflow Bridge Profile The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. The profile ties the workflows together. Profile Configuration Types Profiles contain configuration options relevant to the associated agent. They usually require credentials configurations and are used to collect and enrich the data that is processed by the system. Configuration Description Configuration Description 5G Profile The 5G Profile enables 5G communication with HTTP/2 agents. Aggregation Profile Aggregation consolidates related UDRs that originate from either a single source or from several sources, into a single UDR. Related UDRs, are grouped into "sessions" according to the value of their respective fields, and a set of configurable conditions. Amazon Profile The Amazon Profile is a generic profile used for setting up Amazon S3 credentials and properties that can be used by various other profiles or agents. Analysis APL Code Editor The APL Code Editor is used to create generic APL code that can be imported and used by several agents and workflows. APL Collection Strategy The APL Collection Strategies are used to set up rules for handling the collection of files from the Disk, FTP, SFTP, and SCP collection agents. Archive Profile The Archive Profile contains storage, naming scheme, and lifetime for targeted files. Audit Profile The Audit Profile offers the possibility to output information to user-defined database tables. Azure Profile The Azure Profile is used for setting up the access credentials and properties to be used to connect to an Azure environment. Categorized Grouping Profile The Categorized Grouping Profile is loaded when you start a workflow that depends on it. Conditional Trace Templates Conditional Trace is a trouble-shooting function that allows you to set a trace filter on agents and/or UDRs in real-time workflows route(s) and/or Analysis agent(s) for either a specific field value or a range of field values. Couchbase Profile The Couchbase profile is used to read and write bucket data in a Couchbase database and can be accessed by workflows using Aggregation, Distributed Storage, or PCC. Data Masking Profile The Data Masking profile selects the masking method you want to use, which UDR types and fields you want to mask/unmask, and any masking method-specific settings. Data Veracity Profile The Data Veracity Profile is used to select the particular database to which Data Veracity will be connected. Diameter Application Profile The D iamete r Application Profile captures a set of AVP and command code definitions that are recognized by the Diameter Stack agent during runtime. Diameter Routing Profile The Diameter Routing Profile enables you to define the Peer Table and the Realm Routing Table properties for the Diameter Stack agent. Duplicate Batch Profile Duplicate Batch Detection agent provides duplication control on incoming batches. Duplicate UDR Profile Duplicate UDR Detection agent provides duplication control on incoming UDRs. Elasticsearch Profile The Elasticsearch profile is used to read and write data in an Elasticsearch Service in AWS and can be accessed by batch workflows using Aggregation agents. Encryption Profile TheEncryption Profile you make encryption configurations to be used by the Encryptor agent. Event Notifications The Event Notification configuration offers the possibility to route information from events generated in the system to various targets. External Reference Profile The External Reference profile enables you to use configuration values that originate from property files or exported environment variables from the Platform's startup shell, File System Profile The File System Profile is used for making file system-specific configurations, currently used by the Amazon S3 collection and forwarding agents. GCP Profile The GCP Profile is used for setting up the access credentials and properties to be used to connect to a Google Cloud Platform service. GCP PubSub Profile The GCP PubSub Profile is used for setting up the Google PubSub Subscription and Topic for a Google Cloud Project. Inter Workflow Profile The Inter Workflow profile enables you to configure the storage server that the Inter Workflow forwarding and collection agents use for communication. JMS Profile The JMS profile contains settings that you use to connect and acquire both a Connection Factory object and a Destination object from a JNDI service. Kafka Profile The Kafka profile enables you to configure which topic and which embedded service key to use. Open API Profile If you want to use Open API 3.0 with HTTP/2 agents, you require an Open API profile configuration. You select the profile that you configure in the HTTP/2 Server agent configuration. The Prometheus Filter Use this filter to configure the Prometheus metrics that are going to be exposed for scraping. The purpose of the Prometheus Filter is to prevent the flooding of metrics in the storage of the Prometheus host. Python Interpreter Profile With the Python Interpreter profiles, you can configure which executable to use, and also which working directory you want to have. Python Module With the Python Module configurations, you can write shared Python code that can be imported by multiple Python agents. Redis Profile A Redis profile is used to read and write data in a Redis database and can be accessed by real-time workflows using Aggregation agents. REST Server Profile The REST Server Profile is used to define the endpoint URI for any particular REST server agent. SAP RFC Profile The SAP RFC profile dynamically generates UDRs based on selected SAP RFC functions that are part of an SAP system. Security Profile The Security profile is a generic profile that you can use to make encryption configurations that can be used by several agents and profiles. Shared Table Profile This section describes the Shared Table profile. This profile enables workflow instances to share tables for lookups. SNMP Collection Profile The SNMP Collection profile allows you to import the MIB files you want to use to build your target UDRs. SNMP OID Profile With the SNMP OID profile, you can select to configure which OIDs, UDR types, and fields to poll, outside of the SNMP Request agent itself, which enables several agents to use the same configuration. Suspend Execution This section includes information about the configuration option Suspend Execution. System Insight Profile The System Insight Profile allows you to create, edit or remove profiles and filters that you want to use to display or store statistics using the system insight service. Workflow Bridge Profile The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. Workflow Group The workflow group configuration enables you to manage workflow groups.

---

# Document 944: CSV Formats - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658076/CSV+Formats
**Categories:** chunks_index.json

MediationZone includes support for decoding and encoding data in CSV format.

---

# Document 945: Inter Workflow Real-Time Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033638/Inter+Workflow+Real-Time+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Inserted batch: filename Reported when a file has been closed in the target directory (hence ready for collection). The message is only used in batch workflows. Debug Events There are no debug events for this agent. Loading

---

# Document 946: Services - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655702/Services
**Categories:** chunks_index.json

Plugin agents may utilize built-in services for various purposes. Some services come with separate user interfaces in order to be configurable. Services with a graphical user interface are added as an extra tab in the agent's configuration. To use a service, the Inspectable class name of the service must be returned by the getServices method in the Inspectable class of the agent. At runtime, the agent executable must gather the service's executable, using the getService method in either DRBatchServerEnv or DRRealtimeServerEnv , depending on the type of workflow. The getService method takes the service Inspectable name as an argument and returns the Executable for the service. During runtime, each service Executable acts differently, that is, the Executable for the specific service must be examined. Filename Template The Filename Template service is a service that file-based Forwarding agents may use. It is a service with a user interface where the user can define how to build the file name, based on MIM or user-defined values with the possibility to pad, align, etc. To use this service, the DRFNTServiceInsp class name must be returned in the agent's Inspectable class. The Filename Template executable is called DRFNTServiceExec and it provides one method to get the generated filename. Since the service may depend upon MIM values published in drain , this method must be invoked at endBatch to get the generated filename. Filename Sequence This service allows the user to define a part of a filename to contain a sequence. This is a service with a user interface that is also possible to turn off. The service keeps track of what the next sequence number is. It is also possible to change what the next sequence number will be in its user interface. The Executable class is called DRFNSServiceExec . At runtime, the agent will check whether the service is enabled before invoking methods on it (except for the state handling). The service Executable defines a method called isServiceConfigured that returns information regarding whether it has been configured or not. The agent must manage the state of this service as well as save the result from the getSequenceNoChk and getNextSequenceNo methods. The isSequenceNoChangedByGUI must be invoked at startup to find out if the user has changed the next sequence number. The arguments supplied to the method are the ones saved with the state, that is, the ones returned by getSequenceNoChk and getNextSequenceNo . If the result is true , the agent must consider an update to its state. To find out if the next file(s) to collect have correct sequence numbers, the isNextFileToRetrieve or getConsecutiveSequenceNumbers are used. The isNextFileToRetrieve returns true if the given file has the next sequence number. The getConsecutiveSequenceNumbers method takes a list of filenames and returns them, sorted according to the sequence number. If there is a gap in the sequence, an error is returned. The agent must manually tell the service to update the sequence number, since services are not part of the transaction. This must be done prior to calling endBatch. Sort Order This is a service with a user interface that could be used by Collection agents. The Sort Order service allows the user to define the order in how files will be collected. The user may define sorting based on a segment of the filename, or by using a regular expression. This service may also be turned off. An agent that uses both the Filename Sequence service and this service must make sure that not both services are turned on at the same time. At runtime, the Executable of the agent must check whether the service is configured, using the isServiceConfigured method. The service defines two methods for sorting the files and the agent may use either, based on an array of strings, or based on a list of strings. File Sort Order This service adds the option to sort on file modification time, on top of the functionality of the Sort Order service previously described. The only difference is that the sort methods operate on the Java File object instead of String. ECS Batch To support the cancellation of batches in batch-based collectors, an ECS Batch service must be used. If the workflow has been configured to abort immediately, the chosen service manages this by ignoring calls, which means that the agent does not need to consider ECS. There are two ECS Batch services: DRECSBatchServiceExec - This service uses temporary files for sending data to ECS. DRECSStreamBatchServiceExec - This service sends a stream to ECS, without creating temporary files. Note! Using DRECSStreamBatchServiceExec will result in less I/O overhead since it does not make use of a cache. DRECSBatchServiceExec By using this ECS Batch service, raw data will be written to a temporary file and sent to ECS when the agent receives a cancelBatch call. Since services are not aware of the transaction state, the agent must call the methods on the batch service according to the transaction state. When starting a collection of a new batch, the begin method must be called. For every block routed into the workflow, the consume method must be called with the same data. If the file has been correctly processed by the workflow, i.e. the agent goes to commit , the cancel method must be called on the service. This removes the temporary file from the file system. If the workflow goes to cancelBatch , the commit method must be called. In case of a failure, i.e. neither commit nor cancel has been called on the service, it is recommended to call cancel in the deinitialize method as well. DRECSStreamBatchServiceExec This service does not make use of a cache during batch workflow routing. No temporary files are created which results in less I/O overhead. When the batch workflow is canceled through a cancelBatch call, the service takes a java.io InputStream and sends it to ECS through its insertBatch method. Note! The insertBatch method can only be called from the agent's cancelBatch method, otherwise an exception will be thrown.

---

# Document 947: Basic Administration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783187/Basic+Administration
**Categories:** chunks_index.json

This section contains step-by-step procedures describing the basic system administration tasks. The procedures and commands described expect that the $MZ_HOME/bin directory exists in the $PATH variable for the mzadmin user. After a properly executed installation, this should be the case. This chapter includes the following sections: File System Permissions Remote Access to Containers Starting and Stopping the System Managing Picos System Properties Desktop Properties Legacy Desktop Properties Clearing the Pico Cache Resetting the mzadmin Password Changing Database Password Increasing the Maximum Number of File Descriptors Out of Memory Info in System Log Access Controller Extensions Reading the License File

---

# Document 948: Workflow Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816436/Workflow+Configurations
**Categories:** chunks_index.json

Workflows consist of software agents, each providing specific functionality, that is linked into automated data flows of virtually any complexity. An intuitive and powerful drag-and-drop management user interface covers all aspects of workflow life-cycle management. Using the workflow concept, interface and processing agents can be configured, deployed, and maintained in a modular way. Workflows may also be inter-linked into scenarios of virtually any complexity, where the output of one workflow is the input of another. Workflow hierarchies can thus be defined, from technology-specific collection flows to convergent service-specific flows for downstream systems. Once configured, workflows are automatically distributed on designated servers that are part of an deployment and executed in accordance with defined scheduling criteria. High availability capabilities ensure that real-time workflows are executing at all times. Batch mode workflows can be configured to execute at period intervals. Execution plans can be defined to ensure workflow execution according to internal strategies and processes.

---

# Document 949: Websocket Server Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644085/Websocket+Server+Agent+Configuration
**Categories:** chunks_index.json

The Websocket Server agent configuration tab is displayed when right clicking on the Websocket Server agent and selecting the Configuration option, or when double-clicking on the agent in the Work flow Editor. Open The Websocket Server agent configuration dialog Setting Description Setting Description Host Enter the hostname or IP that the server should bind to. Port Enter the port to bind the server to. By default this is set to 4200. Ping Timeout (s) Timeout for the connections configured in Servers. When no data is received on the Websocket, a ping will regularly be sent to detect if a connection has been lost. Use TLS Select this check box if you want to use TLS. The fields beneath will then be activated. Require Client Authentication Select this check box if you want to make client authentication mandatory. Security Profile

---

# Document 950: Azure Event Hub Consumer Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204738221/Azure+Event+Hub+Consumer+Agent+Configuration
**Categories:** chunks_index.json

To open the Azure Event Hub Consumer agent configuration dialog from a workflow configuration, you can do one of the following: double-click the agent icon select the agent icon and click the Edit button Configuration The Azure Event Hub Agent Configuration consists of the following tabs: 1 General Tab 2 Advanced Tab General Tab Open Azure Event Hub Consumer agent configuration - General tab Setting Description Setting Description Profile Select the Azure profile that has been configured with the connection details for the Azure Event Hub. For more information about how to configure the profile, refer to Azure Profile . Partition Enter the partition ID of the event hub to be used by the agent. This field is optional. Storage Directory Enter the directory path of where the data checkpoints are to be stored on the local directory of the EC server the workflow is running on. The data checkpoint consists of a sequence number stored in a binary file that is used by the Event Hub Consumer agent to resume data collection from where it stopped. Should the field be left empty, the agent will either Start at latest or Start at earliest , depending on the configuration set below. Consumer Group Enter the name of the consumer group the agent will be retrieving from. Start at latest (from end) Events are selected from the last offset from when the workflow was started. If you select this option, there is a risk that data can be lost after a restart. This configuration will only be applied when the workflow is running for the first time or if the Storage Directory is left empty or missing. Start at earliest (from beginning) Events are collected from the first offset. If you select this option, there is a risk that events will be processed multiple times after a restart. This configuration will only be applied when the workflow is running for the first time or if the Storage Directory is left empty or missing. Use EventDataUDR Select this option to send the EventDataUDR which contains the body as a string along with additional metadata that the Event Hub provides. Note! By default, the Azure Event Hub Consumer agent sends the event body as a bytearray. Advanced Tab The Advanced tab contains additional settings for checkpointing the offset of events collection when the workflow is initiated. When a directory path is specified in the Storage Directory field, the settings under the Advanced tab will override the settings for Start at latest (from end) and Start at earliest (from beginning) in the General tab. Open Azure Event Hub Consumer agent configuration - Advanced tab Setting Description Setting Description Ignore out of sequence range S elect this checkbox to enable the following options to predefine where you would like the reading of offset to take place after a workflow restarts. Start at latest (from end) Events are selected from the last offset from when the workflow was started. If you select this option, there is a risk that data can be lost after a restart. Start at earliest (from beginning) Events are collected from the first offset. If you select this option, there is a risk that events will be processed multiple times after a restart.

---

# Document 951: APN Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/285638780/APN+Agent+Configuration
**Categories:** chunks_index.json

To open the APN agent configuration dialog from a workflow configuration, you can do one of the following: Double-click the agent icon Select the agent icon and click the Edit button Open APN agent configuration dialog Setting Description Setting Description Profile Select the APN profile that contains the the Apple Push Certificate details. For more information on configuring the profile, see APN Profile .

---

# Document 952: APL Code Editor - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999066/APL+Code+Editor
**Categories:** chunks_index.json

The A PL Code E ditor, as opposed to the Aggregation and Analysis agents' code areas, is used to create generic APL code, that is, code that can be imported and used by several agents and workflows. To open the APL Code Editor, click the New Configuration button in the upper left part of the Desktop window, and then select APL Code from the menu. Open The menu items that are specific to APL Code Editor are described in the following table: Button Description Button Description Edit Edits the entered APL code. New Creates a new APL code entry. Open Opens the list of saved APL code entries. Save As Opens the Save As dialog box where you can save the APL code entry. You must enter the mandatory parameters to save the entry. Specify the Folder by using the Browse button and navigating to the destination, then confirm the selection by pressing the OK button. Specify the APL code name in the Name field and you can type an optional Version Comment. Open Save As Dialog Box Permissions View/edit the APL code permissions. Open Permissions Dialog Box References View the External References available for the APL code entry. History View the version history for the APL code entry. Help Open the online documentation. The generic code is imported by adding the following code in the agent code area, using the import keyword: import apl.<foldername>.<APL Code configurationname> If generic code is modified in the APL Code Editor, the change is automatically reflected in all agents that contain this code the next time each workflow is executed. Since function overloading is not supported, make sure not to import functions with equal names, since this causes the APL code to become invalid, even if the functions are located in different APL modules. This also applies if the functions have different input parameters, for example, a(int x) and a(string x) . Note! Not all functions work in a generic environment, for example, functions related to specific workflows or MIM-related functions. This type of functionality must be included in the agent code area instead. Example - An APL code definition An APL code definition, saved as MyGenericCode in the Default directory, is available to an agent by adding the following into its code area: import apl.Default.MyGenericCode;

---

# Document 953: A TCP/IP Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654339
**Categories:** chunks_index.json



---
**End of Part 42** - Continue to next part for more content.
