# RATANON/MZ93-DOCUMENTATION - Part 1/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 1 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.3 KB
---

# RATANON/MZ93-DOCUMENTATION - Complete Knowledge Base

**Dataset:** ratanon/mz93-documentation
**Documents:** 6007 items
**Source:** https://huggingface.co/datasets/ratanon/mz93-documentation
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation

---

# Document 1: GTP' Agent MZSH Commands, Events and Limitations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673383
**Categories:** chunks_index.json

mzsh Commands In case you want to see the counters that are published as MIM values, you can use the mzsh wfcommand . See the Commandline Tool User's Guide for further information about this. Agent Message Events There are no agent message events for this agent. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . Limitations - GTP' Transported Over TCP Node Alive Request and Redirection Request are not transmitted to GSN nodes when using the TCP protocol. Loading

---

# Document 2: Encoders - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678339
**Categories:** chunks_index.json

An encoder specifies how data is to be encoded. The syntax for the encoders is as follows: encoder <encoder_name> : <encoder options>; The encoder options are: Option Description out_map(<map name>) Specifies what out-maps to use. At least one is required. block_size(<size>) Specifies that this is a blocked format with a certain block size. terminated_by(<terminator>) Specifies the block filler used. This option has no effect if the block_size has not been specified. When encoding a record, the encoder tries each out-map in the order specified. If the out-map can encode the record, then this out-map is used, otherwise, the next out-map is tried. An out-map can encode the data if: The record type matches the internal type specified in the out-map. All format-specific requirements are met. At the moment this evaluation is performed only for sequential data with an identified_by condition. Only data where the mapped fields meet the identification rule is accepted. This is to support mapping to different external record types from the same internal type. Note! There is no such thing as a constructed encoder. If the records are required to be forwarded in a specific order, an APL agent handling the output logic must precede the Encoder. Loading

---

# Document 3: Data Model for PCC Buckets - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204743088/Data+Model+for+PCC+Buckets
**Categories:** chunks_index.json

In order to make full use of PCC Buckets and create the needed business logic, it is important to understand the underlying data model for Buckets and Products. This chapter outlines the structure and underlying fields of each object in the data model. This chapter includes the following sections: Misc Field Buckets Data Model Product Data Model Periods Data Model Buckets Batches Data Model

---

# Document 4: SAP CC Secured Connection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642859/SAP+CC+Secured+Connection
**Categories:** chunks_index.json

We support TLS/SSL handshaking for one-way authentication between a SAP CC client and SAP CC core server. In one-way mode, only SAP CC Client validates the SAP CC Core Server to ensure that it receives data from the intended SAP CC Core Server. For implementing one-way mode, the SAP CC Core Server shares its Certificate(s) with the SAP CC Client. To allow SAP CC agents to connect to the SAP CC Core Server with TLS enabled, you must: Configure SAP CC Core Server with one-way authentication for the respective Instance and Services. Configure MediationZone Client to trust SAP CC Core Server. Configure SAP CC Core Server To secure SAP CC Core Server communication service, follow this SAP Support page: Secure an SAP CC Core Server communication service Before we can start configuring SAP CC Core Server, we need to know that SAP CC agents in MZ are connecting to the Dispatcher instance through the TCP-IP layer: Open SAP CC Architecture Diagram including SAP CM as a third party element Note! For more information, please read Identifying services involved in the Client/Server communication For our case, you will turn on one-way for ExternalSecure targeted service on the Dispatcher instance. Example - SAP CC Core Server Instance Map Example! Example SAP CC Core Server Instance Map: #InstanceId ; HCISecure ; HCIHost ; HCIPort ; WSSecure ; WSHost ; WSPort ; ExternalSecure ; ExternalHost ; ExternalPort ; InternalSecure ; InternalHost ; InternalPort updater#1 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9000 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9080 ; ; ; ; ; ; dispatcher#1 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9100 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9180 ; oneway ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 2000 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 2100 Configure SAP CC To Trust SAP CC Core Server Take the SAP CC Core Servers Public Certificate ( X.509v3 format encoded in DER ), and configure in SAP CC client to trust the SAP CC Core Server. One of the example method is using the keytool command to add this server certificate to client truststore, and use this truststore for your SAP CC agent. Example - Importing Server Certificate Import the server certificate  certificate.x509.pem  to generate  client.truststore file. keytool -importcert -alias sapcc -file certificate.x509.pem -keystore client.truststore -storetype pkcs12 -storepass examplepw In the SAP CC agent, tick Enable Secured Connection checkbox and configure the following fields: Keystore Path: /path/to/client.truststore Keystore Password: examplepw Note! SAP CC agent will only support a Keystore that is in PKCS#12 format.

---

# Document 5: Distributed Storage Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671675/Distributed+Storage+Profile
**Categories:** chunks_index.json

The Distributed Storage profile enables you to access a distributed storage solution from APL without having to provide details about its type or implementation. The use of the Distributed Storage profile and profiles for specific distributed storage types, such as Couchbase and Redis, makes it easy to change the database setup with a minimum impact on the configured business logic. This simplifies the process of creating flexible real-time solutions with high availability a nd perform ance. Note! If you switch between selecting Couchbase and Redis, the Distributed Storage iterator functions differ. For Couchbase you use dsCreateKeyIterator , and for Redis you use dsCreateREKeyIterator . For further information, see Distributed Storage Functions in the APL Reference Guide . Open Distributed Storage profile concept APL provides functions to read, store and remove data in one or multiple distributed storage instances within the same workflow. It also provides functions for transaction management and bulk processing. For information about which APL functions are applicable to the Distributed Storage profile, see the APL Reference Guide . The Couchbase and Redis profiles are available for use with the Distributed Storage profile . For further information about these profiles, see Couchbase Profile and Redis Profile . The Distributed Storage profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Distributed Storage profile configuration, click the New Configuration button in the upper left part of in Build View , and then select Distributed Storage Profile from the menu. The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently displayed tab. The Distributed Storage profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . Open The Distributed Storage profile dialog Setting Description Setting Description Storage Type Select a storage type from the drop-down list. Profile Select the storage profile that you want to apply.

---

# Document 6: mzcli Textual Pattern Matches - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547980447
**Categories:** chunks_index.json

In resemblance to Regular Expressions, when searching through text strings of names and other textual patterns in mzcli, there are two characters that help you filter text according to certain criteria: The asterisk '*' is a wildcard for one or more characters. The question mark '?' is a wildcard for any single character. Note! If you want to use the '*' and '?' wildcards when you are not logged in, the wildcards have to either be enclosed with single or double quotation marks or preceded with a backslash ''. For example: mzcli mzadmin/dr wfgrouplist * will work. mzcli mzadmin/dr wfgrouplist "*" will work. mzcli mzadmin/dr wfgrouplist * -mode D will not work. The period '.' punctuation mark is not a wildcard and is treated as a normal punctuation mark character.

---

# Document 7: Azure Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032036/Azure+Profile
**Categories:** chunks_index.json

The Azure Profile is used for setting up the access credentials and properties to be used to connect to an Azure environment. Currently, the profile can be used with the following agents: ADLS2 File collection agent ADLS2 File forwarding agent Azure Event Hub Consumer agent Azure Event Hub Producer agent and APL functions: kustoTableCreate, for more information, see Database Table Functions . Buttons The contents of the buttons in the button bar may change depending on which configuration type has been opened. The Azure Profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit button is specific to the Azure Profile configurations. Item Description Item Description External References Open Select this menu item to enable the use of External References in the Azure profile configuration. This can be used to configure the following fields: Shared Key Storage Account Name Key Connection String Connection String Secret Key Storage Account Name Namespace Event Hub Name Client ID Tenant ID Client Secret Certificate Storage Account Name Namespace Event Hub Name Client ID Tenant ID Certificate Path Certificate Password For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . General Tab Azure Data Lake Storage Authentication Method - Shared Key The following settings are available in the Shared Key authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Shared Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Key Enter the authorized shared access key used to access the Azure storage account, or use Secret Profile. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Authentication Method - Secret Key The following settings are available in the Secret Key authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Lake Storage. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Lake Storage. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client secret will only be visible when registering the client ID. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Authentication Method - Certificate The following settings are available in the Certificate authentication method for the Azure Data Lake Storage application in the Azure profile. Open Azure profile - Azure Data Lake Storage Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For ADLS2 file agents, select Azure Data Lake Storage. Authentication Method Select the authentication method for accessing the Azure Data Lake Storage. There are 3 choices with Shared Key, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Storage Account Name Enter the name of the Azure storage account name that will be used by the Azure Data Lake Storage. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Lake Storage. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Lake Storage. Use Security Profile Click this to use a Keystore from a Security Profile Security Profile Certificate Type Set the certificate format that is used by the Azure AD application. You can set it to either a PEM or PFX formatted certificate. Certificate Path Define the full local path of the certificate. The certificate must be stored in the same location as the EC that will be running the workflows with the ADLS2 file agents. The certificate must be the same one used by the Azure AD application. Certificate Password Enter the password for the PFX certificate, where the password value can also be an empty string. Password locked PEM certificates are not supported. Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Note! For the Test Connection button to work while using certificate authentication, the certificate path must point to a certificate located in the Platform. However, when running workflows, the certificate path must point to a certificate located in the EC. Azure Event Hub Authentication Method - Connection String The following settings are available in the Connection String authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Connection String configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Connection String Enter the connection string-primary key of the event hub instance that the profile will be accessing. You can locate the connection string from the shared access policies menu in the target event hub instance. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. Authentication Method - Secret Key The following settings are available in the Secret Key authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Namespace Enter the namespace of the Event Hub that the profile will be accessing. Event Hub Name Enter the name of the Event Hub Instance within the Event Hub Namespace above that the profile will be accessing. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Event Hub. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Event Hub. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client's secret will only be visible when registering the client ID. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. Authentication Method - Certificate The following settings are available in the Certificate authentication method for the Azure Event Hub application in the Azure profile. Open Azure profile - Azure Event Hub Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. For Azure Event Hub agents, select Azure Event Hub. Authentication Method Select the authentication method for accessing the Azure Event Hub. There are 3 choices with Connection String, Secret Key and Certificate. Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Namespace Enter the namespace of the Event Hub that the profile will be accessing. Event Hub Name Enter the name of the Event Hub Instance within the Event Hub Namespace above that the profile will be accessing. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Event Hub. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Event Hub. Use Security Profile Click this to use a Keystore from a Security Profile Security Profile Certificate Path Define the full local path of the certificate. The certificate must be stored in the same location as the EC that will be running the workflows with the Event Hub agents. The certificate must be the same one used by the Azure AD application. Certificate Password Enter the password for the PFX certificate, where the password value can also be an empty string. Password-locked PEM certificates are not supported. Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. The following settings are available in the Certificate authentication method for the Azure Event Hub application in the Azure profile. Note! For the Test Connection button to work while using certificate authentication, the certificate path must point to a certificate located in the Platform. However, when running workflows, the certificate path must point to a certificate located in the EC. Azure Data Explorer The following settings are available in the Secret Key authentication method for the Azure Data Explorer application in the Azure profile. Open Azure profile - Azure Data Explorer Secret Key configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. To select Azure Data Explorer, select it from the dropdown menu list. Authentication Method Select the authentication method for accessing Azure Data Explorer. There are 2 choices  Secret Key and Certificate . Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Choosing Secret Key enables this method. Cluster Name Enter the cluster name. Location Enter the associated location. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Explorer. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Explorer. Client Secret Enter the client secret provided when creating the application for the Azure Active Directory with the client ID above, or use Secret Profile. The client's secret will only be visible when registering the client ID. Use Secrets Profile Click this to use stored credentials from a Secrets Profile . Test Connection Test the connectivity to the selected Azure service using the authentication credentials provided. The following settings are available in the Certificate authentication method for the Azure Data Explorer application in the Azure profile. Open Azure profile - Azure Data Explorer Certificate configuration Setting Description Setting Description Application Select Allows you to select the Azure resource that the profile will connect to. To select Azure Data Explorer, select it from the dropdown menu list. Authentication Method Select the authentication method for accessing Azure Data Explorer. There are 2 choices  Secret Key and Certificate . Choosing one of the options will display the appropriate configuration menu for the chosen authentication method. Choosing Certificate enables this method. Cluster Name Enter the cluster name. Location Enter the associated location. Client ID Enter the client ID (application ID) used to create the application for the Azure Active Directory that will allow the profile to access the Azure Data Explorer. The ID entered here should correlate with the client ID that is used when registering the application on the Azure Active Directory. Tenant ID Enter the tenant ID (directory ID) linked to the Azure AD application that will be used by the profile to access the Azure Data Explorer. Security Profile Test Connection Test the connectivity to the selected azure service using the authentication credentials provided. Advanced Tab Open Advanced Tab The content of this tab changes depending on the selected method in the General Tab . The following fields are available for each option: Field Description Field Description Authority Host Enter the URL to the directory the Microsoft Authentication Library will request tokens. If left empty, the following default values will be used accordingly: Azure Data Lake Storage - https://login.microsoftonline.com Azure Event Hub - https://login.microsoftonline.com API Endpoint Enter the API endpoint in Azure to be used for accessing and managing the services. If left empty is not entered, the following default values will be used accordingly: Azure Data Lake Storage - blob.core.windows.net Azure Event Hub - servicebus.windows.net Azure Data Explorer - kusto.windows.net Additional Information To find out more about the configuration for both authority and endpoints, refer to https://docs.microsoft.com/en-us/azure/active-directory/develop/authentication-national-cloud#azure-ad-authentication-endpoints and https://docs.microsoft.com/en-us/azure/azure-government/compare-azure-government-global-azure .

---

# Document 8: Workflow Validation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638608/Workflow+Validation
**Categories:** chunks_index.json

Workflow configurations may be designed, configured, and saved step-by-step, but are still not valid for activation until fully configured and valid. A valid workflow configuration contains three types of configuration data: Workflow data : General information related to the workflow configuration, for instance, error handling. Workflow structure data : Contains the agents and routes. A route indicates the flow of data depending on the name of the route and the internal behavior of its source agent. Agent specific data : Each agent has a different behavior. Thus, each agent in the workflow configuration requires different configuration data in order to operate. When a workflow is saved, it is silently validated and if some of its configuration is invalid or missing, a dialog will state this and ask whether to still save the workflow or not. Validity is not necessary in order to save a workflow configuration. The workflow can be incomplete or the agent configuration can be faulty. The only exception is that all workflows in the workflow configuration must have unique names. When data is imported to the workflow table, the content is not validated, only the correct number of columns and types are checked. If validation errors occur during the import, the user is asked whether the import should be aborted or continued (that is, importing with errors). Aborting an import results in restoration/rollback to the previous table. How Workflow Validation Works When you click the ( Validate ) button, the workflow configuration validation is started. The validation is done in two steps: Validation of the workflow configuration. If the workflow configuration is invalid, an Information dialog is opened showing details, such as if configuration data or routes are missing, if referenced MIM resources are no longer available, or if configuration data in an agent is missing. The details can be changed by modifying the agent configuration, after clicking OK . If the workflow configuration is invalid the validation process ends there. Example of an invalid workflow Information dialog If the workflow configuration is valid, the validation of the workflow table starts. The values in the table are validated according to each agent's specifications. There is also a check that values have been added for all cells in the per workflow columns. The result is presented in a validation dialog and possible workflow errors are indicated in the workflow table. You can view the validation message for a specific workflow by selecting the corresponding action in the pop-up menu. If none of the workflows in the workflow configuration are valid, you get a dialog saying none of the workflows are valid. If it is not evident why the workflow(s) is erroneous, you can select one or more rows in the workflow table and then click on Validation Message to display a dialog with error message(s). Open Validation dialog for workflow table Note! External References are validated only during runtime. How to tell when the workflow is valid or invalid There is a label at the top of the workflow template, to the right of the workflow template name, that indicates if the workflow configuration is valid or invalid. Open Open Validity labels for workflow template For workflows, the workflow table for each row has the following symbols: Open which indicates that the workflow and related fields in that row are valid Open which indicates that the workflow and related values for the configuration in that row are invalid For example, data type can be invalid (if there are words in a numeric value only field), or a mandatory field could be empty. You can also tell if a workflow is invalid in the Configuration Browser in the Build view if the workflow configuration name is RED in color. Open Configuration Browser with an invalid workflow configuration

---

# Document 9: APL Collection Strategy Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656761/APL+Collection+Strategy+Functions
**Categories:** chunks_index.json

APL Collection Strategy configurations are used on top of pre-defined collection strategies to customize how files are collected from the local file system. Note! The APL function blocks that are described in this chapter are automatically invoked from the collection agent and cannot be called manually. The following functions blocks may be used in the APL Collection Strategy code: The initialize , deinitialize , begin , commit , and rollback function blocks are equivalent to the corresponding APL function blocks described in Function Blocks . The collection specific functions blocks are invoked in the following order: prepareBaseDirList accept filterFiles preFileCollection postFileCollection Note! During run-time, when each of the functions blocks are invoked, the workflow first runs the function blocks base part and then it executes your APL extension code. The following APL functions cannot be used within an APL Collection Strategy configuration. udrRoute mimSet mimPublish cancelBatch hintEndBatch In the following APL functions you cannot assign a persistent variable with a value. For information about persistent variables, see Variables and Variable Scope initialize deinitialize commit rollback initialize The initialize function block is used to prepare for the collection and set up an environment to be used in a later stage, when the batch workflow is executed. void initialize() deinitialize The deinitialize function block is used to clean-up and close resources. void deinitialize() prepareBaseDirList The prepareBaseDirList function block prepares for a list of base directories. A base directory is the directory that includes the files to be collected. This directory is defined when configuring an agent's Base Collection Strategy. Base directory paths can be added, removed, and modified. The prepareBaseDirList function block is invoked by the Collection agent right after the collection preparation activities that were initiated by the initialize function. void prepareBaseDirList( list<string> dirList ) Parameter Description dirList This parameter is used to define one or more collection sources. The collection order of the files is defined using the filterFiles function. The dirList parameter refers to a list that, by default, contains the directory that is defined when configuring an agent's Base Collection Strategy. The dirList parameter is an in/out parameter that serves both as input and output values for the prepareBaseDirList function. In the following example, prepareBaseDirList adds a subdirectory named sub to the directory that is already on the base directories list. For example: If /home/in is a directory that is on the directory list, the prepareBaseDirList function adds /home/in/sub to the directory list. Example - Using prepareBaseDirList void prepareBaseDirList(list<string> dirList) { string dir = listGet(dirList, 0); string subDir = dir + "/sub"; listAdd(dirList, subDir); } accept The collection agent processes the base directory and its included files and creates an internal file list including all files to be collected. The accept function block is invoked by the collection agent each time a new file is about to be added to the list, and, based on each file's fileInfo object, either accepts or rejects the file. boolean accept ( FileInfo file ) Parameter Description file The file parameter includes the properties of the file to collect or a directory where files are stored. The FileInfo object includes: Item Description isDirectory(boolean) Set to True if FileInfo represents a directory. isFile(boolean) Set to True if FileInfo represents a file. name(string) The name of the file or directory size(long) The size of the file or directory timestamp(long) The timestamp for when the file or directory was last modified Returns True if the file shall be added to the list of files that are about to be collected Example - Using accept In this example, only files that have a name that starts with "INFILE_ascii" are collected. boolean accept(FileInfo file) { if(file.isFile) { if ( strStartsWith(file.name, "INFILE_ascii") ) { return true; } else { return false; } } else{ return false; } } filterFiles The filterFiles function block is invoked by the collection agent right after the accept function has been executed and when the list of files has been created. Files to collect can be removed, but not added or modified. Collection order can be modified by sorting the list. void filterFiles ( list<FileInfo> fileInfoList ) Parameter: Parameter Description fileInfoList The fileInfoList parameter contains a reference to the list that includes the files to collect. fileInfoList is an in/out parameter that serves both as input and output values for the filterFiles function. In the following example, file1 is not collected if there is another file with the same name that includes the prefix "ignore_" ( ignore_file1 ) in the collected directory. Example - Using filterFiles In this example, file1 is not collected if there is another file with the same name that includes the prefix "ignore_" ( ignore_file1 ) in the collected directory. void filterFiles(list<FileInfo> fileInfoList) { // put the files into a map string ignorePrefix = "ignore_"; map<string, FileInfo> fileInfoMap = mapCreate(string, FileInfo); int i = listSize(fileInfoList); while(i > 0) { i = i - 1; FileInfo fileInfo = listGet(fileInfoList, i); mapSet(fileInfoMap, fileInfo.name, fileInfo); } // Remove from the map files that are indicated as files // that should be ignored, along with their ignore_ indicator // files . i = listSize(fileInfoList); while(i > 0) { i = i - 1; FileInfo fileInfo = listGet(fileInfoList, i); // check if the filename start with the ignore prefix boolean ignore = strStartsWith(fileInfo.name, ignorePrefix); if(ignore) { string fileToIgnore = strSubstring(fileInfo.name, strLength(ignorePrefix), strLength(fileInfo.name)); mapRemove(fileInfoMap, fileInfo.name); mapRemove(fileInfoMap, fileToIgnore); } } // put the remaining files in the fileInfoList listClear(fileInfoList); list<FileInfo> remainingFiles = mapValues(fileInfoMap); i = listSize(remainingFiles); while(i > 0) { i = i - 1; listAdd(fileInfoList, listGet(remainingFiles, i)); } } preFileCollection The preFileCollection function block is invoked by the Collection agent right before a file is collected. void preFileCollection ( string fileName ) Parameter Description fileName The fileName parameter includes the name of the file to be collected. postFileCollection The postFileCollection function block is invoked by the Collection agent right after a file has been collected. void postFileCollection ( string fileName ) Parameter Description fileName The fileName parameter includes the name of the file that has been collected. begin The begin function block is invoked by the Collection agent during the Begin Batch processing phase and marks a start point for the file based transaction handling. On severe errors, such as when a transaction fails, the APL command abort("reason") should be invoked. void begin ( long transactionID ) Parameter Description transactionID The transactionID parameter is the identifier of this transaction. commit The commit function block is invoked by the Collection agent right after the End Batch processing phase and marks the end of the file-based transaction handling. On severe errors, such as when a transaction fails, the APL command abort("reason") should be invoked. void commit ( long transactionID , boolean isRecover ) Parameter Description transactionID The transactionID parameter is the identifier of this transaction. isRecover The isRecover parameter is set to "true" if this is a recover operation. If the last commit failed, a commit recover operation is executed upon workflow startup. rollback The rollback function block is invoked in case of a system failure, right after the End Batch processing phase. On severe errors, such as when a transaction fails, the APL abort("reason") command should be invoked. void rollback ( long transactionID , boolean isRecover ) Parameter Description transactionID The transactionID parameter is the identifier of this transaction. isRecover The isRecover parameter is set to "true" if this is a recover operation. If the last rollback failed, a rollback recover operation is executed upon workflow startup.

---

# Document 10: Conditional Trace Access Permissions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742517/Conditional+Trace+Access+Permissions
**Categories:** chunks_index.json

There are two different permission levels for Conditional Trace; Conditional Trace Template , which allows you to create and edit Conditional Trace templates in Legacy Desktop, and Conditional Trace , which allows you to use the created templates to do traces in Desktop. To set Conditional Trace access permissions, go to Manage  Tools & Monitoring and then select Access Controller where you can either configure permissions in a new or existing group. Open Access permissions for user who can use existing Conditional Trace filters in Desktop Permission settings Members of the default Administrator access group are configured to manage Conditional Trace templates and filters by default. To enable other users to manage templates and filters, they must be granted Execute rights for the applications Conditional Trace and Conditional Trace Template . To enable users to manage filters only, they must be granted Execute rights for the application Conditional Trace . Write permissions are not required to be set.

---

# Document 11: JMS Request Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686049/JMS+Request+Agent
**Categories:** chunks_index.json

The JMS Request agent sets up a publishing session to the destination that is specified in the profile. The JMS Request agent uses JNDI to connect to a JMS server. Depending on the type in the Request field, a JMS Message of the appropriate type is sent. For a list of supported types see JMS UDRs . The message is then sent to the specified destination. While in session, the agent creates either a temporary queue or a temporary topic and sets up a consumer for it. If the getResponse field is set to true , the agent expects a reply from the temporary destination. The JMSCycle is then saved in a local cache and sent to the JMS Server. When the reply arrives, the correlation ID of the message is used to match the incoming JMS message with the correct JMSCycle in the local cache. When found, the JMSCycle is removed from the cache and the response field is set to the incoming JMS message. Then the JMSCycle UDR is inserted into the workflow. If the JMS Request agent encounters a connection failure the agent tries to re-establish the connection with the JMS server. If the agent has not managed to re-establish the connection to the JMS server after five attempts, it will instead try to re-establish the connection for each UDR. Note! A property can be set to get the JMS Request agent to connect and acquire a Connection Factory object and a Destination object from a JNDI service for every connection attempt. To enable this, add the following property to the relevant EC using the mzsh topo command: mzsh topo set topo://container:<container>/pico:<ec name>/val:config.properties.mz.jms.reinitialize true If no response is expected, as getResponse is false , the agent sends the requested content to the JMS server and discards the UDR. If the send operation fails, the UDR is inserted once again into the workflow with the status set to 30 or 31 . If a reply is not received within the specified timeout, the JMSCycle UDR is forwarded with the Response field set to NULL and the status is set to 20 . Loading

---

# Document 12: GCP Storage Forwarding MultiForwardingUDR Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607857/GCP+Storage+Forwarding+MultiForwardingUDR+Example
**Categories:** chunks_index.json

Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDR s. import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previously in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub-folders in the root directory. The Create Non-Existing Directories check box under the Filename Template tab in the configuration of the forwarding agent must be checked if the directories do not previously exist.

---

# Document 13: Release of New Functionality - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676877/Release+of+New+Functionality
**Categories:** chunks_index.json

In this scenario, the exported configurations are stored in a Version Control System and then checked out by different teams working with test and integration, before the configurations are put into production. Open Chain of events: Team A develops new workflows and exports them to the local working copy. The resulting xml and schema files are checked in to the Version Control System. Team C checks out the exported configurations prepared by Team A to their local working copy and imports them into a staging MediationZone. Team C prepares the workflows for production, by creating additional instances, assigning parameters, creating workflow groups and configuring scheduling. Team C exports the updated configurations to their local working copy and checks in the new exports into the Version Control System. Team B checks out the new configuration exports and imports them into a system test MediationZone. Team B performs system test and Team A and C resolves issues detected, repeating steps 1-6 as necessary. When the configurations are ready for production, Team D checks out the tested and accepted versions of the configurations to their local working copy. Team D performs an export of the current production version, if one already exists, and compares the xml files with the ones checked out from the Version Control System. This provides a way to verify and understand the scope of the changes made. Team D imports the new version of the configurations into the production MediationZone.

---

# Document 14: Listing Python Processes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740128/Listing+Python+Processes
**Categories:** chunks_index.json

As each Python agent runs a Python process, you can list which Python processes are being run, and where. For example the following command lists the Python processes per Python agent: ps -ef | grep python Where each process is being run is identified as follows: <Desktop folder>.<workflow name>.<workflow instance name>.<agent name> Example of list of Python processes running ubuntu 3661 3334 0 11:06 pts/0 00:00:00 python -c 'Default.MyWorkflow.workflow_1.Python_1'; import sys; exec(sys.stdin.read()) ubuntu 3670 3334 0 11:06 pts/0 00:00:00 python -c 'Default.MyWorkflow.workflow_1.Python_2'; import sys; exec(sys.stdin.read()) ubuntu 3678 3334 0 11:06 pts/0 00:00:00 python -c 'Default.MyWorkflow.workflow_1.Python_3'; import sys; exec(sys.stdin.read())

---

# Document 15: JMS Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000796/JMS+Profile
**Categories:** chunks_index.json

In the JMS profile configuration, enter the details required to connect and acquire both the Connection Factory object and Destination object from a JNDI service. The JMS profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration Open JMS profile configuration - JNDI Properties Setting Description Setting Description Connection Factory Name Enter the name of the connection factory object that is to be searched for within JNDI. This is used to create a connection with a JNDI provider (a messaging provider such as Weblogic, activeMQ, etc). Destination Name Enter the name of the destination object that is to be searched for within JNDI. This represents the target of messages that are produced and the source of messages that are consumed (e g, queue name or topic name). Properties List Enter <Name:Value> pairs that you want to use when creating the InitialContext. For a list of available options, refer to the JNDI implementation documentation and Context (Java SE 17 & JDK 17) (oracle.com) . Note! There may be instances where setting the username and password for the JNDI server in the JMS agents will result in an error where the agent will not be able to access the server. When this happens, you will have to set the username and password in the properties list, using the following security context: For username, use: java.naming.security.principal For password, use: java.naming.security.credentials

---

# Document 16: DRBatchProcessor - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676595/DRBatchProcessor
**Categories:** chunks_index.json

This is the class to be used for Processing and Forwarding agents. Since they do not differ more than from an output perspective, the system uses the same class for both types. This agent is fed data through the consume method, applies the logic on the data and then either routes it forward in the workflow (processing) or to an external system (forwarding). drain Called before the current batch ends. The agent must flush all internal buffers to make sure all pending data has been processed before the transaction is ended. This method is the last point in the batch processing where it is legal for the agent to route data. splittingBatch Called when the collector has split the input batch. This is the result of an agent calling DRBatchServerEnv.hintEndBatch . If the agent keeps internal buffers to be flushed differently depending on the nature of the transaction, this method serves as a hint to the drain call. The following sequence diagram shows in which order methods are called for the Batch Processor. Open Sequence diagram for DRBatchProcessor For examples of processing and forwarding agent plugins, see: com.digitalroute.devkit.examples.maxmin.* and com.digitalroute.devkit.examples.diskforwarding.* (forwarding).

---

# Document 17: Security Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031381
**Categories:** chunks_index.json

With the Security Profile, you can make encryption configurations that can be used by various agents. The profile consists of three tabs: General , Advanced, and External Keystore . General Tab Open Security Profile - General Tab Keystore Settings The following settings are available: Settings Description Settings Description Type You have the following options: Java Keystore External Keystore <None> Selecting External Keystore or <None> disables the rest of the keystore settings. Selecting External Keystore will require additional input in the External Keystore tab . Path Enter the location of the keystore from which you want to read the key. Password Enter the relevant keystore password. Public Key Alias The encryption alias to use. In a client, it should be the alias to the server public certificate. If left empty the Keystore Alias will be used to encrypt the message. Private Key Alias If the keystore contains more than one key, specify the alias of the key that you want to use. Key Password The Key Password fields is optional. You can enter the key password, or if you leave this field empty, the Password that you entered is the default. Example - How to Create a Symmetric Crypto Key keytool -keystore test.ks -storepass password -genseckey -keysize 128 -alias testkey -keyalg AES Example - How to Create a Keystore File with Security Contents The example code below shows how to create a Java keystore file for both the server and client connection. In this example, the file will be generated containing the associated security certificate, public and private key. Code Block keytool -genkey -alias server -keyalg RSA -keystore ./server.jks Note! Remember the password issued for the server.jks file. Example - How to Create a Client-Specific Keystore File To create a client-specific Java Keystore file, you can use the keytool command with the required variables. In this example, the generated file will be for a specific client and contain only their certificate and public key. Code Block keytool -export -alias server -keystore ./server.jks -file ./server.cer ... keytool -import -alias client -file ./server.cer -keystore ./client.jks ... Note! Execution of these commands will present password entry prompts, and you will need to remember the entered passphrase. Truststore Settings The following settings are available: Field Description Field Description Type You can select from the following options: Java Truststore Use Java Keystore External Truststore Use External Keystore <None> Selecting Use Java Keystore disables the rest of the truststore settings and the keystore specified in Keystore Settings is used. Selecting External Truststore or Use External Keystore disables the rest of the truststore settings and will require more input in External Keystore tab. Selecting <None> disables the rest of the truststore settings. Path Enter the location of the truststore that you want to use. Password Enter the relevant truststore password. Advanced Tab Open Security Profile - General Tab The Advanced tab enables you to make more detailed configurations for which cipher suites to accept. The following settings are available: Settings Description Settings Description Enable TLS Settings If you want to change the TLS security parameters, select this check box. The default setting is to use the settings from the Java installation. Accepted Protocols You can select if you want agents using this profile to accept only TLS version 1.3 or any TLS version. The default setting is to only accept version 1.3. Used Cipher Suites You can select if you want agents using this profile to use only suites that are enabled by default, or any suites. The default setting is to only use suites that are enabled by default. Cipher Suite Must Match In this field, you can enter any characters that you want the cipher suites to match. You can also enter lists of regular expressions, one per row, that you want the cipher suites to match. Suites not matching your entry are greyed out in the Result on this JVM field. Cipher Suite Must Not Match If you want to exclude cipher suites, you can enter any characters in this field which excludes suites matching the characters. You can also enter lists of regular expressions, one per row, for cipher suites to exclude. Result on this JVM This field displays the cipher suites available on the current JVM. External Keystore Tab The External Keystore tab enables you to store your SSL certificates in one secure location. Currently, it can be stored in Azure KeyVault, Google Secret Manager or HashiCorp Vault. Note! Using the Security profile with External Keystore configured with Kafka agents is not supported. Azure KeyVault Open Azure KeyVault as External Keystore For information about the installation and setup of an Azure KeyVault, see https://azure.microsoft.com/en-us/products/key-vault . Settings Description Settings Description Azure KeyVault Profile Choose an Azure KeyVault Profile to use for the credentials. Certificate name The name of the certificate in Azure KeyVault Google Secret Manager Open Google Secret Manager as External Keystore For information about the installation and setup of Google Secret Manager, see https://cloud.google.com/secret-manager/docs . MediationZone requires a base64 encoded PFX certificate to be stored as a Secret in Google Secret Manager. Settings Description Settings Description Google Secret Manager Profile Choose a Google Secret Manager Profile to use for the credentials. Name The name of the certificate stored in Google Secret Manager. Version The version of the Secret. Key Password Password of the certificate. Generating and Uploading a Certificate Run the following command to create a self-signed PFX keystone file: keytool -genkey -keyalg RSA -keystore Server.pfx -storetype PKCS12 keystore = name of the pfx file, for example, server.pfx Note! When prompted for first and last name, the hostname where the certificate is valid should be entered, for example, localhost. Other values can be anything. Encode the PFX file with base64 by running this command: base64 -i Server.pfx -o Server.b64 -i = name of the input file -o = name of the output file for the base64 string Create a secret on Google Secret Manager with the value of the Server.b64 . HashiCorp Vault Open HashiCorp Vault as External Keystone For information about the installation and setup of a vault, see https://learn.hashicorp.com/vault . Info! When setting up your vault, it is recommended that you have the following set up: Set up a Key-Value (kv) Secret Enable Userpass authentication instead of the default token authentication. Set up a policy with read and list permissions and assign it to a user. Settings Description Settings Description Auth Methods Select the authentication method used to access the vault. Address The address for the vault. The format of the address begins with the hypertext transfer protocol, either HTTP or HTTPS, followed by the IP address of the vault and the TCP port used by the TCP listener of the vault. Example https://127.0.0.1:8200 Username Enter the vault username. Password Enter the vault password. Path The full path of the vault secret engine that contains the relevant keystore or truststore. Example secret/digitalroute/mz/security/server Uploading a Keystore into Your Vault MediationZone requires certain criteria to be met when uploading the keystore into your vault. The following command will help show you how to upload. vault kv put secret/digitalroute/mz/security/<PATH_PREFIX>/keystore filecontent="$(cat <PATH_TO_KEYSTORE>.jks | base64)" password=<PASSWORD> keyalias=<KEYALIAS> keypassword=<KEYPASSWORD> You need to configure the mandatory attributes. The workflow will abort if it calls a Security profile with vault credentials saved in a different format than listed in the table below. Supported Formats Attribute Value Format Attribute Value Format filecontent Base64 String keyalias String keypassword String password String Uploading a Truststore into Your Vault MediationZone requires certain criteria to be met when uploading the truststore into your vault. The following command will help show you how to upload it. vault kv put secret/digitalroute/mz/security/<PATH_PREFIX>/truststore filecontent="$(cat <PATH_TO_TRUSTSTORE>.jks | base64)" password=<PASSWORD> You need to configure the mandatory attributes. The workflow will abort if it calls the security profile with the vault credentials that are saved in a different format as listed in the table below. Attribute Value Format Attribute Value Format filecontent Base64 String password String

---

# Document 18: Excel Encoder Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607431/Excel+Encoder+Agent+Transaction+Behavior
**Categories:** chunks_index.json

Transaction Behavior This section includes information about the Excel Encoder agent's transaction behavior. For information about the general transaction behavior, see Workflow Monitor . Emits The Excel Encoder agent does not emit any commands. Retrieves The Excel Encoder agent does not retrieve any commands.

---

# Document 19: ECS Maintenance System Task - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032851
**Categories:** chunks_index.json

The ECS Maintenance system task removes outdated ECS data, provided that the state is Reprocessed . The number of days to keep data is set in the ECS Maintenance configuration dialog (see ECS Maintenance System Task Configuration ). It is also possible to fully turn off the cleanup of UDRs, Batches, Statistics, or all of them. The Statistics can be reported by email. This is configured in the Report tab of the task. When the ECS Maintenance System Task is executed, a number of things happen: UDRs, batches, and ECS statistics are removed from the ECS according to the settings in the Cleanup tab. See Cleanup Tab in ECS Maintenance System Task Configuration . An ECS Statistics Event is generated containing information about the number of UDRs associated with every error code. This occurs at time intervals configured in the ECS Maintenance System Task Configuration . See ECS Statistics Event for further information about how to configure notifications for the ECS Statistics Event. Statistical information is sent to the ECS Statistics, according to what is configured in the Report Tab in ECS Maintenance System Task Configuration . An email containing statistical information is sent to the email recipient specified in the Report Tab in ECS Maintenance System Task Configuration . For further information on how to configure mail server and port, see Platform in the System Administrator's Guide. Note! The ECS is designed to store a fairly limited amount of erroneous UDRs and batches. It is therefore important that the data is extracted, reprocessed, or deleted from the ECS on a regular basis. Loading

---

# Document 20: Spark, kafka and zookeeper - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645630/Spark+kafka+and+zookeeper
**Categories:** chunks_index.json

Kafka and zookeeper are required for sending data to and from the Spark cluster. Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to Spark. Before you can create the topics you must start the Kafka and Zookeeper services. See Preparing and Creating Scripts for KPI Management on how to start Spark, Kafka, and Zookeeper. The topics are for transferring data to the Spark Application, receiving calculated KPIs from Spark, and a third topic for alarms. The default names of the topics are kpi-input , kpi-output, and kpi-alarm , but the names can be altered in the KPI Management Profile. Ensure that the number of partitions must match the number of Kafka brokers. Retention Settings The default data retention period in Kafka is one day. You can change the length of this period to conserve disk space. Set the following properties in the file server.properties in the config-folder of Kafka: log.retention.bytes - Must be greater than value of the property log.segment.bytes log.segment.bytes - Must exceed the size of the input/output segments to and from Kafka log.retention.hours - Must be greater than the largest window size in the service model by at least factor 3. Hint! The instruction above will change the retention settings for all topics in the Kafka cluster. You can also override the retention setting for individual topics during creation. For further information see Starting Clusters and Creating Topics . For further information about Kafka, see 9.48 Kafka Agents in the Desktop User's Guide .

---

# Document 21: LDAP Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653206/LDAP+Agent
**Categories:** chunks_index.json

This section describes the LDAP agent. This is a processing agent for realtime workflow configurations. The LDAP agent can be used to establish LDAP sessions to servers when a workflow is started. Data can be sent to the LDAP agent in a UDR, which in turn communicates with the LDAP server and sends a request. The LDAP agent is in compliance with RFC 4511. For further information on the Lightweight Directory Access Protocol (LDAP), see https://tools.ietf.org/html/rfc4511 . Open Example of a workflow with the LDAP agent There are a number of UDR types specifically for the LDAP agent. For information on the UDRs, see LDAP Agent UDRs . Operations Supported The LDAP agent supports the following operations: Abandon Add Compare Delete Extended Operation Modify ModifyDN Search Prerequisites The reader of this information should be familiar with: LDAP version 3 and the terminology used within RFC 4511, https://tools.ietf.org/html/rfc4511 The section contains the following subsections: LDAP Agent Configuration LDAP Agent UDRs LDAP Agent Input/Output Data and MIM LDAP Agent Events LDAP Agent Example

---

# Document 22: Alarms and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605353/Alarms+and+Events
**Categories:** chunks_index.json

Alarms and Events is a monitoring tool used to keep track of the various alarms and events generated by your Alarm Detection and Event Notification configurations. To access Alarm and Event, go to Manage  Tools & Monitoring and click Alarm and Event . There are two tabs in Alarms and Events; Alarm Detection and Event Notification . See Alarm Detection and Event Notification for more information about these configurations. Alarm Detection In the Alarm Detection tab there is an alarm detection table displaying a list of all Alarm Detection configurations in your system. Open Alarms and Events - Alarm Detection tab You can choose to activate or deactivate your Alarm Detection configurations by selecting or deselecting the Active checkbox for each configuration. In the table action bar you have the following buttons: Button Description Button Description Enable all Click on this button to activate all the Alarm Detections in the table. Disable all Click on this button to deactivate all the active Alarm Detections in the table. Refresh Click on this button to refresh the Alarm Detection in the table. Event Notification In the Event Notification tab you can view all Event Notification configurations in your system. Open Alarms and Events - Alarm Detection tab You can choose to activate or deactivate your Event Notification configurations by selecting or deselecting the Active checkbox for each configuration. Button Description Button Description Enable all Click on this button to activate all the Event Notifications in the table. Disable all Click on this button to deactivate Event Notifications in the table. Refresh Click on this button to refresh the Event Notification in the table.

---

# Document 23: Details - Overview Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671071/Details+-+Overview+Tab
**Categories:** chunks_index.json

Below is an example of the content of the Overview Tab. Open Open

---

# Document 24: XML Schema Extensions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205815809/XML+Schema+Extensions
**Categories:** chunks_index.json

You can use the following XML Schema extension attributes in your Ultra code: contentFieldName ultraFieldName contentFieldName Attribute In an element that is defined to contain text, the text part is normally defined by the name content in the result Ultra type. Another Ultra field name for this data can be specified by specifying the contentFieldName attribute in the containing element. ultraFieldName Attribute For any attribute or element specification that maps to an Ultra field, the Ultra field name is identical to the XML name (possible namespace is removed). To create a different field name, specify the explicit Ultra field name by using the ultraFieldName attribute.

---

# Document 25: Unlimited Scalability - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848949/Unlimited+Scalability
**Categories:** chunks_index.json

MediationZone centralizes the control logic and distributes execution, and thus addresses the two main problem areas related to vertical and horizontal scaling architectures: Vertical scaling can lead to CPU contention where the scalability of a centralized architecture is limited to the number of plug-in processors supported by the hardware. MediationZone offers virtually unlimited scalability by its support for any number of collaborating networked servers. Horizontal scaling can lead to inconsistent maintenance of a distributed architecture when code and data potentially reside on a large number of servers. MediationZone supports automatic, on-demand distribution of code and configuration to servers configured to be part of the installation, thus reducing operation and maintenance efforts and ensuring a consistent configuration store. Open Scale-out architecture Logically, the MediationZone platform is layered into three different zones: Access Zone is the layer where users access the system through a graphical interface or command line interface to perform operations and maintenance tasks. Control Zone hosts configurations and provides storage and a range of services that are essential to the MediationZone system. Execution Zone is a scale-out layer that provides processing capacity in the system. This layer contains one or several Execution Contexts and Service Contexts, which are distributed over any number of servers. Execution Contexts are responsible for executing and supervising workflows. Service Contexts host services that workflows running in Execution Contexts can share. Functionality in any of the Access Zone, Control Zone, and Execution Zone may be distributed over any number of servers, or be deployed on a single server. Distribution across several servers enables deployment of right-sized and cost-effective mediation solutions for optimal hardware utilization. The workflow service uses a configurable load-balancing algorithm to determine which Execution Context is most suitable to receive the execution of the workflow. Decisions for deploying a workflow on a specific server may, for example, be either location or hardware dependent. Specific network hardware might be required for the collection subsystem, or improved LAN bandwidth utilization could be achieved by deploying collection and aggregation workflows close to the network elements. The system processes in the various zones are referred to as "pico instances" and can be of different types: Platform Execution Context (EC) Command Line Tool (mzsh) Desktop

---

# Document 26: Web Service Example - Creating a Web Service Provider Workflow - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610008/Web+Service+Example+-+Creating+a+Web+Service+Provider+Workflow
**Categories:** chunks_index.json



---
**End of Part 1** - Continue to next part for more content.
