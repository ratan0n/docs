# RATANON/MZ93-DOCUMENTATION - Part 12/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 12 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~70.0 KB
---

The IPDR SP agent generates the following UDRs: SAMIS SAMIS_TYPE_1 SAMIS_TYPE_2 V3_0 V3_1 Data Simple TemplateData For a complete list of UDRs generated by the IPDR SP agent, refer to the UDR Assistance. Open UDR Internal Format Browser - IPDRSP UDRs SAMIS UDRs The following fields are included in the SAMIS UDRs: Field Description Field Description CMcpelpv4List (list<ipaddress>) This field contains a list of CPE IPv4 addresses that is associated with the cable modem. CMdocsisMode (int) This field indicates the registration mode of the cable modem. CMipAddress (ipaddress) This field contains the IP address of the cable modem. CMmacAddress (string) This field contains the MAC address of the cable modem. CMTScatvIfIndex (long) This field contains the ifIndex for the CMTS CATV interface. CMTScatvIfName (string) This field contains the ifName of the Interfaces Group MIB for the row entry corresponding to the CMTS CATV interface (ifType = 127) for this cable modem. CMTSdownIfName (string) This field contains the ifName of the Interfaces Group MIB for the row entry corresponding to the CMTS downstream interface for this cable modem. CMTShostName (string) This field indicates the fully qualified domain name for the CMTS, if available. CMTSipAddress ( ipaddress ) This field contains the IP address for the CMTS. CMTSsysUpTime (long) This field indicates the time (in hundredths of a second) since system initialization. CMTSupIfName (string) This field contains the ifName of the Interfaces Group MIB for the row entry corresponding to the CMTS upstream interface for this cable modem. CMTSupIfType (int) This field indicates the type of interface the cable modem is registered with. CollConfigId (short) This field contains the collector configuration identifier of the IPDR SP record. ConnectionKey (string) This field contains the connection key which is used to identify the connection between agent and the exporter. CreationTime (date) This field indicates the creation time of the UDR. DocumentId (bytearray) This field indicates the document identifier of the IPDR SP record. ExporterAddress (string) This field contains the connection address of the IPDR SP exporter address. ExporterBootTime (date) This field contains the details of the exporter boot time received from the IPDR SP exporter. ExporterPort (int) This field contains the connection port of the IPDR SP exporter port. GateId (long) This field contains the GateID of the SF. The field can also contain the value 0. IsDuplicate (boolean) This field designates the value of the duplicate flag for the IPDR SP record. RawData (bytearray) This field contains the raw data of the IPDR SP record. RecCreationTime (bigint) This field contains the timestamp (in milliseconds) of the time the data for the record was acquired. RecType (int) This field identifies the record type for the IPDR SP record. SeqNo (long) This field identifies the sequence number for the IPDR SP record. ServiceClassName (string) This field contains the Service Class Name (SCN) of the service flow. ServiceDirection (int) This field indicates the direction of the service flow from the CMTS cable interface. ServiceIdentifier (long) This field contains the service flow ID. ServiceOctetsPassed (bigint) This field contains a counter value of the octets that has passed through the service flow. ServicePktsPassed (bigint) This field contains a counter value of the packets that have passed through the service flow. ServiceSlaDelayPkts (long) This field contains a counter value of the packets that exceeds the SLA in the downstream service flow. ServiceSlaDropPkts (long) This field contains a counter value of the packets that has dropped as it has exceeded the SLA in the downstream service flow. ServiceTimeActive (long) This field describes the time service flow is active. ServiceTimeCreated (long) This field describes the creation time of the service flow. SessionId (int) This field contains the session ID of the IPDR SP record. TemplateId (int) This field contains the template ID of the IPDR SP record. TemplateUDR (TemplateData (IPDRSP)) This field contains the TemplateData UDR representing the actual template structure used by the IPDR SP record. OriginalData (bytearray) This field contains the original data in bytearray format. SAMIS_TYPE_1 UDRs The following fields are included in the SAMIS_TYPE_1 UDRs: Field Description Field Description CmIpv4Addr (ipaddress) This field contains the IPv4 address that is associated with the cable modem. CmIpv6Addr (ipaddress) This field contains the IPv6 address that is associated with the cable modem. CmIpv6LinkLocalAddr (ipaddress) This field contains the IPv4 Link Local address that is associated with the cable modem. CmLastRegTime (long) This field contains the date and time for when the cable modem was registered last. CmMacAddress (string) This field contains the MAC address of the cable modem. CmQosVersion (int) This field describes the queuing services registered by the cable modem. CmRegStatusValue (int) This field contains the unique identifying value for the cable modem. CmtsHostName (string) This field indicates the fully qualified domain name for the CMTS, if available. CmtsIpv4Addr (ipaddress) This field contains the IPv4 address of the CMTS. CmtsIpv6Addr (ipaddress) This field contains the IPv6 address of the CMTS. CmtsMdIfIndex (long) This field contains the ifIndex for the CMTS Mac Domain interface. CmtsMdIfName (string) This field contains the ifName of the Interfaces Group MIB for the row entry corresponding to the CMTS Mac Domain interface (ifType = 127) for this cable modem. CmtsSysUpTime (long) This field indicates the time (in hundredths of a second) since system initialization. CollConfigId (short) This field contains the collector configuration identifier of the IPDR SP record. ConnectionKey (string) This field contains the connection key which is used to identify the connection between agent and the exporter. CreationTime (date) This field indicates the creation time of the UDR. DocumentId (bytearray) This field indicates the document identifier of the IPDR SP record. ExporterAddress (string) This field contains the connection address of the IPDR SP exporter address. ExporterBootTime (date) This field contains the details of the exporter boot time received from the IPDR SP exporter. ExporterPort (int) This field contains the connection port of the IPDR SP exporter port IsDuplicate (boolean) This field designates the value of the duplicate flag for the IPDR SP record. RawData (bytearray) This field contains the raw data of the IPDR SP record. RecCreationTime (bigint) This field contains the timestamp (in milliseconds) of the time the data for the record was acquired. RecType (int) This field identifies the record type for the IPDR SP record. SeqNo (long) This field identifies the sequence number for the IPDR SP record. ServiceAppId (long) This field contains the application identifier of the service flow. ServiceClassName (string) This field contains the Service Class Name (SCN) of the service flow. ServiceDirection (int) This field indicates the direction of the service flow from the CMTS cable interface. ServiceDsMulticast (boolean) This field indicates if the service flow is multicast or unicast. ServiceFlowChSet (bytearray) This field contains a set of channels configured for the service flow. ServiceGateId (long) This field contains the GateID of the service flow. The field can also contain the value 0. ServiceIdentifier (long) This field contains the service flow ID. ServiceOctetsPassed (bigint) This field contains a counter value of the octets that has passed through the service flow. ServicePktsPassed (bigint) This field contains a counter value of the packets that have passed through the service flow. ServiceSlaDelayPkts (long) This field contains a counter value of the packets that exceeds the SLA in the downstream service flow. ServiceSlaDropPkts (long) This field contains a counter value of the packets that has dropped as it has exceeded the SLA in the downstream service flow. ServiceTimeActive (long) This field describes the time service flow is active. ServiceTimeCreated (long) This field describes the creation time of the service flow. SessionId (int) This field contains the session ID of the IPDR SP record. TemplateId (int) This field contains the template ID of the IPDR SP record. TemplateUDR (TemplateData (IPDRSP)) This field contains the TemplateData UDR representing the actual template structure used by the IPDR SP record. OriginalData (bytearray) This field contains the original data in bytearray format. SAMIS_TYPE_2 UDRs The following fields are included in the SAMIS_TYPE_2 UDRs: Field Description Field Description CmMacAddress (string) This field contains the MAC address of the cable modem. CmtsHostName (string) This field indicates the fully qualified domain name for the CMTS, if available. CmtsMdIfIndex (long) This field contains the ifIndex for the CMTS Mac Domain interface. CmtsMdIfName (string) This field contains the ifName of the Interfaces Group MIB for the row entry corresponding to the CMTS Mac Domain interface (ifType = 127) for this cable modem. CmtsSysUpTime (long) This field indicates the time (in hundredths of a second) since system initialization. CollConfigId (short) This field contains the collector configuration identifier of the IPDR SP record. ConnectionKey (string) This field contains the connection key which is used to identify the connection between agent and the exporter. CreationTime (date) This field indicates the creation time of the UDR. DocumentId (bytearray) This field indicates the document identifier of the IPDR SP record. ExporterAddress (string) This field contains the connection address of the IPDR SP exporter address. ExporterBootTime (date) This field contains the details of the exporter boot time received from the IPDR SP record. ExporterPort (int) This field contains the connection port of the IPDR SP exporter port. IsDuplicate (boolean) This field designates the value of the duplicate flag for the IPDR SP exporter. RawData (bytearray) This field contains the raw data of the IPDR SP record. RecCreationTime (bigint) This field contains the timestamp (in milliseconds) of the time the data for the record was acquired. RecType (int) This field identifies the record type for the IPDR SP record. SeqNo (long) This field identifies the sequence number for the IPDR SP record. ServiceAppId (long) This field contains the application identifier of the service flow. ServiceClassName (string) This field contains the Service Class Name (SCN) of the service flow. ServiceDirection (int) This field indicates the direction of the service flow from the CMTS cable interface. ServiceDsMulticast (boolean) This field indicates if the service flow is multicast or unicast. ServiceFlowChSet (bytearray) This field contains a set of channels configured for the service flow. ServiceGateId (long) This field contains the GateID of the service flow. The field can also contain the value 0. ServiceIdentifier (long) This field contains the service flow ID. ServiceOctetsPassed (bigint) This field contains a counter value of the octets that has passed through the service flow. ServicePktsPassed (bigint) This field contains a counter value of the packets that have passed through the service flow. ServiceSlaDelayPkts (long) This field contains a counter value of the packets that exceeds the SLA in the downstream service flow. ServiceSlaDropPkts (long) This field contains a counter value of the packets that has dropped as it has exceeded the SLA in the downstream service flow. ServiceTimeActive (long) This field describes the time service flow is active. ServiceTimeCreated (long) This field describes the creation time of the service flow. SessionId (int) This field contains the session ID of the IPDR SP record. TemplateId (int) This field contains the template ID of the IPDR SP record. TemplateUDR (TemplateData (IPDRSP)) This field contains the TemplateData UDR representing the actual template structure used by the IPDR SP record. OriginalData (bytearray) This field contains the original data in bytearray format. V3_0 UDRs Open IPDRSP DOCSIS UDRs - V3_0 UDR The following UDR Types are available in the V3_0 UDR: CMTS_CM_REG_STATUS_TYPE CMTS_CM_SERVICE_FLOW_TYPE CMTS_CM_US_STATS_TYPE CMTS_DS_UTIL_STATS_TYPE CMTS_TOPOLOGY_TYPE CMTS_US_UTIL_STATS_TYPE CPE_TYPE DIAG_LOG_DETAIL_TYPE DIAG_LOG_EVENT_TYPE DIAG_LOG_TYPE IP_MULTICAST_STATS_TYPE SPECTRUM_MEASUREMENT_TYPE V3_1 UDRs Open IPDRSP DOCSIS UDRs - V3_1 UDR The following UDR Types are available in the V3_1 UDR: CMTS_CM_DS_OFDM_PROFILE_STATUS_TYPE CMTS_CM_DS_OFDM_STATUS_TYPE CMTS_CM_DS_OFDMA_PROFILE_STATUS_TYPE CMTS_CM_DS_OFDMA_STATUS_TYPE DS_OFDM_PROFILE_STATS_TYPE US_OFDMA_STATS_TYPE For more information on the fields included in the V3_0 and V3_1 UDRs, refer to Annex C in CM-SP-OSSIv3.0-I05-071206.pdf (cablelabs.com) . IPDRSP Data UDRs The following fields are included in the IPDRSP Data UDRs: Field Description Field Description CollConfigId (short) This field contains the collector configuration identifier of the IPDR SP record. ConnectionKey (string) This field contains the connection key which is used to identify the connection between agent and the exporter. CreationTime (date) This field indicates the creation time of the UDR. DocumentId (bytearray) This field indicates the document identifier of the IPDR SP record. ExporterAddress (string) This field contains the connection address of the IPDR SP exporter address. ExporterBootTime (date) This field contains the details of the exporter boot time received from the IPDR SP exporter. ExporterPort (int) This field contains the connection port of the IPDR SP exporter port. IsDuplicate (boolean) This field designates the value of the duplicate flag for the IPDR SP record. RawData (bytearray) This field contains the raw data of the IPDR SP record. SeqNo (long) This field identifies the sequence number for the IPDR SP record. SessionId (int) This field contains the session ID of the IPDR SP record. TemplateId (int) This field contains the template ID of the IPDR SP record. TemplateUDR (TemplateData (IPDRSP)) This field contains the TemplateData UDR representing the actual template structure used by the IPDR SP record. OriginalData (bytearray) This field contains the original data in bytearray format. IPDRSP Simple UDRs For more information on the fields in IPDRSP Simple UDRs, refer to IPDRSP Data UDRs . IPDRSP TemplateData UDRs The following fields are included in the IPDRSP TemplateData UDRs: Field Description Field Description Fields (FieldDescriptor) This field contains a list of 5 other fields: FieldId (int): Denotes the identifier of the field FieldName (string): Describes the name of the field IsEnabled (boolean): Set to true if enabled TypeId (int): Denotes the identifier for the type of field OriginalData (bytearray): Contains the original data in bytearray format SchemaName (string) This field describes the schema name of the IPDR SP template. TemplateId (short) This field contains the template ID for the IPDR SP template. TypeName (string) This field contains the type name for the IPDR template type. OriginalData (bytearray) This field contains the original data in bytearray format.

---

# Document 252: Diameter Request Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032633/Diameter+Request+Agent+Configuration
**Categories:** chunks_index.json

To open the Diameter Request agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , click Realtime  OK . Click Add agent . In the Agent Selection dialog click the Processing tab and select Diameter Request . Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. Open The Diameter Request agent - Diameter Request tab Item Description Item Description Associated Diameter Stack From the drop-down list that includes all the Diameter Stack agents in the workflow, select the Diameter Stack that you want requests to be sent from.

---

# Document 253: Auditing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658230/Auditing
**Categories:** chunks_index.json

MediationZone includes comprehensive functionality to provide auditing and statistical information related to workflows. This function can record audit information on a per workflow basis, or on a consolidated basis. The audit capability covers normal workflow execution as well as reconciliation of reprocessed ECS data. Audit information can include any data available at workflows execution such as workflow name, source filename, route name, source and receiving agent names, number of bytes, UDRs or duration passed along each route in the workflows. Record reconciliation is also possible using the audit subsystem of MediationZone. Using this mechanism, it is possible to track the history of records while they for example reside in error correction. Example of Audit usage: an external system requires a statistical report on a daily basis from MediationZone to keep track of the traffic. To solve this, all workflows are configured to log the record count to the very same audit table. The daily summary can be calculated using, for example, an SQL Executor in the Task Manager, or by a workflow collecting, formatting and forwarding the information.

---

# Document 254: Duplicate Batch Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672784
**Categories:** chunks_index.json

The Duplicate Batch profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Configuration To create a new Duplicate Batch profile configuration, go to Build  New Configuration . Select Duplicate Batch Profile from the Configurations dialog. Duplicate Batch profile configuration Note! If the Detection Method is modified after a Duplicate Batch Detection agent has been executed, the already stored information will not match any records processed with the new profile version. The items in the menu bar may change depending on the configuration type. The Duplicate Batch profile uses the standard menu items, as described in Build View . The Duplicate Batch profile configuration contains the following settings: Setting Description Setting Description Max Cache Age (Days) Enter the number of days you want to keep the batch information in the database. Use CRC Select this checkbox to create a checksum from the batch file data. You can use this checksum when searching for duplicate batch files by comparing the files' checksums. Use Byte Count Select this checkbox to use the number of bytes in the batch files for duplicate detection. Use MIM Value Select this checkbox to use a MIM value for duplicate detection. A MIM name defined in the Named MIMs table is compared with a MIM Resource that can be connected both with batches and workflows. Note! When you use MIM values as a detection method, both empty and non-empty batches will be evaluated. The duplicate batch detection will also be performed regardless of the data on the incoming routes, as well as if there is no data on the incoming routes. This is not the case for the other detection methods. Named MIMs Click the Add button to create a list of user-defined MIM names. When the Duplicate Batch Detection agent is configured, each MIM name is assigned to one MIM Resource for which detection will be applied. Within the same workflow configuration, the profiles configured to Use MIM Value must map to the same MIM names.

---

# Document 255: ECS Search and Update Tuning - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647814/ECS+Search+and+Update+Tuning
**Categories:** chunks_index.json

The ECS Search and Update functionality can be tuned: To define the maximum number of ECS entries that are displayed in GUI simultaneously (default value 500) To define the maximum number of ECS entries that can be updated at one time (default value 15,000,000) with for example Bulk Edit.

---

# Document 256: Installation of Redis - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638100/Installation+of+Redis
**Categories:** chunks_index.json

To install Redis on AWS, take the following steps: Log into your AWS account and go to Services. In Services, go to Database and select ElastiCache. In the ElastiCache Dashboard, click Create. Choose Cluster engine and specify the basic setup of the cluster as required. You can also define advanced settings by expanding the Advanced Redis Settings tab at the bottom of the page. Note! The Subnet group that the ElastiCache cluster belongs to must be open for access from your MediationZone installation. ElastiCache dialog to create an ElastiCache cluster The ElastiCache Dashboard provides you with an overview of your ElastiCache clusters based on type. In addition to creating a cluster in the dashboard, you can also backup, reboot, delete, or modify clusters by selecting the buttons at the top of the dashboard. For further information on creating an ElastiCache cluster, see https://aws.amazon.com/documentation/elasticache/ .

---

# Document 257: Creating Pico Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204647137
**Categories:** chunks_index.json

Create new pico configurations by manually creating a new configuration file in the container ( <pico name>.conf) or use the mzsh topo set command . Run the following commands to create a new pico configuration. $ mzsh topo set topo://container:<container>/pico:<pico> <conf> The <config> argument may contain a key-value pair that specifies a template or a pico configuration in HOCON format. Example - Creating a new pico configuration based on a template $ mzsh topo set topo://container:main1/pico:ec2 template:mz.standard-ec When the target container is in the local MZ_HOME, you can add the local flag and omit the container name. Example. Creating a new pico configuration in the local container $ mzsh topo set --local pico:ec2 template:mz.standard-ec When you specify a pico configuration with multiple attributes, it is recommended that you use multi-line strings. Example - Creating pico configuration in HOCON format $ mzsh topo set --local pico:ec2 '{ template:mz.standard-ec config { properties { ec.httpd.port : 9092 } classpath { jars=["lib/picostart.jar"] } } }' If you do not specify a template, you need to ensure that the pico type is specified in the configuration: mzsh topo set --local pico:ec2 '{ type:ec config { properties { ec.httpd.port : 9092 } classpath { jars=["lib/picostart.jar"] } } }' Note! It is possible to create identical pico configurations with the same name in multiple containers. However, only one of these can run at a time. When you want to set the value of a property so that it is shared by all pico instances in a container, you can set it on the container level instead. Similarly, if the value should be shared by all pico instances in all containers, your can set properties on the cell level. Example - Setting a property in a container $ mzsh topo set topo://container:<container>/val:common.pico.tmpdir '${mz.home}/"tmp"' Example - Setting a property in a cell $ mzsh topo set topo://cell:default/val:common.pico.rcp.platform.host examplehost Note! Properties in the pico configuration overrides properties on the container level. Properties that are set on the container level overrides properties on the cell level. For further information about how to set properties on container- and cell level, see Updating Pico Configurations .

---

# Document 258: Email Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652451/Email+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . Loading

---

# Document 259: Automatic Statistics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205658277/Automatic+Statistics
**Categories:** chunks_index.json

Automatic Statistics includes the System Log, the System Statistics and the ECS Statistics. The System Statistics monitors system resources and record throughput for each workflow. The ECS Statistics gives information on records sent to and collected from the central error repository. All automatically recorded information can be viewed in comprehensive GUI views. MediationZone constantly collects information from the different sub-systems and hosts in the system. This information is among other things used for load balancing. By using the System Statistics tool, it is possible to view collected statistics. There are various kinds of statistics that are described more in detail in subsequent chapters. Host Statistics MediationZone collects statistics from the different machines hosting a Platform or Execution Context. For example, the load of the CPU or the number of context switches. This is called host statistics. MediationZone uses an external binary called vmstat to collect the information. This binary must be installed to have statistics collected and to perform load-balancing work for workflows. All values that are collected from a host can be selected from a list. On newer operating systems some of these may not be available for collection due to changes in the kernel of the operations system. In this example, CPU User Time (%) has been selected for a certain host and is displayed in a graph: Open Example of host statistics viewed from the System Statistics tool Workflow Statistics When a workflow is running, MediationZone continuously collects information on how much data is being processed. This (throughput) is measured per second. In most cases this number will be UDRs per second but e there are no UDRs routed in the workflow it will instead be raw data per second. The throughput is sampled every five seconds. Throughput is also defined as a MIM value for the workflow in case it would be convenient to delegate this to external systems, or to generate alarms if the throughput falls to low. The System Statistics window also shows a calculated number of simultaneously running workflows. Open Example of workflow statistics viewed from the System Statistics tool Pico Instance Statistics Every minute MediationZone collects memory information from the different Java processes defining the Platform and the Execution Context. This information shows how much memory is used and how much memory that can be used for the running process. The following is an example from the standard GUI view: Open Example of system statistics in Pico Viewer Error Correction Statistics An overview of different errors that exist in the environment can be viewed from the ECS Statistics tool Open Overview of errors in the system Offline Analysis of System Statistics The System Statistics sub-system has an import & export capability for the statistical data. This feature enables the users to export data from a production system and to import it into a test/development system for offline root-cause analysis. System Access/System Event From the System Log, which is also available from the Web Interface, an audit trail of a certain user can be displayed. Criteria such as user, date, Entity (workflow), and action updated, logged etc can be displayed. Further different formats of the report can be customized if direct access to the database is granted, or if data is exported to an external system. Open Example of collected statistics viewed in the System Log

---

# Document 260: SAP CC Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001750
**Categories:** chunks_index.json

This section describes the SAP CC agents. Supported Versions The following versions of SAP CC are currently supported: Version 2023 Note! Update : SAP CC team have announced the following on official support for SAP CC versions For SAP CC 2023 or newer, please use Java 17 based clients such as MZ 9.x For SAP CC 2022 or earlier, please use Java 8 based clients such as MZ 8.3.x Prerequisites SAP Convergent Ch argi ng concepts. Before starting with the SAP CC agents, the following is required: SAP Convergent Charging installation materials that matches the version of your SAP CC core system (i.e the back end servers). Info! Find your desired version of SAP CC installation materials from the SAP CC server provider. For example, here are some links that will direct you to the SAP CC 2023 installation material downloads. SAP CC 2023 Patch Release Note SAP CC 2023 Release Note It is recommended to upgrade your SAP CC server to the latest patch whenever possible. Setting up SAP CC Core SDK jar files Info! For more information on how to set up the SAP CC Core SDK jar files in Execution Container, refer to SAP CC Agent Preparation The section contains the following subsections: SAP CC Online Agent SAP CC Batch Agents SAP CC Notification Agent SAP CC REST Agent SAP CC UDRs SAP CC Agent Preparations SAP CC Secured Connection

---

# Document 261: Configuration Security - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612961/Configuration+Security
**Categories:** chunks_index.json

For each configuration in MediationZone it is possible to set read, write, and execute permissions. This is done by setting the properties for the configuration. It is also possible to choose if a configuration should be encrypted and protected by a password. For further information on setting the configuration permissions, see the Common Configuration Buttons and Encryption in Administration and Management in Legacy Desktop .

---

# Document 262: Data Veracity Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606738/Data+Veracity+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no message events for this agent. For further information about the agent message event type, see Agent Event . Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Start collecting This event is reported when the UDR collection from Data Veracity Collection Agent starts. Commit <count> records from Table <table name> This event is reported at end batch upon a successful commit. Total <count> records reprocessed This event is rep orted a t end batch showing the total number of records that were reprocessed.

---

# Document 263: SNMP Collection Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674790/SNMP+Collection+Profile
**Categories:** chunks_index.json

The SNMP Collection profile allows you to import the MIB files you want to use to build your target UDRs. It also allows you to specify the set of default SNMP parameters that will be used by the SNMP Request agent and the SNMP Trap agent. Note that one profile is usually for one family of network equipment. In case you have several network equipment families you must create one profile for each. Configuration The SNMP Collection profile contains the following tabs: 1 MIB Workspace Tab 2 Default SNMP Parameters Tab 3 Advanced Tab MIB Workspace Tab In the MIB Workspace tab, you load the MIB files to be used by SNMP collection agents in the MIB Workspace section. You can verify that the target network element responds to SNMP requests directly from the MIB Workspace. Use the SNMP Query Results section to send out SNMP requests including SNMP GET, GET-NEXT, GET-BULK, GET-WALK, and SET. Open SNMP Collection Profile - MIB Workspace tab Button and field Description Button and field Description MIB Workspace Load Select the MIB files you want to load and use to build the target UDR. These types of files are often defined with the extension .mib by the vendors, but there is no naming convention and it can be anything. Note! For any MIB files with dependencies, you are required to upload these files manually through the Load button feature. This is not the case if you are using the legacy desktop as all MIB files are loaded automatically by default. Unload Select the MIB file you want to unload and click this button. Unload All Click this button to unload all MIB modules. Expand All Click this button to e xpand all MIB tree branches . MIB Object Properties This panel shows the properties of the selected MIB file. SNMP Query Results OID Object Identifier of the currently selected MIB Object. This OID is used to send out SNMP requests to remote network element for testing purposes. VALUE The Value field is used to specify the value of the SNMP SET operation, which you can edit. The value must follow the syntax and semantics of the selected MIB Object. This field is also used to populate the result of the SNMP Get command, which is grayed out and cannot be edited. SNMP GET Performs SNMP GET operations on the selected MIB object. SNMP GET-NEXT Performs SNMP GET-NEXT operations on the selected MIB object. SNMP GET-BULK Performs SNMP GET-BULK operations on the selected MIB object. SNMP WALK Walks the selected MIB tree branch and retrieves all values. SNMP SET Performs SNMP SET operations on the selected MIB object. Specify the value to be set in the Value field. STOP Stops the current SNMP operation. CLEAR Clears the SNMP results area. Default SNMP Parameters Tab The SNMP Request agent uses the CSV files as information source to poll network elements. The parameter from the default set is used when the corresponding parameter is unspecified in the input CSV file. This is valid for all parameters except two in the profile: Polling interval Trap port Open SNMP Collection Profile - Default SNMP Parameters tab Field Description Field Description General Host Define the host used to perform SNMP queries for testing purposes directly from the MIB workspace tab. This value is optional since the hosts should be defined in the CSV file of the SNMP Request agent. Polling Interval (ms) The default polling interval for SNMP is 300 seconds. This value is used if no other value is defined in the CSV file of the SNMP Request agent. The value must always be set here, it can not be set in the input CSV file. Polling Distribution Period (ms) This field is used to configure how long the distribution period should be in milliseconds (ms). By default this field is set to zero ("0"), meaning that no distribution period is configured, and polling will be distributed evenly over the polling interval. If you configure the distribution period, the polling will be distributed within this period only during the polling interval, meaning the distribution period has to be equal to, or shorter than, the polling interval. The distribution period will start at the beginning of the polling interval. Port Default port for SNMP is 161. If no other value is defined in the CSV file of the SNMP Request agent this value is used. Trap Port Default trap port for SNMP trap is 162. This value is used by the SNMP Trap agent to listen for incoming SNMP notifications (TRAPs and INFORMs). This value must always be set here. SNMP Version SNMPv1 Use read and write communities to access the network element via SNMPv1. Note! If there is no other value defined in the CSV file of the SNMP Request agent, this value or the SNMPv2c and SNMPv3 value is used based on the selected option. SNMPv2c Use read and write communities to access the network element via SNMPv2c. For more information about the SNMPv2c, see SNMP_Wiki . Read Community The SNMP community string is like a user ID or password that allows access to a target network element. The read community is used for GET, GET-NEXT, and GET-BULK requests. Write Community The SNMP community string is like a user ID or password that allows access to a target network element. The Write community is used for SET requests. SNMPv3 Use the same parameters and authentication as defined in the network element. For more information about the SNMPv3, see SNMP_Wiki . Note! The parameters "Retry" and "Timeout" are not used as expected. This is due to an error in the underlying library used. "Retry" is used during the discovery call, as well as the actual call, when it should only be used during the actual call. "Timeout" is not only used during actual timeouts, but between any "Retry" attempts. User Name The principal on whose behalf access is requested. This must be a human-readable string representing the user in a format that is Security Model independent. Context Name The collection of management information accessible by an SNMP entity. Context Engine ID An SNMP engine provides services for sending and receiving messages, authenticating and encrypting messages, and controlling access to managed objects. There is a one-to-one association between an SNMP engine and the SNMP entity which contains it. Within an administrative domain, an EngineID is the unique and unambiguous identifier of an SNMP engine. Context Name - the name of an SNMP Context. Auth Protocol The authentication protocol that is used to authenticate the user. Four protocols are defined: SHA-2 224, SHA-2 256, SHA-2 384, and SHA-2 512. Auth Password The password required for authentication service. Priv Protocol The privacy protocol that is used to protect the message from disclosure. Two such protocols are defined: DES-CBC Symmetric Encryption Protocol and CFB-AES-128. Priv Password The password required for privacy (encryption) service. Request Handling Timeout (ms) The timeout is the time interval that the SNMP Request agent waits for a response message from the target network element. The timeout value is given in milliseconds. Note! This timeout must be set so that the following is true: "Polling Interval > Timeout * (Number of Retries +1)". This is checked when validating the SNMP profile. However, take care when using the CSV_TIMEOUT and CSV_RETRIES in the Network Element file. UDR Timeout (ms) UDR Timeout in milliseconds. If the time exceeds the timeout, the UDR will be sent on the next route. Note! This UDR timeout must be set so that the following is true: "UDR timeout > Timeout * (Number of Retries +1)". This is checked when validating the SNMP profile. Number of Retries Retries are the number of times a SNMP request is sent when a timeout occurs. If the retry value is zero (0), the request is not re-transmitted during timeout. Max Outstanding Per Element The maximum number of simultaneous outstanding requests per element allowed. When this limit is reached, no new requests are sent to the element until responses for the outstanding requests are received. The default value is set to zero (0), which means that no limit is set and any number of simultaneous outstanding requests is allowed. Advanced Tab The following fields are available in the SNMP Profile Advanced tab. Open SNMP Collection Profile - Advanced tab Field Description Field Description General Max PDU Size The maximum size of request PDUs that this target is able to receive. The response PDU has to fit in a single UDP packet. The maximum limit is 65535 bytes. Get-Bulk Parameters Non Repeaters Number of variables in the variable list for which a simple GETNEXT operation has to be done. Max Repetitions Number of continuous GETNEXT operations. Scalar Value Retrieval Max Variables Per PDU The maximum number of variable bindings per request. Table Retrieval Max Columns Per PDU Maximum number of columns to retrieve per SNMP GETNEXT or GETBULK request. Max Rows Per PDU Maximum number of rows to retrieve per SNMP GETBULK request. Send Table Requests in Separate PDUs By default all column OIDs from a single table are now sent in one GETBULK request in accordance with the Max Variables Per PDU. Use this if All column OIDs from a single table are to be sent in separate GETBULK requests .

---

# Document 264: RESTful Interface for Reference Data Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677668
**Categories:** chunks_index.json

Database operations are supported via a RESTful HTTP Interface on the Platform. Note! Temporary files are created on the Platform host when the user executes queries via Reference Data Management. These temporary files, which are stored in $MZ_HOME/tmp/rowset , are removed automatically when the user session expires. User sessions expire automatically after six (6) hours. Restful Operations The tables below describe the various operations that are available for Reference Data Management. The service endpoint URI is http://<Platform host>:<port>/api/v1. If encryption is enabled, the URI is https://<Platform host>:<port>/api/v1 . For further information about enabling encryption, see Network Security in the System Administrator's Guide. Basic authentication is used and you must pass user credentials for each RESTful call. Create and Get Session This operation returns a session id which should be appended to all subsequent API calls where this parameter is applicable. Resource path /session HTTP method GET Example $ curl -u  user:passw http://localhost:9000/api/v1/session Close Session This operation closes a session. This session id is no longer valid for any subsequent API calls. Resource path /session/close?sessionid=<sessionid> HTTP method GET Example - Closing a session $ curl -u user:passw  http://localhost:9000/api/v1/session/close?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Reference Data Profiles This operation retrieves a list of all Reference Data profiles that are available to the user. Resource path /refdatas HTTP method GET Example - Retrieving a list $ curl -u  user:passw http://localhost:9000/api/v1/refdatas Get Metadata This operation retrieves table metadata that can be used for viewing or to derive parameters for other REST APIs. If the Last Update feature is enabled, the values stored in the most recent Last Update user and timestamp column are retrieved as well. The result is returned synchronously. Resource path /refdatas/<Reference Data profile>/table/<table name>?sessionid=<session id> HTTP method GET Example - Get metadata $ curl -u user:passw  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Query This operation executes database queries on a specific Reference Data profile and database table. The query is performed asynchronously and control is returned immediately. You can retrieve the result of the query by using /rowset/<rowset number>?sessionid=<session id> . This operation requires Input parameters that are passed in a JSON format as part of the HTTP message body. Note! Any ongoing query process running on the same session will be aborted and a new process for the latest query will be executed. This operation will clear any uncommitted changes saved in the same session. Resource path /refdatas/<Reference Data profile>/table/<table name>/rowset?sessionid=<session id> HTTP method PUT Body This is where the executeQuery is included. The executeQuery JSON payload includes these options: rowsPerPage - the max number of rows (Data Set size) that are allowed in a rowset of the retrieved result. selectedColumns - allows for specific columns to be selected. filterExpression - allows optional definition of query expressions. The query will match all of the specified query expressions. Example - Get query without query expression in JSON payload body $ curl -X PUT -u user:passw -H 'content-type: application/json'  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t  -d '{"executeQuery": {"rowsPerPage":500,"selectedColumns":["ID","FIRSTNAME","LASTNAME"]}}' Example - Get query with query expression in JSON payload body $ curl -X PUT -u user:passw -H 'content-type: application/json'  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t  -d '{"executeQuery": {"rowsPerPage":500,"selectedColumns":["ID","FIRSTNAME","LASTNAME"],"filterExpression":[{"col":"ID","expr":9,"op":">="},{"col":"LAST_NAME","expr":"Smith","op":"not like"}]}}' Example - Get query using a JSON payload file $ curl -X PUT -T="Example.json" -u user:passw -H 'content-type: application/json'  'http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t Example.json file { "executeQuery": { "rowsPerPage": 500, "selectedColumns": ["ID", "FIRSTNAME", "LASTNAME"] } } Get Data Sets Data sets can be retrieved once downloaded to the file system of the Platform. This operation returns a data set for the given rowset (sequence number). The total number of available data sets can be queried with the Get Status operation. Resource path /rowset/<rowset number>?sessionid=<session id> HTTP method GET Example - Get data sets $ curl -u user:passw  http://localhost:9000/api/v1/rowset/0?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Status This operation returns a status message. It can be used to retrieve active processes and to query the number of available rows, data sets, and the status of imports and exports. If the Last Update feature is enabled, the values stored in the most recent Last Update user and timestamp column are retrieved as well. Resource path /status?sessionid=<session id> HTTP method GET Example - Get status $ curl -u user:passw  http://localhost:9000/api/v1/status?sessionid=vusncl88sjghv7h8nkb0ohja6t Abort Process This operation requests the active process to abort. Note! To prevent a user from initiating another operation before the first operation initiated is complete, Abort Process can be used before an operation is complete. Resource path /status/abort?sessionid=<session id> HTTP method GET Example - Get status $ curl -u user:passw  http://localhost:9000/api/v1/status/abort?sessionid=vusncl88sjghv7h8nkb0ohja6t Table Export This operation performs a database table export. Input parameters are passed in a JSON format as part of the HTTP message body. Resource path /refdatas/<Reference Data profile>/table/<table name>/download?sessionid=<sessionid> HTTP method POST Body This is where the exportParams are included. The exportParams JSON payload includes these options: opts textQualifier - designated as double quotes by default. separator - designated as a comma by default. extent - designated as all by default. all - export all rows in the table. selected - export rows from the result of Get Query . selectedColumns - allows for specific columns to be selected. Note! For the extend option, selected value is only applicable when Get Query is applied prior Table Export . Example - Table export without options $ curl -X POST  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/download?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -u "user:passw"  -d 'exportParams={}'  > Export.csv Example - Table export with options $ curl -X POST  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/download?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -u "user:passw"  -d 'exportParams={"opts":{"textQualifier":"'''","separator":";","extent":"all"},"selectedColumns":["CITY","CUSTOMER_NAME"]}' > Export.csv Table Import This operation performs a database table import. Input parameters are passed in a JSON format as part of the form-data in the HTTP message body. Resource path /refdatas/<Reference Data profile>/table/<table name>/upload?sessionid=<sessionid> HTTP method POST Body This is where the file and the input parameters are included. The exportParams format includes these options: textQualifier - designated as double quotes by default. separator - designated as a comma by default. opts - designated as append by default. append - imported rows are appended to the table. truncate - the existing data in the table are truncated before the import is executed. force - designated as false by default. Example - Table import without options $ curl -i -u "user:passw"  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/upload?sessionid=rssrh20dcd8lc1j505b3bqnstc'  -F file=@/path/to/import_test.csv Example - Table import with options $ curl -i -u "user:passw"  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/upload?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -F file=@/path/to/import_test.csv  -F 'opts={"textQualifier":"'","separator":",","op":"truncate","force":false}' Save Changes This operation saves data modification (insert/update/delete). Changes are saved within the client session. Input parameters are passed in a JSON format as part of the HTTP message body. Note! tableName and refProfile are mandatory in order to save. A single table can be modified in a single session only. Note! The save operation is supported either on Oracle (based on the ROWID pseudo column) or non-Oracle type tables containing a Primary Key constraint. Non-Oracle tables without a Primary Key are not supported for data modifications. Resource path /save?sessionid=<session id> HTTP method PUT Body This is where the dataEdits are included. The dataEdit JSON payload includes these options: refProfile - Reference Data Management Profile. tableName - Database table name. updates - modification parameters. action - to specify the type of modification. insert - insert a new row to the table. update - edit an existing row in the table. delete - delete an existing row in the table. ids - to specify column value pairs of primary key(s). column - private key column name. value - private key value for the respective column. values - to specify column value pairs to be inserted/updated. column - column name to be inserted/updated. value - insert/update value for the respective column. Note! When inserting a row, specifying a pseudo ids column value pair allows Cancel Changes to be applied to that specific insert row modification. Example - Save changes with a JSON payload body $ curl -X PUT -u "user:passw" -H 'content-type: application/json  -d '{"dataEdit":{"refProfile":"Default.refTest","tableName":"MZADMIN.REFRENCE_DATA","updates":[{"action":"delete","ids":[{"column":"ID","value":8}]}]}}'  http://localhost:9000/api/v1/save?sessionid=vusncl88sjghv7h8nkb0ohja6t Example - Save changes using a JSON payload file $ curl -X PUT -T="Example.json" -u "user:passw" -H 'content-type: application/json  http://localhost:9000/api/v1/save?sessionid=vusncl88sjghv7h8nkb0ohja6t Example.json { "dataEdit" : { "refProfile" : "Default.refTest", "tableName" : "MZADMIN.REFRENCE_DATA", "updates" : [ { "action" : "insert", "ids" : [ { "column" : "ROWID", "value" : "ins0" } ], "values" : [ { "column" : "ID", "value" : 645 }, { "column" : "FIRST_NAME", "value" : "Roberts" }, { "column" : "LAST_NAME", "value" : "Polis" } ] }, { "action" : "update", "ids" : [ { "column" : "ID", "value" : "6" } ], "values" : [ { "column" : "LAST_NAME", "value" : "Wick" }, { "column" : "FIRST_NAME", "value" : "John" } ] }, { "action" : "delete", "ids" : [ { "column" : "id", "value" : "8" } ] } ] } } Commit Changes This operation applies saved edits in the database and commits the work in case of success. You can use force commit in case of errors. If the Last Update feature is enabled, the user name and modification timestamp values for insert/update modifications are stored in the Last Update columns specified in the Reference Data Management Profile. The Last Update information is used by the Get Status operation to retrieve the most recent Last Update user and timestamp. Resource path /save/commit?force=<true|false>&sessionid=<session id> HTTP method GET Example - Commit changes $ curl -u user:passw  http://localhost:9000/api/v1/commit?force=false&sessionid=vusncl88sjghv7h8nkb0ohja6t List Changes This operation returns a list of the modifications saved. Resource path /save/list?sessionid=<session id> HTTP method GET Example - List changes $ curl -u user:passw  http://localhost:9000/api/v1/save/list?sessionid=vusncl88sjghv7h8nkb0ohja6t Cancel Changes This operation cancels the changes made from being saved. Input parameters are passed in a JSON format as part of the HTTP message body. Note! Pseudo Primary Keys for inserted rows can be included in the Save Changes operation to allow the cancel function for a specific insert row modification. Note! SaveSize in the Cancel Changes response indicates the number of changes remaining. Resource path /save/cancel?sessionid=<session id> HTTP method PUT Body This is where the dataEdits are included. The dataEdit JSON payload includes these options: scope - specify the scope of changes to be canceled. single - allow only specific modifications identified by ids to be cancelled. all - allow all the modifications to be canceled. ids - to specify column value pairs of primary key(s). This is only required when the scope is single . column - private key column name. value - private key value for the respective column. Example - Cancel changes $ curl -u user:passw -H 'content-type: application/json'  -d '{"dataEdit":{"scope":"single","ids": [{"id":[{"column":"ORDER_NUM","value":10398005}]}]  http://localhost:9000/api/v1/save/cancel?sessionid=vusncl88sjghv7h8nkb0ohja6t Show Demo Query This operation shows an example JSON payload format that applies for a Get Query operation. Resource path /demo/queryRequestParameters HTTP method GET Show Demo Changes This operation shows an example JSON payload format that applies for a Save Changes operation. Resource path /demo/dataEditRequestParameters HTTP method GET

---

# Document 265: Managing Picos in Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657533/Managing+Picos+in+Desktop
**Categories:** chunks_index.json

This section describes how to manage pico configurations in Pico Management screen in Desktop. To open the Pico Management screen, click on the Manage screen option in Desktop and then click on the Pico Management button. Open Pico Management You can perform the following operations on the Pico Management page: View, update, and create Picos Start, stop, and restart Picos This chapter includes the following sections: Starting, Stopping and Restarting Picos View, Update, Create, and Delete Picos

---

# Document 266: High Availability for PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678679/High+Availability+for+PCC
**Categories:** chunks_index.json

The PCC solution manages high availability in different ways. For [EZ] and [DR], redundancy is achieved by having several of each, while [CZ] requires a stand-by machine to fail over to. The minimum solution described in the PCC Installation Instructions offers 99.999% Availability. Refer to the PCC Installation Instructions documentation for further information about high-availability installations.

---

# Document 267: Kafka Batch Forwarding Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138300/Kafka+Batch+Forwarding+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data This section includes information about the data type that the agent expects and delivers. The agent receives KafkaRecords and produces Kafka messages. Meta Information Model (MIM)

---

# Document 268: Logging - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611574
**Categories:** chunks_index.json

Problems related to submission of a Spark application are logged in the Platform log. Errors that occur after submission of a Spark application, i e runtime errors, are logged in the the Spark environment. Error information related to Kafka and Zookeeper services can be found in the SC-logs for the respective service. Runtime Errors Cluster Runtime errors that occur in the cluster are logged in SPARK_HOME/logs . Spark Application Runtime errors that occur in the Spark application when it is running are logged in the file SPARK_HOME/work/driver-<number>-<number>/stderr . Runtime errors on the executor level are logged in the file SPARK_HOME/work/app-<number>-<number>/stderr You can also access these logs from the Spark Master Web UI: Click a Worker id under Running Drivers . Open Spark UI - Master Click stderr under Logs . Open Spark UI - Worker KPI Processing Accumulators When a Spark batch has finished processing, a set of accumulators are logged in the file SPARK_HOME/work/driver-<number>/stdout . These accumulators serve as a summary of what has been collected and calculated within the batch. The following accumulators are logged: Accumulator Description Accumulator Description CalculatedKPIs This accumulator includes GeneratedKPIOutputs and calculated KPIs that are not closed yet. DiscardedKPIs This accumulator is incremented by one for each calculated KPI that belongs to a previously closed period . FailedMetricCalculations This accumulator is incremented by one for each metric calculation that fails, e g due to invalid data in the input records . If there are several nodes in the node tree(s) that contain the metric, one input record may affect several metric calculations. FailedKPICalculations This accumulator is incremented by one for a KPI calculation that fails due to undefined metrics in the KPI expression. In order for the accumulator to be incremented, the following conditions must apply: - The period for the KPI ends during the Spark batch. - The KPI expression uses multiple metrics and one or more of these are undefined. GeneratedKPIOutputs This accumulator is incremented by one for each successfully calculated and delivered KPI. MissingExpressionForInputType This accumulator is increased by one for each input record that does not match a metric and a dimension object in the service model. Example - Counters in stdout The example below indicates that 20 input records failed to match both a metric and dimension expression in the service model. ============= SPARK BATCH: 2023-10-19 12:35:20:0 =============== CalculatedKPIs = 222 GeneratedKPIOutputs = 200 MissingExpressionForInputType = 20 DiscardedKPIs = 0 FailedMetricCalculations = 0 FailedKPICalculations = 0 You can also access these accumulators from the Spark Master Web UI: Click a Worker id under Running Drivers . Open Click stdout under Logs . Note! The accumulators are logged using log4j, meaning that the configured log level will decide whether or not the accumulators will be logged. The log level is specified in submit.sh by assigning log4j_setting and supply --conf spark.driver.extraJavaOptions=$log4j_setting. The default log level in Spark is WARNING and the log level for the accumulators is INFO . Note! It is possible to log the accumulators to a separate log file by adding the log4j.properties log4j.appender.accumulatorlog=org.apache.log4j.RollingFileAppender log4j.appender.accumulatorlog.File=accumulators.log log4j.appender.accumulatorlog.layout=org.apache.log4j.PatternLayout log4j.appender.accumulatorlog.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n log4j.logger.com.digitalroute.mz.spark.StreamOperations$=INFO, accumulatorlog log4j.additivity.com.digitalroute.mz.spark.StreamOperations$=false

---

# Document 269: Resume Workflow Execution - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997002/Resume+Workflow+Execution
**Categories:** chunks_index.json

Restart all ECs that were shut down or disconnected during the upgrade. $ mzsh restart <ec name> Start the workflows on the restarted ECs. Workflow groups can be enabled either through the Execution Manager in the Desktop or command line. Based on the recorded state during the shutdown process (refer to Shut Down Workflows and Desktops ), you can use different approaches to resume Workflow Groups with the wfgroupenable command. If there was a mix of enabled and disabled groups previously, run the following command to enable only the groups that were enabled: $ mzsh mzadmin/<password> wfgroupenable <folder name>.* # Run the following command only if you wish to enable the SystemTask group. $ mzsh mzadmin/<password> wfgroupenable SystemTask.* Otherwise if all groups, including System Task-related cleaner groups, were enabled before, use the following command to enable all groups: $ mzsh mzadmin/<password> wfgroupenable * Important! In some cases, you may not want to enable the System Task cleaner groups, as they might delete logs or data needed for audits or operational reviews. If all groups have been enabled, you can disable the System Task group using the following command: $ mzsh mzadmin/<password> wfgroupdisable SystemTask.*

---

# Document 270: Configuration Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638352
**Categories:** chunks_index.json

Overview The Configurations Browser is used to access the different configurations available in MediationZone. Each individual configuration has its own set of properties and can differ visually from others. The standard menu buttons are shared between each of them, but depending on the complexity and available fields some configurations can be presented to the users in a different way. Common Configurations Common configurations refer to functions that are not dependent on specific profiles or agents. They usually serve to configure a specific process that modifies the way configurations are executed. For all configurations, except Workflow, a dialog will open up where you create your configuration. For Workflows, the Workflow Editor will open up. The following table shows a list of all common configurations: Configuration Description Configuration Description External References Profile The External Reference profile enables you to use configuration values that originate from property files or exported environment variables from the Platform's startup shell. KPI Profile The KPI profile contains a KPI Management service model and is used by the KPI Cluster, KPI Cluster Out, and KPI agents. Reference Data Profile In a Reference Data profile configuration, you can select the tables that should be available for query and editing via the Reference Data Management Web UI, or RESTful interface. Unit Test The Unit Test configuration enables the creation of tests used in continuous deployment scenarios. Workflow The Workflow configuration enables the creation and management of three different types of workflows: batch, real-time, and task. See the Working with Workflows section to learn more. W orkflow Bridge Profile The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. The profile ties the workflows together. Profile Configuration Types Profiles contain configuration options relevant to the associated agent. They usually require credentials configurations and are used to collect and enrich the data that is processed by the system. Configuration Description Configuration Description 5G Profile The 5G Profile enables 5G communication with HTTP/2 agents. Aggregation Profile Aggregation consolidates related UDRs that originate from either a single source or from several sources, into a single UDR. Related UDRs, are grouped into "sessions" according to the value of their respective fields, and a set of configurable conditions. Amazon Profile The Amazon Profile is a generic profile used for setting up Amazon S3 credentials and properties that can be used by various other profiles or agents. Analysis APL Code Editor The APL Code Editor is used to create generic APL code that can be imported and used by several agents and workflows. APL Collection Strategy The APL Collection Strategies are used to set up rules for handling the collection of files from the Disk, FTP, SFTP, and SCP collection agents. Archive Profile The Archive Profile contains storage, naming scheme, and lifetime for targeted files. Audit Profile The Audit Profile offers the possibility to output information to user-defined database tables. Azure Profile The Azure Profile is used for setting up the access credentials and properties to be used to connect to an Azure environment. Categorized Grouping Profile The Categorized Grouping Profile is loaded when you start a workflow that depends on it. Conditional Trace Templates Conditional Trace is a trouble-shooting function that allows you to set a trace filter on agents and/or UDRs in real-time workflows route(s) and/or Analysis agent(s) for either a specific field value or a range of field values. Couchbase Profile The Couchbase profile is used to read and write bucket data in a Couchbase database and can be accessed by workflows using Aggregation, Distributed Storage, or PCC. Data Masking Profile The Data Masking profile selects the masking method you want to use, which UDR types and fields you want to mask/unmask, and any masking method-specific settings. Data Veracity Profile The Data Veracity Profile is used to select the particular database to which Data Veracity will be connected. Diameter Application Profile The D iamete r Application Profile captures a set of AVP and command code definitions that are recognized by the Diameter Stack agent during runtime. Diameter Routing Profile The Diameter Routing Profile enables you to define the Peer Table and the Realm Routing Table properties for the Diameter Stack agent. Duplicate Batch Profile Duplicate Batch Detection agent provides duplication control on incoming batches. Duplicate UDR Profile Duplicate UDR Detection agent provides duplication control on incoming UDRs. Elasticsearch Profile The Elasticsearch profile is used to read and write data in an Elasticsearch Service in AWS and can be accessed by batch workflows using Aggregation agents. Encryption Profile TheEncryption Profile you make encryption configurations to be used by the Encryptor agent. Event Notifications The Event Notification configuration offers the possibility to route information from events generated in the system to various targets. External Reference Profile The External Reference profile enables you to use configuration values that originate from property files or exported environment variables from the Platform's startup shell, File System Profile The File System Profile is used for making file system-specific configurations, currently used by the Amazon S3 collection and forwarding agents. GCP Profile The GCP Profile is used for setting up the access credentials and properties to be used to connect to a Google Cloud Platform service. GCP PubSub Profile The GCP PubSub Profile is used for setting up the Google PubSub Subscription and Topic for a Google Cloud Project. Inter Workflow Profile The Inter Workflow profile enables you to configure the storage server that the Inter Workflow forwarding and collection agents use for communication. JMS Profile The JMS profile contains settings that you use to connect and acquire both a Connection Factory object and a Destination object from a JNDI service. Kafka Profile The Kafka profile enables you to configure which topic and which embedded service key to use. Open API Profile If you want to use Open API 3.0 with HTTP/2 agents, you require an Open API profile configuration. You select the profile that you configure in the HTTP/2 Server agent configuration. The Prometheus Filter Use this filter to configure the Prometheus metrics that are going to be exposed for scraping. The purpose of the Prometheus Filter is to prevent the flooding of metrics in the storage of the Prometheus host. Python Interpreter Profile With the Python Interpreter profiles, you can configure which executable to use, and also which working directory you want to have. Python Module With the Python Module configurations, you can write shared Python code that can be imported by multiple Python agents. Redis Profile A Redis profile is used to read and write data in a Redis database and can be accessed by real-time workflows using Aggregation agents. REST Server Profile The REST Server Profile is used to define the endpoint URI for any particular REST server agent. SAP RFC Profile The SAP RFC profile dynamically generates UDRs based on selected SAP RFC functions that are part of an SAP system. Security Profile The Security profile is a generic profile that you can use to make encryption configurations that can be used by several agents and profiles. Shared Table Profile This section describes the Shared Table profile. This profile enables workflow instances to share tables for lookups. SNMP Collection Profile The SNMP Collection profile allows you to import the MIB files you want to use to build your target UDRs. SNMP OID Profile With the SNMP OID profile, you can select to configure which OIDs, UDR types, and fields to poll, outside of the SNMP Request agent itself, which enables several agents to use the same configuration. Suspend Execution This section includes information about the configuration option Suspend Execution. System Insight Profile The System Insight Profile allows you to create, edit or remove profiles and filters that you want to use to display or store statistics using the system insight service. Workflow Bridge Profile The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. Workflow Group The workflow group configuration enables you to manage workflow groups.

---



---
**End of Part 12** - Continue to next part for more content.
