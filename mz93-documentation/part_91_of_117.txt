# RATANON/MZ93-DOCUMENTATION - Part 91/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 91 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.0 KB
---

The Desktop Interface enables interaction with MediationZone deployments. http://localhost:9001/desktop/ After a successful login, the Desktop start page is displayed . Open The Dashboard shows the splash screen. From here you can access the main sections: The top menu panel lists the main options available to the users, subdivided into the Build View and Manage View screens. Clicking on the MediationZone logotype takes you back to the start page. On the top right, there is the User Settings List . You can access the dropdown menu by clicking on the icon. Top Menu Bar The Desktop user interface has a top menu bar consisting of the following options: Item Description Item Description Screen Selection The Screen Selection is located at the top left in Desktop. You can select to open the Build screen or the Manage screen and MediationZone icon takes you back to the start screen. Each selection is displayed in a full-screen vi ew. Open Screen Selection List Search Allows you to search the Desktop for any configuration or content. For more information on using search terms to better aid your querying, refer to Search . User The User menu is located on the top right in Desktop and contains various options for your account or deployment. See User Settings for more information. Open User icon Accessibility Support Across all views, there is screen reader support that enables Accessibility access. In addition, the overall display theme can be selected for easier viewing. For more information, see Desktop Accessibility Options . The Desktop interface is redesigned to be easily accessible using web browsers. This all-new experience has many advantages over the Legacy Desktop as it is more intuitive to use. However not all functionality from the Legacy Desktop is available yet, for some advanced features and functionality you still have to use the Legacy variant. F or more information, see the Legacy Desktop . This section contains the following subsections: Build View Manage View

---

# Document 2148: Starting and Managing the Legacy Desktop - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205655073/Starting+and+Managing+the+Legacy+Desktop
**Categories:** chunks_index.json

You can start the Legacy Desktop for multiple instances of the MediationZone Platform via the Desktop Launcher application, or you can start it in your browser. When the Desktop Launcher connects to the Platform, it automatically downloads and installs the various files that are required to run the Desktop. After a system upgrade, the Desktop Launcher updates these files to ensure that the Legacy Desktop is compatible with the connected Platform. If a new version of MediationZone includes an updated version of the Desktop Launcher it will update itself.

---

# Document 2149: metric - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676958
**Categories:** chunks_index.json

The metric objects represent values that are extracted from KDR UDRs and aggregated according to the tree structure in the model. Expressions are applied on the various fields in the UDRs to calculate a value, e g a sum, average, or min/max value. The following JSON schema describes the data format of the metric object type: Loading Property Description fun fun must contain a string that identifies one of the following aggregation functions: avg - This function yields the average result of the expression . min - This function yields the minimum result of the expression . max - This function yields the maximum result of the expression . sum - This function yields the sum of results from expression . expr expr must contain an arithmetic or relational expression (or combination thereof) based on the fields in the KDR input and/or constant values. Syntax: "<KDR.type>" : "<expression>" The functions and operators that you can use in expressions are described below. Conditional functions: isSet(<field>) - Returns 1 if the field is set to a value, otherwise 0 isNotSet(<field>) - Returns 1 if the field is not set to a value, otherwise 0 Operators: + - Addition - - Subtraction/negation * - Multiplication / - Division Relational operators: = - Equal != - Not equal > - Greater than < - Less than A relational expression or sub-expression evaluates to 1 if it is true, or 0 if it is false. You can use parentheses to modify the order of the operations, i e apply precedence rules. Example. Expressions Arithmetic expression: "expr": { "kdr_record_type_a": "field2-field1", "kdr_record_type_b": "field3" } Expression using conditional function: "expr": { "kdr_record_type_a": "isSet(field1)" } Relational expression: "expr": { "kdr_record_type_a": "field2<(field1+10)" } In the case of division by zero, the value of the output of the expression will be positive infinity, negative infinity, or NaN (Not a Number) as defined in the JVM specification. Example - JSON Representation "metric": { "TotalNumber": { "fun": "sum", "expr": { "kdr_record_type_a": "isSet(field1)" } }, "TotalSuccessful": { "fun": "sum", "expr": { "kdr_record_type_a": "field1=200" } }, "AvgDuration": { "fun": "avg", "expr": { "kdr_record_type_a": "field2-field1", "kdr_record_type_b": "field3" } } }

---

# Document 2150: Inter Workflow Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673584/Inter+Workflow+Collection+Agent
**Categories:** chunks_index.json

The collecting Inter Workflow agent collects batch files from a storage server. The data that it collects has previously been submitted to the storage server by a forwarding Inter Workflow agent. Note! An Inter Workflow profile cannot be used by more than one Inter Workflow collection agent at a time. A workflow trying to use an already locked profile will abort. Note! In a batch workflow, the collecting Inter Workflow agent will hand over the data, in UDR form, to the next agent in turn, one at a time. In a real-time workflow, on the other hand, the collecting Inter Workflow agent routes the UDRs into the workflow, one batch at a time. It is possible to restrict memory consumption by setting the property mz.iwf.max_size_block in the <ec>.conf or the platform.conf , on the EC or Platform that runs the Inter Workflow storage. For further information on how to modify properties, see Updating Pico Configurations . If the agent wants to allocate more memory than the given property value during collection, the collection will abort instead of suffering a possible "out of memory". The value representation should be in bytes. See the following example: Example - Restricting memory consumpltion mz.iwf.max_size_block="65535" Note! The minimum value is 32000 bytes, and even if a lower value is configured, 32000 will apply. Every batch file that the agent routes to the workflow is preceded with a special UDR that is called NewFileUDR , and contains the n name of the batch file. Open The Inter Workflow collection agent in batch and real-time workflows This section contains the following subsections: Inter Workflow Real-Time Collection Agent Input/Output Data and MIM Inter Workflow Batch Collection Agent Input/Output Data and MIM Inter Workflow Batch Collection Agent Transaction Behavior Inter Workflow Real-Time Collection Agent Events Inter Workflow Collection Agent in a Batch Workflow Inter Workflow Batch Collection Agent Events Inter Workflow Collection Agent in a Real-Time Workflow Inter Workflow Real-Time Collection Agent Transaction Behavior

---

# Document 2151: SAP CC Secured Connection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642859
**Categories:** chunks_index.json

We support TLS/SSL handshaking for one-way authentication between a SAP CC client and SAP CC core server. In one-way mode, only SAP CC Client validates the SAP CC Core Server to ensure that it receives data from the intended SAP CC Core Server. For implementing one-way mode, the SAP CC Core Server shares its Certificate(s) with the SAP CC Client. To allow SAP CC agents to connect to the SAP CC Core Server with TLS enabled, you must: Configure SAP CC Core Server with one-way authentication for the respective Instance and Services. Configure MediationZone Client to trust SAP CC Core Server. Configure SAP CC Core Server To secure SAP CC Core Server communication service, follow this SAP Support page: Secure an SAP CC Core Server communication service Before we can start configuring SAP CC Core Server, we need to know that SAP CC agents in MZ are connecting to the Dispatcher instance through the TCP-IP layer: Open SAP CC Architecture Diagram including SAP CM as a third party element Note! For more information, please read Identifying services involved in the Client/Server communication For our case, you will turn on one-way for ExternalSecure targeted service on the Dispatcher instance. Example - SAP CC Core Server Instance Map Example! Example SAP CC Core Server Instance Map: #InstanceId ; HCISecure ; HCIHost ; HCIPort ; WSSecure ; WSHost ; WSPort ; ExternalSecure ; ExternalHost ; ExternalPort ; InternalSecure ; InternalHost ; InternalPort updater#1 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9000 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9080 ; ; ; ; ; ; dispatcher#1 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9100 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 9180 ; oneway ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 2000 ; off ; ec2-13-229-84-66.ap-southeast-1.compute.amazonaws.com ; 2100 Configure SAP CC To Trust SAP CC Core Server Take the SAP CC Core Servers Public Certificate ( X.509v3 format encoded in DER ), and configure in SAP CC client to trust the SAP CC Core Server. One of the example method is using the keytool command to add this server certificate to client truststore, and use this truststore for your SAP CC agent. Example - Importing Server Certificate Import the server certificate  certificate.x509.pem  to generate  client.truststore file. keytool -importcert -alias sapcc -file certificate.x509.pem -keystore client.truststore -storetype pkcs12 -storepass examplepw In the SAP CC agent, tick Enable Secured Connection checkbox and configure the following fields: Keystore Path: /path/to/client.truststore Keystore Password: examplepw Note! SAP CC agent will only support a Keystore that is in PKCS#12 format.

---

# Document 2152: System Statistics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/312868905
**Categories:** chunks_index.json

MediationZone constantly collects information from the different sub-systems and hosts within the system. Among other things, this information is used for load balancing. Using System Statistics, you can view, export, and import statistical information. Host Statistics The system collects statistics from the different machines hosting a Platform or Execution Context, such as the load of the CPU or the number of context switches. This is called host statistics. MediationZone uses the vmstat command to collect the information. This binary must be installed to have statistics collected and to perform load-balancing work for workflows. The following list holds all values collected from each host. On newer operating systems, some of these may not be available for collection due to changes in the kernel of the operation system. CPU User Time - This value shows how much time was spent in non-kernel specific code. This value is displayed in percentage. 100% means that all processing power is spent. See CPU System Time as well. CPU System Time - This value shows how much time was spent in kernel-specific code, such as scheduling of different processes or network transfer. This value is displayed in percentage. 100% means that all processing power is spent. Context Switches - The number of context switches per second. A context switch occurs when one process hands over information to another process. The more context switches, the less effective and scalable the system will be. Swapped To Disk - The amount of data that was swapped out. A large value indicates that the system does not have enough RAM to manage the memory requirements of the different processes. Swapped In From Disk - The amount of data that was read from swap. Processes Waiting For Run - Shows how many processes that are waiting to run. A high number indicates that the machine is not fast enough to manage the load. Processes Swapped Out - Processes that have persisted in swap due to insufficient available memory, or due to aggressive management of the memory layer. Processes In Sleep - The number of processes that are presently not doing anything. Pico Instance Every minute, the system collects memory information from the different Java processes defining the Platform and the Execution Context. This information shows how much memory is used and how much memory is available for the running process. Used Memory - Shows the amount of memory currently allocated by the running process. As Java is a language using garbage collection, this number may very well get close to the maximum memory limit without being a problem for the running process. However, if the amount of used memory is close to the maximum limit for a long time, the process needs more memory. This value is displayed in bytes. See the -Xmx and -Xms properties defined in the XML file defining the process. Maximum Memory - Shows the amount of memory that the process can use. This value is displayed in bytes. Process CPU Time - Shows the percentage of CPU time that has been used. Open File Descriptors - This is a Unix measurement that enables you to create a statistical diagram over the number of open files during the last minute, hour, or day. Garbage Collection Count - Shows the number of times the garbage collector has run since the last time statistics was collected. Garbage Collection Time - Shows the amount of time the garbage collector has run since the last time statistics was collected. This value is displayed in milliseconds. Thread Count - Shows the number of allocated threads. Workflow Statistics The system collects statistical data that is sampled every 5 seconds as long as a workflow is being executed. This information includes: Throughput - Displays workflow throughput. As long as a workflow is being executed, the system continuously samples the number of processed UDRs, or raw data, per second. Queue Throughput - Displays queue throughput per second for real-time queues. Statistics for real-time queues are only available when routing UDRs, not raw data. Note! To enable its convenient delegation to external systems, or to generate an alarm if the throughput falls too low, throughput is also defined as a MIM value for the workflow. For further information, see Throughput Calculation in Workflow Properties . Simultaneous - Displays the number of simultaneously running workflows. Queue Size - The size of the queue space that is being used at the time of the sample for each individual queue. Viewing the System Statistics To open the System Statistics, go to Manage and then select System Statistics under the Tools and Monitoring section. Upon first entry, you will see the landing page: Open System Statistics Landing Page You will be provided a choice of searching local statistics by clicking the Filter Local Statistics button or importing statistics with the Import External Statistics button. There are three different types of statistics: host, pico instance, and workflow. To display statistics in System Statistics, you have to use the Filter Local Statistics button, see the section below, Searching the System Statistics, or, for Import statistics, see System Statistics | Importing Statistics . Searching the System Statistics To search, click the Filter Local Statistics button to open the Filter dialog. In the Filter dialog, search criteria may be defined in order to single out the Statistics of interest. Open Filter dialog Setting Description Setting Description Period Select this option to display the statistics from the chosen time interval. Select one of the predefined time intervals; Last Hour , Today, Yesterday , This Week , Previous Week , Last 7 Days , This Month , Previous Month , or select the option User Defined and enter the start and end dates and times of your choice in the From and To fields. Note! If several criteria are enabled, an absolute match is displayed. For instance, if Host and Workflow are specified as well as Period , only the time for which there are both workflow measures and host measures is displayed. Scale Specifies the time scale to be used. There are three different time resolutions on which statistics are collected. The default value is Hour. Host Choose from a list of available hosts by selecting items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended host to filter by. Picos Choose from a list of available picos by selecting items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended pico to filter by. Workflows Choose from a list of available workflows by items from the drop-down list or simply click on the Select All button. Users may also type out the name of the intended workflow to filter by. Options in System Statistics Host / Pico / Workflow Views After selecting the hosts, picos and / or workflows, users may click on the Apply button to perform the search. The search cannot be performed when no options are selected from all three types of instances. Note! The default formula that the filter results will be displayed in is Average . Users may select either Minimum or Maximum later after filter result have been displayed. When the new formula is selected, all the values of the charts will be updated accordingly. Another feature within the filter dialog is the ability to select the Picos either by All available picos or by Picos that are running. Open Picos with the [R] label. Picos that are running are denoted with the label [R] before the pico name in the Picos dropdown list. Open System Statistics Simply click on the chart of choice to add to the instance view and deselect to remove it. For each type of statistics you have selected to view, you will see a Statistics type view drop-down displaying the statistical charts selected for the statistics measurement type. To add or remove other measurement types, click the Show / Hide Charts menu button found above each statistics view section of either hosts, pico or workflows statistic. The types of available charts in the drop menu to select from are mentioned above in the System Statistics | Host Statistics section for hosts, System Statistics | Pico Instance section for pico instances, and System Statistics | Workflow Statistics section for workflows. Take for example the show/hide charts list for pico as shown below Open Statistics type charts for pico instances For each statistics type you can see: View Description View Description Host View Each host has its statistics displayed in a separate color. A default measurement type chart is displayed upon first filter application Pico Instance View Each pico instance has its statistics displayed in a separate color. A default measurement type chart is displayed upon first filter application Workflow View Each workflow has its statistics displayed in a separate color, and if you have selected to view queue statistics, each queue has its own color. A default measurement type chart is displayed upon first filter application If you select to view Queue Throughput statistics and you have more than one route with the same name in a workflow, they are displayed separately by default, and are named in the format route name (source agent name), e.g. r_1 (collection1). If you want to group these queues together, e.g. due to legacy settings, select the Ignore Route Owner checkbox, and the source agent is no longer displayed per route. Note! When the Ignore Route Owner checkbox is selected, and there is more than one route with the same name, if the statistics are displayed in hour or day resolution, the average of all routes with the same name is shown. If the statistics are displayed on minute resolution, the statistics for one of the queues with the same name is shown, and there is no rule for which queue is shown. Exporting Statistics Exporting statistics may be useful for several purposes, for example if you want to share the statistical information with someone who does not have access to your system. To export the statistics: Click on the Export button. The Export dialog opens. Open Export Statistics dialog The export dialog resembles the filter dialog. You may choose which hosts / picos / workflows to be part of your export file according your desire time range as per your selection of the time period. After selecting your desired configurations, click on the Export button to save the zip file. It should be in your default download folder as per your browser setting. Hint! The export functionality can also be used to save statistics on a regular basis. Such as every month or every year, to use for comparison with the current statistics. Importing Statistics The Import functionality is used to import statistics that have previously been exported from a MediationZone system. When using the Import functionality you do not have to perform a search in order to display the statistics. Note! An import of statistics does not affect the data in the database, it just displays a snapshot of the statistics at the time it was exported. To import the statistics: In System Statistics, click Import . The Open dialog opens. Browse to the directory where the *.zip file you want to import is located. Select the file and click Open . The statistical information is now displayed in System Statistics. The same search criteria that were set in the Search System Statistics dialog when the statistics were exported is displayed. The date information at the top of the dialog displays the time interval for the imported statistics, and the text "Imported Statistics" also appears in red beside the date information. Disabling Collection of Statistics You can disable the collection of statistics on the Platform by setting the following Platform properties to false in platform.conf : mz.statistics.collect.all - All statistics mz.statistics.collect.pico - Pico statistics mz.statistics.collect.workflow - Workflow Statistics The default value of the properties above is true . When you set the property mz.statistics.collect.all to false , this overrides the settings of mz.statistics.collect.pico and mz.statistics.collect.workflow

---

# Document 2153: DNS Lookup Function - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612352/DNS+Lookup+Function
**Categories:** chunks_index.json

lookUpDNS Sends a lookup query to stated host and returns the result drudr lookUpDNS ( string queryType, string queryString, string host, int port, int timeout, int retries ) Parameter Description Parameter Description queryType The query type to be used, e g NAPTR, AAA, CNAME, etc. For full list, see https://javadoc.io/static/dnsjava/dnsjava/3.3.1/org/xbill/DNS/Type.html . queryString The string to be used in the query. host The host that should be used for the lookup. port The port that should be used for the lookup. timeout The number of milliseconds to wait before timeout. retries The number of retries you want to make before returning an error code and proceeding. Returns A DNSResponseUDR containing the lookup result ( list<DNSRecordUDRs> ), status code ( int ) and error message ( string )

---

# Document 2154: MSMQ Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686257/MSMQ+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the MSMQ Collection agent configuration dialog from a workflow configuration, you can do the following: right-click the agent icon and select Configuration... select the agent icon and click the Edit button Configuration The Agent Configuration dialog contains the following settings: Open MSMQ collection agent configuration dialog The configuration tab contains the settings required to connect to the MSMQ service. Setting Description Setting Description Host The host name or IP address of the MSMQ server. Domain The domain name that the login user belongs to. Username The account used to login to the MSMQ server to send/receive memory. Password The account password used to login to the MSMQ server. Path Specify the name of the queue you want to send/receive message in this field. Example: .PRIVATE$mzqueue

---

# Document 2155: Configuring Searchable Fields in the ECS - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205000024
**Categories:** chunks_index.json

If you want to search for UDRs with specific values in certain fields, you can configure such fields in the ECS Searchable Fields dialog (ECS Inspector  Searchable Fields button). Open Searchable Fields dialog Note! These configurations must be made before UDRs are sent to the ECS by the ECS forwarding agent. To configure searchable fields: In the ECS Searchable Fields dialog, Labels tab, click the Add button at the bottom of the dialog. The Add Label dialog opens. Open Add Label dialog Enter a name in the Label field. Click the Add button to add the label into the Defined Field Labels list. Repeat the previous step for all the labels you want to add, and then click the Close button. Click the Mappings tab to map UDR fields to the different labels. Click the Add button to open the UDR Internal Format Browser. Select the UDR type you want and click OK (to add and close the browser) or Apply (to add more UDR types without having to reopen the browser). The UDR type(s) are added in the UDR Types list. Select a UDR type in the UDR Types list, and double click on the UDR Field row to select a UDR Field to associate to the chosen label. The Select UDR Field dialog opens. Select a UDR Field and click OK. The selected UDR Field is listed in the UDR Field row for the label. Repeat the previous step for all the UDR Types where you want to map UDR Fields. Click the Save button when you are finished. The configuration is saved, and the next time the ECS receives UDRs from an ECS forwarding agent, the configured UDR Fields are added as meta data and can later be used for making searches, see Searching the ECS .

---

# Document 2156: Inter-Workflow Communication, Using Database Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672322
**Categories:** chunks_index.json

Data may propagate between workflows or MediationZone systems by combining a Database forwarding agent with a Database collection agent, where the exchange point is a mutual database table. Open When using the same table, the collection agent must make sure that it does not collect data that the forwarding agent is simultaneously feeding with data from its current batch. Note! Transfer of UDRs between workflows is ideally handled using the Inter Workflow agents. The Database agent approach is useful in case of wanting to change the content of the UDRs. Another use is when wanting to pass on MIM values and merge batches at the same time. In the Inter Workflow agent case, only the MIM values for the first (Header MIMs) and last batch (Trailer and Batch MIMs) are considered. Using the Database agents, MIM values may be mapped into database columns. Pending Transaction Table The MediationZone database, hosts a table where pending transactions are registered. A pending transaction is an ongoing population of a table by a Database forwarding agent. The pending transaction continues from a Begin Batch to an End Batch. The purpose of this table is for Database collection agents to avoid collecting pending data from the table that a Database forwarding agent is currently distributing to. The pending transaction table holds database names and table names. Thus, before a collection session starts, the collector evaluates if there are any pending Transaction IDs registered for the source database and table. If there are, rows matching the Transaction IDs will be excluded. In the following figure, the Database collection agent will exclude all rows with transaction ID 187. Open A Database forwarding agent may be configured to target a stored procedure, instead of a table directly. In such cases the user must specifically select the table that the stored procedure will populate ( SP Target Table ). The reason for that is that the pending transaction table must contain the table name, not the SP name, so that the selected table name in the Database collection agent can be matched. Exchanging Storable Data All UDRs have a special field named Storable . This field contains the complete UDR description and all its data. If UDRs, having many fields or a complex structure to be exchanged, it could be suitable to store the content of the Storable field in the database. In that way the table would only need one column. The database type of that column must be a RAW, LONG RAW or a BLOB. Note! The data capacity of the column types RAW, LONG RAW and BLOB differs. Consult the database documentation. For performance reasons it is advised to use the smallest type possible that fits the UDR content. When configuring the Database forwarding agent, the Storable field from the UDR is be assigned to the table column in a straight forward fashion. However, when collecting that type of data the column assignment must not be made to the Storable field. Instead To UDR is selected in the Value Type field. When the Database collection agent detects a mapping of type To UDR , the selected UDR type is not consulted for what UDR type to create. The information about the UDR type will be found in the data of the column itself. Thus, if the UDR stored in the column is of another type than selected in the Source tab, the type to be distributed by the Database collection agent is the type actually found.

---

# Document 2157: FTP Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685502
**Categories:** chunks_index.json

You open the FTP collection agent configuration dialog from a workflow configuration. To open the FTP collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select FTP from the Collection tab of the Agent Selection dialog. The Filename Sequence and Sort Order tabs are described in Workflow Template . Connection Tab The Connection tab is used to configure the remote server connection. Open The FTP Collection Agent Configuration - Connection tab Setting Description Setting Description Connection Information Host Enter the hostname or IP address of the remote host. If a connection cannot be established to this host, the Additional Hosts specified in the Advanced tab, are tried. Username Enter the username for the remote host account. Password Enter the associated password. Transfer Type Select the data transfer type to be used during file retrieval. Binary - agent uses binary transfer type. Default setting. ASCII - agent uses ASCII transfer type. Collection Retries Enable Select this check box to enable repetitive connection attempts. When this option is selected, the agent will attempt to connect to the host as many times as is stated in the Max Retries field. If the connection fails, a new attempt will be made after the number of seconds entered in the Retry Interval (s) field. Retry Interval(s) Enter the time interval in seconds, between retries. If a connection problem occurs, the actual time interval before the first attempt to reconnect will be the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval(s) field. For the remaining attempts, the actual time interval will be the number of seconds entered. Max Intervals Enter the maximum number of trial intervals. Restart Retries Settings En able Select this check box to enable the agent to send a RESTART command if the connection is lost during a file transfer. The RESTART command contains the necessary interrupt information and will continue from that spot. Before selecting this option, ensure that the target FTP server supports the RESTART command. When this option is selected, the agent will attempt to re-establish the connection, and resume the file transfer from the point in the file stated in the RESTART command, as many times as is entered in the Max Restarts field. When a connection has been re-established, a RESTART command will be sent after the number of seconds entered in the Retry Restarts Interval(s) field. Note! The RESTART Retries settings will not work if you have selected to decompress the files in the Source tab! Note! RESTART is not always supported for transfer type ASCII. For further information about the RESTART command, see http://www.w3.org/Protocols/rfc959/ . Retry Restarts Interval(s) Enter the time interval, in seconds, you want to wait before initiating a restart. This time interval will be applied for all restart retries. If a connection issue occurs, the actual time interval before the first attempt to send a RESTART command will be the time set in the Timeout field in the Advanced tab plus the time set in the Retry Interval(s) field. For the remaining attempts, the actual time interval will be the specified number of seconds. Max Restarts Enter the maximum number of restarts per file you want to allow. If more than one attempt to send the RESTART command has been made, the number of used retries will be reset as soon as a file transfer is completed successfully. Source Tab The Source tab contains configuration options related to the remote host, source directories, and source files. The following text describes the configuration options available when no custom strategy has been chosen. Open The FTP collection agent configuration - Source tab Setting Description Setting Description Collection Strategy If there is more than one collection strategy available, a Collection Strategy drop-down menu list will be made visible. For further information about collection strategies, see Appendix 4 - Collection Strategies . File Information Directory Enter the absolute path to the source directory where the source files reside. If the FTP server is of UNIX type, the path name must be specified as relative to the home directory of the account. Include Subfolders Select this check box if you have subfolders in the collection source directory. Note! Subfolders that are in the form of a link are not supported. If you select Enable Sort Order in the Sort Order tab, the sort order selected will also apply to subfolders. Filename Enter the name of the source files on the remote host. Regular expressions according to Java syntax apply. For further information, see http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html . Example To match all file names beginning with TTFILE , type: TTFILE.* Note! When collecting files from VAX file systems, the names of the source files must include both the path and filename when entering the regular expression. Compression Select the compression type. This option determines if the agent will decompress the files before passing them to the workflow. No Compression - the agent will not decompress the files. Gzip - the agent will decompress the files using Gzip. Before Collection Move to Temporary Directory If this option is enabled, the source files will be moved to a subdirectory called DR_TMP_DIR in the source directory, before collection. This option supports safe collection when source files repeatedly use the same name. Append Suffix to Filename Enter the suffix that you want to be added to the file name prior to collection. Important! Before you execute your workflow, make sure that none of the file names in the collection directory include this suffix. Inactive Source Warning (h) If enabled, when the configured number of hours have passed without any file being available for collection, a warning message (event) will appear in the System Log and Event Area: The source has been idle for more than <n> hours, the last inserted file is <file>. After Collection Move to If enabled, the source files will be moved from the source directory (or from the directory DR_TMP_DIR if using Move to Temporary Directory ) to the directory specified in the Destination field, after collection. Note! The Directory has to be located in the same file system as the collected files. The absolute path names must be defined. If a file with the same filename, but with different content, already exists in the target directory, the workflow will abort. If a file with the same file name, and the same content, already exists in the target directory, this file will be overwritten and the workflow continue running. Rename Options Rename If this option is enabled, the source files will be renamed after the collection, and remain (or moved back from the directory DR_TMP_DIR if using Move to Temporary Directory ) in the source directory. Note! When the File System Type for VAX/VMS is selected, there are special considerations. If a file is renamed after collection on a VAX/VMS system, the filename might become too long. In that case, the following rules apply: A VAX/VMS filename consists of <file name>.<extension>;<version>, where the maximum number of characters for each part is: <file name>: 39 characters <extension>: 39 characters <version>: 5 characters If the new filename turns out to be longer than 39 characters, the agent will move part of the filename to the extension part. If the total sum of the filename and extension part exceeds 78 characters, the last characters are truncated from the extension. An example: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_ CHARACTERS.DAT;5 will be converted to: A_VERY_LONG_FILENAME_WITH_MORE_THAN_39_. CHARACTERSDAT;5 Note! Creating a new file on the FTP server with the same file name as the original file, but with other content, will cause the workflow to abort. Creating a new file with the same file name AND the same content as the original file will overwrite the file. Remove If enabled, the source files will be removed from the directory (or from the directory DR_TMP_DIR , if using the Move to Temporary Directory option, after collection. Ignore If enabled, the source files will remain in the source directory after the collection. This field is not available if the Move to Temporary Directory option is enabled. Destination Enter the full pathname to the directory on the remote host into which the source files will be moved after the collection. This field is only available if Move to is enabled. Prefix and Suffix Prefix and/or suffix that will be appended to the beginning and the end of the name of the source files, respectively, after the collection. These fields are only available if Move to or Rename is enabled. Warning! If Rename is enabled, the source files will be renamed in the current (source or DR_TMP_DIR ) directory. Be sure not to assign a Prefix or Suffix , giving files new names still matching the Filename regular expression. That will cause the files to be collected over and over again. Search and Replace Select either the Move or Rename option. Search : Enter the part of the filename that you want to replace. Replace: Enter the replacement text. Search and Replace operate on your entries in a way that is similar to the Unix sed utility. The identified filenames are modified and forwarded to the target agent in the workflow. This functionality enables you to perform advanced filename modifications, as well: Use regular expression in the Search entry to specify the part of the filename that you want to extract. Note! A regular expression that fails to match the original file name will abort the workflow. Enter Replace with characters and meta characters that define the pattern and content of the replacement text Search and Replace Examples To rename the file file1.new to file1.old , use: Search: .new Replace: .old To rename the file JAN2011_file to file_DONE , use: Search: ([A-Z]*[0-9]*)_([a-z]*) Replace: $2_DONE Note that the search value divides the file name into two parts by using parentheses. The replace value applies to the second part by using the place holder $2 . Keep (days) Enter the number of days to keep moved or renamed source files on the remote host after collection. In order to delete the source files, the workflow has to be executed (scheduled or manually) again, after the configured number of days. Note! A date tag is added to the filename, determining when the file may be removed. This field is only available if the Move to or Rename option is enabled. UDR Type Section Route FileReferenceUDR Select this check box if you want to forward the data to an SQL Loader agent. For more information, see SQL Loader Agent . Advanced Tab The Advanced tab contains advanced FTP service configuration options. For example, if the FTP server does not return the file listed in a well-defined format, you can use Disable File Detail Parsing . For further information, see the available options. Open The FTP collection agent configuration - Advanced tab Setting Description Setting Description Command Port Enter the port number of the remote FTP server. Timeout (s) Enter The maximum time, in seconds, to wait for a server response. A value of 0 will result in an indefinite wait. Passive Mode (PASV) This option must be enabled if FTP passive mode is used for the data connection. In passive mode, the channel for data transfer between the client and server is initiated by the client instead of the server. This is used when firewalls block standard FTP connections. Disable File Detail Parsing Disables parsing of file detail information received from the FTP server. This enhances the compatibility with unusual FTP servers but disables some functionality. If file detail parsing is disabled, file modification timestamps will not be available to the collector. The collector does not have the ability to distinguish between directories and simple files, sub directories in the input directory must for that reason not match the filename's regular expression. The agent assumes that a file named DR_TMP_DIR is a directory because a directory named DR_TMP_DIR is used when Move to Temporary Directory under the Source tab is activated. Therefore, it is not allowed to name a regular file in the collection directory DR_TMP_DIR. Note! When collecting files from a VAX file system, this option has to be enabled. Additional Hosts Here you can enter additional host names or IP addresses that can access the source directory for file collection. These hosts are tried, in sequence from top to bottom. Use the Add , Edit , Remove , Up , and Down buttons to configure the order of the hosts in the list.

---

# Document 2158: Conditional Trace Templates - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611035
**Categories:** chunks_index.json

To be able to perform traces on workflows in Desktop, there must be at least one Conditional Trace template defined. Note! You can use regular expressions in the Conditional Trace templates, but be careful of overusing " .*" . The more specific the filter is, the better the result you will get in terms of minimized amount of output and performance impact. You can create templates for any real-time workflows, giving you results for all agents in the workflow, and you can also use the trace() APL function in Analysis or Aggregation agents to get results for specific functions in the APL code. This section describes: Creating Conditional Trace Templates Tracing Functions Using APL in a Workflow

---

# Document 2159: Starting Clusters and Creating Topics - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611497
**Categories:** chunks_index.json

Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to the Spark cluster. Before you can create the topics you must start Zookeeper and Kafka. Prerequisites: Prepare scripts according to Preparing and Creating Scripts for KPI Management Starting Clusters To start a cluster follow the steps: Start Zookeeper and Kafka To start Zookeeper, run the following: bin/zookeeper-server-start.sh config/zookeeper.properties To start Kafka, run: bin/kafka-server-start.sh config/server.properties Create Kafka topics and partitions using the scripts included in the Kafka installation. The names of the topics must correspond to the Spark application configuration. In order for the Spark KPI Application to work, the required number of partitions for each topic must be equal to the setting of the property spark.default.parallelism in the Spark application configuration. Use a replication factor that is greater than one (1) to make sure that data is replicated between Kafka brokers. This decreases the risk of losing data in case of issues with the brokers. This is how to create topics, assuming the current working directory is the Kafka software folder: $ ./bin/kafka-topics.sh --create --topic <input topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <output topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <alarm topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> Example - Creating Kafka Topics ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 Example - Creating Kafka topics, overriding retention settings ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 --config retention.ms=86400000 Run the following command to start Spark: $ start_master_workers.sh ... To submit the app to the Spark cluster . Submit the app: $ submit.sh kpiapp ... You can now confirm the status of the Spark cluster. Open a browser and go to http://<master host>:8080 . Open Spark UI

---

# Document 2160: Agent Failure Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638885/Agent+Failure+Event
**Categories:** chunks_index.json

An agent failure event message is not a failure literally speaking, but is reported when an agent acts upon a predefined Error Case. For instance, when duplicates are found by the Duplicate UDR Detection agent. Not all agents can issue these sort of events. For further information, see the relevant agent user's guide. The following fields are included: agentErrorMessage - Error message issued by the agent. agentName - The name of the agent. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents - Workflow: <Workflow name>, Agent: <Agent name>, Message: <errorMsg> eventName origin receiveTimeStamp severity timeStamp Fields inherited from the Workflow event The following fields are inherited from the Workflow event, and described in more detail in Workflow Event : workflowKey workflowName workflowGroupName

---

# Document 2161: Netflow Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608411/Netflow+Configuration
**Categories:** chunks_index.json

You open the Netflow collection agent configuration dialog from a workflow configuration. To open the Netflow collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type, select Realtime . Click Add agent and select Netflow from the Collection tab of the Agent Selection dialog. Connection Tab Open NetFlow agent configuration dialog - Connection tab Field Description Field Description Received Settings Port The port number where the NetFlow agent will listen for packets from the routers. Note! Since the routers will be configured to communicate with a specific host on this port, it is important that the workflow containing the NetFlow agent is configured to execute on that specific host and not on a random host. Two NetFlow agents may not be configured to listen on the same port on the same host. IP Address Enter the IP address of the target host. Only from Predefined Hosts If enabled, the agent will only accept packets from hosts specified in the Interface Mapping tab. Data from other hosts will be discarded. If disabled, all arriving data will be accepted. This may be suitable if a combination of routers is used. When a majority of the routers only send from one interface (IP-address) each, and some is set according to the Interface Mapping tab. Hence, when disabling this option, the one-interface-routers do not have to be added to the interface mapping list. Warn on Sequence Gap Determines if a warning will be raised in the System Log when the sequence number gap between two sequential PDUs from the same router is equal to or larger than specified in the Minimum Gap . Minimum Gap The minimum sequence number gap between two flow records that will cause warning in the System Log. Receive Buffer Size Enter the buffer size for messages to read in this field. The value should not exceed the value set for the kernel parameter net.core.rem_max . Example on how to set the kernel parameters $ sudo sysctl -w net.core.rmem_max=18388600 $ sudo sysctl -w net.core.netdev_max_backlog=100000 UDR Type Tab Open NetFlow agent configuration dialog - UDR Type tab Field Description Field Description Add Select this button to open the Add UDR Internal Format Browser. Here you can add the desired UDR types. Remove Removes the selected entry. Interface Mapping Tab Open NetFlow agent configuration dialog Interface Mapping tab Maps several interface IP addresses to one main IP address. Each router using more than one interface IP address when sending data to the agent must be registered here. One of the IP addresses, supported by the router, must be registered as Main IP Address . The others are configured in the IP Address list. If a packet arrives from an IP address configured in the IP Address list, it will be mapped to the corresponding Main IP Address. This way it will appear as if all packets originating from the same IP address. Field Description Field Description Main IP Address Each router that supports multiple interfaces must add one address to this list. When an existing row is selected, the content in the IP Address table will reflect the slave IP addresses for the selected Main IP Addresses. IP Addresses Additional IP addresses are mapped to their corresponding main IP address by the agent. You can use the buttons below the two main sections to Add, Edit, or Remove the available entries.

---

# Document 2162: HTTP Batch Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685849/HTTP+Batch+Agent
**Categories:** chunks_index.json

This section describes the HTTP batch agent. The HTTP batch collection agent collects from either a single URL or, in Index Based Collection mode, it collects all linked-to URLs found in an HTML-formatted document (Anchor HREF attributes). The agent will download the files from the web server as a byte stream and route the content of the file into the workflow in parts of up to 32768 bytes. The system supports both HTTP and HTTPS. The filename of the file to be collected must be known before collection. Prerequisites The reader of this information should be familiar with: HTTP/HTTPS protocol UDR structure and contents The section contains the following subsections: HTTP Batch Agent Input/Output Data and MIM HTTP Batch Appendix - Database Requirements for Duplicate Check HTTP Batch Agent Configuration HTTP Batch Events HTTP Batch Agent Transaction Behavior

---

# Document 2163: RESTful Interface for Reference Data Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677668/RESTful+Interface+for+Reference+Data+Management
**Categories:** chunks_index.json

Database operations are supported via a RESTful HTTP Interface on the Platform. Note! Temporary files are created on the Platform host when the user executes queries via Reference Data Management. These temporary files, which are stored in $MZ_HOME/tmp/rowset , are removed automatically when the user session expires. User sessions expire automatically after six (6) hours. Restful Operations The tables below describe the various operations that are available for Reference Data Management. The service endpoint URI is http://<Platform host>:<port>/api/v1. If encryption is enabled, the URI is https://<Platform host>:<port>/api/v1 . For further information about enabling encryption, see Network Security in the System Administrator's Guide. Basic authentication is used and you must pass user credentials for each RESTful call. Create and Get Session This operation returns a session id which should be appended to all subsequent API calls where this parameter is applicable. Resource path /session HTTP method GET Example $ curl -u  user:passw http://localhost:9000/api/v1/session Close Session This operation closes a session. This session id is no longer valid for any subsequent API calls. Resource path /session/close?sessionid=<sessionid> HTTP method GET Example - Closing a session $ curl -u user:passw  http://localhost:9000/api/v1/session/close?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Reference Data Profiles This operation retrieves a list of all Reference Data profiles that are available to the user. Resource path /refdatas HTTP method GET Example - Retrieving a list $ curl -u  user:passw http://localhost:9000/api/v1/refdatas Get Metadata This operation retrieves table metadata that can be used for viewing or to derive parameters for other REST APIs. If the Last Update feature is enabled, the values stored in the most recent Last Update user and timestamp column are retrieved as well. The result is returned synchronously. Resource path /refdatas/<Reference Data profile>/table/<table name>?sessionid=<session id> HTTP method GET Example - Get metadata $ curl -u user:passw  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Query This operation executes database queries on a specific Reference Data profile and database table. The query is performed asynchronously and control is returned immediately. You can retrieve the result of the query by using /rowset/<rowset number>?sessionid=<session id> . This operation requires Input parameters that are passed in a JSON format as part of the HTTP message body. Note! Any ongoing query process running on the same session will be aborted and a new process for the latest query will be executed. This operation will clear any uncommitted changes saved in the same session. Resource path /refdatas/<Reference Data profile>/table/<table name>/rowset?sessionid=<session id> HTTP method PUT Body This is where the executeQuery is included. The executeQuery JSON payload includes these options: rowsPerPage - the max number of rows (Data Set size) that are allowed in a rowset of the retrieved result. selectedColumns - allows for specific columns to be selected. filterExpression - allows optional definition of query expressions. The query will match all of the specified query expressions. Example - Get query without query expression in JSON payload body $ curl -X PUT -u user:passw -H 'content-type: application/json'  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t  -d '{"executeQuery": {"rowsPerPage":500,"selectedColumns":["ID","FIRSTNAME","LASTNAME"]}}' Example - Get query with query expression in JSON payload body $ curl -X PUT -u user:passw -H 'content-type: application/json'  http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t  -d '{"executeQuery": {"rowsPerPage":500,"selectedColumns":["ID","FIRSTNAME","LASTNAME"],"filterExpression":[{"col":"ID","expr":9,"op":">="},{"col":"LAST_NAME","expr":"Smith","op":"not like"}]}}' Example - Get query using a JSON payload file $ curl -X PUT -T="Example.json" -u user:passw -H 'content-type: application/json'  'http://localhost:9000/api/v1/refdatas/Default.refprofile_star/table/APP.STAR/rowset?sessionid=vusncl88sjghv7h8nkb0ohja6t Example.json file { "executeQuery": { "rowsPerPage": 500, "selectedColumns": ["ID", "FIRSTNAME", "LASTNAME"] } } Get Data Sets Data sets can be retrieved once downloaded to the file system of the Platform. This operation returns a data set for the given rowset (sequence number). The total number of available data sets can be queried with the Get Status operation. Resource path /rowset/<rowset number>?sessionid=<session id> HTTP method GET Example - Get data sets $ curl -u user:passw  http://localhost:9000/api/v1/rowset/0?sessionid=vusncl88sjghv7h8nkb0ohja6t Get Status This operation returns a status message. It can be used to retrieve active processes and to query the number of available rows, data sets, and the status of imports and exports. If the Last Update feature is enabled, the values stored in the most recent Last Update user and timestamp column are retrieved as well. Resource path /status?sessionid=<session id> HTTP method GET Example - Get status $ curl -u user:passw  http://localhost:9000/api/v1/status?sessionid=vusncl88sjghv7h8nkb0ohja6t Abort Process This operation requests the active process to abort. Note! To prevent a user from initiating another operation before the first operation initiated is complete, Abort Process can be used before an operation is complete. Resource path /status/abort?sessionid=<session id> HTTP method GET Example - Get status $ curl -u user:passw  http://localhost:9000/api/v1/status/abort?sessionid=vusncl88sjghv7h8nkb0ohja6t Table Export This operation performs a database table export. Input parameters are passed in a JSON format as part of the HTTP message body. Resource path /refdatas/<Reference Data profile>/table/<table name>/download?sessionid=<sessionid> HTTP method POST Body This is where the exportParams are included. The exportParams JSON payload includes these options: opts textQualifier - designated as double quotes by default. separator - designated as a comma by default. extent - designated as all by default. all - export all rows in the table. selected - export rows from the result of Get Query . selectedColumns - allows for specific columns to be selected. Note! For the extend option, selected value is only applicable when Get Query is applied prior Table Export . Example - Table export without options $ curl -X POST  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/download?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -u "user:passw"  -d 'exportParams={}'  > Export.csv Example - Table export with options $ curl -X POST  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/download?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -u "user:passw"  -d 'exportParams={"opts":{"textQualifier":"'''","separator":";","extent":"all"},"selectedColumns":["CITY","CUSTOMER_NAME"]}' > Export.csv Table Import This operation performs a database table import. Input parameters are passed in a JSON format as part of the form-data in the HTTP message body. Resource path /refdatas/<Reference Data profile>/table/<table name>/upload?sessionid=<sessionid> HTTP method POST Body This is where the file and the input parameters are included. The exportParams format includes these options: textQualifier - designated as double quotes by default. separator - designated as a comma by default. opts - designated as append by default. append - imported rows are appended to the table. truncate - the existing data in the table are truncated before the import is executed. force - designated as false by default. Example - Table import without options $ curl -i -u "user:passw"  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/upload?sessionid=rssrh20dcd8lc1j505b3bqnstc'  -F file=@/path/to/import_test.csv Example - Table import with options $ curl -i -u "user:passw"  'http://localhost:9000/api/v1/refdatas/ref_data_mgmt_oracle.ref_data_mgmt_pf/table/TABLE.CUSTOMERS/upload?sessionid=p3hce86dkb4rmls9peh4e8rps9'  -F file=@/path/to/import_test.csv  -F 'opts={"textQualifier":"'","separator":",","op":"truncate","force":false}' Save Changes This operation saves data modification (insert/update/delete). Changes are saved within the client session. Input parameters are passed in a JSON format as part of the HTTP message body. Note! tableName and refProfile are mandatory in order to save. A single table can be modified in a single session only. Note! The save operation is supported either on Oracle (based on the ROWID pseudo column) or non-Oracle type tables containing a Primary Key constraint. Non-Oracle tables without a Primary Key are not supported for data modifications. Resource path /save?sessionid=<session id> HTTP method PUT Body This is where the dataEdits are included. The dataEdit JSON payload includes these options: refProfile - Reference Data Management Profile. tableName - Database table name. updates - modification parameters. action - to specify the type of modification. insert - insert a new row to the table. update - edit an existing row in the table. delete - delete an existing row in the table. ids - to specify column value pairs of primary key(s). column - private key column name. value - private key value for the respective column. values - to specify column value pairs to be inserted/updated. column - column name to be inserted/updated. value - insert/update value for the respective column. Note! When inserting a row, specifying a pseudo ids column value pair allows Cancel Changes to be applied to that specific insert row modification. Example - Save changes with a JSON payload body $ curl -X PUT -u "user:passw" -H 'content-type: application/json  -d '{"dataEdit":{"refProfile":"Default.refTest","tableName":"MZADMIN.REFRENCE_DATA","updates":[{"action":"delete","ids":[{"column":"ID","value":8}]}]}}'  http://localhost:9000/api/v1/save?sessionid=vusncl88sjghv7h8nkb0ohja6t Example - Save changes using a JSON payload file $ curl -X PUT -T="Example.json" -u "user:passw" -H 'content-type: application/json  http://localhost:9000/api/v1/save?sessionid=vusncl88sjghv7h8nkb0ohja6t Example.json { "dataEdit" : { "refProfile" : "Default.refTest", "tableName" : "MZADMIN.REFRENCE_DATA", "updates" : [ { "action" : "insert", "ids" : [ { "column" : "ROWID", "value" : "ins0" } ], "values" : [ { "column" : "ID", "value" : 645 }, { "column" : "FIRST_NAME", "value" : "Roberts" }, { "column" : "LAST_NAME", "value" : "Polis" } ] }, { "action" : "update", "ids" : [ { "column" : "ID", "value" : "6" } ], "values" : [ { "column" : "LAST_NAME", "value" : "Wick" }, { "column" : "FIRST_NAME", "value" : "John" } ] }, { "action" : "delete", "ids" : [ { "column" : "id", "value" : "8" } ] } ] } } Commit Changes This operation applies saved edits in the database and commits the work in case of success. You can use force commit in case of errors. If the Last Update feature is enabled, the user name and modification timestamp values for insert/update modifications are stored in the Last Update columns specified in the Reference Data Management Profile. The Last Update information is used by the Get Status operation to retrieve the most recent Last Update user and timestamp. Resource path /save/commit?force=<true|false>&sessionid=<session id> HTTP method GET Example - Commit changes $ curl -u user:passw  http://localhost:9000/api/v1/commit?force=false&sessionid=vusncl88sjghv7h8nkb0ohja6t List Changes This operation returns a list of the modifications saved. Resource path /save/list?sessionid=<session id> HTTP method GET Example - List changes $ curl -u user:passw  http://localhost:9000/api/v1/save/list?sessionid=vusncl88sjghv7h8nkb0ohja6t Cancel Changes This operation cancels the changes made from being saved. Input parameters are passed in a JSON format as part of the HTTP message body. Note! Pseudo Primary Keys for inserted rows can be included in the Save Changes operation to allow the cancel function for a specific insert row modification. Note! SaveSize in the Cancel Changes response indicates the number of changes remaining. Resource path /save/cancel?sessionid=<session id> HTTP method PUT Body This is where the dataEdits are included. The dataEdit JSON payload includes these options: scope - specify the scope of changes to be canceled. single - allow only specific modifications identified by ids to be cancelled. all - allow all the modifications to be canceled. ids - to specify column value pairs of primary key(s). This is only required when the scope is single . column - private key column name. value - private key value for the respective column. Example - Cancel changes $ curl -u user:passw -H 'content-type: application/json'  -d '{"dataEdit":{"scope":"single","ids": [{"id":[{"column":"ORDER_NUM","value":10398005}]}]  http://localhost:9000/api/v1/save/cancel?sessionid=vusncl88sjghv7h8nkb0ohja6t Show Demo Query This operation shows an example JSON payload format that applies for a Get Query operation. Resource path /demo/queryRequestParameters HTTP method GET Show Demo Changes This operation shows an example JSON payload format that applies for a Save Changes operation. Resource path /demo/dataEditRequestParameters HTTP method GET

---

# Document 2164: Database Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205032206/Database+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Ready with table: tablename Reported, along with the name of the working table, when all rows are collected from it. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Start collecting Indicates that possible cleanup procedures are finalized and that the actual collection begins. No rows selected from table tablename Reported, along with the name of the working table, if no rows have been selected for collection. Avoids reading the following pending txn ids: list of ids Reported, along with a list of Transaction IDs, if the constructed SQL select statement finds any pending Transaction IDs in the pending transaction table. Rows marked with these transaction IDs will be excluded by the query. Marking collected data as cancelled Reported when a Cancel Batch is received. Deleting collected data Reported when collected rows are removed after collection, if Remove is selected in After Collection . Has deleted n rows in tablename Subevent to the Deleting collected data event, stating the number of rows removed by each SQL commit command. The maximum number depends on the Commit Window Size . Marking collected data Reported when collected rows are marked, if Mark as Collected is selected in After Collection . Has updated n rows in tablename Sub event to the Marking collected data event, stating the number of rows marked as collected by each SQL commit command. Taking care of collected data via SPname Reported, along with the name of the stored procedure, when it is called after collection, if Run SP is selected in After Collection. Try to force running Stored Procedure to Stop Reported when stopping a workflow that has a Stored Procedure running.

---

# Document 2165: Radius Client Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740215/Radius+Client+Agent+Configuration
**Categories:** chunks_index.json



---
**End of Part 91** - Continue to next part for more content.
