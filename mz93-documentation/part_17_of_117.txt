# RATANON/MZ93-DOCUMENTATION - Part 17/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 17 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.3 KB
---

A Couchbase profile is used to read and write bucket data in a Couchbase database and can be accessed by workflows using Aggregation, Distributed Storage, or PCC. The profile should not be used to operate on data that has been inserted or updated by external third-party software. As a client of Couchbase, the profile operates in synchronous mode. When sending a request to Couchbase, the profile expects a server response, indicating success or failure, before proceeding to send the next one in queue. The Couchbase profile is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. Note! Created or updated Couchbase profiles that are used for PCC do not become effective until you restart the ECs. Configuration To create a new Couchbase profile, click the New Configuration button from the Configuration dialog available from Build View , and then select Couchbase Profile from the menu. The contents of the menus in the menu bar may change depending on which configuration type has been opened in the currently displayed tab. The Couchbase profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit menu is specific for Couchbase profile configurations. Setting Desciption Setting Desciption External References Select this menu item to Enable External References in an agent profile field. This can be used to configure the use of the following fields, and the respective external reference keys available: Bucket Name Bucket Password Bucket User Bucket User Password Operation Timeout Max Retries Retry Interval Host For further information, see Using External Reference in Agent Profile Fields and External Reference Profile . In a Couchbase profile, there are three tabs: Connectivity , Management , and Advanced . Connectivity Tab The Connectivity tab is displayed by default. Couchbase profile - Connectivity tab The following settings are available in the Connectivity tab: Setting Description Setting Description Bucket Configuration Bucket Name Enter the bucket that you want to access in Couchbase in this field. Bucket User Enter the user who has access rights to the bucket. User Password Enter the password for the user who has access to the bucket. Parameterized When enabled, here you may enter parameterized bucket configuration. Cluster Connectivity Operation Timeout (ms) Enter the number of milliseconds after which you want Couchbase "CRUD" operations, that is, create, read, update, and delete, to timeout. Setting a lower value than the default 1000 ms may have a positive impact on throughput performance. However, if the value is set too low, indicated by a large number of operation timeouts errors in the EC logs, a lower throughput can be expected. As mentioned, this value is used for timing out CRUD operations in Couchbase. This means that the system sends a request towards Couchbase and if an answer is not returned in time, the operation fails, and the system is no longer waiting for an answer. Retry Interval Time (ms) Enter the time interval, in milliseconds, that you want to wait before trying to read the cluster configuration again after a failed attempt. In other words, in case a request sent to Couchbase returns an unsuccessful answer, the number specified here is the time that the system waits before retrying the same request. Max Number Of Retries Enter the maximum number of retries. This is the number of retries the system does before treating the Couchbase operation as failed Enable Security Select this checkbox to connect to Couchbase cluster with SSL. If you want to set a parameter, select the Parameterized checkbox and enter the parameter name using ${} syntax, see Profiles for more information on how parameterization works (in this mode the regular Enable Security checkbox is disabled). The parameter needs to be set as true or false . Parameterized When enabled, you may enter a parameterized cluster configuration. Security Profile Cluster Nodes Cluster Nodes In this section, add IP addresses/hostnames of at least one of the nodes in the cluster. This address information is used by the Couchbase profile to connect to the cluster at workflow start, and to retrieve the IP addresses of the other nodes in the cluster. If the first node in the list cannot be accessed, the Couchbase profile will attempt to connect to the next one in order. This is repeated until a successful connection can be established. Hence it is not necessary to add all the nodes, but it is good practice to do so for a small cluster. For example, if there are just three nodes, you should add all of them. Hint! The Operation Timeout , Retry Interval Time , and Max Number Of Retries settings together with the Advanced tab setting mz.cb.lock.timeout.in .secs , work jointly. To understand how, see the following explanation. If you for example use the lookup function with transaction, a LOCK ERROR means that the lookup failed because there was already a transaction lock held on the object that the lookup was trying to get from Couchbase. That is, the lookup was not able to get a lock on the object. What is most important here is how the communication between MediationZone and Couchbase works. In case you use the lookup function it works like this: The lookup function with transaction ID tries to get an object from Couchbase with a lock. MediationZone sends a request to Couchbase to get an object and lock it. If the object exists and is not locked, Couchbase creates a lock and sends the answer back with a successful result code and the data. Alternatively, if the object is locked, Couchbase immediately sends an answer with the result LOCK_ERROR. Couchbase does not wait to answer until the lock is released. This is why the Operation Timeout parameter does not play a crucial role here. The Operation Timeout parameter is only used when Couchbase is stalling and does not send an answer for a long time. Assuming that the object is locked and the system received answer with LOCK_ERROR, it will wait for the amont of time specified in Retry Interval Time and then retry the request towards Couchbase. This process is repeated until Max Number Of Retries is exhausted or the answer contains a successful result code and the object is retrieved from Couchbase. So, if you for example set Retry Interval Time to 100 ms and Max Number of Retries to 10, you get 100 ms * 10 retries = 1000 ms. However, from a MZ perspective, the timeout of 1000 ms is just for a single lookup attempt and not for the whole lookup operation. To clarify, if you get a failed lookupMany with LOCK_ERROR in the system log, it means that the lookup failed because it tried to get an object with a lock but failed after Max Number Of Retries. That is, some other process (or thread) was holding a lock on that particular object for longer than 1000 ms. This is possible if the Advanced tab setting mz.cb.lock.timeout.in .secs is larger than 1000 ms. In other words, you need to decide what is more important - that the operation succeeds or that the operation is fast. Management Tab The Management tab contains Cluster Management and Monitoring settings. Open Couchbase profile - Management tab The following settings are available in the Management tab: Setting Description Setting Description Admin User Name If you want to create a new bucket that does not exist in your Couchbase cluster, enter the user name that you stated when installing Couchbase in this field. Admin Password If you want to create a new bucket that does not exist in your Couchbase cluster, enter the password that you stated when installing Couchbase in this field. Parameterized When enabled, here you may enter parameterized cluster management settings. Bucket Size (MB) Enter the size of the bucket you want to create, in MB. Number of Replicas Enter the number of replicas you want to have. If the bucket you want to access in Couchbase does not exist, the Couchbase profile can be used for creating the bucket in runtime for you, provided that the Admin User Name and Admin Password for your Couchbase cluster have been entered in the Management tab. If the bucket you want to access already exists in your cluster, these two fields do not have to be filled in. Advanced Tab In the Advanced tab, you can configure additional properties. These can typically be left unchanged in the standard Couchbase configuration. It is recommended that you change these properties when using the Couchbase profile in Aggregation. For more information about using the Couchbase profile in Aggregation, see Performance Tuning with Couchbase Storage in Aggregation Agent Configuration - Real-Time . Open Couchbase profile - Advanced tab Important! Client Management Timeout If there is a need to control a timeout of any synchronous operation, a system property mz.cb.management.timeout can be used. It should be set for every execution context used to execute a workflow that may require longer timeout. See the text in the Properties field for further information about the properties that you can set. Logging Couchbase Statistics You can use the property mz.cb.statistics.class if you want to log Couchbase statistics to file. The default value is com.digitalroute.mimexposer.statistics.NoDbStatistics . To enable this property, set the value to com.digitalroute.mimexposer.statistics.MemDbStatistics or com.digitalroute.mimexposer.statistics.FileDbStatistics . If you set the property value to com.digitalroute.mimexposer.statistics.MemDbStatistics , you can use the MIM agent to send statistics to System Insight. To do this, you must configure a workflow with a MIM agent. For further information, see 9.78.6 MIM Agent . If you set the property value to com.digitalroute.mimexposer.statistics.FileDbStatistics , you can use the MIM agent to send statistics to System Insight or you can log to file. If you want to log to file only, you do not require the MIM agent. The log file is stored in $MZ_HOME/log per bucket, for example <bucket name>_couchbase_stats.log, and the file is flushed every 60 seconds by default. If you want to modify how often the file is flushed, set the property mz.cb.statistics.flush.period.in .seconds with the required value in the EC. The Couchbase statistics that you can log include the following: Maximum answer time Average answer time Number of failed CRUDS Maximum lock time Average lock time Number of failed locks Number of timed out locks Support for Scope and Collection Refer to the following properties in the Advanced tab to configure Scope and Collection. Couchbase scope name: The default value is "_default". mz.cb.bucket.scopeName _default Couchbase collection name: The default value is "_default". mz.cb.bucket.collectionName _default Note! When naming scopes and collections, ensure to follow the standards mentioned in Naming for Scopes and Collections . When using scopes and collections, ensure that proper resources are assigned to the scope/collection within the Couchbase bucket. For example, if there are 5 buckets each with its own resource, and if it is decided to use only 1 bucket using scope and collection and removing the other 4. In this case, resources should be appropriately assigned to scopes/collections based on the previous usage of individual buckets.

---

# Document 337: wfdisable - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612629/wfdisable
**Categories:** chunks_index.json

usage: wfdisable <pattern match expression for workflow names> ... This command disables one or more workflows. With this command you compare a single pattern matching expression, or several, with the full workflow name, <folder>.<workflowconfigurationname>.<workflowname>, of all the workflows. The command accepts wild cards, such as '*' and '?'. For further information see Textual Pattern Matches . Return Codes Listed below are the different return codes for the wfdisable command: Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count is incorrect. 2 Will be returned if the user is not found or not logged in. 3 Will be returned if no matching workflow was found.

---

# Document 338: SQL Statements - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034720
**Categories:** chunks_index.json

The format of the SQL statements differs depending on which database type you are using. For MySQL, Netezza, PostgreSQL, SybaseIQ and Vertica, the path to the file (stated as <path to file> in the code blocks below) can be entered in three different ways: Entering the path explicitly, e g 'Users/mine/mypath' Stating the UDR field containing the path by entering $(UDR.fullpath) Stating the MIM value containing the path, e g $(Source_Pathname) for the Disk agent MySQL For remote loading (the file resides in a local directory) LOAD DATA LOCAL INFILE <path to file> INTO TABLE TABLENAME FIELDS TERMINATED BY ',' LINES TERMINATED BY 'n'; For server-side loading (file resides in the server file system of the database) LOAD DATA INFILE <path to file> INTO TABLE TABLENAME FIELDS TERMINATED BY ',' LINES TERMINATED BY 'n'; Netezza For remote loading (the file resides in a local directory) INSERT INTO TABLENAME SELECT * FROM EXTERNAL <path to file> USING (delim ',' REMOTESOURCE 'JDBC') For server-side loading (file resides in the server file system of the database) INSERT INTO TABLENAME SELECT * FROM EXTERNAL <path to file> USING (delim ',') PostgreSQL For server-side loading (file resides in the server filesystem of the database) COPY table_name [(column1,colunm2,...,columnN)] FROM <path to file> DELIMITER ',' csv; Stating columns is optional. Note! Postgres does not support remote file loading. SAP HANA For server-side loading (file resides in the server file system of the database) Create a control file containing code below (this example ctl file name is abc.ctl, abc.txt is the csv file): import data into table SYSTEM."TEST_LOADER" from 'abc.txt' record delimited by 'n' fields delimited by ',' optionally enclosed by '"' error log 'abc.err' Run the workflow with the following command: import from '/<path>/abc.ctl' Note! SAP HANA does not support remote file loading. Sybase IQ For server-side loading (file resides in the server file system of the database) LOAD TABLE TABLENAME (COLUMNNAME, COLUMNNAME2) USING FILE <path to file> FORMAT BCP ESCAPES OFF DELIMITED BY ',' Note! The Sybase JConnect driver does not support remote file loading. Vertica For server-side loading (file resides in the server file system of the database) COPY <table name> FROM LOCAL '<path to file>' DELIMITER AS '<character>'; If the file contains a header, you can add SKIP 1 to the SQL query, like so: COPY <table name> FROM LOCAL '<path to file>' DELIMITER AS '<character>'SKIP 1;

---

# Document 339: SFTP Forwarding Agent MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643070/SFTP+Forwarding+Agent+MultiForwardingUDR+Input
**Categories:** chunks_index.json

When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the FNT folder. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see 16. FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent has been saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is selected, if not, a runtime error will occur. When MultiForwardingUDR s are expected configuration options referring to bytearray input are ignored. For further information about Filename Template, see Filename Template Tab in Workflow Template(old) . Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDR s. import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previously in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the root directory. If the directories do not exist, the Create Non-Existing Directories check box in the forwarding agent Configuration dialog under the Filename Template tab must be selected.

---

# Document 340: User Defined Action UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205035367/User+Defined+Action+UDRs
**Categories:** chunks_index.json

It is possible to define an Action UDR and send it from the Collection workflow, in a ConsumeCycleUDR , to communicate actions back to the forwarding workflow. The UDR can be sent regardless of the Send Reply over Bridge setting in the Workflow Bridge profile.

---

# Document 341: MQTT UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205686229/MQTT+UDRs
**Categories:** chunks_index.json

The MQTT agent generates Error, PublishAck and SubscribeResponse while it receives Publish, Subscribe and Unsubscribe. Error UDRs The following fields are included in the Error UDRs: Field Description Field Description affectedUDR (DRUDR) This field stores the unprocessed UDR in case of an error in processing by the MQTT agent. message (string) This field contains the error message from the MQTT agent. OriginalData (bytearray) This field contains the original data in bytearray format. PublishAck UDRs The following fields are included in the PublishAck UDRs: Field Description Field Description broker (string) This field contains the broker connection details in the following format: tcp://<ip address/hostname>:<port> complete (boolean) This field contains a value from the broker that indicates if the topic has been completely published. messageID (int) This field contains the identifier for the message. topics (list<string>) This field contains the list of subscribed topics for this particular broker. OriginalData (bytearray) This field contains the original data in bytearray format. SubscribeResponse UDRs The following fields are included in the SubscribeResponse UDRs: Field Description Field Description broker (string) This field contains the broker connection details in the following format: tcp://<ip address/hostname>:<port> data (bytearray) This field contains the data sent by the MQTT broker in bytearray format. id (int) This field contains the identifier for the message. qos (int) This field indicates the QoS value for this particular message. topic (string) This field indicates the MQTT topic for this particular message OriginalData (bytearray) This field contains the original data in bytearray format. Publish UDRs The following fields are included in the Publish UDRs: Field Description Field Description data (bytearray) This field contains the data sent to the MQTT broker in bytearray format. messageID (int) This field contains the identifier for the message. qos (int) This field indicates the QoS value for this particular message. retain (boolean) This field indicates if retain is set to true or false. topic (string) This field indicates the MQTT topic for this particular message OriginalData (bytearray) This field contains the original data in bytearray format. Subscribe UDRs The following fields are included in the Subscribe UDRs: Field Description Field Description qos (list<int>) This field contains a list of QoS values for the topics in the UDR. topics (list<string>) This field contains a list of topics for the UDR OriginalData (bytearray) This field contains the original data in bytearray format. Unsubscribe UDRs The following fields are included in the Unsubscribe UDRs: Field Description Field Description topics (list<string>) This field contains a list of topics for the UDR OriginalData (bytearray) This field contains the original data in bytearray format.

---

# Document 342: GCP BigQuery Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641507
**Categories:** chunks_index.json

This section describes the transaction behavior of the GCP BigQuery agent. For more information about general transactions, see, Transactions, in Workflow Monitor . The GCP BigQuery batch forwarding agent uses the streaming insert API that is designed to ensure that data can be loaded at extremely high volumes and also that loaded data is available to queries in real-time. As part of the implementation that does both of these things, newly inserted data is added to a streaming buffer where it is immediately available for queries. However, this data is not moved into standard storage until more than an hour after being loaded. While the data is in the streaming buffer, it can only be queried. It cannot be updated, deleted, or copied. You will need to refer to the GCP documentation for further information on the streaming insert API and the streaming buffer. Due to the restriction on modifying rows in the streaming buffer, the GCP Bigquery agent does not modify the Data table at commit and rollback stage. Instead, the agent adopts a related design utilizing a Transaction ID , unique for each batch. The Data table must have a Transaction ID column. The Batch Status table must be created with a Transaction ID column and a Status column. At commit and rollback stage, the Batch Status is updated with a status code reflecting the current stage that can be used for auditing. Consumers of the loaded data are expected to always access that data through a view. This view should join the two tables on Transaction ID where status = 0, for example: Example - View Joining Data Table and Batch Status Table The following DDL query is used in the BigQuery Query Editor to create a view under the user_analytics Dataset with the table named view1 . CREATE VIEW IF NOT EXISTS user_analytics.view1 AS SELECT * FROM user_analytics.data_tbl1 AS t1 FULL JOIN user_analytics.batch_status_tbl1 AS t2 USING (id) WHERE t2.status = 0;

---

# Document 343: Log Properties - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848638/Log+Properties
**Categories:** chunks_index.json

This section describes the log related properties that you can set in the STR and the Desktop. All Pico Types These properties are applicable to all pico types in the STR. Property Description Property Description pico.logdateformat Default value: "YYYY-MM-DD" This property specifies the date format to be used in the log files. See http://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html for further information. pico.log.header Default value: " " This property enables the addition of a header to the log file generated, which indicates which pico, and in which version of MediationZone the log file is generated. To add headers to the log of the pico in which you set this property, set this property to mz.version-pico . After setting this property you must restart the relevant pico. pico.log.level Default value: "WARNING" This property specifies the log level. The available values for pico.log.level are: ALL FINEST FINER FINE INFO WARNING SEVERE OFF Note! Changing the default log level to INFO, FINE, FINER, FINEST, or ALL, may have a significant impact on performance. pico.pid Default value: $MZ_HOME/log This property specifies the directory you want the EC or Platform to write process ID (PID) file to. If this property is not included in the pico configuration, the default directory, $MZ_HOME/log will be used. Note! If using several ECs, a directory has to be added for each EC. pico.stderr Default value: $MZ_HOME/log This property specifies the directory you want the EC or Platform to write standard errors to. If this property is not included in the pico configuration, the default directory, $MZ_HOME/log will be used. Note! If using several ECs, a directory has to be added for each EC. pico.stdout Default value: $MZ_HOME/log This property specifies the directory you want the EC or Platform to write standard output to. If this property is not included in the pico configuration, the default directory, $MZ_HOME/log will be used. Note! If using several ECs, a directory has to be added for each EC. log4j.configurationFile Default value: $MZ_HOME/etc/log4j2.xml This property specifies the location of log4j2 configuration file. If this property is not included in the pico configuration, the default directory, $MZ_HOME/etc/log4j2.xml will be used. Note! If using several ECs, a directory has to be added for each EC. log4j APL Logs - Platform The Platform property mz.logging.refreshinterval defines how often log4j APL logs are updated. It is set to 1000 ms by default. For further information about log4j APL logs, see log4j APL Logging Configurations .

---

# Document 344: Oracle Connection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204669950/Oracle+Connection
**Categories:** chunks_index.json

The following steps are applicable when using an Oracle database. For configuration of Oracle RAC, skip to Oracle RAC Connection . Go to the Oracle JDBC driver download page: https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html Download the JDBC driver ( ojdbc<version>.jar ) for Oracle database that is compatible with the supported JDK version and Oracle database version mentioned in https://infozone.atlassian.net/wiki/x/PAY0D . Store the JDBC driver in a directory that is available on the Platform Container host, such as mz.3pp.dir . For more information, see 3pp Properties . If you are using Oracle 12, it is recommended that you set the property jvmargs.args to -Djava.security.egd=file:///dev/urandom for all pico instances that are expected to open connections towards Oracle. The jdbc driver has performance using /dev/urandom .

---

# Document 345: UDP Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609718/UDP+Agent
**Categories:** chunks_index.json

The UDP collection agent allows data to be collected and inserted into a workflow, using the standard UDP protocol. It is also possible to send responses back to the source in the form of a bytearray or in the case of several connections as a UDR containing a response field. All response handling is done through APL commands. The UDP collection agent supports IPv4 and IPv6 environments. This section describes the UDP collection agent. The agent is listed among the collection agents. Prerequisites The reader of this information should be familiar with: The UDP protocol, RFC 768 Supported Protocol Specification The UDP protocol has been implemented according to RFC 768. A UDP workflow may be configured to send responses to the source Upon activation, the collector binds to the defined port and awaits incoming UDP packets. Note the absence of a Decoder in the workflows. The collector has built-in decoding functionality, supporting any format as defined in the Ultra Format Editor . The section contains the following subsections: The UDP Format UDP Related UDR Types UDP Agent Configuration UDP Agent Input/Output Data and MIM UDP Agent Events

---

# Document 346: Firebase Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641232/Firebase+Agent
**Categories:** chunks_index.json

This section describes the Firebase agent. The agent is a forwarding agent available in real-time workflow configurations. The Firebase agent allows push notifications to be sent to mobile devices using Firebase Cloud Messaging. Open Example workflow with a Firebase agent There are two UDR types specific to the APN agent; FirebaseUDR and FirebaseErrorUDR . Prerequisites The user of this information should be familiar with: Firebase Cloud Messaging ( https://firebase.google.com/docs/cloud-messaging ) The section contains the following subsections: Firebase UDRs Firebase Agent Configuration Firebase Agent Meta Information Model and Events

---

# Document 347: MSMQ Processing Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205653328/MSMQ+Processing+Agent
**Categories:** chunks_index.json

The MSMQ processing agent enables the user to forward a message to the MSMQ queue server. For example, consider this workflow: Open MSMQ Processing Workflow Example: Sending messages to an MSMQ server consume { if (instanceOf(input, httpd)) { MSMQ msg = udrCreate(MSMQ); msg.label = Test Label; msg.body = Test body current TS =  + dateCreateNow(); msg.context = input; udrRoute(msg, queue); } } The section contains the following subsections: MSMQ Processing Agent Configuration

---

# Document 348: Workflow Bridge Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610103/Workflow+Bridge+Profile
**Categories:** chunks_index.json

The Workflow Bridge profile enables you to configure the bridge that the forwarding and collection agents use for communication. The profile ties the workflows together and is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. The section contains the following subsections: Workflow Bridge Profile Configuration

---

# Document 349: Analysis Agent Input and Output Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640064
**Categories:** chunks_index.json

UDRs entering an Analysis agent are referred to as input types, while UDRs leaving the agent are referred to as output types. The input types must be specified, while the output types are calculated from the input types and the AP L code. Example - Input and Output types Suppose there is a workflow with one Analysis agent, one input route streaming two different input types (typeA and typeB), and two output routes. The two output routes take two different UDR types - the first equaling one of the input types (typeA), and the second is a new UDR type (typeC) which is created out of information fetched from the other input type (typeB). Open The APL code: if (instanceOf(input, typeA)) { udrRoute((typeA)input,"r_2"); } else { typeC newUDR = udrCreate(typeC); newUDR.field = ((typeB)input).field; // Additional field assignments... udrRoute(newUDR, ,"r_3"); } The first udrRoute statement explicitly typecasts to the typeA type, while there is no typecasting at all for the second udrRoute statement. This is because the input variable does not have a known type (it can be either typeA or typeB), while newUDR is known by the compiler to be of typeC. Without any typecasting, the output type on r_2 would have been reported as an undefined UDR, drudr, and the workflow would not have been valid.

---

# Document 350: SCP Agents Attributes and Authentication - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740493/SCP+Agents+Attributes+and+Authentication
**Categories:** chunks_index.json

Attributes The SCP collection agent and the SCP forwarding agent share a number of common attributes. They are both supported by a number of algorithms: 3des-cbc, 3des-ctr, blowfish-cbc, aes128-cbc, aes192-cbc, aes256-cbc, aes128-ctr, aes192-ctr, aes256-ctr, arcfour, arcfour128, arcfour256. Authentication The SCP agents support authentication through either username/password or private key. Private keys can optionally be protected by a Key password. Most commonly used private key files, can be imported into the system. Typical command line syntax (most systems): ssh-keygen -t <keyType> -f <directoryPath> Argument Description Argument Description keyType The type of key to be generated. Both RSA and DSA key types are supported. directoryPath Where to save the generated keys. Example The private key may be created using the following command line: > ssh-keygen -t rsa -f /tmp/keystore Enter passphrase: xxxxxx Enter same passphrase again: xxxxxx Then the following is stated: Your identification key has been saved in /tmp/keystore Your public key has been saved in /tmp/keystore.pub When the keys are created the private key may be imported to the SCP agent: Open Edit Private Key dialog Finally, on the SCP server host, append /tmp/keystore.pub to $HOME/.ssh/authorized_keys . If the $HOME/.ssh/authorized_keys is not there it must be created.

---

# Document 351: Aggregation Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031698/Aggregation+Agent+Events
**Categories:** chunks_index.json

Agent Message Events The agent does not itself produce any message events. However, APL offers the possibility of producing events. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Aggregation Storage implementation This event is reported during workflow initialization. It shows the selected storage type i e file storage ,Couchbase, or Redis. You may also configure debug messages in the APL code.

---

# Document 352: Data Veracity in Data Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204672534
**Categories:** chunks_index.json

The Data Veracity feature in Data Management allows the user to inspect and maintain Data Veracity records which includes erroneous UDRs and batch files located in Data Veracity. Operators can view and remove Error Codes, Filters, Repair Rules, inspect UDRs as well as Approve or Reject Data Veracity records that have been marked for deletion and edit the content of UDRs. Apart from simply sending a Data Veracity record to Data Veracity, a workflow can be configured to associate user-defined information with the Data Veracity data such as Error Code and MIM information. For cancelled batches, the Error UDR and Cancel Message may contain additional user defined information. Note! Take special precaution when updating the Ultra formats. It is not possible to collect data from Data Veracity if the corresponding UDR has been renamed. If the format definition has changed, you can still collect the data. Changes to the formats are handled as follows: Added or renamed fields will be assigned default values. Removed fields will be ignored. Fields that have changed data types will be assigned default values. Note! Operations that modifies the Data Veracity tables via Data Veracity Data Management will only be allowed by users with appropriate application write access. Data Veracity Dashboard From the Manage view, click on Data Veracity and this will lead you to the Data Veracity dashboard where you can select the action to perform on the UDRs. Open Data Veracity Dashboard The section contains the following subsections: Search & Repair Approve Delete Repair Jobs Error Codes Filters Repair Rules Restricted Fields Data Masking Fields

---

# Document 353: Archiving Agents MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031972/Archiving+Agents+MultiForwardingUDR+Input
**Categories:** chunks_index.json

When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the package FNT. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see 16. FNTUDR Functions APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Archiving Example Example - Archiving This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDRs. In this example, the data is being buffered in the consume block. This makes it possible to route a complete batch to multiple files from the drain block. Note that the execution context needs available memory to buffer the whole file. import ultra.FNT; bytearray data; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent) { //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } beginBatch { data = baCreate(0); } consume { data = baAppend(data, input); } drain { //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR("dir1", "file1", data)); udrRoute(createMultiForwardingUDR("dir2", "file2", data)); } Local Archiving Example Example - Local Archiving This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDRs. import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previous in the example will send two MultiForwardingUDRs to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the root directory.

---

# Document 354: Exit Codes - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744049/Exit+Codes
**Categories:** chunks_index.json

Below are the defined exit codes listed. Exit Code Message Info Exit Code Message Info 0 Run completed. 50 <Prints the help command> No parameters to the command. 51 <varying, error description> Error parsing the parameters. 60 Timed out. Workflow/group did not completed/aborted within the defined timeout. 70 No matching workflow. Workflow/group not found. If wfgroupstart is used the message will be: No matching group found. 71 Only one workflow is allowed to be started when option -w is used. Workflow is replaced with workflowgroup if wfgroupstart is used. [-w] is replaced with [-b] in case it is used instead. 80 <varying, depending on why it didn't start> Reason why the workflow/group did not start. 90 Unexpected error: <error message + stack trace> Unexpected error, abort command. 100 Aborted. 101 Platform shutdown, group aborted. 110 Unknown status of run. Command has been out of contact with platform and lacks information about how the run completed. 230 Already active (ignoring request). 231 Permission denied. 232 No such configuration. 240 Configuration is invalid.

---

# Document 355: Aggregation Agent Configuration - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639887
**Categories:** chunks_index.json

The real-time Aggregation agent's configuration dialog includes the following tabs: General APL Code Storage General Tab The General tab enables you to assign an Aggregation profile to the agent and to define error handling. With the Error Handling settings, you can decide what you want to do if no timeout has been set in the code or if there are unmatched UDRs. Open The Aggregation agent configuration dialog - General tab Setting Description Setting Description Profile Click Browse... and select an Aggregation profile. All the workflows in the same workflow configuration can use different Aggregation profiles. For this to work, the profile has to be set to Default in the Field settings tab in the Workflow Properties dialog. After this, each workflow in the Workflow Table can be assigned with the correct profile. Force Read Only Select this check box to only use the aggregation storage for reading aggregation session data. If you enable the read only mode, timeout handling is also disabled. When using file storage and sharing an Aggregation profile across several workflow configurations, the read and write lock mechanisms that are applied to the stored sessions must be considered: There can only be one write lock at a time in a profile. This means that all but one Aggregation agent must have the Force Read Only setting enabled. If all of the Aggregation agents are configured with Force Read Only , any number of read locks can be granted in the profile. If one write lock or more is set, a read lock cannot be granted. If Timeout is Missing Select the action to take if timeout for sessions is not set in the APL code using sessionTimeout . The setting is evaluated after each consume or timeout function block has been called (assuming the session has not been removed). The available options are: Ignore - Do nothing. This may leave sessions forever in the system if the closing UDR does not arrive. Abort - Abort the agent execution. This option is used if timeout must be set at all times. Hence, a missing timeout is considered being a configuration error. Use Default Timeout - Allow the session timeout to be set here instead of within the code. If enabled, a field becomes available. In this field, enter the timeout, in seconds. If No UDR Match is Found Select the action that the agent should take when a UDR that arrives does not match any session, and Create Session on Failure is disabled: Ignore - Discard the UDR. Log Event - Discard the UDR and generate a message in the System Log. Route - Send the UDR on the route selected from the on list. This is a list of output routes through which the UDR can be sent. The list is only activated if Route is selected. APL Code Tab Storage The Storage tab contains settings that are specific for the selected storage in the Aggregation profile. Different settings are available in batch and real-time workflows. File Storage When using file storage for sessions in a batch workflow, the Storage tab contains a setting to control how often the timeout block should be executed. In this tab, it is also specified when the changes to the aggregation data is written to file. Open The Aggregation agent configuration dialog - Storage tab for File Storage Setting Description Setting Description Session Timeout Interval (seconds) Determines how often, in seconds, the timeout block is invoked for outdated sessions. Storage Commit Interval (seconds) Determines how often, in seconds, the in-memory data is saved to files on disk. Storage Commit Interval (#Processing Calls) Determines the number of Processing Calls before the in memory data is saved to files on disk. A 'Processing Call' is an execution of any of the blocks consume , command or timeout . If both this option and the Storage Commit Interval (seconds) are configured, commits are made when any of them are fulfilled. Note! If Storage Commit Interval (seconds) and/or Storage Commit Interval (#Processing Calls) are configured, data left in memory when the workflow stops will be saved to file. If Storage Commit Interval (seconds) and Storage Commit Interval (#Processing Calls) are not configured, none of the sessions in-memory are stored on disk. The session count displayed in the Aggregation Inspector will not include these sessions. When the Max Cached Sessions in the Aggregation profile is exceeded, and Storage Commit Interval (seconds) and Storage Commit Interval (#Processing Calls) are not configured, the agent deletes the oldest session. This is done in order to allocate space for the new session while still staying within the limit. Couchbase Storage Open The Aggregation agent configuration dialog - Storage tab for Couchbase Setting Description Setting Description If Error Occurs in Storage Select the action that the agent should take when an error occurs in the storage: Ignore - Discard the UDR. Log Event - Discard the UDR and generate a message in the System Log. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is only activated only if Route is selected. Disable Timeout Select this check box to disable the timeout handling. Redis Storage Open The Aggregation agent configuration dialog - Storage tab for Redis Setting Description Setting Description If Error Occurs in Storage Select the action that the agent should take when an error occurs in the storage: Ignore - Discard the UDR. Log Event - Discard the UDR and generate a message in the System Log. Route - Send the UDR on the route selected from the on list. This is a list of output routes on which the UDR can be sent. The list is only activated if Route is selected. Disable Timeout Select this check box to disable the timeout handling.

---

# Document 356: Websocket Client Agent Input/Output Data, Meta Information Model and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610064/Websocket+Client+Agent+Input+Output+Data+Meta+Information+Model+and+Events
**Categories:** chunks_index.json

Input/Output Data The agent emits UDRs of the following types: BinaryMessage TextMessage which are both used for communication between the client and the server after a connection has been established. The agent retrieves UDRs of the following types: BinaryMessage TextMessage CloseConnection of which the first two is used for communication between the client and the server after a connection has been established, and the last one can be sent to the agent to signal that the connection should be closed. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . The agent publishes the following MIM parameter: MIM Value Description Open Connections (list<string>) This MIM parameter is of type list of string. This MIM shows list of one URL per open connection. Agent Message Events There are no message events for this agent. Debug Events There are no debu g events for this agent.

---

# Document 357: resumeexecution - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612551/resumeexecution
**Categories:** chunks_index.json

usage: resumeexecution This command enables you to resume the execution of a workflow, or a workflow group, that is stuck in the Hold state. Example 9. If the workflow, or the workflow group, does not leave the Hold state after running systemimport -holdexecution , either due to a system crash or because you performed Ctrl+C , use this command to resume execution. For further information, see Section 2.2.2.23.6, [ -he|-holdexecution [ r | sr | sir | wr ] ] For information about workflow and workflow group states, see the Desktop User's Guide . Return Codes Listed below are the different return codes for the resumeexecution command: Code Description Code Description 0 Will be returned if all workflows and groups resume execution normally. 1 Will be returned if any errors occur.

---

# Document 358: FTPS Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685616/FTPS+Agents
**Categories:** chunks_index.json

This section describes the FTPS collection and FTPS forwarding agents. These agents are for batch workflow configurations and support the Transport Layer Security (TLS) and the Secure Sockets Layer (SSL) cryptographic protocols. Prerequisites The reader of this information should be familiar with: Standard FTP (RFC 959, http://www.ietf.org/rfc/rfc0959.txt ) FTP Security Extensions ( RFC 2228, http://www.ietf.org/rfc/rfc2228.txt ) The section contains the following subsections: FTPS Collection Agent FTPS Forwarding Agent

---

# Document 359: Kafka Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138071/Kafka+Profile
**Categories:** chunks_index.json

In the Kafka profile configuration, you define which Kafka broker to consume messages from or produce messages to, if security should be applied, and if other specific settings for communication are needed. Configuration To create a new Kafka profile configuration, click the New Configuration button and select Kafka Profile from the Configurations dialog. The Kafka profile configuration contains two tabs; Connectivity and Advanced . Connectivity tab The Connectivity tab is displayed by default when creating or opening a Kafka profile. Open Kafka profile configuration - Connectivity tab The Connectivity tab contains the following settings: Setting Description Setting Description Host The hostname of the Kafka broker. Port The port of the Kafka broker. Security Profile If you want to have a secure connection, click Browse to select a Security profile with the certificate and configuration you want to use, see Security Profile . Advanced tab In the Advanced tab, you can configure properties for optimizing the performance of the Kafka Producer and Consumer. The Advanced tab contains two tabs; Producer and Consumer . Producer tab In the Producer tab, you can configure the properties of the Kafka forwarding agent. Open Kafka profile configuration - Producer tab in the Advanced tab For information on how to configure the properties for SSL and Kerberos, see Configuring Apache Kafka Security | 4.1.x | Cloudera Documentation . Note! Once you have edited the JAAS file required for Kerberos, restart the EC to register the changes made. Note! If you make any changes to the security configuration of the Kafka Producer, any topics in use must be recreated before they can be used. For further information on the properties, see https://kafka.apache.org . Enabling compression for Kafka Compression for messages sent to Kafka brokers can now be enabled from the Advanced producer properties . The compression codec utilized by the system follows the standard Kafka library, where Gzip, Lz4, and Snappy are supported. To enable compression, just add the property compression.type into the Advanced producer properties and add the value, gzip , lz4 , snappy or, none. Consumer tab In the Consumer tab, you can configure the properties of the Kafka collection agent. Open Kafka profile configuration - Consumer tab in the Advanced tab See the text in the Advanced consumer properties field for further information about the properties.

---

# Document 360: Configuration Object Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645064/Configuration+Object+Example
**Categories:** chunks_index.json

This is a simple configuration object example from the DTK examples. The example shows a Disk Collection agent configuration including a directory field and a fileNamePrefix field. @DRStorableId("devkitexamples.DiskCollectionConfig") @DRConfigTypeInfo(version = 10f, section = "Disk Collection") public class DiskCollectionConfig extends DRAbstractConfigObject { public static final String DIRECTORY = "Directory"; public static final String FILENAME_PREFIX = "Filename Prefix"; private String _directory = ""; private String _fileNamePrefix = ""; public DiskCollectionConfig() {} @DRConfigFieldInfo(title = DIRECTORY, description = "The path to the source directory") public void setDirectory(String directory) { _directory = directory; } public String getDirectory() { return _directory; } @DRConfigFieldInfo(title = FILENAME_PREFIX, description = "The filename prefix") public void setFileNamePrefix(String fileNamePrefix) { _fileNamePrefix = fileNamePrefix; } public String getFileNamePrefix() { return _fileNamePrefix; } @Override public String validateField(String fieldName, Object fieldValue, DREnvironment env) throws DRException { if (fieldName.equalsIgnoreCase("directory")) { if (fieldValue == null || ((String) fieldValue).length() < 1) return "Directory may not be empty"; } return null; } }

---

# Document 361: Services Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638570/Services+Tab
**Categories:** chunks_index.json

There are two predefined services In the Services tab Order Service and Supervision. Note! The Services tab is only available in real-time workflows. Open Workflow Properties - Services tab Button Description Button Description Add Service Click to select a service to be used by the workflow, then click OK to apply it. Remove Service Visible when a service has been selected. Click to remove the service from the workflow. Subsections This section contains the following subsections: Ordered Service Supervision Service

---

# Document 362: Application Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644798/Application+Tab
**Categories:** chunks_index.json

The Application Tab contains the information listed below. RCP Connections Tab Open Application with the RCP Connections tab displayed Tab Description Tab Description RCP Connections Information about RCP connections. For detailed information, see 7.4 RCP Latency Monitoring and the description of Oracle's Package java.lang.management : https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Event Listeners Tab Open Application with the Event Listeners tab displayed Tab Description Tab Description Event Listeners Information about Events. For detailed information, see 7.2 Event Monitoring and Oracle's Package java.lang.management Description: https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Open Application with the Event Sending tab displayed Tab Description Tab Description Event Sending Information about Events. For detailed information, see 7.2 Event Monitoring and Oracle's Package java.lang.management Description: https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Application with the Workflow tab displayed Tab Description Tab Description Workflows Information about Workflows. For detailed information, see 7.3 Workflow Monitoring and Oracle's Package java.lang.management Description: https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html Application with the Logging tab displayed Tab Description Tab Description Logging Oracle's Package java.lang.management Description: https://docs.oracle.com/javase/8/docs/api/java/lang/management/package-summary.html

---

# Document 363: Tools & Monitoring - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605336/Tools+Monitoring
**Categories:** chunks_index.json

MediationZone provides different tools and monitoring services to, for example, view logs, statistics, and pico instance information, and to import and export configurations. The section describes all Tools & Monitoring services. For further information about the Ultra specific tools, see the Ultra Reference Guide . To access Tools & Monitoring, go to Manage  Tools & Monitoring . This chapter includes the following sections: Access Controller Alarms and Events Conditional Trace EC Groups Encrypt Password Execution Manager Log Files Log Filter Pico Management Python Manager System Exporter System Importer System Log System Statistics UDR File Editor and Ultra Format Converter Operations REST Interface

---

# Document 364: Database Sizing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/511639988/Database+Sizing
**Categories:** chunks_index.json

Generally, 10 GB of allocated storage space for the platform database is sufficient for typical installations. Note! Some customers use many times more than 10GB, depending on their processing volume and specific use cases. These figures are based on our experience with large customers who have complex use cases and their calculated database storage usage. The database storage should be regularly monitored, and additional space provided as needed to ensure there is always enough free space to accommodate any increase in database storage usage. This takes into account that the platform database storage space used will be maintained by ensuring SystemTask maintenance tasks listed below are scheduled to run and purge expired data automatically for: Archive data cleaner Configuration Cleaner ECS Maintenance Statistic Cleaner System Logs Cleaner User Cleanup To configure the maintenance tasks, click on the Build menu and expand the System Task folder. Sizing conditions The recommended sizing described here is valid as long as excessive database utilization is avoided. For production systems, it is crucial to minimize excessive logging and database inserts to ensure uninterrupted operation. Before transitioning a solution from development to production, it is important to Remove non-crucial system logging in APL as extensive logging to the system log could cause the database tables to grow unnecessarily Disable Debug on all workflows to reduce disk space used. In the same manner, if Error Correction System (ECS) is used, it is essential to have proper handling of all error cases so that the ECS database tables are not growing and that there is valid reprocessing logic in place for all types of error cases. In addition, database storage space used by Duplicate Batch functionality depends on its profile Max Cache Age settings. Exceptions Some product features can optionally store data in the database instead of disk. The sizing recommendation mentioned in this section does not take these into account since the required additional database space depends on each use case. As such the additional database space required must be calculated separately to be sufficient for accommodating these operations, if they are used. We recommend using a separate database for these functions rather than the platform database. These functions are: Audit Profile | Configuration Aggregation Profile | Session Tab Data Veracity Duplicate UDR Profile | SQL Storage Inter Workflow Profile | Configuration

---

# Document 365: Function Blocks for the Python Processing Agent - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204642259/Function+Blocks+for+the+Python+Processing+Agent+-+Real-Time
**Categories:** chunks_index.json

When writing code for the Python processing agent, the function blocks in this section apply. For examples and further information on writing code in the Python processing agent, see Python Writer's Guide . The following function blocks are supported by the Python processing agent: Function Block Description def initialize() This function block initializes resources and state. def consume(input) This function block consumes and processes UDRs. def timeout(obj) This function block is called when using the setTimeout function. UDRs may be routed in a timeout function block. When possible, always route to an asynchronous route from the timeout function block. def stop() This function block is called when the workflow is about to stop. def deinitialize() This function block will cleanup resources and state.

---

# Document 366: Snowflake - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671654/Snowflake
**Categories:** chunks_index.json

This section contains information that is specific to the database type Snowflake. Supported Functions The Snowflake Server database can be used with: sqlExec Function (APL) SQL Collection/Forwarding Agents Preparations The drivers that are required to use the Snowflake database are bundled with the software and no additional preparations are required. Advanced Connection Configuration for Snowflake The Advanced Connection Setup is used for Snowflake configurations. To make the Connection String text area and the Notification Service text field appear, select the Advanced Connection Setup radio button. The Username , Password and Database Type fields will remain. Open Database Profile Configuration - Advanced Connection Setup for Snowflake Setting Description Setting Description Connection String In this field you should enter the following string: jdbc:snowflake://<ip address>/<db query> Notification Service This field is not applicable for the Snowflake db. Note! When using the Snowflake JDBC driver, you might encounter the following exception: net.snowflake.client.jdbc.SnowflakeSQLException: JDBC driver internal error: exception creating result java.lang.NoClassDefFoundError: Could not initialize class net.snowflake.client.jdbc.internal.apache.arrow.memory. RootAllocator at net.snowflake.client.jdbc.SnowflakeResultSetSerializableV1.create(Snowflake ResultSetSerializa bleV1.java:586). To resolve the issue, you can override the default result format for JDBC by setting the JDBC_QUERY_RESULT_FORMAT property. Use the following ALTER statements to set the property at the account or user level: ALTER ACCOUNT SET JDBC_QUERY_RESULT_FORMAT='JSON'; ALTER USER <user_name> SET JDBC_QUERY_RESULT_FORMAT='JSON'; See this Snowflake Community Article for more information regarding this error.

---

# Document 367: Data Veracity Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204999421/Data+Veracity+Forwarding+Agent+Transaction+Behavior
**Categories:** chunks_index.json

Transaction Behavior The agent utilizes a unique Transaction ID to ensure that the inserted (distributed) rows are removed incase the batch is cancelled. This is to avoid duplicated rows. To handle this, the agent inserts the unique batch Transaction ID in the assigned Transaction ID column. If the batch is cancelled, all rows matching the batch Transaction ID will be removed again. Emits This agent does not emit anything. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Command Description Begin Batch Retrieves a new Transaction ID. End Batch Do nothing. Cancel Batch Removes the distributed rows with the current Transaction ID or calls the configured Cleanup SP. The pending Transaction ID row is deleted.

---

# Document 368: EC Groups - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204671006/EC+Groups
**Categories:** chunks_index.json

The EC Groups tool allows you to view/add EC Groups. The EC groups can be added here or by using the mzsh command pico for example. See the Execution Tab for more information on how to add ECs to an EC Group. In the following sections, we will explain the EC Groups tool and the configuration files. EC Groups Tool The EC Groups tool shows the list of EC groups. From the list, you are able to add, delete or edit groups. You can sort the group names alphabetically. To refresh the list, click the Refresh button at the top of the list. Open EC Groups Add a New Group Click the Add button to add a new EC Group. A window pops up where you can fill in the name of the EC Group. Click OK and the group becomes visible in the list. See the mzsh command pico for an example on how to add an EC group. Editing an Existing Group To edit an existing EC group, select the checkbox of the EC Group and click the Edit button. A new window is displayed using which you can edit the group name. Click OK to close the window and submit the changes. The list is updated with your changes. Open EC Groups - Edit Removing a Group To delete one or many groups, select the group to remove and click the Delete button. EC groups from Topo or Pico Management If you have defined EC Groups using topo commands or in the Pico Management tool, the list will automatically display these EC groups, when they are started.

---

# Document 369: Bit Operation Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204677832/Bit+Operation+Functions
**Categories:** chunks_index.json

The functions described in this section are used to perform operations on variables of the bitset type. The following functions for Bit Operation described here are: 1 bitIsSet and bitIsCleared 2 bsCreate 3 bsSet and bsClear bitIsSet and bitIsCleared The bitIsSet and bitIsCleared functions evaluate whether an individual bit is set or cleared. This can be done against either an integer, or a bitset value. For integer type, these are convenience functions that can be used instead of the regular logical operators. boolean bitIsCleared ( int|bitset value, int bitnumber ) boolean bitIsSet ( int|bitset value, int bitnumber ) Parameter Description Parameter Description value Bitset or integer to evaluate bitnumber The bit to evaluate. 0 (zero) is the least significant bit, 31 is the most significant. Returns false or true bsCreate The bsCreate function creates an empty bitset. bitset bsCreate() Parameter Description Parameter Description Returns An (empty) bitset bsSet and bsClear The bsSet and bsClear functions set or clear a bit in a bitset. void bsClear ( bitset bitset, int bitnumber ) void bsSet ( bitset bitset, int bitnumber ) Parameter Description Parameter Description bitset The bitset to modify bitnumber The bit to evaluate. 0(zero) is the least significant bit, 31 is the most significant. Returns Nothing

---

# Document 370: SCP Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674705/SCP+Forwarding+Agent+Transaction+Behavior
**Categories:** chunks_index.json



---
**End of Part 17** - Continue to next part for more content.
