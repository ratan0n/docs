# RATANON/MZ93-DOCUMENTATION - Part 46/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 46 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~67.0 KB
---

Prometheus UDR You use PrometheusUDR s to create metrics. The PrometheusUDR has to be populated with a value and optional Prometheus labels. You can decide if you want to create a PrometheusUDR per each value, or aggregate them using the Aggregation agent and send a cumulative value after desired conditions are met. Labels should follow the Prometheus recommendation, for details see the official Prometheus documentation. The PrometheusUDR type can be viewed in the UDR Internal Format Browser . The PrometheusUDR contains data that can be sent to the endpoints running on each Execution Context. The following fields are included in the PrometheusUDR : Field/Option Description Name (string) This field is populated with the metric name, for example mim_realtime_processing_analysis_outbound_udrs . MetricType (string) This field is populated with one of the metric types supported by Prometheus. Supported types are: COUNTER GAUGE UNTYPED (default) Value (double) This field is populated with a numeric value and is what, in most cases, is displayed in your Grafana graph. For example, the field can be populated by 6.5 . Labels (map<string.string>) This field is populated with the labels applied to the metric, for example origin_country . Description (string) This field is populated with a description of a metric.

---

# Document 1040: Data Masking Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998444
**Categories:** chunks_index.json

In the Data Masking profile, you configure the masking method you want to use, which UDR types and fields you want to mask/unmask, and any masking method-specific settings. There are four different masking methods that you can use: Crypto - Uses cryptographic algorithm that can be configured to either derive its key from a passphrase or a Keystore. It uses either AES-128 or AES-256 for data encryption. The data can be unmasked later when required. Database - Enables data model masking to store masked and unmasked data. The data can be unmasked later when required. Hash (one way) - Employs a salt-based encryption scheme for obscuring data only. All masked data using this method cannot be unmasked. Hash/Database - Uses a combination of the database and hash mode. The data can be unmasked later when required. For more information on the supported data types, see Supported Data Types . Configuration To create a new Data Masking profile, click the New Configuration button from Build View , and then select Data Masking Profile from the selection screen. The contents of the buttons bar may change depending on which configuration type has been opened in the currently displayed tab. The Data Masking profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . The Edit button content is specific to the Data Masking profile configurations. The Data Masking profile consists of the following tabs: 1 Fields Tab 2 Crypto Tab 3 Database Tab 4 Hash Tab 5 Hash/Database Tab Fields Tab The masking method that is selected in the Fields tab determines which of the other four tabs that will be active as these tabs contain masking method specific configurations. The Fields tab in the Data Masking profile configuration contains the following settings: Open Data Masking Profile - Fields tab Setting Description Setting Description Masking Method Select the masking method to be used from the drop-down list. Storage Fields Add the fields to map the UDR fields to. This section is only applicable to Database Storage and Hash/Database masking methods. UDR Field Mappings Add all the UDR types and fields for the profile to process. Random Algorithm (Only for String type) Specify the algorithm to be used for generating the random character. The supported algorithms are: Default : Default random algorithm. For Crypto, it only supports Base64 format where the Hash or Database are using mixture of alphanumeric and special characters. The supported characters list are: [!, ", #, $, %, &, ', (, ), *, +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, <, =, >, ?, @, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, [, , ], , _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, {, |, }, ~] UUID 4 : Generate UUID string in 8-4-4-4-12 format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Custom : Filtered character based on Default character sets, filter condition can be configured through Regex Pattern For more information on the algorithms for each masking method, see Supported Random Algorithm Type . This section is only applicable to Database Storage , Hash and Hash/Database masking methods. This section is disabled if other masking method is selected. Regex Pattern This is field is enabled when the Custom option is selected. It is a regular expression to extract characters based on the default characters list. String Length This is field is enabled when the Custom option is selected. Specify the length of the output string. Output Format This field is non-editable. It displays the supported character list and sample output preview. Supported Random Algorithm Type The supported random algorithm types for each masking method are as follows: Algorithm Crypto Database Hash Hash/Database Algorithm Crypto Database Hash Hash/Database Default UUID 4 Custom Crypto Tab This tab is enabled only when the Crypto masking method is selected in the Fields tab. Open Data Masking Profile - Crypto tab Setting Description Description Setting Description Description Cipher Mode Cipher Mode to use. The two modes CTR and GCM are non-deterministic in the sense that they will give different outputs for the same input. This means that it will not be possible to correlate data from separate UDRs, but if this is not a requirement then it gives a more complete anonymization. The CBC mode is deterministic and can be used when correlation must be possible on pseudonymized data. It includes a transposition scrambling to protect against prefix matching. The ECB mode is not recommended since it allows for prefix and suffix matching and thus gives weaker security than the CBC mode. It is only included for backwards compatibility. Derive Key from Passphrase Select this option for the cryptographic engine to use a key from the passphrase. The Passphrase and Algorithm fields will be enabled. Passphrase Enter a passphrase manually or click the Random button to generate a random key. The passphrase is then hashed and it is use as the key. If you use a random passphrase and it has been changed, you will not be able to unmask any masked data prior to the change. Algorithm Select the algorithm to be used, either the AES-128 or AES-256 . This can only be used for fields of string and bytearray types. Read Key from Keystore Select this option to use a key from a designated keystore. The keystore must be a JCEKS. The Keystore Path , Keystore Password , Key Name and Key Password fields will be enabled. Example - Creating a symmetric crypto key $ keytool -keystore test.ks -storepass password -storetype jceks -genseckey -keysize 128 -alias testkey -keyalg AES Keystore Path Enter the path to the keystore file. Keystore Password Enter the associated password. Key Name This field is optional. Enter the associated key name. Key Password This field is optional. Enter the associated key password if required, otherwise the Keystore Password is used as the default password. Database Tab This tab is enabled only when the Database Storage masking method is selected in the Fields tab. Open Data Masking Profile - Database tab Setting Description Setting Description Database Model Database Browse and select the Database profile to use. Table Select the database table to view the following information: Field : Shows the field name Unmasked : Shows the unmasked content Masked : Shows the masked content Key : The selected checkbox shows the fields that will be searched when unmasking data. If you have a large table or huge amount of lookups, you may consider to select the necessary fields only for searching when unmasking data Advanced Queue Size Set the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the maximum number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements to retrieve data. Hash Tab This tab is enabled only when the Hash masking method is selected in the Fields tab. Open Data Masking Profile - Hash tab Setting Description Setting Description Salt Enter the entry of the relevant hash or click the Random button to generate a random entry. Hash/Database Tab This tab is enabled only when the Hash/Database masking method is selected in the Fields tab. Open Data Masking Profile - Hash/Database tab Setting Description Setting Description Data Model Database Browse and select the Database profile to use. Table Select the database table to view the following information: Field : Shows the field name Unmasked : Shows the unmasked content Masked : Shows the masked content Key : The selected checkbox shows the fields that will be searched when unmasking data. If you have a large table or huge amount of lookups, you may consider to select the necessary fields only for searching when unmasking data Hash Salt Enter the entry of the relevant hash or click the Random button to generate a random entry. Advanced Queue Size Set the queue size for the workers. The queue size will be split between the workers. Max Number of Workers Enter the number of workers. Max Select Batch Size Enter the maximum size of the batch when making large select statements to retrieve data. Supported Data Types The supported data types for each masking method are as follows: Data type Crypto Database Hash Hash/Database Data type Crypto Database Hash Hash/Database string integer long short double byte bytearray

---

# Document 1041: Updating Pico Configurations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657479
**Categories:** chunks_index.json

Update existing configurations by manually editing the pico configuration file (<pico name>.conf) or use the mzsh topo set command. If you want to make changes for a set of pico configurations, you can do so by using templates. Attributes, i e properties, that that are set on container level (in container.conf ) or cell level ( cell.conf ) are inherited by pico configurations. Pico Level Attributes Run the following commands to add or update an attribute of a pico configuration in a container. $ mzsh topo set topo://container:<container>/pico:<pico>/val:<attribute> <attribute value> Example - Updating a system property $ mzsh topo set topo://container:main1/pico:platform/val:config.properties.mz.subfolder.enabled true Run the following command to add or update an object that contains one or more attributes. $ mzsh topo set topo://container:<container>/pico:<pico>/obj:<object name> '<config>' The <config> argument may contain a key-value pair that specifies a template or a pico configuration in HOCON format. Example - Updating a pico object $ mzsh topo set topo://container:main1/pico:ec2 '{ template:mz.standard-ec config { properties { ec.webserver.port=9092 } classpath { jars=["lib/picostart.jar"] } } }' Container Level Attributes Run the following command to add or update an attribute on container level. $ mzsh topo set topo://container:<container>/val:<attribute> <attribute value> Run the following command to add or update an object that contains one or more attributes. $ mzsh topo set topo://container:<container>/obj:<object name> '<config>' You cannot add JVM arguments or classpaths on the container level. If you need to add JVM arguments that are applied to all pico processes, it is recommended that you do so by using templates. Cell Level Attributes Run the following commands to add or update an attribute on cell level. $ mzsh topo set topo://val:common.<attribute> <attribute value> Run the following commands to add or update an object that contains one or more attributes. $ mzsh topo set topo://obj:common.<object name> '<config>' You cannot add JVM arguments or classpaths on the cell level. If you need to add JVM arguments or classpaths that are applied to all pico processes, it is recommended that you do so by using templates. Desktop Attributes Run the following commands to add or update a custom default attribute for Desktop instances. $ mzsh topo set topo://client:desktop/val:config.<attribute> <attribute value> Run the following commands to add or update an object that contains one or more attributes. $ mzsh topo set topo://obj:client:desktop.config.<object name> '<config>' Example - Updating default JVM arguments $ mzsh topo set topo://client:desktop/obj:config '{ jvmargs { xms=["-Xms128M"] xmx=["-Xmx256M"] client=["-client"] } }'

---

# Document 1042: Aggregation Agent Overview - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606134/Aggregation+Agent+Overview
**Categories:** chunks_index.json

The Aggregation consolidates related UDRs that originate from either a single source or from several sources, into a single UDR. Related UDRs, are grouped into "sessions" according to the value of their respective fields, and a set of configurable conditions. The first UDR to match a set of conditions triggers the generation of a new session. Consecutive UDRs that match the same conditions can then be handled in the context of the same session. The agent stores the session data in a file system, Couchbase, Redis or Elasticsearch. It is also possible to use the settings for file storage to keep the session data in-memory only. When selecting which storage type you require, note that File Storage can be used in batch and real-time workflows; Couchbase and Redis can only be used in real-time workflows; Elasticsearch can only be used in batch workflows. To ensure the integrity of the session's data in the storage, the Aggregation agent may use read- and write locks. When you have selected file storage and an active agent has write access, no other agent can read or write to the same storage. It is possible to grant read-only access for multiple agents, provided that the storage is not locked by an agent with write access. When you have selected Couchbase or Redis storage, multiple Aggregation agents can be configured to read and write to the same storage. In this case, write locks are only enforced for sessions that are currently updated and not the entire storage. For further information about how to configure read-only access, see Aggregation Profile . In a batch workflow, the aggregation agent receives collected and decoded UDRs one by one. Open Aggregation in a batch workflow In a real-time workflow, the aggregation agent may receive UDRs from several different agents simultaneously. Open Aggregation in a real-time workflow The aggregation flow chart below illustrates how an incoming UDR is treated when it is handled by the Aggregation agent. If the UDR leaves the workflow without having called any APL code, it is handed over to error handling. For detailed information about handling unmatched UDRs see the section General Tab in Agent Configuration - Batch, Aggregation Agent Configuration - Batch , or Agent Configuration - Real-Time in Aggregation Agent Configuration - Real-Time . Open The aggregation flow chart When several matching sessions are found, the first one is updated. If this occurs, redesign the workflow. There must always be zero or one matching session for each UDR. High level steps for configuration of Aggregation Follow the steps below to configure Aggregation: Create a session UDR in Ultra format. Create a storage profile, i e Couchbase or Redis profile (not required for file storage). Create an Aggregation profile. Create a workflow containing an Aggregation agent.

---

# Document 1043: Configuration Contract Example - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645298/Configuration+Contract+Example
**Categories:** chunks_index.json

This is a simple configuration contract example from the DTK examples. The example shows a Disk Collection agent configuration including a directory field and a fileNamePrefix field. <?xml version="1.0" encoding="UTF-8"?> <contract object-version='1.0'> <class-name>DiskCollectionConfig</class-name> <package-name>com.domain.diskcollection</package-name> <storable-id>companyname.DiskCollectionConfig</storable-id> <section id='Disk Collection'> <field id='directory'> <title>Directory</title> <name>Directory</name> <description>The path to the source directory</description> <type><object-type name='java.lang.String'/></type> <default-value>""</default-value> <validation> <validate minLength='1' message='Directory may not be empty'/> </validation> </field> <field id='fileNamePrefix'> <title>Filename Prefix</title> <name>Filename Prefix</name> <description>The filename prefix</description> <type><object-type name='java.lang.String'/></type> <default-value>""</default-value> </field> </section> </contract>

---

# Document 1044: GTP' LGU Collection Agent MZSH Commands, Events and Limitations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204673409/GTP+LGU+Collection+Agent+MZSH+Commands+Events+and+Limitations
**Categories:** chunks_index.json

mzsh Commands In case you want to see the counters that are published as MIM values, you can use the mzsh wfcommand . See the Command Line Tool Reference Guide for further information about this. Agent Message Events There are no agent message events for this agent. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 1045: Inter Workflow Collection Agent in a Real-Time Workflow - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739394/Inter+Workflow+Collection+Agent+in+a+Real-Time+Workflow
**Categories:** chunks_index.json

To open the Inter Workflow collection agent configuration, click Build>New Configuration . Select Workflow from the Configurations dialog. When prompted to select a workflow type , click Realtime . Click Add agent and select Inter Workflow from the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent to display the Agent Configuration dialog. Inter Workflow Tab Open Inter Workflow collection agent real-time configuration dialog - Inter Workflow tab Setting Description Setting Description Profile Click Browse to select a profile to assign to the agent. All workflows in the same workflow configuration can use separate Inter Workflow profiles, if that is preferred. In order to do that the profile must be set to Default in the Workflow Table tab found in the Workflow Properties dialog. After that each workflow in the table can be appointed different profiles. Use Custom Stream Select this checkbox to enable Stream ID-based connections across multiple workflows using the same profile. When checked, collector and forwarding agents establish connections based on both the profile and a Stream ID, allowing workflows to link dynamically. In cases where a real-time workflow connects to a batch workflow, they scale as a unit, ensuring backend/frontend pairs stay linked via the Stream ID. Note! If this checkbox is cleared, the Inter Workflow profile is fixed at design time and cannot be changed dynamically, preventing chained workflows from scaling. Example - Configuring a Stream ID in Inter Workflow Forwarder and Collection Agents Scenario: You have multiple processing workflows that each need to send data to a specific collection workflow. Instead of creating separate Inter Workflow profiles for each pair, you can configure a stream ID to manage these connections within a single profile. For example, if three processing workflows (A, B, and C) need to send data to three corresponding collection workflows (X, Y, and Z), you can define stream IDs like "A-X", "B-Y", and "C-Z". This ensures each processing workflow sends data to the correct collection workflow while maintaining a simpler, more scalable configuration. Stream ID If you have checked the Use Custom Stream checkbox add a Stream ID. Decoder Tab Open Inter Workflow collection agent real-time configuration dialog - Decoder tab Setting Description Decoder Click Browse to select from a list of available decoders created in the Ultra Format Editor, as well as the default built-in decoders: CSV Format JSON Format MZ Tagged Format Different settings are available depending on the Decoder you select. Full Decode This option is only available when you have selected a decoder created in the Ultra Format Editor. Select this option to fully decode the UDR before it is sent out from the decoder agent. This action may have a negative impact on performance, since not all fields may be accessed in the workflow, making decoding of all fields in the UDR unnecessary. If it is important that all decoding errors are detected, you must select this option. If this checkbox is cleared (default), the amount of work needed for decoding is minimized using "lazy" decoding of field content. This means that the actual decoding work may be done later in the workflow, when the field values are accessed for the first time. Corrupt data (that is, data for which decoding fails) may not be detected during the decoding stage, but can cause a workflow to abort later in the process. MZ Tagged Specific Settings Tagged UDR Type Click the Add button to select from a list of available internal UDR formats stored in the Ultra and Code servers to reprocess UDRs of an internal format and send them out. If the compressed format is used, the decoder automatically detects this. JSON Specific Settings UDR Type Click Browse to select the UDR type you want the Decoder to send out. You can either select one of the predefined UDRs or the DynamicJson UDR, which allows you to add a field of type any for including payload. Unmapped Fields If you have selected DynamicJson as UDR Type , you can select the option data in this field in order to include payload. If you have selected another UDR type that contains an any, or a map field, you can select to put any unmapped fields into the field you select in this list. All fields of any or map type in the selected UDR type will be available. If set to (None), any unmapped fields will be lost. Schema Path Enter the path to the JSON schema you want to use in this field. CSV Specific Settings UDR Type Click Browse to select the UDR type you want the Decoder to send out. You can either select one of the predefined UDRs or the DynamicCsv UDR if the CSV format is not known. Format Select the CSV format you want to use; Unix , Mac , Windows , or Excel , or select to define your own customized format. If you select Custom , the following four settings will be enabled. Delimiter Enter the delimiter character(s) for the fields in the CSV. Use Quote Select this option if quotes are used in the CSV. Quote If Use Quote is selected, enter the type of quotes used in the CSV. Line Break Enter how line breaks are expressed in the CSV.

---

# Document 1046: Changing Database Password - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657646/Changing+Database+Password
**Categories:** chunks_index.json

Use the procedure below to change the password of the users mzadmin and mzowner in Oracle, PostgreSQL and SAP HANA databases. Note! A prerequisite for this to work properly is that the database is only used by the Platform and not by any configurations. Disable all workflow groups, either from the Execution Manager in Desktop, or by using the mzsh wfdisable command. Let all batch workflows finish execution. Note! Batch workflows containing Inter Workflow agents that do not have the Deactivate on Idle check box selected will have to be shutdown manually. Use the command mzsh topo get to list the jdbc parameters. $ mzsh topo get -s --format data-only topo://container:<mz.container>/pico:platform/obj:config.properties.mz.jdbc Example - List of jdbc parameters mzsh topo get -s --format data-only topo://container:platform1/pico:platform/obj:config.properties.mz.jdbc { "password": "DR-4-48851644227183C2041D838568E117EC", "oracle": { "ons": "" }, "type": "oracle", "user": "mzadmin", "url": "jdbc:oracle:thin:@//<install.ora.host>:1521/MZ" } Use the mzsh topo set command to update the password for mzadmin: $ mzsh topo set topo://container:<mz.container>/pico:platform/val:config.properties.mz.jdbc.password `mzsh encryptpassword <new password>` Example - Updating mzadmin password $ mzsh topo set topo://container:platform1/pico:platform/val:config.properties.mz.jdbc.password `mzsh encryptpassword mypassword` Shut down the platform using the mzsh shutdown command. Update the Oracle, PostgreSQL or SAP HANA passwords to correspond to your new mzadmin password. Startup the Platform. Enable all workflow groups and start all workflows again.

---

# Document 1047: SFTP Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674742/SFTP+Collection+Agent
**Categories:** chunks_index.json

The SFTP collection agent collects files from a remote host and inserts them into a workflow, using the SFTP protocol over SSH2. Upon activation, the agent establishes an SSH2 connection and an SFTP session towards the remote host. If this fails, additional hosts are tried, if configured. On success, the source directory on the remote host is scanned for all files matching the current filter. In addition, the Filename Sequence service may be utilized for further control of the matching files. All files found will be fed one after the other into the workflow. When a file has been successfully processed by the workflow, the agent offers the possibility of moving, renaming, removing, or ignoring the original file. The agent can also automatically delete moved or renamed files after a configurable number of days. In addition, the agent offers the possibility of decompressing (gzip) files after they have been collected before they are inserted into the workflow. When all the files have been successfully processed, the agent stops, awaiting the next activation, scheduled or manually initiated. The SFTP collection agent supports IPv4 and IPv6 environments. If you require to run the SFTP collection agent in a real-time workflow, for further information, see Batch-Based Real-Time Agents . Loading

---

# Document 1048: Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610886/Agent+Configuration
**Categories:** chunks_index.json

The agent configuration object specifies the configuration interface between the agent and the system core. It is specified by extending the DRAbstractConfigObject using application specific annotations for configuration object information such as field and type names. This chapter includes the following sections: Annotations Configuration Object Example Porting agent configuration from Version 8 DTK

---

# Document 1049: External Reference Profile Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639604/External+Reference+Profile+Configuration
**Categories:** chunks_index.json

To create a new External Reference Profile configuration, click the New Configuration button in the upper left part of the Desktop window, and then select External Reference Profile from the menu. The contents of the menus in the menu bar may change depending on which configuration type has been opened. The External Reference profile uses the standard menu items and buttons that are visible for all configurations, and these are described in Common Configuration Buttons . Open External Reference Profile Configuration Setting Description Setting Description External Reference Type From the drop-down list select the External Reference source type. The following types are available: S3 Properties File - A file on an Amazon Web Service S3 bucket. For more information, refer to AWS S3 Support for External Reference Profile Properties File - A file on the Platform Container host Environment Variable - Exported environment variables from the Platform's startup shell Properties Database - Use the internal database as an External Reference source. To store values in the internal database, use the web interface and enter values for needed Database Keys. Local Key The name to use when referring to this external reference in the workflow table and profiles which allow the use of external references. Properties File Key This field is available when you have selected Properties File in External Reference Type . The name of the External Reference in the properties file or environment variable. Database Key This field is available when you have selected Properties Database in External Reference Type . The name of the External Reference in the Database. Value The current value of the External Reference. To update the value from the Properties file, use the Refresh button. If values are not displayed, make sure that a properties file is available in the specified path or that the environment variables are set. External Reference Applications With External Reference Profile and External Reference Values , you can set different type of permissions for saving External Reference Profile keys and values. To know more about these applications, see Access Controller Applicable only if you select Properties Database from External Reference Type. For example, the following screenshot shows an access group with the name as 'Values' and different permission that you can assign to that access group for External Reference Values application. Open Access Controller - Applications List The following permissions are available: Write - In order to create a new External Reference profile you must have Write access to the External Reference Profile application, however to be able to edit or set Value, you must have Write access to the External Reference Values application. If you try to set or edit Value of an External Reference Profile for which you do not have the write permission a dialog window appears stating that you need the correct permission. Execute - You cannot make any changes to the External Reference Values permission if you have only Execute access. To be able to open an existing External Reference Profile, you must have the Read access for that particular External Reference Profile. Read Configuration Browser to know more about the different permission groups pertaining to a configuration.

---

# Document 1050: Desktop User's Guide - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638163
**Categories:** chunks_index.json

Search this document: Chapters The following chapters are included: Desktop Overview Working with Workflows Event Notifications Alarm Detection Inspection Tools & Monitoring Appendix 1 - Profiles Appendix 2 - Batch and Real-Time Workflow Agents Appendix 3 - Task Workflow Agents Appendix 4 - Collection Strategies Legacy Desktop

---

# Document 1051: Conditional Trace Templates - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611035/Conditional+Trace+Templates
**Categories:** chunks_index.json

To be able to perform traces on workflows in Desktop, there must be at least one Conditional Trace template defined. Note! You can use regular expressions in the Conditional Trace templates, but be careful of overusing " .*" . The more specific the filter is, the better the result you will get in terms of minimized amount of output and performance impact. You can create templates for any real-time workflows, giving you results for all agents in the workflow, and you can also use the trace() APL function in Analysis or Aggregation agents to get results for specific functions in the APL code. This section describes:

---

# Document 1052: SAP CC REST UDRs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/319717377
**Categories:** chunks_index.json

The SAP CC REST UDR types are designed to formalize the exchange between workflows and the SAP Convergent Charging Core Server. The SAP CC REST agent translates the received charging or loading request UDRs into its corresponding SAP Convergent Charging API request and the response from the server is translated into the appropriate UDRs stored in the same request UDR provided, which becomes available in the workflow for subsequent processing. The UDRs were generated from the API specification provided by SAP for both the Charging and Loading services, and the released API version. The REST API version configured on the SAP CC REST agent must be aligned with the REST API version used by the SAP CC server for Charging and Loading services to be sent and received. Prerequisites The reader of this information should be familiar with: SAP Convergent Charging API for Charging and Loading Services ( SAP Business Accelerator Hub ) Overview There are two primary request UDRs, the usage being dependent on which REST API service is configured on the agent: SAPCCChargingRequest UDR. The main UDR expected by the SAP CC REST agent when the Charging Service REST API is selected and can be returned to the agent with a response attached. SAPCCLoadingRequest UDR. The main UDR expected by the SAP CC REST agent when the Loading Service REST API is selected and can be returned to the agent with a response attached. Supported Services Within each services provided by SAP Convergent Charging, there are a number of possible APIs to be called. The following table details the currently supported services and APIs, along with the expected UDR to be passed into the primary request UDRs for each. SAP Service SAP Service APIs Request UDR Response UDR SAP Service SAP Service APIs Request UDR Response UDR Charging Service Event Based Rating /chargeableItemProcess ChargingRequest ChargingResponse , OnErrorResponse Session Based Rating (Create session) /chargingSessions ChargingSessionCreateRequest ChargingSessionCreateResponse , OnErrorResponse Session Based Rating (Update or Stop session) /chargingSessions/{sessionId} ChargingSessionUpdateRequest , ChargingSessionStopRequest ChargingSessionUpdateResponse , OnErrorResponse Loading Service Usage Event Loading /chargeableItemsLoad List of ChargeableItem List of LoadingResponse , OnErrorResponse The request UDR needs to be passed into either SAPCCChargingRequest or SAPCCLoadingRequest s request parameter, as shown below, using the above Event Based Rating API as an example. SAPCCChargingRequest requestUdr = udrCreate(SAPCCChargingRequest); ChargingRequest udr = udrCreate(ChargingRequest); ...... requestUdr.request = udr; Response Handling Responses from SAP Convergent Charging are returned under the original request UDR, in the request.response.body parameter. The output will first have to be casted to the correct UDR, then cast the body into the respective UDRs described above, for further processing. An example using the above Event Based Rating API as below. if(instanceOf(input, SAPCCChargingRequest)){ SAPCCChargingRequest resp = (SAPCCChargingRequest) input; if(instanceOf(resp.response.body, ChargingResponse)){ // Logic if response body is ChargingResponse } else{ // Logic if response body is not ChargingResponse } } else if(instanceOf(input, erroUDR)){ // Logic if there are errors when sending request } Error Handling The response HTTP status code can be accessed using input.response.statusCode . For HTTP status code 200, the error will be present in the APIs respective response UDR, as response.error . An example using the Event Based Rating API as below. SAPCCChargingRequest resp = (SAPCCChargingRequest) input; if(instanceOf(resp.response.body, ChargingResponse)){ ChargingResponse response = (ChargingResponse) resp.response.body; if(response.error != null){ // Logic to handle API error } } The response UDR OnErrorResponse from SAP Convergent Charging will be returned in the event of a HTTP status code of 400, 401, 403 or 500. The output UDR errorUDR will only be routed if the Route Error UDR option is enabled in the agent configuration. The UDRs for the use of SAP CC REST agent are generated from the API specifications provided by SAP. For more information on structure of the API requests and responses, you may refer to the following. Charging Service - Event Based Rating: SAP Business Accelerator Hub Charging Service - Session Based Rating (Create Session): SAP Business Accelerator Hub Charging Service - Session Based Rating (Updated or Stop Session): SAP Business Accelerator Hub Loading Service - Usage Event Loading: SAP Business Accelerator Hub The list of available SAP CC REST UDRs can be viewed in the UDR Internal Format Browser in the sapcc_rest folder. To open the browser, open an APL Editor, and, in the editing area, right-click and select UDR Assistance . SAPCCChargingRequest The following fields are included in the SAPCCChargingRequest UDR: Field Description Field Description request (DRUDR) The following UDRs can be included in this field: ChargingRequest ChargingSessionCreateRequest ChargingSessionStopRequest ChargingSessionUpdateRequest requestUri (string) The request URI included in the response from SAP Convergent Charging. response (Response (http)) This field is populated by the contents of SAP CC REST UDRs | Response . sessionId (string) The sessionId identifier when sending a request to Session Based Rating (Update or Stop session). SAPCCLoadingRequest Field Description Field Description request (list<DRUDR>) The following Loading request item can be included in this field: List of ChargeableItem requestUri (string) Set the URI path to be used for the request. response (Response (http)) This field is populated by the contents of SAP CC REST UDRs | Response . errorUDR Field Description Field Description errorMessage Description of the error. originalUDR The input UDR received by the agent. Cookie The Cookie UDR is used for including cookie information in SAP CC REST UDRs | Response . The following fields are included in the Cookie UDR: Field Description Field Description domain (string) The domain to which the cookie belongs. expires (string) The date and time when the cookie expires. httponly (boolean) If set to true , cookies will not be exposed through channels other than HTTP and HTTPS requests. name (string) The name of the cookie. path (string) The path to the page the cookie belongs to, for example "http://mydomain.com/mypage.html ". secure (boolean) Indicates whether the data is sent in secure mode or not. value (string) The value of the cookie. Response The Response UDR contains the response to the request and is incorporated in the SAP CC REST UDRs | SAPCCChargingRequest and SAP CC REST UDRs | SAPCCLoadingRequest . The following fields are included in the Response UDR: Field Description Field Description body (any) This field contains the message body, that is, the payload, of the response. Responses are present as follows: For Charging Service Requests: ChargingResponse ChargingSessionCreateResponse ChargingSessionUpdateResponse OnErrorResponse For Loading Service Requests: LoadingResponse OnErrorResponse cookies (list<CookieUDR>) This field contains any SAP CC REST UDRs | Cookie UDRs included. headers (map<string,list<string>>) This field is populated by all the headers present in the response. The map keys contain header field names and the content is stored in the corresponding map values. httpVersion (string) This field contains the HTTP version; HTTP/1 or HTTP/2. mimetype (string) The type of the content, for example "text/html", "image/gif", etc. openAPIUDR (DRUDR) Used for custom OpenAPI specification validation, which is currently unavailable. statusCode (int) The HTTP status code, for example: 200 - OK, 404 - Not Found, etc. validationErrors (string) Used for custom OpenAPI specification validation, which is currently unavailable.

---

# Document 1053: Data Hub Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644935
**Categories:** chunks_index.json

The Data Hub forwarding agent is a batch agent that bulk loads data to an Impala database specified by a Data Hub Profile . Loading

---

# Document 1054: Password UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205002453/Password+UDR
**Categories:** chunks_index.json

The Password UDR is used to create an input field for passwords. It acts like a text field but the text entered will be hidden by dots. You can use the following APL code to create a password field that is required Password password = udrCreate(Password); password.label = "Password"; password.required = true; The following fields are included in the Password UDR : Field Description attributes (map<string,string>) This field may contain extra attributes to be added. cssClasses (list<string>) This field may contain a list of extra values added to class attribute. This is typically used to style the component. Please read more on Bootstrap . disabled (boolean) This field may contain a boolean if the component should be disabled or enabled. id (string) This field may contain the id of the component label (string) This field may contain the label for the password. labelCssClasses (list<string>) This field may contain a list of extra values added to class attribute of the label. This is typically used to style the component. Please read more on Bootstrap . name (string) This field may contain the name of the component. If the component is present in a Form UDR , the name will be submitted with the form as the key in the Params Map in Request UDR . placeholder (string) This field may contain a placeholder can be used as a help text. readonly (boolean) This field may contain a boolean if the field is readonly. required (boolean) This field may contain a boolean if the component is required. Typically used inside a Form UDR. value (int) This field may contain a value.

---

# Document 1055: Updating Instance Settings - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204675827/Updating+Instance+Settings
**Categories:** chunks_index.json

You can update the connection parameters and Desktop properties for manually added instances from the Instance Settings dialog. Click the cogwheel icon ( ) next to the name or right-click and then select Instance Settings from the pop-up menu. Open Connection Parameters The URL to start the Desktop is specified in the following format: <protocol>://<ip address or hostname>:<port>/launch/desktop The protocol can be either http or https. Properties Tab Each instance that you have added to the Desktop Launcher can be configured with a set of properties that control display and behavior. To update the properties, select the Properties tab. The values that are grayed out are set in the Platform Container and fetched by Desktop Launcher. Enter a new value in a field to override its initial value. To revert back to the initial values, clear all text in the field. You can also add properties that are not already set in the Platform Container by using the rows at the bottom of the table. Security Tab The Security tab contains settings related to digital certificates that are required for a secure connection, via HTTPS, and the cryptographic key that is required when client authentication is enabled on the Platform. Open The Platform Certificate Chain field contains a certificate downloaded from the Platform. This field is for information only. The first time that you establish a secure connection to the Platform, the Trusted Certificates field will be populated automatically . If the certificate from the Platform changes, you will be prompted to accept the new certificate. Alternatively, right-click on the downloaded certificate and select Add As Trusted Certificate . When client authentication is enabled, you must import a key in the Client Key field. You can import the key from a keystore file or you may copy the key from an existing instance in the Desktop Launcher. Importing Key from File Right-click on the field and select Import Key from File . Select the keystore file and then click OK . Enter the password at the prompt. Copy Key from Existing Instance or Launcher Service Right-click on the Client Key field and select Copy Key From Instance or Copy from Service. Select a name of an instance or launcher service from the sub-menu. To remove an instance Remove an instance by right-clicking on the name and then select Delete from the pop-up menu . In rare cases, locally stored runtime data may prevent the Desktop from starting correctly. To remove this data, right-click on the name and then select Delete Runtime Data from the pop-up menu.

---

# Document 1056: SFTP Forwarding Agent Transaction Behavior - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643085
**Categories:** chunks_index.json

This section includes information about the SFTP forwarding agent transaction behavior. For information about the general transaction behavior, see Transactions in Workflow Monitor . Emits This agent does not emit anything. Retrieves The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Begin Batch When a Begin Batch message is received, the temporary directory DR_TMP_DIR is first created in the target directory, if not already created. Then, a target file is created and opened in the temporary directory. End Batch When an End Batch message is received, the target file in DR_TMP_DIR is closed and, finally, the file is moved from the temporary directory to the target directory. If backlog functionality is enabled, an additional step is taken where the file is moved from DR_TMP_DIR to DR_POSTPONED_MOVE_DIR and then to the target directory. If the last step failed, the file will be left in DR_POSTPONED_MOVE_DIR and marked as backlogged. Cancel Batch If a Cancel Batch message is received, the target file is removed from the DR_TMP_DIR directory.

---

# Document 1057: Disk Agents in Batch Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205685116/Disk+Agents+in+Batch+Workflows
**Categories:** chunks_index.json

This section describes how to configure the Disk agents in batch workflows. The section contains the following subsections: Disk Collection Agent - Batch Disk Forwarding Agent - Batch

---

# Document 1058: GTP' LGU Collection Agent UDR - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205652917/GTP+LGU+Collection+Agent+UDR
**Categories:** chunks_index.json

The UDR type used by the GTP' LGU Collection agent can be viewed in the UDR Internal Format Browser. To open the browser, open an APL Editor, and, in the editing area, right-click and select UDR Assistance . GTPCollectionUDR The following fields are included in the GTPCollectionUDR format: Field Description Field Description dataRecords (list<GTPDataRecord>) This field contains a list of the GTP data records in bytearray and the length of the record in integer. The list contains the following fields for each element: dataRecords(bytearray) length(int) port (int) This field contains the port number of the GTP' server. sequenceNumber (long) This field contains the sequence number of the collected data records. source (string) This field should contain the source from where the data was collected. This can be the IP address or hostname of the GTP' server.

---

# Document 1059: Out of Memory Info in System Log - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204744580/Out+of+Memory+Info+in+System+Log
**Categories:** chunks_index.json

If a pico server instyance runs out of memory during, for example, a system import, the System Log might not be properly updated with the out of memory information. To make sure that this critical error information is properly stored in System Log, an oom script is run each time an OutOfMemoryError exception is thrown. The JVM is started using the -XX:OnOutOfMemoryError=oom flag, which enables the JVM to catch all OutOfMemoryError exceptions from the oom script. OOM Script The oom script uses three parameters to generate output: $1 - the name of the running pico process, for example, ec1 or platform, where the OutOfMemoryError occurred $2 - the regular log file, for example ec1.log or platform.log, where further information is stored $3 - the name of the critical error log file where the OutOfMemoryError info ($1 and $2) is added, together with current time and hostname The name of the critical error log file is, in case of a platform process, platform_criticalerror.log and, in case of an EC process, for example, ec1_criticalerror.log. Note! It is not allowed to update the oom script to return any other info, since this might cause the logging to System Log to fail or incorrect info will be included in System Log. Each time the platform or an EC is restarted, the contents of the critical error log will be added to the System Log (one line per OutOfMemoryError occasion) and the critical error log will then be removed. Example - OOM info in the Critical Error Log Tue Jan 17 17:21:11 CET 2012 OutOfMemoryError on ec1at host-e6410. See ec1.log for more information User Defined Script It is possible to use a user defined script for sending, for example, e-mail notifications related to the OutOfMemoryError exception. This script must be called user_oom and located in MZ_HOME/bin. If user_oom is available, the oom script will call it and forward the $1 and $2 parameter values.

---

# Document 1060: FTP Collection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641346/FTP+Collection+Agent
**Categories:** chunks_index.json

The FTP collection agent collects files from a remote file system and inserts them into a workflow, using the standard FTP (RFC 959) protocol. When activated, the collector establishes an FTP session towards the remote host. On failure, additional hosts are tried if so configured. On success, the source directory on the remote host is scanned for all files matching the current Filename settings, which are located in the Source tab. In addition, the Filename Sequence service may be used to further control the matching files. All files found will be fed one after the other into the workflow. The agent also offers the possibility to decompress compressed (gzip) files after they have been collected before they are inserted into the workflow. When all the files are successfully processed, the agent stops to await the next activation, scheduled or manually initiated. The FTP collection agent supports IPv4 and IPv6 environments. If you require to run the FTP collec tion agen t in a real-time workflow, for further information, see Batch-Based Real-Time Agents . Loading

---

# Document 1061: Additional Platform Properties in install.xml - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736101
**Categories:** chunks_index.json

This page contains additional sets of platform related properties described below in the install.xml file. The properties are described in the order they appear in the install.xml file. The properties are categorized as follows: 1 General Database Properties 2 3pp Properties 3 Common Properties 4 Config Properties 5 Notification Properties 6 Execution Context Properties - Not Used in Platform Only Installations 7 Desktop Properties General Database Properties Property Description Property Description mz.jdbc.type Default value: derby This property specifies the platform database you want to use. The supported options are derby , oracle , postgresql , or saphana . mz.jdbc.user Default value: mzadmin This property specifies the username of the database that the Platform will use during runtime. mz.jdbc.password Default value: mz This property specifies the password of the jdbc user. The password can be changed at a later stage. For further information about changing the database password, see Changing Database Password in the System Administrator's Guide . mz.jdbc.url Default value: "" This property can be used to explicitly specify the JDBC URL. If it is not specified, it is calculated based on the other database parameters described above. 3pp Properties Property Description Property Description mz.3pp.dir Default value: /opt/3pp This property specifies the directory where to store 3pp files required by various parts of the system, such as jar files for Oracle, the SAP RFC agent, the SAP JCo agent, the SAP CC agents, the JMS agents, Data Veracity, the Kafka profile, and the various DB connectors. The files are moved to the $MZ_HOME/3pp directory during installation. Note! Ensure that the directory specified for this property is located outside of MZ_HOME during installation. Common Properties Property Description Property Description pico.rcp.platform.host Default value: "" Example value: 192.168.0.190 This property specifies the IP address or the hostname of the Platform to be used by other pico instances such as Execution Contexts, Service Contexts, or Command Line. If a failover occurs and you have entered a hostname as the value of this property, the hostname is retrieved from the DNS enabling reconnection. If you have entered a static IP address as the value of this property, reconnection issues may occur after a failover, if the IP address has changed. pico.rcp.platform.port Default value: 6790 This property specifies the port for connecting the Execution Contexts to the Platform. pico.rcp.server.host Default value: "" This property specifies the IP address or hostname of the pico instances. It is used to determine the interface that the pico instances must bind to and the IP address/hostname that to use by connecting processes. If a failover occurs and you have entered a hostname as the value of this property, the hostname is retrieved from the DNS enabling reconnection. If you have entered a static IP address as the value of this property, reconnection issues may occur after a failover, if the IP address has changed. When the value of this property is left blank, the pico instance binds to all IP addresses of the host. This means that the pico listens for inbound network traffic on all network interfaces, and may attempt to use any local IP address for outbound network traffic. Note! If the host has more than one IP address, this property has to be set with the correct IP address. Make sure to set the property if you use IPv6, or if a high availability environment is configured. For information about high availability, see High Availability . mz.webserver.port Default value: "9000" This property specifies the port of the Platform Web Interface. When installing a second MediationZone instance, this port value must be replaced in order to not clash with the first instance. Config Properties Property Description Property Description mz.name Default value: DR This property specifies the name of the MediationZone deployment. Notification Properties Property Description Property Description mz.notifier.mailfrom Default value: "" Example value: mzadmin@company.com This property specifies the sending signature to use for event generated e-mails, presented as a label. mz.mailserver.host Default value: "" Example value: SMTP.server This property specifies the name or IP address of the mail server to use for event generated e-mails. Execution Context Properties - Not Used in Platform Only Installations Property Description Property Description mz.eclist Default value: ec1 This property specifies the EC(s) to be installed. Only applicable if install.types includes ec. ec.backlog.dir Default value: ${mz.home}/tmp This property specifies the directory where ECs can store their backlogged events. If this parameter is removed, the EC's events are not logged. Only applicable if install.types includes ec. ec.webserver.enabled Default value: true This property specifies if the web server for Execution Contexts should be active. Only applicable if install.types includes ec. ec.webserver.port Default value: 9090 This property specifies the port of the EC web server. Only applicable if install.types includes ec. Note! If you specify several ECs, you need to modify this value to use separate port numbers for each EC once the installation is completed. Do this by using the mzsh topo set command: mzsh topo set topo://container:<container>/pico:<name>/val:config.properties.ec.webserver.port <new_port_number> Desktop Properties Property Description Property Description ui.webserver.port Default value: 9001 This property specifies the port of the Desktop interface. Only applicable if install.types includes ui.

---

# Document 1062: System Overview - Security - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646979/System+Overview+-+Security
**Categories:** chunks_index.json

This chapter includes the following sections: Access Zone - Security Execution Zone - Security Control Zone - Security

---

# Document 1063: Python Connector Agent Events - Real-Time - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204608778/Python+Connector+Agent+Events+-+Real-Time
**Categories:** chunks_index.json

Agent Message Events

---

# Document 1064: SAP RFC Agent Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609222/SAP+RFC+Agent+Preparations
**Categories:** chunks_index.json

This document contains the steps to set up the SAP JCo library's jar file in Platform and Execution Container. To be able to use the SAP RFC profile and processing agent: Extract the SAP JCo library binaries: tar xvfz sapjco3-<platform_version>.tgz Add the extracted file sapjco3.jar to the classpath for the jar files for the relevant EC and the Platform. In the example below, the SAP jar files are located in MZ_HOME/3pp. The configuration includes the jar files required to use any of the SAP CC agents, and the Platform database is Derby. Note! Ensure that you include existing paths, so that they are not overwritten. $ mzsh topo get topo://container:<container>/pico:<ec>/obj:config.classpath $ mzsh topo get topo://container:<container>/pico:platform/obj:config.classpath Example - Setting classpath EC (ec1): $ mzsh topo set topo://container:main1/pico:ec1/obj:config.classpath.jars ' ["lib/picostart.jar", "3pp/common_message.jar", "3pp/common_util.jar", "3pp/core_chargingplan.jar", "3pp/core_chargingprocess.jar", "3pp/core_client.jar", "3pp/logging.jar", "3pp/sap.com~tc~logging~java.jar", "3pp/sapjco3.jar"]' Platform: $ mzsh topo set topo://container:main1/pico:platform/obj:config.classpath.jars ' ["lib/derby.jar", "lib/picostart.jar", "lib/javassist.jar", "lib/codeserver.jar", "lib/codeserver_common.jar", "3pp/sapjco3.jar"]' Create a jni folder in the MZ_HOME directory in both the Platform and Execution Container. Place the libsapjco3.so file in both the MZ_HOME/jni directories. Add the MZ_HOME/jni directory as an additional library file along with MZ_HOME/common/lib/native using the following topo command. mzsh topo set topo://container:main1/val:common.java.library.path '${mz.home}"/common/lib/native:"${mz.home}"/jni"' Note! If the common/lib/native directory does not exist in your MZ_HOME, you can create the directory before running the command above.

---

# Document 1065: Pico Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030732
**Categories:** chunks_index.json

The Pico Management interface provides a comprehensive view of the various pico instances that are currently deployed as well as allow you to create and manage the pico instances. Open Pico Management dashboard view The dashboard displays a list of all pico instances in the system. You can search and filter them for easy viewing. Columns and Buttons Description Columns and Buttons Description Name Name of the pico instances. Type The pico instance type of the JVM will be displayed here. These types will be labeled as platform , ec , sc , and ui Status The status of the JVM will be shown here. If the pico instance is up and can be pinged, it will be shown as running . If it cannot be pinged, it will be shown as unreachable . If the pico instance is shut down it will show as not-started . Memory Use The amount of memory being used by the pico instance will be shown here. Response Time The response time of the pico instance will be shown here. Up Time The amount of time that the pico instance has been running will be shown here. EC Groups If the pico instance belongs to any EC Groups, these will be shown here. Actions These are buttons that allow you to make changes to the selected pico. Edit - Allows you to make changes to the properties related to that particular pico instance. See Editing a Pico Instance for more details. Details - Allows to check more details about the pico instance from the Pico Management dashboard. Restart - Restarts the pico instance from the Pico Management dashboard. Stop - Stops the pico instance from the Pico Management dashboard. Start - Starts the pico instance from the Pico Management dashboard. Delete - Removes the pico instance from the Pico Management dashboard. Managing the Pico Instances You can start, stop or restart pico instances in the Pico Management dashboard. Selecting one or more of the pico instances will allow you to perform any of the highlighted actions shown at the top of the pico list. The available actions depend on the type of pico instance. Open Selecting multiple pico instances to start. Adding a Pico Instance You can add an EC or SC pico instance using the New Pico button on the Pico Management dashboard. The following are properties found in the Create New Pico dialog: Property Description Property Description Pico Container Choose between all available containers in your installation to list the pico instance under. This is a mandatory field. Name Provide a unique identifier for your pico instance. The name cannot be shared across other pico instances and has to be unique. This is a mandatory field. Template Choose between the standard-ec template or the standard-sc template for your pico instance. You can then make changes to the JSON template in the text field below, such as add on properties in Execution Context for EC's and Service Context Properties for SC's. Open Create New Pico dialog window with the standard-ec option selected. Editing a Pico Instance You can add or modify properties pertaining to the particular pico instance as well as their values. The Edit Pico dialog consists of a JSON template text field where you can add new properties, edit the values and View Resolved Config . The View Resolved Config lets you view the current valid configuration for that particular pico instance. You cannot make changes to the text field while viewing the resolved config. For more details to the properties you can add, refer to: Platform Properties for the Platform pico. Execution Context for Execution Context pico. Service Context Properties for Service Context pico. Open Edit Pico dialog window for an ec container. Details of a Pico Instance The user can learn more about each pico instance by clicking Details in the Actions buttons, where its possible to see information separated by the following tabs: Details - Overview Tab Details - Application Tab Details - Virtual Machine Tab Details - Operating System Please note that the information presented in the tabs comes directly from JMX MBeans and is primarily intended for troubleshooting activities.

---

# Document 1066: Salesforce Streaming API Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001720/Salesforce+Streaming+API+Agent
**Categories:** chunks_index.json

This section describes the Salesforce Streaming API agent. The agent is a processing agent for real-time workflow configurations allowing you to subscribe to Salesforce topics and receive messages. The Salesforce Streaming API agent can be used to connect to Salesforce as a client and listen to changes to specific objects. The agent uses the Salesforce Streaming API. This allows the workflow to receive requests in real-time from Salesforce for creating, updating, or deleting users, for example. Open Example workflow using the Salesforce Streaming API agent Prerequisites The reader of this information should be familiar with: The Salesforce Streaming API ( https://developer.salesforce.com/docs/atlas.en-us.api_streaming.meta/api_streaming/intro_stream.htm ) This section includes the following subsections: Salesforce Streaming API Agent Configuration Salesforce Streaming API UDRs Salesforce Streaming API Agent Input/Output Data and MIM Salesforce Streaming API Agent Events

---

# Document 1067: Disconnect ECs - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/240844801/Disconnect+ECs
**Categories:** chunks_index.json

ECs can be disconnected from the Platform. This allows workflows and services to continue processing throughout the upgrade as long as the Execution Contexts are installed in a separate container from the platform. Any standalone workflow can continue executing during the platform container upgrade, the downtime of the workflows will be minimized to the upgrade of the individual Execution Container, and each container can be handled separately. To disconnect the ECs, run the following command: $ mzsh mzadmin/<password> disconnect <ec name> ... Note! Disconnected ECs may still attempt to contact the Platform. These attempts will fail and cause warning messages (No open connection to <Platform host> in the log files. Ignore these warning messages.

---

# Document 1068: ADLS2 File Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998797/ADLS2+File+Forwarding+Agent+Configuration
**Categories:** chunks_index.json



---
**End of Part 46** - Continue to next part for more content.
