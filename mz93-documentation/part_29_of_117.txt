# RATANON/MZ93-DOCUMENTATION - Part 29/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 29 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~61.2 KB
---

The Workflow Monitor controls workflow execution and presents a detailed view of the workflow execution status. Agent states and events can be monitored during workflow execution. The monitor also allows for dynamic updates of the configuration for certain agents by sending commands. A command can, for example, tell an agent to flush or reset data in memory. For further information about applicable commands, see the section for the relevant agent. See also Dynamic Update below. Note! The Workflow Monitor can apply commands to only one workflow at a time, in one monitor window per workflow. The monitor functionality is not available for groups or the whole workflow configuration. The Workflow Monitor displays the active version of the workflow. Monitoring a workflow does not imply exclusive rights to start or stop it, the workflow can be activated and deactivated by another user while monitored, or by scheduling. Opening the Workflow Monitor You can open the workflow monitor from either a workflow configuration or the Execution Manager . To open the monitor from a workflow configuration: Click the ( Run ) button. From the Run Workflow dialog, Select the workflow you want to view in the workflow monitor. Click the Run button to open the selected workflow in the workflow monitor. The workflow will only run once you click on the Start button in the workflow monitor. Open Workflow Monitor To open the monitor from the Execution Manager, see the section The Right-Click Menu, in the Execution Manager . Viewing Agent Events While a workflow is running, agents report progress in terms of events. These events can be monitored in the two Agent debug panels at the bottom. You can also Enable Debug to view more events or messages from the debug command in the Analysis or Aggregation agents. Note! When you Enable Debug , it may slow down the workflow due to logging, especially if you set it to log debug into files in the workflow properties. For more regarding debug in workflow properties, refer to Debug Type in the Execution Tab . The following steps must be taken in order to view these events. To view events for all agents in the workflow, click Select All from either of the Agent debug panels. To only view events for some selected agents, click Select agent(s) to monitor event from either of the Agent debug panels and select only the agents you want to view events for. Workflow Execution States All running workflows are monitored by the workflow server on the Platform. A workflow can be in any one of the following states: Open The workflow state diagram State Description Building When a workflow, or any of a workflow's referenced configurations, are being re-built, for example when saving or recompiling, the workflow will be in the Building state. When a workflow is in the Building state, in the worklow monitor, the text "Workflow is building" is also displayed. Workflows started by scheduling configurations will wait until the workflow leaves the Building state before they start. Executed A Workflow becomes Executed after one of the following: Note! This is a transient state, after the workflow has run, where cleanup is handled. This state is not reached if the workflow is aborted. In the GUI, the workflow is shown as Stopped even after it has reached the Idle state. Hold A workflow that is in the Idle state and is being imported - either by the mzsh systemimport r | sr | sir | wr or by the System Importer configured to Hold Execution - enters the Hold state until the import activity is finished. The Workflow group then resumes its Idle state. Idle A workflow is in Idle state if there is no activity going on in the workflow. It means that the workflow is not running, not in process of starting or stopping, or not in any other state (Invalid or Hold). After execution, although the workflow is indeed Idle, the state space on the display may remain as one of the following states: Invalid The workflow configuration is erroneous. Once you correct the error, the workflow assumes the Idle state. Note! The workflow cannot be executed if it is in invalid state. Loading The platform is uploading the workflow to the Execution Context. When the transfer is complete, the Execution Context initializes the agents. When the workflow starts running the state changes to Running. Running The workflow is currently executing. Unreachable If the Platform fails to establish connection with the EC where a workflow is executing, the workflow enters the Unreachable state. When the workflow server successfully reestablishes the connection, the workflow is marked as Running, Aborted, or Executed, depending on the state that the workflow is in. An Unreachable workflow may require manual intervention if the workflow is not running any more. For further information see Execution Context in Desktop User's Guide . Waiting The Waiting state applies only to workflows that are included as members in a workflow group. In the Waiting state, the workflow cannot start execution due to two parameters in the workflow group configuration: The Startup Delay parameter, and the Max Simultaneous Running quota. A workflow in the Waiting state changes to Running when triggered either by a user, the scheduling criteria of its parent workflow group, or by a more distant ancestor's scheduling criteria. For further information see the Scheduling section in Managing a Workflow Group . Locked Locked state is a special case that is only relevant when a batch workflow is present in multiple versions. That is, there are multiple workflows with the same name and instance id from the same workflow configuration. These can be in different versions of a package or be the same as a workflow outside any package. This means that in case of same workflows, the packages that these workflows are part of are ignored when considering whether they are different versions of the same workflow. If this is the case (and again, only for batch workflows), then only one version is allowed to run at a time. When one version of the workflow starts, the other ones enter into the locked state until the started one completes its execution. Only then is the next version allowed to start. This allows for collection using the file sequence number service to stay consistent even if the workflow version is updated. Workflow Execution State The workflow execution state provides granular information about a workflow's current execution status. In APL, there are function blocks that are called for each workflow execution state. For more information about which APL functions that can be used in each state, see Analysis Agent . Real-time agents only have three different execution states, that occur at every execution of the workflow. Open Execution flow for real-time agents The batch agents are more complex and contain additional states in order to guarantee transaction safety. Open Execution flow for batch agents State Description State Description initialize The initialize state is entered once for each invocation of the workflow. During this phase the workflow is being instantiated and all agents are set up according to their configuration. beginBatch This state is only applicable for batch workflows. At every start of a new batch, the batch collection agent emits a beginBatch call. All agents then prepare for a new batch. This is normally done every time a new file is collected, but differs depending on the collection agent. consume The agents handle all incoming UDRs or bytearrays during the consume state. drain This state is only applicable for batch workflows. When all UDRs within a batch have been executed, the agents enter the drain state. This state can be seen as a final consume state with the difference that there is no incoming UDR or bytearray. The agent may however send additional information before the endBatch state. endBatch This state is only applicable for batch workflows. The collection agent calls for the endBatch state when all UDRs or the byte arrays are transferred into the workflow. This is normally done at the file end, but is dependent on the collection agent or when a hintEndBatch call is received from any agent capable of utilizing APL code. commit This state is only applicable for batch workflows. Once the batch is successfully processed or sent to the ECS, the commit state is entered. During this phase, all actions that concern transaction safety are executed. deinitialize This is the last execution state for each workflow invocation. The agents clean and release resources, such as memory and ports, and stop execution. cancelBatch This state is only applicable for batch workflows. If an agent fails the processing of a batch it may emit a cancelBatch call and the setting in Workflow Properties defines how the workflow should act. For more information regarding the Workflow Properties , see Error Tab in Workflow Properties . Note! The execution states cancelBatch and endBatch are mutually exclusive - only one per batch can be executed. rollback This state is only applicable for batch workflows. If the last execution of the workflow aborted, the agents enter the rollback execution state right after the initialize state. The agents recover the state prior to the failing transaction and then enter beginBatch or deinitialize depending on if there are additional batches to process. Viewing Abort Reasons In most cases if a workflow has aborted, one of its agents has a red square outline surrounding it. Double-clicking such an agent displays the Agent Status dialog with the abort reason. The System Log also holds valid information for these cases. Open A workflow with an aborted agent, indicated by the red square outline. In a batch workflow, a detected error causes the workflow to abort and the detected error is shown as part of the abort reason and inserted into the System Log. A real-time workflow handles errors by only sending them to the System Log. The only time a real-time workflow aborts is when an internal error has occurred. It is therefore important to pay attention to the System Log or subscribe to workflow error events to fully understand the state of a real-time workflow. Note! Although a workflow has aborted, its scheduling is still valid. Thus, if it is scheduled to execute periodically, it is automatically started again the next time it is due to commence. This is because the abortion might be a lost connection to a network element, which may be available again later. Therefore, a periodically scheduled workflow, which has aborted, is treated as Active until it is manually deactivated. Open Status details for an aborted agent In the Agent Status dialog, there is a Stack Trace section provided in the dialog. Use the information that it provides when consulting Support. Agent States An agent can have one of the following states: State Description State Description Created The agent is starting up. No data may be received during this phase. This state only exists for a short while during workflow startup. Idle The agent is started, awaiting data to process. This state is not available for a real-time workflows. Running The agent has received data and is executing. For agents that are running, there is a green square outline surrounding them. Stopped The agent has successfully finished processing data. Aborted The agent has terminated erroneously. For an agent that is aborted, there is a red square outline surrounding the aborted agent. The error reason can be tracked either by double-clicking the agent or by examining the System Log. Agent Configuration Some agents allow parts of them to be reconfigured while active. When double-clicking the agent in monitor mode, the agent's configuration is shown if the agent can be reconfigured. When the configuration has been updated it can be committed to the active workflow using the Dynamic Update button in the toolbar. Dynamic Update While a real-time workflow is being executed, you can change the value of the following parameters: The Host and Port parameters of the TCP/IP agent The NAS list of the Radius agent The Interval of the Pulse agent Open Example of the TCP/IP configuration in a running Realtime workflow To be able to dynamically update TCP/IP Host and Port parameters, you need to set them to either Default or Per Workflow in the Workflow Properties dialog. See the figure The Workflow Table Tab in Workflow Table Tab . To update, click Dynamic Update . On the bottom left of the workflow monitor, the text Dynamic Update followed by a number appears. It represents the number of times that you have updated the workflow configuration while running it, that is, since the last time you started it. Open Example of a dynamic update status after clicking the Dynamic Update button Transactions A workflow operates on a data stream and MediationZone supplies a transaction model where persistent synchronization is made from the workflow data and agent-specific counters. In theory the synchronization could be performed continuously on a byte level, but in practice this would drastically decrease the performance of the system. The transaction model is based on the premises that collection agents are free to initiate a transaction to the transaction server. At the moment the complete workflow is frozen and the transaction server saves the state of the workflow data that is queued for each agent. In practice, agents indirectly emit a transaction when an End Batch is propagated. When all data is secured, the workflow continues the execution. Debug A workflow can be configured to send debug logs as an event or to a file. When an event is selected, the debug logs will be present in the text areas in Workflow Monitor View. Open Workflow Properties to select Debug type When the Debug Type is File, the debug will be written to a file in MZHOME of the Execution context. The default file location is in a debug directory inside the temporary directory decided by this property pico.tmpdir . It can also be set directly using this property, mz.wf.debugdir . When the debug is sent to a file, an additional button will be visible in the Workflow Monitor, Download debug files . Open Monitor buttons when debug is sent to file After a run the Download debug files button is enabled and once you click on the button, the following dialog appears. Open Download debug files dialog

---

# Document 612: File System Type - Git - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/315392053
**Categories:** chunks_index.json

Git is the default File System Type selected when you create a new File System Profile. There is one tab below this - General . Open New Configuration - File system profile General tab The following settings are available in the General tab in the Git File System Type: Setting Description Setting Description Repository URL The URL to the repository. Git Provider Open the dropdown to select the Git Provider you want to connect to. The options are: GITHUB BITBUCKET GITLAB Username This field will only be available when BITBUCKET is selected as the Git Provider. Enter your Bitbucket username here. Token Token to access the repository. This field is optional. Use Secrets Profile Select the checkbox to use a Secrets Profile to get the Token. Get Branches Click this button to fetch the branches from the repository. If the connection is working, the Branch combo box will be populated. If the connection fails, an error dialog will be shown. Branch Select the branch to use. Note! It is not possible to create a new branch using Usage Engine. The branch must already exist in the repository specified in the Repository URL . View Repository Click here to browse the folders in the repository. It is only possible when the configuration is saved. Note! When you Save As, the remote repository is cloned to the platform and may take some time. This directory is $MZHOME/gitrepos by default. It can be changed by setting the property mz.git.basePath to some other path accessible from the Platform It is not possible to change the Repository URL or branch once the configuration is saved. Import of Git File System Profile An imported new Git File System Profile configuration will always be invalid since the repository has not been cloned. You clone the repository in the profile by clicking the Clone Repository button. Open Open When the cloning is done the text on the button will change to Preview Repository , and the configuration should now be valid, which you can verify by clicking the Validate button.

---

# Document 613: Backup and Disaster Recovery Procedures - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204613213
**Categories:** chunks_index.json

Backup and disaster recovery procedures are recommended tasks to be carried out at a regular basis to avoid performance degradation. Complete system backups to an external disk should be performed at regular time intervals, and preferably at times when the traffic load is low, e.g. at night. The reasonable time interval might be depending on how the system is configured, the amount of traffic, etc, and may vary from installation to installation. General Requirements Regardless of the method chosen for the provisioning of backup and restore operations, there are requirements that must be met: When an instance is restored from a backup image, the database backup must be older than the disk backup. The reason for this is to avoid database references to non-existing data. Temporary files must be excluded from the backups. This is to avoid inconsistent or partial data. Temporary files can be identified by various identifiers that form the file names: Temporary files  Prefixed with "TEMP." DupUDR function temporary files  They are in a separate folder called "tmp". InterWorkFlow temporary files  Stored in a separate folder called "DR_TMP_DIR". Archiving function files  Stored in a separate folder called "pending". This chapter includes the following sections: Derby Database Backup and Restore Oracle Database Online Backup and Restore Oracle Database Online Backup and Restore in Amazon Web Services

---

# Document 614: pcreate - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646413/pcreate
**Categories:** chunks_index.json

usage: pcreate <name> <version> <package-file> [ -level <default level> ] [-revision <revision>] [-repository <repository>] [-metadata <data>] [-hidden] [[-level <level name>] file=<file-to-include>] [-osgi <true/false> ] [-exportpackages <classpath>]... ] Creates a software package (.mzp) that can be installed into the system. It is usually used by software developers to create additional functionality and updates. Arguments Argument Description Argument Description <name> The name of the package. <version> The version string of the package name. <package-file> The resulting package. For further information, see the Development Toolkit User's Guide . Return Codes Listed below are the different return codes for the pcreate command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the argument count was incorrect. 2 Will be returned if the package name is too long. 3 Will be returned if if the package version is too long. 4 Will be returned if an install class or a jar argument is missing. 5 Will be returned if a meta data argument is missing, or if a revision argument is missing. 6 Will be returned if a repository argument is missing, or if a level argument is missing. 7 Will be returned if the level is invalid. 8 Will be returned if a file argument is missing 9 Will be returned if no level is given for the jar file. 10 Will be returned if a file argument is missing. 11 Will be returned if the file cannot be read. 12 Will be returned if the package filename cannot be found. 13 Will be returned if the install jar file cannot be read.

---

# Document 615: JSON Functions - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656924/JSON+Functions
**Categories:** chunks_index.json

The JSON functions are used to decode and encode JSON-formatted strings. Note! The JSON parser is designed according to the specification RFC7159 and may accept non-JSON input, as stated in section 9 of RFC7159 . This chapter includes the following sections: JSON Decoding Functions JSON Encoding Functions

---

# Document 616: MariaDB - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204605795/MariaDB
**Categories:** chunks_index.json

This section contains information that is specific to the database type MariaDB. Supported Functions The MariaDB database can be used with: Callable Statements (APL) Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Event Notifications Prepared Statements (APL) Reference Data Management SQL Collection/Forwarding Agents SQL Loader Agent Task Workflows Agents (SQL) Preparations A database driver is required to connect to a MariaDB database. This driver must be stored in the Platform Container. Follow the steps below if MariaDB is not set up during the installation of the Platform Container: Download the JDBC driver ( mariadb-java-client - <version>.jar ) for the appropriate MariaDB database version. Copy the downloaded file(s) to the directory MZ_HOME/3pp in the Platform Container. Restart the Platform and ECs for the change to take effect. You should be able to select the MariaDB option from the Database profile after this step.

---

# Document 617: Prepared Statements - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646256/Prepared+Statements
**Categories:** chunks_index.json

A prepared statement is used to execute a statement multiple times efficiently. The Prepared Statement related functions support database failover, which is useful in a replicated database environment, by receiving multiple database profiles. If a workflow failure occurs using one database profile, a failover process starts to replace that profile with the next available one. If none of the supplied profiles could be used a null value will be returned. All functions without the parameter disableCommit will be auto committed after the function has been executed. Note! Failovers can be triggered by all errors that occur while connecting to the database or executing commands, not exclusively due to connection problems. Please refer to the Notes section on the Database Functions page for details on the allowed database data type. The following functions for Database Prepared Statements are described here: 1 sqlPrepSelect 2 sqlPrepDynamicSelect 3 sqlPrepUpdate sqlPrepSelect The sqlPrepSelect function is used when selecting data from database tables. table sqlPrepSelect (string sqlStatement, any parameter [,...], string DBProfile [,...] ) Parameter Description Parameter Description sqlStatement SQL string containing one or more parameter placeholders ('?'). parameter(s) Value(s) to bind against parameter placeholders. DBProfile/s) Name of the database profile(s) to use for access. disableCommit (Removed since MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement to be performed at the end of every SQL transaction for this particular function. By default, the system has the disableCommit set to true unless changed by this parameter. It should be noted that on recent Oracle versions, the DBLink SQL transaction behaviour has changed, where every single SQL statement for remote database transaction requires a commit or rollback statement in order to close a connection. Returns Null in case of failure the error details can be found in System Log, otherwise a table, holding the SQL select result. Example - Using sqlPrepSelect final string sql = "select col2 from tab1 where col1=?"; consume { table tab = sqlPrepSelect(sql, myUdr.aValue, "Default.myPrimaryDB", "Default.mySecondaryDB"); if ( tab != null ) { if ( tableRowCount(tab) > 0 ) { debug("Found following value in col2:" + tableGet(tab, 0, 0)); } } else { debug("Both primary and secondary database nodes are unavailable"); } } sqlPrepDynamicSelect The sqlPrepDynamicSelect function is used when selecting data from database tables with dynamically created SQL statements. table sqlPrepDynamicSelect (string sqlStatement, list<any> parameters, string DBProfile [,...]) Parameter Description Parameter Description sqlStatement SQL string containing one or more parameter placeholders ('?'). The string may also contain variables that substitute any part of the statement such as table- or column names. Note! Validation of sqlStatement is performed in runtime and not during configuration. Make sure that number of placeholders matches the actual number of parameters. parameter(s) Value(s) to bind against parameter placeholders DBProfile/s) Name of the database profile(s) to use for access disableCommit (Removed since MediationZone 9.3.0.1) An optional parameter to disable the commit statement from being performed at the end of every SQL transaction for this particular function. Setting this parameter to false will result in the commit statement to be performed at the end of every SQL transaction for this particular function. By default, the system has the disableCommit set to true unless otherwise changed via this parameter. Note! It should be noted that on recent Oracle versions, the DBLink SQL transaction behaviour has changed, where every single SQL statement for remote database transaction requires a commit or rollback statement in order to close a connection. Returns Null in case of failure the error details can be found in System Log, otherwise a table, holding the SQL select result Example - Using sqlPrepDynamicSelect final string EXT_REF_CONFIG = "Default.extrefprop"; string TABLE_KEY = externalReferenceGet(EXT_REF_CONFIG, "mytable"); consume { string sql = "select col2 from " + TABLE_KEY + " where col1=?"; list <any> parameters = listCreate(any, myUDR.aValue); table tab = sqlPrepDynamicSelect(sql, parameters, "Default.myPrimaryDB", "Default.mySecondaryDB"); if ( tab != null ) { if ( tableRowCount(tab) > 0 ) { debug("Found following value in col2:" + tableGet(tab, 0, 0)); } } else { abort("Both primary and secondary database nodes are unavailable"); } } sqlPrepUpdate The sqlPrepUpdate function is used when updating, inserting or deleting data into a database table. int sqlPrepUpdate( string sqlStatement, any parameter [,...], string DBProfile [,...] ) Parameter Description Parameter Description sqlStatement SQL string containing one or more parameter placeholders ('?') parameter(s) Value(s) to bind against parameter placeholders DBProfile/s) Name of the database profile(s) to use for access Returns -1 in case of failure the error details can be found in System Log, otherwise an integer equal to the number of rows updated or inserted Example - Using sqlPrepUpdate final string sql = "update table1 set name = ? where id = ?"; consume { int cnt = sqlPrepUpdate(sql, aNewName, anIdent, "Default.myPrimaryDB", "Default.mySecondaryDB"); if ( cnt == -1 ) { debug("Both primary and secondary database nodes are unavailable"); } }

---

# Document 618: UDP Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204643487/UDP+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data an agent expects and delivers. The agent receives UDP packets and produces UDRs in accordance with the settings in the Decoder tab. If responses are sent back to the host, the agent consumes Packet UDRs . MIM For information about the MIM and a list of the general MIM parameters, see Meta Information Model in Administration and Management in Legacy Desktop . The agent publishes the following MIM parameter: MIM Value Description UDRs (long) This MIM parameter contains the number of received UDRs. UDRs is of the long type and is defined as a global MIM context type. The string values in the map contain the host-names of the peers.

---

# Document 619: FTAM IOG Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607522/FTAM+IOG+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data

---

# Document 620: Event Setup Tab - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638784
**Categories:** chunks_index.json

In the Event Setup tab you add the event(s) for which you want notifications to be sent. For each event, the parameters that are populated by using the type Event Field , Formatted (with variables) or SQL , can be assigned different values values from selected event fields. Event Notification - Event Setup tab Setting Description Setting Description Filter Settings Select the event types you want to send notifications for in the Filter table. For each event type you can define that certain values in the event type must be matched in order for a notification to be sent, e g a specific workflow, or certain severities Apart from matching existing values (for instance Workflow name) you can also use regular expressions and hard coded strings. When using regular expressions, Java syntax applies, see http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html Event Field Name The contents of this column varies depending on the selected event type, Event Fields for more information. Note that only fields of string type are visible. Match Value(s) In this column you define the matching criteria for the events you want to send notifications for. Double-click on a cell to open the Edit Match Value dialog box where you can configure matching criteria. The values you can select from are pre-defined suggestions, but you can also use hard coded strings and regular expressions. Example For example, entering: .*idle.* will match any single lines containing "idle". Some fields also contain several lines, so entering: (?s).*idle.* will match any multi-line content containing "idle". The default value for each of the components is All . Note! Some of the Event Fields let you select from four Match Value types: Information , Warning , Error , or Disaster . For the rest of the Event Fields Match Value you use a string. Make sure you enter the exact string format of the relevant Match value. For example: the Event Field timeStamp can be matched against the string format yyyy-mm-dd. Field Map Settings Maps variables against event fields. The Field Map table exists only if any of the parameters for the selected notifier type is set to Formatted , Event Field or SQL . Notifier Field Displays the Notifier parameter available in the Notifier Setup tab. If a specific parameter has more than one variable, it will claim one line per variable. Variable Displays the name of the variable, as entered in the Notifier Setup tab. If the parameter type is Event Field , this field will be empty. Event Field Select the event field from which you want to pick values from in this drop-down list. Add Event... Click on this button to add an event that you can call from APL or Ultra Code. The new event type is added as a separate tab in the Event Setup . Remove Event... Click on this button to remove the event type tab you have selected. Refresh Field Map Click on this button to updates the Field Map table. This is required if the parameter population types or formatting fields have been modified in the Notifier Setup tab.

---

# Document 621: Aggregation Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205031494
**Categories:** chunks_index.json

This section describes the Aggregation profile and the Aggregation agent. The agent is a processing agent for batch and real-time workflow configurations. The Aggregation consolidates related UDRs that originate from either a single source or from several sources, into a single UDR. Related UDRs are grouped into "sessions" according to the value of their respective fields, and a set of configurable conditions. The first UDR to match a set of conditions triggers the generation of a new session. Consecutive UDRs that match the same conditions can then be handled in the context of the same session. The agent stores the session data in a file system, Couchbase, Redis, or Elasticsearch. It is also possible to use the settings for file storage to keep the session data in memory only. When selecting which storage type you require, note that File Storage can be used in batch and real-time workflows; Couchbase and Redis can only be used in real-time workflows; Elasticsearch can only be used in batch workflows. To ensure the integrity of the session's data in the storage, the Aggregation agent may use read- and write locks. When you have selected file storage and an active agent has write access, no other agent can read or write to the same storage. It is possible to grant read-only access for multiple agents, provided that the storage is not locked by an agent with write access. When you have selected Couchbase or Redis storage, multiple Aggregation agents can be configured to read and write to the same storage. In this case, write locks are only enforced for sessions that are currently updated and not the entire storage. For further information about how to configure read-only access, see Aggregation Profile . In a batch workflow, the aggregation agent receives collected and decoded UDRs one by one. Open Aggregation in a batch workflow In a real-time workflow, the aggregation agent may receive UDRs from several different agents simultaneously. Open Aggregation in a real-time workflow The aggregation flow chart below illustrates how an incoming UDR is treated when it is handled by the Aggregation agent. If the UDR leaves the workflow without having called any APL code, it is handed over to error handling. For detailed information about handling unmatched UDRs see the section General Tab in Agent Configuration - Batch, Aggregation Agent Configuration - Batch , or Agent Configuration - Real-Time in Aggregation Agent Configuration - Real-Time . Open The aggregation flow chart When several matching sessions are found, the first one is updated. If this occurs, redesign the workflow. There must always be zero or one matching session for each UDR. High-level steps for configuration of Aggregation Follow the steps below to configure Aggregation: Create a session UDR in Ultra format. Create a storage profile, i.e. Couchbase or Redis profile (not required for file storage). Create an Aggregation profile. Create a workflow containing an Aggregation agent. Prerequisites The reader of this information should be familiar with: Couchbase Redis SQL The section contains the following subsections: Aggregation Agent Overview Aggregation Example - Association of IP Data Aggregation Performance Tuning Storage Profiles Aggregation Profile Aggregation Agents Configuration Session UDR Type Aggregation Session Inspector

---

# Document 622: File System Type - GCP Storage - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/315162665
**Categories:** chunks_index.json

When selecting GCP as a file system, you will see one tab - General . Open File System profile - GCP - General tab General tab The following settings are available in the General tab in the File System profile: Setting Description Setting Description Authentication Details - Service Account Settings Environment-Provided Service Account Select this option, if you are configuring an environment-provided service account with this profile. This will disable the Input Option and Credentials File fields. Input Option Using this option you delegate how the GCP connection credentials are acquired, the available options are to select a JSON File or to fill in the information from a Form. Credentials File Enter the path to the delegated credentials file. This option is visible only when the JSON File option is selected as the input option. Import Credentials from File This button allows for credentials to be imported from a locally stored file. This option is visible only when the Form input option is selected. Project ID Enter the project ID. This option is visible only when the Form input option is selected. Private Key ID Enter the Private key ID. This option is visible only when the Form input option is selected. Client Email Enter the client's email. This option is visible only when the Form input option is selected. Client ID Enter the client ID. This option is visible only when the Form input option is selected. Other Information Enter other information that might be used with this profile. This option is visible only when the Form input option is selected. Location Settings Bucket Enter the target bucket name. Use GCP Profile Select this check box if you already have a GCP Profile set up, this will disable the fields above and allow you to utilize the credentials that you have defined in your chosen GCP Profile.

---

# Document 623: GCP PubSub Subscriber Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739012/GCP+PubSub+Subscriber+Agent+Events
**Categories:** chunks_index.json

Agent Message Events There are no agent message events for this agent. For information about the agent message event type, see Agent Event . Debug Events There are no debug events for this agent.

---

# Document 624: FTP Forwarding Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641329
**Categories:** chunks_index.json

You open the FTP forwarding agent configuration dialog from a workflow configuration. To open the FTP forwarding agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Batch . Click Add agent and select FTP from the Forwarding tab of the Agent Selection dialog. This dialog contains two tabs: FTP and Filename Template . The FTP tab in turn contains four tabs; Connection Target , Advanced , and Backlog , which will be described on this page. For information about Filename Template configuration, see Workflow Template . Connection The Connection tab is used to configure the remote server connection. Open The FTP forwarding agent configuration - Connection tab Setting Description Setting Description Connection Information Host Enter the hostname or IP address of the remote host. If a connection cannot be established to this host, the Additional Hosts specified in the Advanced tab, are tried. Username Enter the username for the remote host account. Password Enter the associated password. Transfer Type Select the data transfer type to be used during file retrieval. Binary - agent uses binary transfer type. Default setting. ASCII - agent uses ASCII transfer type. File System Type Specify the file system type of the remote host. You can choose between Unix , Windows NT, and VAX/VMS . Forwarding Retries Enable Select this check box to enable forwarding connection retry attempts. When this option is selected, the agent will attempt to connect to the host as many times as is stated in the Retry Intervals(s) field. The maximum number of attempts is defined in the Max Entries field. Retry Interval(s) Enter the retry time interval in seconds. Max Entries Enter the maximum number of forwarding retry attempts. Target Tab Open The FTP forwarding agent configuration - Target tab Setting Description Setting Description Input File Handling Input Type The agent can act on two input types. Depending on which one the agent is configured to work with, the behavior will differ. The default input type is byte array , that is the agent expects byte arrays. If the input type is MultForwardingUDR , the behavior will change. For further information about the agent's behavior with MultiForwardingUDR input, refer to FTP Forwarding Agent MultiForwardingUDR Input . File Information Directory Enter the absolute pathname of the target directory on the remote host. The pathname may also be given relative to the home directory of the user's account. The files will be temporarily stored in the automatically created subdirectory DR_TMP_DIR . When an endBatch message is received, the files are moved from the subdirectory to the target directory. Create Directory Select this check box to create the directory, or the directory structure, in the specified path. Note! The directories are created during workflow execution. Compression Select the compression type of the target files. When enabled this will compress the files before they are stored on the server. No Compression - The agent will not compress the files. Gzip - The agent will compress using Gzip. Note! No extra extension will be appended to the target filenames, even if compression is selected. Target File Handling Produce Empty Files Enable this option if you need to create empty files. Handling of Already Existing Files Select the behavior of the agent when handling existing files. The available options are: Overwrite - The old file will be overwritten and a warning entry will be created in the System Log. Add Suffix - If the file already exists the suffix ".1" will be added. An ascending order of numeral suffixes will be applied if it already exists, starting from ".2". Abort - This is the default option and it is applied for upgraded configurations. Temporary File Handling Use Temporary Directory If this option is selected, the agent will move the file to a temporary directory before placing it in the target directory. After the whole file has been transferred to the target directory, and the endBatch message has been received, the temporary file is deleted from the temporary directory. Use Temporary File If there is no write access to the target directory and, hence, a temporary directory cannot be created, the agent can move the file to a temporary file that is stored directly in the target directory. After the whole file has been transferred, and the endBatch message has been received, the temporary file will be renamed. The temporary filename is unique for every execution of the workflow. It consists of a workflow and agent ID, and a file number. Abort Handling Select the file handling behavior in case of cancelBatch or rollback is received. The options are to Delete Temporary File or Leave Temporary File . Note! When a workflow aborts, the file will not be removed until the next time the workflow is run. Advanced Tab Open The FTP forwarding agent configuration - Advanced tab Setting Description Setting Description Advanced Settings Command Port Select the port number of the remote FTP server. Timeout(s) The maximum time, in seconds, to wait for a response from the server. A value of "0" will wait indefinitely. Passive Mode (PASV) This enables Passive Mode. In passive mode, the channel for data transfer between the client and server is initiated by the client instead of by the server. This is useful when a firewall is situated between the client and the server. Additional Hosts Here you can specify additional host names or IP addresses that can access the data storage. Connections attempts to these hosts are initiated, in sequence from top to bottom, if the agent fails to connect to the remote host set in the Connection tab. Use the Add , Edit , Remove , Up , and Down buttons to configure the host list. Note! The names of the created files are determined by the settings in the Filename Template tab. The use and setting of private threads for an agent, enabling multi-threading within a workflow, is configured in the Thread Buffer tab. For further information, see Thread Buffer Tab in Workflow Template . Backlog Tab The Backlog tab contains configurations related to backlog functionality. If the backlog is not enabled, the files will be moved directly to their final destination when an end batch message is received. If the backlog however is enabled, the files will first be moved to a directory called DR_POSTPONED_MOVE_DIR and then to their final destination. Refer to Retrieves in FTP Forwarding Agent Transaction Behavior for further information about transaction behavior. When the backlog is initialized and when backlogged files have been transferred a note is registered in the System Log. Open The FTP forwarding agent configuration - Backlog tab Setting Description Setting Description Enable Backlog Check to enable the backlog option. Directory Base directory in which the agent will create sub-directories to handle backlogged files. Absolute or relative path names can be used. Max Size Type Files is the maximum number of files allowed in the backlog folder. Bytes refer to the total sum of all files that reside in the backlog folder. If a limit is exceeded the workflow will abort. Size Enter the maximum number of files or bytes that the backlog folder can contain. Processing Order Determine the order by which the backlogged data will be processed once the connection is reestablished, and select between First In First Out (FIFO) or Last In First Out (LIFO). Duplicate File Handling Specifies the behavior if a file with the same file name as the one being transferred is detected. The options are Abort or Overwrite and the action is taken both when a file is transferred to the target directory or to the backlog.

---

# Document 625: mzsh Commands - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205656979/mzsh+Commands
**Categories:** chunks_index.json

The mzsh can be used both as a system administration tool and as a system client tool. When using mzsh without running the platform, mzsh enables you to startup or shut down the system and perform other tasks that are not dependent on having a Platform running. The Platform server process is started by entering the following in the shell: mzsh <username>/<password> startup platform The following text will appear: Starting platform...done. If mzsh is entered while the platform is running, mzsh will become a pico instance, with the ability to manage the system processes. For further information about Pico Clients, see the Desktop User's Guide . Using commands downloaded from Platform requires that the user is logged in. For further information about the Platform, see the System Administrator's Guide . $ mzsh mzadmin/<password> startup platform For example, if the platform has already been started, the following command: $ mzsh startup platform This will generate the following message: Platform is already running. You can only use mzsh in non-interactive mode. Note! Some commands are only available when the Platform is running. This chapter includes the following sections: Always Available Available When the Platform Is Running

---

# Document 626: mzcli - help - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979510/mzcli+-+help
**Categories:** chunks_index.json

Usage usage: help [command] This command provides information about available commands and their use. If help is used without an argument, a list containing available commands and their required arguments will be displayed. When an argument is added to the help command information about the entered command is displayed. Return Codes Listed below are the different return codes for the help command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if the command supplied in the argument does not exit.

---

# Document 627: Oracle RAC Connection - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204637920
**Categories:** chunks_index.json

The following steps are applicable when using Oracle RAC: Go to JDBC and UCP Downloads page . Download the Zipped JDBC driver (ojdbc8.jar) and Companion Jars tar file for Oracle database driver 19c. Unzip the file. After successful extraction copy the ons.jar and ojdbc<version>.jar files from the root directory. Store ons.jar and ojdbc<version>.jar in the directory specified by the mz.3pp.dir property in the installation properties, see Additional Platform Properties in install.xml .

---

# Document 628: UDR File Editor and Ultra Format Converter - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737474
**Categories:** chunks_index.json

UDR File Editor For information about the UDR File Editor, see the Ultra Reference Guide . Ultra Format Converter For information about the Ultra Format Converter, see the Ultra Reference Guide . Loading

---

# Document 629: ADLS2 File Forwarding MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737945
**Categories:** chunks_index.json

Warning! MultiForwarding is not fully supported for ADLS2 File Forwarding as of version 8.1.5.0 When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the package FNT. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note! After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is checked. If not checked, a runtime error will occur if a previously unknown directory exists in the FNTUDR of an incoming MultiForwardingUDR . Every configuration option referring to bytearray input is ignored when MultiForwardingUDR s are expected. Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDR s. import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned previous in the example will send two MultiForwardingUDR s to the forwarding agent. Two files with different contents will be placed in two separate sub folders in the root directory. The Create Non-Existing Directories check box under the Filename Template tab in the configuration of the forwarding agent must be checked if the directories do not previously exist.

---

# Document 630: system - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204678049
**Categories:** chunks_index.json

usage: system <subcommand> <options> This command starts the Platform if it is not running, and then starts/stops pico instances on containers that have been set up for remote execution. Note! This command is valid only for the MZ_HOME owner. You can specify which pico instances that are to be started, stopped, or restarted by adding a target path to the subcommands. The target path is specified as follows: container:<container>/pico:<pico> or container:<container> Note! Set your desired environment variables in $MZ_HOME/bin/mzshr.env as this file will be loaded with local variables. Add the desired profiles, such as ". /home/mzadmin/.profile_mz". You can specify both the container and pico instance as a regular expression. Example - Regular expression in target paths container:.*/pico:.* By adding tag attributes you can perform additional filtering of the pico instances: Example - Adding tags to pico instances $ mzsh topo set -l pico:ec1 'settings.tags=[tag1,tag2]' $ mzsh topo set -l pico:ec2 'settings.tags=[tag1]' Run the following command to start the pico instances with the tag tag1 . $ mzsh system start -t tag1 The following subcommands are available with mzsh system : help restart start stop help Usage: system help [<subcommand>] Use system help to retrieve a description of the help command or its subcommands. Run the following command for an overview of the various subcommands: $ mzsh system help Run the following command for a description of a specific subcommand: $ mzsh system help <command> restart Usage: system restart [--dry-run] [-l, --local] [-services] [-t, --tag <tag>] [--timeout-seconds] [-v, --verbose] [<target path>] Use system restart to stop and start pico instances in one or more containers. The Platform will be started if it is not already running. However, the command does not stop the Platform. Option Description Option Description [--dry-run] Lists the picos instances that are addressed by the command, but the command is not executed. [-l, --local] Use this option to select the local container, unless another container is specified in the target path. [-t, --tag <tag>] Filter that excludes all pico instances that do not contain the specified tag. [--timeout-seconds] Sets the maximum allowed time for all calls to complete. The default value is 300 seconds. [-v, --verbose] Use this option for detailed output from the command. start Usage: system start [--dry-run] [-l, --local] [-t, --tag <tag>] [--timeout-seconds] [-v, --verbose] Use system start to start pico instances in one or more containers. The Platform will be started if it is not already running. Option Description Option Description [--dry-run] Lists the picos instances that are addressed by the command, but the command is not executed. [-l, --local] Use this option to select the local container, unless another container is specified in the target path. [-t, --tag <tag>] Includes pico instances in the target path that contain the specified tag. [--timeout-seconds] Sets the maximum allowed time for all calls to complete. The default value is 300 seconds. [-v, --verbose] Use this option for detailed output from the command. stop Usage: system stop [--dry-run] [-t, --tag <tag>] [--timeout-seconds] [-v, --verbose] [<target path>] Use system stop to stop pico instances in one or more containers. Option Description Option Description [--dry-run] Lists the picos instances that are addressed by the command, but the command is not executed. [-l, --local] Use this option to select the local container, unless another container is specified in the target path. -t, --tag <tag>] Includes pico instances in the target path that contain the specified tag. [--timeout-seconds] Sets the maximum allowed time for all calls to complete. The default value is 300 seconds. [-v, --verbose] Use this option for detailed output from the command. Return Codes Listed below are the different return codes for the system command: Code Description Code Description -1 Will be returned if there is an old process running or if the remote (../temp/.remote) file cannot be deleted. 0 Will be returned if the command was successful, or if there are no startable processes defined. 1 Will be returned if the JVM fails to start. (The JVM has logged too much on stderr.) 102 Will be returned if the JVM fails to start. (The timeout on the callback from the JVM was exceeded.) 103 Will be returned if the command has been interrupted with CTRL-C. 104 Will be returned if the JVM fails to start. (The JVM started with one or more critical errors.)

---

# Document 631: Prometheus Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001266/Prometheus+Agent
**Categories:** chunks_index.json

This section describes the Prometheus forwarding agent. The Prometheus agent is available for both real-time and batch workflow configurations. This agent lets you publish metrics in a format that can be scraped by a Prometheus instance. The metrics can then be visualized using Grafana. Follow the official Grafana documentation here: Grafana OSS and Enterprise | Grafana documentation . To configure Prometheus, see Overview | Prometheus . Use the Prometheus filter to configure the metrics that are going to be exposed for scraping, see The Prometheus Filter . Open The Prometheus forwarding agent stores metrics in a cache. The cached metrics are published in an endpoint that is scraped by Prometheus. You can configure a maximum number of metrics stored until Prometheus scrapes them. When you scrape a metric it is automatically deleted. You can also set an expiration time for a metric. Endpoint Each Execution Context exposes an endpoint that can be scraped by a Prometheus instance at: <ec_host>:<ec_webport>/metrics/ It is possible to configure how many metrics to store and for how long a cache can hold. This is configured via Execution Context Properties: Property Description Property Description mz.metrics.cache.size.max Maximum number of records in the metrics cache. The default value is 10000. mz.metrics.cache.expire Maximum time in seconds before a metric is removed from the cache. The default value is 300. Note! This property is only valid for data that is not read (and the number of records in the mz.metrics.cache.size.max property is not reached). Data that are read are removed immediately. The cache is shared by all workflows running in the Execution Context so its size has to be set accordingly, that is, to the expected metric flow throughput. Each Prometheus scrape empties the cache so the cache size should be set to minimum <number_of_metrics_expected_per_second> * <prometheus_scrape_interval>. Note! Metrics stored in the cache are read-only-once. Manual querying of the endpoint will result in data missing in Prometheus. Compatibility with System Insight It is possible to replace the System Insight agent with the Prometheus agent without making any changes to the existing workflows. This is done by simply replacing the System Insight Forwarding agent with the Prometheus Forwarding Agent. A few restrictions apply: All metrics created with the use of a Measurement UDR are exposed as a GAUGE type. Each value stored inside the fields field of the Measurement UDR is reported as a separate metric. The CATEGORY field is not used. If it is still needed it has to be assigned to NAME fields of the Prometheus UDR. See Prometheus UDR Type for more information. If the System Insight UDR field fields contains many measurements, each is reported as a separate metric. Use the Prometheus Node Exporter to obtain the host hardware metrics. See Monitoring Linux host metrics with the Node Exporter | Prometheus for more information. The section contains the following subsections: Prometheus Forwarding Agent Prometheus UDR Type The Prometheus Filter

---

# Document 632: Agent Event - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737074
**Categories:** chunks_index.json

The following fields are included: agentName - The name of the agent issuing the event. Fields inherited from the Base event The following fields are inherited from the Base event, and described in more detail in Base Event : category contents - Workflow: <Workflow name>, Agent: <Agent name> eventName origin receiveTimeStamp severity timeStamp Fields inherited from the Workflow event The following fields are inherited from the Workflow event, and described in more detail in Workflow Event : workflowKey workflowName workflowGroupName

---

# Document 633: Duplicate UDR Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607178/Duplicate+UDR+Profile
**Categories:** chunks_index.json



---
**End of Part 29** - Continue to next part for more content.
