# RATANON/MZ93-DOCUMENTATION - Part 56/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 56 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~69.5 KB
---

This section describes functions that relates to manipulating string values in the APL. Warning! The String functions cannot operate on null arguments. The following functions for String described here are: 1 String Concatenation 2 strEndsWith 3 strEqualsIgnoreCase 4 strIndexOf 5 strLastIndexOf 6 strInsert 7 strLength 8 strREContains 9 strREIndexOf 10 strREMatches 11 strREReplaceAll 12 strReplaceChars 13 strSplit 14 strStartsWith 15 strSubstring 16 strToLower 17 strToUpper 18 strTrim String Concatenation Strings are concatenated with the arithmetic '+' operator: string str1 = string str2 + string str3 ... strEndsWith Returns true if str ends with substr , otherwise false . boolean strEndsWith ( string str , string substr ) Parameter Description Parameter Description str String to examine substr String to look for Returns true or false strEqualsIgnoreCase Compares two strings and returns true if they are equal, and false if they are not. This comparison ignores case. boolean strEqualsIgnoreCase ( string str1 , string str2 ) Parameter Description Parameter Description str1 A string to compare str2 Another string to compare Returns true or false strIndexOf Returns the first position where substr can be found in str . If substr is not found, -1 is returned. The position starts to count from 0 (zero), which is the first character. int strIndexOf ( string str , string substr, int startIndex ) //Optional Parameter Description Parameter Description str String to examine substr String to look for startIndex Index where to start the string search. If startIndex is not specified, the string is searched from the beginning. Returns The position of the first character of substr within str . If not found, -1 is returned. strLastIndexOf Returns the last position where substr can be found in str . If substr is not found, -1 is returned. The position starts to count from 0 (zero), which is the first character. int strLastIndexOf ( string str , string substr, int startIndex ) //Optional Parameter Description Parameter Description str String to examine substr String to look for startIndex Index where to start the string search. If startIndex is not specified, the string is searched from the beginning. Returns The position of the last character of substr within str . If not found, -1 is returned. strInsert Inserts a string into another at a specific position, and returns the result string. string strInsert ( string str1 , int position , string str2 ) Parameter Description Parameter Description str1 The string that str2 is inserted into str2 The string to insert position Position where str2 will be inserted. Position 0 (zero) indicates prior to the first character of str1 . Returns The result string. Note, the str1 and str2 are not modified. strLength Returns the number of characters in a string. int strLength( string str ) Parameter Description Parameter Description str String to examine Returns The number of characters in the string strREContains Returns true if a string contains a substring, else false . The function is case sensitive. boolean strREContains ( string str , string regexp ) Parameter Description Parameter Description str String to examine regexp The substring to look for. Regular expressions are allowed. Returns true or false Note! Regular expressions according to Java syntax applies. For further information, see https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html Warning! Using nonconstant regular expressions may consume large amounts of memory and should therefore be avoided. strREIndexOf Returns the first position where a regular expression can be found in a string. If the regular expression is not found, -1 is returned. The position starts to count from 0 (zero), which is the first character. int strREIndexOf ( string str , string regexp ) Parameter Description Parameter Description str String to examine regexp Regular expression to be used when examining str , using case sensitive and contains (not matches ). Returns The position of the first character of regexp within str . If not found, -1 is returned. Example - Using strREIndexOf strREIndexOf( "Hello There!", "[Tt]he" ) // Returns 6 Note! Regular expressions according to Java syntax applies. For further information, see https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html . Warning! Using nonconstant regular expressions may consume large amounts of memory and should therefore be avoided. strREMatches Returns true if the first stated string matches the content of the second string completely. The function is case sensitive. boolean strREMatches ( string str , string regexp ) Parameter Description Parameter Description str The first string regexp The second string. Regular expressions are allowed Returns true or false Example - Using strREMatches strREMatches( "abc", "ab." ) Will return true . strREMatches( "abc", "a..c" ) Will return false . strREMatches( "abc", "a[a-z]c" ) Will return true . strREMatches( "abc", "a[A-Z]c" ) Will return false . strREMatches( numberString, "[0-9]*" ) Will check that the variable numberString only contain digits. It will return true for "123" and false for "123F". Note! Regular expressions according to Java syntax applies. For further information, please refer to https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html . Warning! Using nonconstant regular expressions may consume large amounts of memory and should therefore be avoided. strREReplaceAll Replaces existing substrings within a string with new values. string strREReplaceAll ( string str , string old , string new ) Parameter Description Parameter Description str String to change old The substring to replace. Regular expressions are allowed. new The new substring to replace the old. If an empty string is entered, the old will be removed without inserting any new value. Returns The result string Example - Using strREReplaceAll string strREReplaceAll ( "flower", "low", "orm" ) This call will change the string "flower" to the string "low". If the new parameter was left empty ("orm" in this case), the resulting string would have been "fer". Note! Regular expressions according to Java syntax applies. For further information, please refer to https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html . Warning! Using nonconstant regular expressions may consume large amounts of memory and should therefore be avoided. strReplaceChars Replaces characters in a string at a given position with another string and returns the new string. If str2 is longer than str1 , then the resulting string will be expanded to fit the complete str2 . string strReplaceChars ( string str1 , int position , string str2 ) Parameter Description Parameter Description str1 The base string position Position where str2 will start replacement in str1 . Position 0 (zero) indicates prior to the first character of str1 . str2 String to use for replacement Returns The result string. Note, the str1 and str2 are not modified. Example - Using strReplaceChars The following example returns the string: Hi Walter strReplaceChars("Hi Sister", 3, "Walt"); strSplit Splits a string where a given regular expression is matched into a list of strings. The function will try to match the regular expression and split the string as many times as possible. list<string> strSplit ( string str , string regex ) Parameter Description Parameter Description str The base string regex The regular expression which is to be used to split elements Returns A list of strings This function would typically be used for splitting strings of values that are comma separated, colon separated, or similar. The list returned will present the substrings in the order they occur in the original string. Example - Using strSplit strSplit("one,two,three", ","); Will return the following list: one two three strSplit("name:date:time", ":"); Will return the following list: name date time strSplit("person a person b person c", "person "); Will return the following list: a b c strStartsWith Returns true if str starts with substr , otherwise false . boolean strStartsWith ( string str , string substr ) Parameter Description Parameter Description str String to examine substr String to look for Returns true or false strSubstring Returns a substring from a given string. The extracted substring is from position start to end . string strSubstring ( string str , int start , int end ) Parameter Description Parameter Description str The base string start Position where to start the substring extraction. Position 0 (zero) points to the first character of str . end Position where to end the extraction. That is, the index of the letter after the last letter in the substring. Returns The substring Example - Using strSubstring The following call returns the string "the". strSubstring("hi there", 3, 6); strToLower Turns all letters in a string to lower-case. string strToLower( string str ) Parameter Description Parameter Description str The base string Returns The result string. Note, the str is not modified. strToUpper Turns all letters in a string to capital letters. string strToUpper( string str ) Parameter Description Parameter Description str The base string Returns The result string. Note, the str is not modified. strTrim Removes leading and trailing spaces from a string and returns the result. string strTrim( string str ) Parameter Description Parameter Description str The base string Returns The result string. Note, the str is not modified.

---

# Document 1319: GTP' LGU ReCollection Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204641600/GTP+LGU+ReCollection+Agent
**Categories:** chunks_index.json

This document describes the GTP' LGU ReCollection agent. This is a collection agent for real-time workflow configurations. The GTP' LGU ReCollection agent is useful for retrieving specific sets of charging data that is already collected by the GTP' agent, GTP' LGU Collection agent, or data that has been found missing. The agent receives UDRs that contain sequence numbers of GTP' Data Record Packages from the workflow. The agent uses these UDRs to initiate an FTP transfer of the specified packages from a network element, such as a GSN node. Open GTP' LGU ReCollection agent workflow Interaction Scenario The diagram below illustrates the data transfer between a workflow and a network element. Open Data transfer between workflow and GSN node The agent waits for a GTPRecollectionRequestUDR from the workflow. The UDR contains a list of sequence numbers, identifying packages that are to be transferred. The agent adds a request for each received UDR in an internal queue. The GTP' LGU ReCollection agent uses UDP to send the queued requests to the connected network element one by one. The network element transfers the requested data via FTP. When the FTP transfer is complete, the network element sends an answer to the GTP' LGU ReCollection agent via UDP. This step is repeated for each request in the queue. When the GTP' LGU ReCollection agent has received an answer for a request, it emits a GTPRecollectionResponseUDR that can be routed back to the workflow. The section contains the following subsections: GTP' LGU ReCollection Agent Configuration GTP' LGU ReCollection Agent Input/Output Data and MIM GTP' LGU ReCollection Agent Events and Limitations GTP' LGU ReCollection Agent UDR Types

---

# Document 1320: SCP Forwarding Agent Memory Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001951/SCP+Forwarding+Agent+Memory+Management
**Categories:** chunks_index.json

A global memory buffer will be allocated per EC. The size of the buffer is specified by using an Execution Context property in the EC's configuration file located in the relevant container. Note that this global backlog memory buffer is used and shared by this and any other forwarding agent that transfers files to a remote server. The same memory buffer is used for all ongoing transactions on the same EC. When several workflows are scheduled to run simultaneously, and the forwarding agents are assigned with the backlog function, there is a risk that the buffer may be too small. In such case, it is recommended that you increase the size of this property. Example - Setting a property to increase the maximum memory To increase a maximum memory to 20 MB: mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.forwarding.backlog.max_memory 20 You must restart the EC for the property to apply. If no property is set the default value of 10 MB will be used. The amount allocated will be printed out in the EC's log file. This memory will not affect the Java heap size and is used by the agent when holding a copy of the file being transferred.

---

# Document 1321: Inter Workflow Batch Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033608/Inter+Workflow+Batch+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events Ready with batch: name Reported when a batch is finished and then sent on to the subsequent agent. Merging batch: name Reported during batch merging. Last merged batch: name Reported upon activation, and shortly before the merging of a new batch is started. Batch cancelled: name Reported when a batch has been cancelled and routed to ECS. Debug Events There are no debug events for this agent.

---

# Document 1322: Oracle Preparations - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204637873/Oracle+Preparations
**Categories:** chunks_index.json

This section describes the preparations necessary when using Oracle as database and includes the following subsections: Extract Database Definition Files for Oracle Oracle Database Creation

---

# Document 1323: Removing and Refreshing Launcher Services - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204644431/Removing+and+Refreshing+Launcher+Services
**Categories:** chunks_index.json

To remove a launcher service, right-click on the name and select Delete from the pop-up menu . If the information provided by a launcher service has changed since the Desktop Launcher, you can refresh the displayed information. Right-click on the name and then select Refresh from the pop-up menu .

---

# Document 1324: Forwarding Agents - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205816537/Forwarding+Agents
**Categories:** chunks_index.json

A forwarding agent is responsible for distributing the data from the workflow to other systems or devices. An example of a forwarding agent could be one that creates a file from a data stream and transfers it to another system using FTP.

---

# Document 1325: Legacy Kafka Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138556/Legacy+Kafka+Profile
**Categories:** chunks_index.json

The Kafka profile enables you to configure which topic and which embedded service key to use, and it is loaded when you start a workflow that depends on it. Changes to the profile become effective when you restart the workflow. The Kafka profile configuration contains two tabs: Connectivity and Advanced . Connectivity tab The Connectivity tab is displayed by default when creating or opening a Kafka profile, and it contains the following settings: Setting Description Setting Description Topic Enter the Kafka topic that you want to use for your configuration. This field supports parameterization using ${} syntax, see Appendix 1 - Profiles for more information on how parameterization works. Use parameterized broker list Enable this checkbox if you want to set Brokers as a parameter instead. When enabled you need to set the parameter name using ${} syntax, see Appendix 1 - Profiles for more information on how parameterization works. Brokers A Broker is a node in a Kafka cluster. If you are using external Kafka, you must add Kafka Brokers. Use the Add button to enter the addresses of the Kafka Brokers you want to connect to. Host If you are using external Kafka, enter the hostname for Zookeeper. Port If you are using external Kafka, enter the port for Zookeeper. Group id Set this id to an identity of your choice, to enable load sharing between several Kafka collectors. Use this in combination with enable.auto.commit=true , in the Consumer and Advanced tab. This field supports parameterization using ${} syntax, see Appendix 1 - Profiles for more information on how parameterization works. Security Profile Select a Security Profile profile if you have defined it for your Kafka server. Advanced tab In the Advanced tab you can configure properties for optimizing the performance of the Kafka Producer and Consumer. The Advanced tab contains two tabs: Producer and Consumer . Producer tab In the Producer tab, you can configure the properties of the Kafka forwarding agent. The property producer.abortunknown=true sets the agent to abort if the broker replies with Unknown topic or partition . For further information on the other properties, see the text in the Advanced producer properties field , or refer to Apache Kafka . When running in Acknowledged execution mode, the property producer.full.response determines if the data sent to the Kafka log is also included in the response UDR. The value is set to true by default. Setting the value to false reduces the memory footprint. For information on how to configure the properties for SSL and Kerberos, please refer to https://www.cloudera.com/documentation/kafka/latest/topics/kafka_security.html . Caution! If you make any changes to the security configuration of the Kafka Producer, any topics used must be recreated before they can be used. This field supports parameterization using ${} syntax. For more information on parameterization see Appendix 1 - Profiles . Consumer tab In the Consumer tab, you can configure the properties of the Kafka collection agent. For further information about the properties, see the text in the Advanced Consumer Properties field, or refer to https://kafka.apache.org/25/documentation.html#consumerconfigs . This field supports parameterization using ${} syntax. For more information on parameterization see Appendix 1 - Profiles . Note! The sasl.jaas.config client property has been added to Advance Producer Properties and Advance Consumer Properties in the Kafka profile. This new property is used to configure SASL authentication directly in the client's properties instead of using a JAAS file. This simplification lets you run multiple clients in the same JVM by using different sets of credentials, which is not possible with a JAAS file. You can still use the existing java.security.auth.login.config system property which points to a JAAS file. However, this option allows only one set of user credentials for all client connections from a JVM. This means that MediationZone users wont be able to run multiple Kafka workflows against different Kafka brokers on the same EC. When both the JAAS configuration system property ( java.security.auth.login.config) and client property ( sasl.jaas.config) are specified, the client property will be used.

---

# Document 1326: Execution Manager - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205030703/Execution+Manager
**Categories:** chunks_index.json

With Execution Manager you can enable, activate, and monitor multiple workflow groups. To open the Execution Manager, go to Manage  Tools & Monitoring and then select Execution Manager . Open The Execution Manager The Execution Manager contains three tabs, Overview , Running Workflows , and Detail Views . You can open a separate Detail View tab for every workflow group or groups, that you want to monitor. Note! The times displayed in the Start time and Next run columns are based on the timezone of the Desktop Client. Overview Tab The Overview tab is comprised of the Status box and the workflow groups table. Status Log In the Status Log, you see a log of events for workflows and workflow groups, which are also displayed in the System Log . Workflow Groups Table The Workflow Groups Table displays the following information about all the workflow groups that are configured in the system: Column Description Open Selection Checkbox The selection check boxes can be used for selecting specific entries that you want to perform actions for. When selecting a check box, the table action bar will change and display a number of buttons you can use to perform different actions, see Table Action Buttons and Right-Click Menu . Name This column displays the workflow group name. Mode This column shows whether the workflow group is enabled or disabled. If the workflow group is enabled it can be activated by its scheduling criteria. If it is disabled, the workflow group can only be started manually. State This column displays the current state of the workflow group, see Workflow Group States . Runtime Info This column displays detailed information about the workflow group state. Started By This column displays how the workflow group was started: Schedule - based on the workflow group scheduling criteria A user The parent workflow group Workflow name if it is auto-started Note! The value will be Unknown if the Platform has been restarted. Start Time This column displays the last time an execution was started. The time will be based on the timezone of the Desktop client. Next Run This column displays the next scheduled execution time. The time will be based on the timezone of the Desktop client. Note! If the workflow is not enabled this space column will be empty. Next Suspend Action This column displays the suspend action which is either the scheduled execution suspension of a configuration (workflow or workflow group) or the removal of such a suspension (activation enabling). Table Action Buttons and Right-click Menu When selecting one or multiple check boxes for workflow groups, the table action bar will change and display a number of buttons you can use to perform different actions. Right-clicking a row in the table opens a menu with the same options although they will only apply for the workflow group you right-click, that is only one single group. The following actions are available: Action Description Clear Clears all selected check boxes. This option is only available in the Table Action Bar since right-clicking a workflow group will always a one-group operation. Detail View/Open in Detail View Opens the workflow group(s) in a separate View tab in the Detail Views tab. Start Triggers execution of the selected workflow group(s). Stop Stops the execution of the selected workflow group(s). Enable Enables the selected workflow group(s). Disable Disables the selected workflow group(s). Note! In the Table Action Bar, this option is available when clicking on the three dots to the right in the panel and selecting it in the menu that is displayed. View Abort Message Opens an error dialog box that specifies the reason for aborting the execution of the particular workflow group or its workflow member. Note! In the Table Action Bar, this option is available when clicking on the three dots to the right in the panel and selecting it in the menu that is displayed. Open Editor Opens the workflow group configuration. Running Workflows Tab The Running Workflows tab displays the workflows that are currently running as well as the ones that are unreachable. Open Note! To stop a workflow, right-click on the workflow and then select Stop . You can also open the Workflow Monitor by selecting Open Monitor . The Running Workflow tab table contains the same columns as the Overview tab, as well as the following: Column Description Name This column displays the name(s) of the running workflow(s). Status This column displays the status of the workflow(s), see Workflow Execution States . EC This column displays the pico name of the EC on which the workflow is running. Debug This column displays whether debug is turned on or off, see Workflow Monitor . Backlog This column displays the number of files that are yet to be processed. The value in the Backlog column is identical to the Source Files Left MIM value. Throughput This column displays the throughput for the workflow's collecting agent. The value shows either number of UDRs or bytes, depending on what the collecting agent produces, and is updated every five seconds as long as the workflow is being executed. Detail Views Tab The Detail Views tab displays the workflow group(s) that you have selected to view details for in the Overview tab. Note! Detail views are saved as part of the user preferences, and therefore enable you to export and import them along with user information. A workflow group that is marked with a yellow warning icon in the Detail View is invalid. The table in this tab contains the same columns as the Running Workflows tab as well as the following: Column Description Prereq This column displays a comma-separated list of the workflow group Prerequisites settings. See Members Execution Order in Managing a Workflow Group . Next Suspend Action The suspend action is either the scheduled execution suspension of a configuration (workflow or workflow group), or the removal of such a suspension (activation enabling). Right-click Menu If you right-click on a row in the table, a menu opens with the following options: Entry Description Debug On/Off This option turns debug on or off for the selected workflow, see Workflow Monitor . Open in Monitor This option opens the selected workflow in the Workflow Monitor. Note! This option is not applicable to workflow groups. Opening a Detail View for a Workflow Group To open a Detail View of a workflow group: In the Overview tab, right-click the workflow group, or select the check boxes for a number of workflow groups. Select Open Detail View option in the right-click menu, or click on the Detail View button in the table action bar. The New Detail View dialog box appears. Open New Detail View dialogue Enter a unique name for the view and click OK. The Detail Views tab opens and displays the selected workflow groups and their members in a separate table. Open Detailed View tab Managing Detail Views Click on the Manage Detail Views button to set the visibility of the Detail Views of workflow groups. To Manage Detail View Tabs: Select Manage Detail Views ; the Manage Detail Views window opens. Open Manage Detail Views Select the Visible checkbox to set the visibility of Detail Views of each workflow group. In the table, you can click the Refresh button to refresh the Detail Views table or click the Delete button to delete the Detail Views table of the workflow group.

---

# Document 1327: Redeploying Spark Workers - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204645718/Redeploying+Spark+Workers
**Categories:** chunks_index.json

Use the provided scripts stop.sh and start.sh to redeploy the Spark workers. This is required after you have updated the number of worker nodes in the spark cluster. increased or decreased the number of slave Service Contexts that are available to the spark service. $ stop.sh Example - Redeploying the Spark workers $ start_master_workers.sh When you change the number of spark slaves, you may also want to update the property spark.default.parallellism in the service configuration. This is to optimize the performance of the Spark cluster. However, for the change in the Spark configuration to become effective, you must restart the cluster and submit the spark application. For further information about spark.default.parallellism , see KPI Management - External Software . If the Spark UI indicates that a worker has the status "DEAD", you must restart the cluster. Note! It is not recommended to run more than one Spark worker per host. The command above will only restart one worker per host .

---

# Document 1328: Categorized Grouping Transaction Behavior and Input/Output Data - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204640289/Categorized+Grouping+Transaction+Behavior+and+Input+Output+Data
**Categories:** chunks_index.json

Transaction Behavior Emits The agents does not emit any commands. Retrieves Command Description Command Description Cancel Batch If a cancelBatch is emitted by any agent in the workflow, all data in the current transaction will be disregarded. No closing conditions will be applied. Input/Output Data The agent receives UDRs of CGAgentInUDR type and emits UDRs of CGAgentOutUDR type.

---

# Document 1329: Installation of Control Zone and Execution Zone for PCC - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736306
**Categories:** chunks_index.json

Install two Platform Containers with separated databases on two separate machines. These installations constitute the [CZ]. For each Platform Container, install an Execution Container. These installations constitute the [EZ]. Open Installing [CZ] and [EZ]

---

# Document 1330: The Prometheus Filter - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205001332/The+Prometheus+Filter
**Categories:** chunks_index.json

Use this filter to configure the Prometheus metrics that are going to be exposed for scraping. The purpose of the Prometheus Filter is to prevent the flooding of metrics in the storage of the Prometheus host. The filter is positive, so you define what you want to get. Except in the situation where no Prometheus Filter exists or all existing Prometheus Filters are disabled, then all available metrics are exposed for scraping. The exception is MIM metrics from batch workflows. In the case of MIMs from batch workflows, no metrics will be collected unless there is a filter whitelisting them. A Prometheus filter consists of one or many metric filters. Each metric filter has a Metric Name Filter and optionally a number of Label Filter(s) . A metric that passes at least one of the metric filters will be exposed for scraping. Open Exa mple o f a Prometheus filter with one metric filter that has no label filters. Setting Description Setting Description Ena bled Whether this Prometheus Filter is enabled or not. Metric Name Filter A filter is applied to the metric name. Label Filter(s) A filter that applies to a label that exists on the metric(s) passes the corresponding metric name filter. To Add a Metric Filter To add a metric filter, take the following steps: Click the Add button. In the dialog that is displayed, enter a Metric Name Filter in the form of a regular expression. Optionally, specify one or many label filter(s) by entering the Label along with a corresponding Label Value Filter The Label Value Filter has to be entered in the form of a regular expression. Click Add to add the metric filter. Click Close when done. Open For example, five metric filters are configured. See the detailed description in the table below. Setting Description Setting Description jvm_.* java_.* process_.* Will give you Global statistics (JVM, processes, event management). com_digitalroute_event.* MIM metrics related to the event handling system. com_digitalroute_wf_.* workflow=Default.myWorkflow.* Workflow MIM metrics for the workflows in the Default folder that has a name beginning with "myWorkflow".

---

# Document 1331: Platform Software Installation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736151/Platform+Software+Installation
**Categories:** chunks_index.json

The final step in the Platform Container installation must be executed as the mzadmin UNIX user. The following command session sets up the directory structure and installs all the software. The release content directory is the working directory for the installation. To install the software: Enter the release content directory: $ cd ./<staging directory>/<release content directory> Pre-installation validations are performed to verify the environment and configuration, ensuring a seamless installation experience. To perform validation without starting the installation, run the following command: $ ./setup.sh install -validate-only Run the following command to initiate the installation process. $ ./setup.sh install The Platform is automatically started and shut down during the installation. Note! If the Platform failed to start, check the platform.log file in $MZ_HOME/log for details. If validation fails but you choose to proceed, you can run the following command: $ ./setup.sh install -skip-validate Caution! You can choose to skip validation, but be aware that this bypasses checks for potential issues in your environment and configuration. If there are undetected problems, the installation may fail or cause unexpected behavior. Proceed with caution and ensure you have backups before continuing. Start the Platform: $ mzsh startup platform The installation of the Platform Container is now complete.

---

# Document 1332: mzcli - wfcommand - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/547979915/mzcli+-+wfcommand
**Categories:** chunks_index.json

Usage usage: wfcommand <pattern matching expression for workflow name> <'Workflow' | workflow service | agent> <instructions> ... Debug The debug command cannot be sent to a specific agent, it is only functional on a workflow level. To turn debug on for a workflow: wfcommand <workflow> Workflow debug on To turn the debug for a workflow off: wfcommand <workflow> Workflow debug off Aggregation and Diameter agents The Aggregation and Diameter agents allow the user to interact with the flow of data via the wfcommand . For further information on these agents, see Aggregation Agent and Diameter Agents . GTP agent The GTP agent allows the user to view the MIM counters with wfcommand printcounters . For further information, see https://infozone.atlassian.net/wiki/x/C5UyD . The request counters and the timestamp, which are published as MIM values are also possible to view from the command line tool. Syntax to run the wfcommand function in mzcli: mzcli wfcommand <Workflow Name> <Agent Name> printcounters Parameters Parameter Description Workflow Name Name of the workflow that contains the GTP agent. Agent Name Name of the GTP agent in the workflow. Example - Executing the wfcommand $ mzcli mzadmin/dr wfcommand "Default.myWorkflow" "GTP_1" printcounters Default.myWorkflow (3): Data Record Count: 138 Cancel Data Count: 2 Message Error Count: 0 Possible Duplicate Count: 5 Release Data Count: 1 Out of Sequence Count: 0 Duplicate Message Count: 0 Redirect Count: 0 Last Request Timestamp: Thu Mar 07 14:38:12 CET 2013 Supervision By stating the Supervision service you can select to trigger and clear supervision actions with the wfcommand command. If you want to use the wfcommand to trigger the Supervision service, you can append the following instructions: manual which has the following syntax: Usage: manual [-action actionName actionParameters] Simply entering manual will deactivate the Supervision Service configurations and switch to manual mode, which means that no conditions will be evaluated and no actions will be executed. When using the -action option, the syntax will be as follows: Usage: manual -action overload ( <ratio> | -trigger | -clear ) [strategy] Options Option Description Option Description ratio If you want to trigger overload and reject a specific ratio of incoming requests, e g reject every fifth request, you can enter the ratio number here. 2 equals every second request, 5 equals every fifth request, etc. -trigger Use this option if you want to trigger overload for everything. An event will then be sent out for monitoring purposes. This will disable the automatic Supervision configuration. -clear Use this option if you want to clear overload for everything. Any Diameter/Radius overload protection strategies will be cleared and no requests will be rejected. The automatic Supervision configuration will then be disabled. Note! Using this option will not enable the automatic Supervision configuration. [strategy] Use this option to state a specific Diameter/Radius strategy, for example, Diameter_AbortSessionRequest, Diameter_CCEventRequest, etc. automatic is used to revert to automatic Supervision configuration. When switching from automatic to manual mode, all active overload protection will remain active. However, in the opposite direction, when switching from manual to automatic mode, all active overload protection will be reset and the appropriate overload protection will be set by the automatic rules. Each time you switch between manual and automatic mode, an entry will be logged in the System Log for monitoring purposes. Example - Trigger Overload Protection To trigger overload protection for a workflow: wfcommand <workflow> "Supervision" manual -action overload -trigger An event will then be sent out for monitoring purposes. To clear overload protection for a workflow: $ wfcommand <workflow> "Supervision" manual -action overload -clear To reject every fifth Diameter AbortSessionRequest in a workflow: $ wfcommand <workflow> "Supervision" manual -action overload 5 Diameter_AbortSessionRequest To switch back to automatic mode: $ wfcommand <workflow> "Supervision" automatic See https://infozone.atlassian.net/wiki/x/zYCbHQ for more information. Return Codes Listed below are the different return codes for the wfcommand : Code Description Code Description 1 Will be returned if command-line arguments are missing. 2 Will be returned if no matching workflows can be found, or if the workflow is not running. 3 Will be returned if the target agent cannot be found, or if the agent is not accepting/supporting the command.

---

# Document 1333: Disk Agents in Real-Time Workflows - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204610658
**Categories:** chunks_index.json

This section describes how to configure the Disk agents in real-time workflows. The section contains the following subsections: Real-Time Disk_Deprecated Collection Agent Real-Time Disk_Deprecated Forwarding Agent

---

# Document 1334: Changing Platform Database - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205657718/Changing+Platform+Database
**Categories:** chunks_index.json

This feature is currently not available. See the known issue Known Issues | Unable to Change Platform Database After Installation

---

# Document 1335: SCP Forwarding Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609320/SCP+Forwarding+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor . For further information about the agent message event type, see Agent Event . Ready with file: filename Reported, along with the name of the target file, when the file is successfully written to the target directory. Debug Events Debug messages are dispatched when debug is used. The messages are during execution shown in the Workflow Monitor and can also be stated according to the configuration done in the Event Notification Editor . For further information about the agent debug event type, see Debug Event .

---

# Document 1336: Documentation Generator - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204676088
**Categories:** chunks_index.json

Using the Documentation Generator, you can create documentation for the configurations that you have created. Generating Automated Documentation To open the Documentation Generator, click the Tools button in the upper left part of the Desktop window, and then select Documentation Generator from the menu. Open The Documentation Generator To generate documentation on the configurations in the system, select the Output Target directory in which you want to generate the documentation. To select a directory click the Browse... button, select the target directory, and click the Save button. Click the Generate button. You can then open the generated HTML file (index.html) in your web browser from the selected target directory. Open Documentation displayed in web browser Content of the Documentation The documentation generated includes up-to-date information on the saved configurations. The sections included vary according to the type of configuration documented and how it is used. The possible sections included are the following: Section Description Section Description Workflow An image of the configuration. This section is only included for workflow configurations. Globals The variables and constants are declared globally. This section is only included for APL Code configurations. Functions The APL functions. This section is only included for APL Code configurations. Description The content is provided by the user in the configuration profile, using the Documentation dialog. For example, you can provide a description and the purpose of the configuration in the dialog. For further information on how to populate this section, see the Documentation section in Desktop User Interface . Uses A list of all the configurations that the configuration uses, for example, APL code or Ultra format. Used By A list of all the configurations using the configuration. Access A list of the users who have access to the configuration.

---

# Document 1337: Network Security - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205848669
**Categories:** chunks_index.json

This chapter includes the following sections: Communication Through Firewalls RCP Encryption HTTP Encryption

---

# Document 1338: pico - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204612513/pico
**Categories:** chunks_index.json

This command is used to: list, add, or delete EC groups view pico instances mark for shutdown usage: pico <command> [<args>] Add a Group Usage: -add GroupName Delete a Group Usage: -delete GroupName List Groups Usage: -list Regexp View Running Groups & Processes Usage: -view Pico Name Mark ec for shutdown Usage: -mark_for_shutdown <Pico Name> <IP/Host> <true/false> -list usage: pico -list Use pico -list to list available EC groups. Example - pico -list The following commands list all EC groups. mzsh <username>/<password> pico -list Output: ecgroup2 ec ecgroup1 ec -view usage: pico -view <pico name> Use pico -view to view the status of pico instances in the system. Example - pico -view The following displays the status for all pico instances in the system. mzsh <username>/<password> pico -view Output: Pico Start Memory Response Error Marked For Name Time (Used, Commited, Max) Time (ms) Status Shutdown ec1 (ip:port) <time> 1.1, 24.2, 27.6 1 OK ec2 (ip:port) <time> 1.2, 23.3, 28.7 1 OK MZSH:45881 (ip:port) <time> 5.9, 15.5, 47.5 2 OK Platform (ip:port) <time> 8.2, 20.0, 27.6 2 OK The Memory column is a comma-separated list of Used, Committed, and Maximum memory, and Response Time is measured in milliseconds. If Error Status indicates "Error", an OutOfMemoryError has occurred and additional information will be included in the output. For example, in case of an OutOfMemoryError on the Platform, the following could be shown: Platform: Mon Jan 23 09:59:47 CET 2012 OutOfMemoryError on platform at platform-e6410. See log/platform.log for more information OutOfMemoryError is further described in Out of Memory Info in System Log If Error Status indicates "Connection failure", an RCP error has occurred. The following command displays the status of the pico instance named ec1. mzsh <username>/<password> pico -view ec1 Output: Pico Name: ec1 (10.0.0.8:37197) OS: LINUX OS Version: 2.6.38-11-generic-pae Architecture: I386 Processors: 8 Java Version: 1.8.0_121 Loaded Classes: 2854 -add usage: pico -add <ec group name> Use pico -add to add an EC group to the system. Example - pico -add The following command adds an EC group named ecgroup1. mzsh <username>/<password> pico -add ecgroup1 Output: Group ecgroup1 added -delete usage: pico -delete <ec group name> Use pico -delete to delete an EC group from the system. Example - pico -delete The following command deletes an EC group named ecgroup1. mzsh <username>/<password> pico -delete ecgroup1 Output: Group ecgroup1 deleted -mark_for_shutdown usage: pico -mark_for_shutdown <ec1 localhost true> Use pico -mark_for_shutdown to signal to the Platform that an EC is scheduled to be shut down. As a result, the Platform will not assign workflows to the EC. Example - pico -mark_for_shutdown The following command signals that the EC named EC1 should not be assigned workflows mzsh <username>/<password> pico -mark_for_shutdown ec1 localhost true Output: OK Return Codes Listed below are the different return codes for the pico command: Code Description Code Description 0 Will be returned if the command was successful. 1 Will be returned if arguments can not be parsed or if arguments are missing. 2 Will be returned if the communication with the Platform fails. 3 Will be returned if checking of user privileges failed, or if pico already exists when trying to add a new pico. 4 Will be returned if the user does not have permission to add or delete picos. 5 Will be returned if an unexpected error occurs.

---

# Document 1339: Radius Example Analysis Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674384/Radius+Example+Analysis+Agent
**Categories:** chunks_index.json

All decoding, validation, and manipulation are performed by the Analysis agent. The code logic is as follows: Each time the workflow is activated, a subscriber table is read into memory. To keep the example simple, the table content is assumed to be static. For a real implementation, it is recommended to re-read the table on a regular basis. Decode the UDP packet. Consider only UDRs of the type Default.extendedRadius.Access_Request_Int . Perform a lookup against the subscriber table, and create a reply of the type Default.extendedRadius.Access_Accept_Int or Default.extendedRadius.Access_Reject_Int , depending on whether the subscriber was found in the table. Route the reply back to the Radius agent. table tmp_tab; initialize { tmp_tab = tableCreate("select SUBSCRIBER from VALID_SUBSCRIBERS"); } consume { list<drudr> reqList = listCreate(drudr); radius.Radius r = (radius.Radius) input; string err = udrDecode("Radius.Request_Dec", reqList, r.requestMessage, true); if ( (err != null) || (listSize(reqList) != 1) ) { debug("Decoding error: " + err); return; } drudr elem = (drudr) listGet(reqList, 0); if (instanceOf(elem, Default.extendedRadius.Access_Request_Int)) { Default.extendedRadius.Access_Request_Int req = (Default.extendedRadius.Access_Request_Int) elem; table rowFound = tableLookup( tmp_tab, "SUBSCRIBER", "=", req.User_Name ); if (tableRowCount(rowFound) > 0) { Default.extendedRadius.Access_Accept_Int resp = udrCreate(Default.extendedRadius.Access_Accept_Int); resp.Identifier = req.Identifier; r.responseMessage = udrEncode("Default.extendedRadius.Response_Enc", resp); udrRoute( r ); } else { Default.extendedRadius.Access_Reject_Int resp = udrCreate(Default.extendedRadius.Access_Reject_Int); resp.Identifier = req.Identifier; r.responseMessage = udrEncode("Default.extendedRadius.Response_Enc", resp); udrRoute( r, "Response" ); } } else { debug("Invalid request type"); } }

---

# Document 1340: KPI Management - Distributed Processing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742761
**Categories:** chunks_index.json

The following scheme demonstrates a KPI Management provisioning and processing scenario. Open Provisioning and processing The steps used (the subsequent pages will describe this in more detail) A user provisions a service model configuration via a KPI profile in the Desktop. The user then submits an application, which performs the KPI calculations, to the Spark cluster. Input data is received by the KPI Cluster In agent as KDR UDRs. The agent then transfers the content through Kafka to the Spark Cluster. The Spark cluster periodically polls the input topic and performs the KPI calculations that are based on the service model and the input data. The polling interval depends on the duration of the Spark batch intervals. When the timestamps of the input data indicate that a configurable time period has elapsed, the Spark cluster sends the calculated KPIs to a dedicated output topic. There is also a separate topic for alarm output. If the service model has been configured to produce immediate alarms, the Spark cluster sends the KPIs that hit an alarm level, within a Spark batch, potentially before their KPI period closes. The data on the output and alarm topics are collected via KPI Cluster Out agents extracts and decodes the KPI data to KPIAggregatedOutput UDRs. This chapter includes the following sections: KPI Management Quick-Start Guide - Distributed Processing Preparing and Creating Scripts for KPI Management KPI Management - External Software KPI Management Agents KPI Management Service Model Deployment - Distributed Processing Managing the Spark Cluster KPI Management Scaling Considerations

---

# Document 1341: SAP RFC Processor Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034531/SAP+RFC+Processor+Agent+Configuration
**Categories:** chunks_index.json

The SAP RFC Processor agent is a realtime processing agent, which obtains the UDR generated from the selected SAP RFC profile. The UDR type for the RFC function selected, is determined according to the structure in SAP. The agent sends the populated UDR to the SAP system with the connection details configured in the SAP RFC profile. To open the SAP RFC agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select SAP RFC in the Processing tab in the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. Open SAP RFC Processor agent configuration dialog Setting Description Setting Description Connection Details Settings Profile Click Browse to select the SAP RFC profile you want to use. Performance Settings Enable Connection Pool Select this check box to use a connection pool to communicate with the SAP Java Connector. Max Idle Connections Enter the maximum number of idle connections kept open by the destination. A value of 0 means there is no connection pooling. The default value is 2. Max Active Connection Enter the maximum number of active connections that can be created for a destination simultaneously. The default value is 10. Max Queue Size Enter the maximum number of records waiting to be processed in the queue. This prevents an out of memory error from occurring if the SAP System is too slow to consume the records. The default value is 1000. Number of Threads Enter the number of threads simultaneously uploading into the SAP System. The minimum value is 1. The default value is 5. Cache Size Enter the cache size for the SAP FunctionTemplate in the agent. The minimum value is 10. The default value is 1000. Cache Expiry (min) Enter the cache expiry in minutes. The minimum value is 1. The default value is 30. Enable Immediate Stop Select this check box to enable the agent to stop immediately and ignore pending items in the queue. Enable Logging for Execution Exceeding Time Threshold Select this check box to enable the System Log notification when an RFC Function execution has exceeded the configured time threshold. Execution Threshold Time (min) This field determines the execution time threshold before the System Log notification event is sent.

---

# Document 1342: SNMP Collection Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204674790
**Categories:** chunks_index.json

The SNMP Collection profile allows you to import the MIB files you want to use to build your target UDRs. It also allows you to specify the set of default SNMP parameters that will be used by the SNMP Request agent and the SNMP Trap agent. Note that one profile is usually for one family of network equipment. In case you have several network equipment families you must create one profile for each. Configuration The SNMP Collection profile contains the following tabs: 1 MIB Workspace Tab 2 Default SNMP Parameters Tab 3 Advanced Tab MIB Workspace Tab In the MIB Workspace tab, you load the MIB files to be used by SNMP collection agents in the MIB Workspace section. You can verify that the target network element responds to SNMP requests directly from the MIB Workspace. Use the SNMP Query Results section to send out SNMP requests including SNMP GET, GET-NEXT, GET-BULK, GET-WALK, and SET. Open SNMP Collection Profile - MIB Workspace tab Button and field Description Button and field Description MIB Workspace Load Select the MIB files you want to load and use to build the target UDR. These types of files are often defined with the extension .mib by the vendors, but there is no naming convention and it can be anything. Note! For any MIB files with dependencies, you are required to upload these files manually through the Load button feature. This is not the case if you are using the legacy desktop as all MIB files are loaded automatically by default. Unload Select the MIB file you want to unload and click this button. Unload All Click this button to unload all MIB modules. Expand All Click this button to e xpand all MIB tree branches . MIB Object Properties This panel shows the properties of the selected MIB file. SNMP Query Results OID Object Identifier of the currently selected MIB Object. This OID is used to send out SNMP requests to remote network element for testing purposes. VALUE The Value field is used to specify the value of the SNMP SET operation, which you can edit. The value must follow the syntax and semantics of the selected MIB Object. This field is also used to populate the result of the SNMP Get command, which is grayed out and cannot be edited. SNMP GET Performs SNMP GET operations on the selected MIB object. SNMP GET-NEXT Performs SNMP GET-NEXT operations on the selected MIB object. SNMP GET-BULK Performs SNMP GET-BULK operations on the selected MIB object. SNMP WALK Walks the selected MIB tree branch and retrieves all values. SNMP SET Performs SNMP SET operations on the selected MIB object. Specify the value to be set in the Value field. STOP Stops the current SNMP operation. CLEAR Clears the SNMP results area. Default SNMP Parameters Tab The SNMP Request agent uses the CSV files as information source to poll network elements. The parameter from the default set is used when the corresponding parameter is unspecified in the input CSV file. This is valid for all parameters except two in the profile: Polling interval Trap port Open SNMP Collection Profile - Default SNMP Parameters tab Field Description Field Description General Host Define the host used to perform SNMP queries for testing purposes directly from the MIB workspace tab. This value is optional since the hosts should be defined in the CSV file of the SNMP Request agent. Polling Interval (ms) The default polling interval for SNMP is 300 seconds. This value is used if no other value is defined in the CSV file of the SNMP Request agent. The value must always be set here, it can not be set in the input CSV file. Polling Distribution Period (ms) This field is used to configure how long the distribution period should be in milliseconds (ms). By default this field is set to zero ("0"), meaning that no distribution period is configured, and polling will be distributed evenly over the polling interval. If you configure the distribution period, the polling will be distributed within this period only during the polling interval, meaning the distribution period has to be equal to, or shorter than, the polling interval. The distribution period will start at the beginning of the polling interval. Port Default port for SNMP is 161. If no other value is defined in the CSV file of the SNMP Request agent this value is used. Trap Port Default trap port for SNMP trap is 162. This value is used by the SNMP Trap agent to listen for incoming SNMP notifications (TRAPs and INFORMs). This value must always be set here. SNMP Version SNMPv1 Use read and write communities to access the network element via SNMPv1. Note! If there is no other value defined in the CSV file of the SNMP Request agent, this value or the SNMPv2c and SNMPv3 value is used based on the selected option. SNMPv2c Use read and write communities to access the network element via SNMPv2c. For more information about the SNMPv2c, see SNMP_Wiki . Read Community The SNMP community string is like a user ID or password that allows access to a target network element. The read community is used for GET, GET-NEXT, and GET-BULK requests. Write Community The SNMP community string is like a user ID or password that allows access to a target network element. The Write community is used for SET requests. SNMPv3 Use the same parameters and authentication as defined in the network element. For more information about the SNMPv3, see SNMP_Wiki . Note! The parameters "Retry" and "Timeout" are not used as expected. This is due to an error in the underlying library used. "Retry" is used during the discovery call, as well as the actual call, when it should only be used during the actual call. "Timeout" is not only used during actual timeouts, but between any "Retry" attempts. User Name The principal on whose behalf access is requested. This must be a human-readable string representing the user in a format that is Security Model independent. Context Name The collection of management information accessible by an SNMP entity. Context Engine ID An SNMP engine provides services for sending and receiving messages, authenticating and encrypting messages, and controlling access to managed objects. There is a one-to-one association between an SNMP engine and the SNMP entity which contains it. Within an administrative domain, an EngineID is the unique and unambiguous identifier of an SNMP engine. Context Name - the name of an SNMP Context. Auth Protocol The authentication protocol that is used to authenticate the user. Four protocols are defined: SHA-2 224, SHA-2 256, SHA-2 384, and SHA-2 512. Auth Password The password required for authentication service. Priv Protocol The privacy protocol that is used to protect the message from disclosure. Two such protocols are defined: DES-CBC Symmetric Encryption Protocol and CFB-AES-128. Priv Password The password required for privacy (encryption) service. Request Handling Timeout (ms) The timeout is the time interval that the SNMP Request agent waits for a response message from the target network element. The timeout value is given in milliseconds. Note! This timeout must be set so that the following is true: "Polling Interval > Timeout * (Number of Retries +1)". This is checked when validating the SNMP profile. However, take care when using the CSV_TIMEOUT and CSV_RETRIES in the Network Element file. UDR Timeout (ms) UDR Timeout in milliseconds. If the time exceeds the timeout, the UDR will be sent on the next route. Note! This UDR timeout must be set so that the following is true: "UDR timeout > Timeout * (Number of Retries +1)". This is checked when validating the SNMP profile. Number of Retries Retries are the number of times a SNMP request is sent when a timeout occurs. If the retry value is zero (0), the request is not re-transmitted during timeout. Max Outstanding Per Element The maximum number of simultaneous outstanding requests per element allowed. When this limit is reached, no new requests are sent to the element until responses for the outstanding requests are received. The default value is set to zero (0), which means that no limit is set and any number of simultaneous outstanding requests is allowed. Advanced Tab The following fields are available in the SNMP Profile Advanced tab. Open SNMP Collection Profile - Advanced tab Field Description Field Description General Max PDU Size The maximum size of request PDUs that this target is able to receive. The response PDU has to fit in a single UDP packet. The maximum limit is 65535 bytes. Get-Bulk Parameters Non Repeaters Number of variables in the variable list for which a simple GETNEXT operation has to be done. Max Repetitions Number of continuous GETNEXT operations. Scalar Value Retrieval Max Variables Per PDU The maximum number of variable bindings per request. Table Retrieval Max Columns Per PDU Maximum number of columns to retrieve per SNMP GETNEXT or GETBULK request. Max Rows Per PDU Maximum number of rows to retrieve per SNMP GETBULK request. Send Table Requests in Separate PDUs By default all column OIDs from a single table are now sent in one GETBULK request in accordance with the Max Variables Per PDU. Use this if All column OIDs from a single table are to be sent in separate GETBULK requests .

---

# Document 1343: Workflow Table - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997546/Workflow+Table
**Categories:** chunks_index.json

The workflow table is located at the bottom of the workflow template. There must always be at least one valid and runnable workflow per workflow configuration, otherwise it will not be valid. The three leftmost columns gather the workflow meta data: Valid, ID, and Name. A workflow table always contains these columns. The ID is automatically generated starting at 1. The ID is unique within the workflow configuration. The Name is generated based on the ID, for example 'Workflow_1'. The names can be edited, however, if two workflows have exactly the same name a validation error will occur. The workflow table is populated depending on settings made in the Workflow Properties dialog, in the Workflow Table tab. For example adding rows and field type settings is done there and will propagate changes in the workflow table. Apart from the three first columns, columns in the table represent fields of Default and Per Workflow type. See Workflow Properties for further information about the field types. You can also configure your own fields to be referenced by your APL code, which can be updated. The fields are configured in the Workflow Properties and referenced by the APL command dynamicFieldGet . The fields can be of either boolean , int , or string type, and you can configure different categories for the fields, for an easier overview. Open Example of a workflow table If there is a default value for a field, it is displayed in the table as <default value> . If no default value is set in the agent in the template, < > is displayed. If the field is of a Per Workflow type and not yet defined, the symbol is shown in the cell pointing out that the cell is not valid. Different icons that can appear in the workflow table: Icon Description Icon Description Open Workflow is not yet validated Open Workflow is valid and runnable Open Workflow is invalid Open The cell is invalid. Open The field contains an External Reference. The order of table rows can be switched to not ordered, ascending or descending by clicking on the header one or multiple times. Info! Number fields will be numerically ordered and other fields will be ordered alphabetically. If External References are used, ordering will be done alphabetically by External Reference name. If the column is for a number field, numbers will be numerically ordered and separately grouped from the alphabetically ordered External References cells. Table Toolbar The workflow table toolbar presents you with options to manipulate each workflow and any data within the rows. The following are the buttons found on the bar: With no workflows selected With no workflows selected Open Option Description Search Allows you to search the text of all columns. The Valid column is excluded from the search as it only have icons for values. Add workflow Creates a new workflow by adding a row into the workflow table. The button opens the Add Workflow dialog and allows you to rename the workflow. If you have set some of the agent configuration to be configurable per workflow in the Workflow Properties then you will be able to configure those fields. Export Opens an export dialog where you select and save workflow configurations in a file. With this export file you transfer and update workflow table data either on your current machine or on a different client. The export file can be created in the following format: .csv The .csv export file contains a header row, comma (,) delimited fields, and text values that are delimited by a quotation mark ("). Exported fields that contain profiles are given a unique string identifier. The ID and Name fields are exported as well. In the export file, External References are enclosed in braces ({}) and preceded by a dollar symbol ($), for example ${OUTPUT_DIR} . For further information see External Reference Profile . Import Opens an import dialog where you can import an exported file. This file might contain, for example, data that has been saved in the workflow table, locally or on a different client. This command supports the following file format: .csv Selecting Keep old workflows will result in the contents of the file being appended into the workflow table. Selecting Save before import will save the workflow configuration before proceeding with the import. This is so you can rollback to the previous version in case the import goes wrong. Importing a table may result in any of the following scenarios: If the ID number of the imported workflow (a table row), is identical to the ID number of one of the rows that are already in the table, the imported entry overwrites the existing one. If the ID number of the imported workflow is -1, the imported entry is added to the bottom of the table. If the ID number of the imported workflow does not exist in the table and is not equal to -1, the imported entry is added to the table. The ID number remains the same number as it was in the import file. With one workflow selected Open Option Description Clear selection(s) Clears the currently selected row so that it is not selected anymore. Edit Opens the Edit Workflow dialog where you are able to edit the agent configuration field values that have been set to be configurable per workflow. For more on what per workflow is, refer to Workflow Properties . Delete Removes the entire workflow and all configurations associated with the selected row. If removed, the ID number of that workflow will never again return within that workflow configuration. Validation Message Opens an Information dialog where a message regarding the validity of the template, workflow, and cell is shown. With many workflows selected Open Option Description Clear selection(s) Clears all currently selected row(s). Delete Removes the entire workflow and all configurations associated with the selected rows. If removed, the ID number of that workflow will never again return within that workflow configuration. Right-Click Menu Item Description Item Description Edit Opens the Edit Workflow dialog where you are able to edit the agent configuration field values that have been set to be configurable per workflow. For more on what per workflow is, refer to Workflow Properties . Delete Removes the entire Workflow that is associated with the selected row. If removed, the ID number of that workflow will never again return within that workflow configuration. To remove more than one workflow, select all the relevant rows. Validation Message Opens an Information dialog where a message regarding the validity of the template, workflow, and cell is shown.

---

# Document 1344: Redis Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881522/Redis+Configuration
**Categories:** chunks_index.json

After Redis has been installed, you can create Redis profiles to communicate with Redis. For information on how to extend the Redis database, see https://aws.amazon.com/documentation/elasticache/ . Note! Created or updated Redis profiles that are used for PCC do not become effective until you restart the ECs. Redis Profiles A Redis profile is used to read and write data in a Redis database. This profile can be accessed by workflows using Aggregation, Distributed Storage or PCC. As a client to Redis, the profile operates in synchronous mode. When sending a request to Redis, the profile expects a server response, indicating success or failure, before proceeding to send the next one in queue. PCC using a Redis profile has been verified using Amazon ElastiCache. For information about how to create a Redis profile, see Redis Profile .

---

# Document 1345: HTTP/2 Client Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739276/HTTP+2+Client+Agent
**Categories:** chunks_index.json



---
**End of Part 56** - Continue to next part for more content.
