# RATANON/MZ93-DOCUMENTATION - Part 40/112

---
**Dataset:** ratanon/mz93-documentation
**Part:** 40 of 112
**GitHub:** https://github.com/ratan0n/docs/tree/main/mz93-documentation
**Size:** ~67.0 KB
---

---

# Document 892: SNMP Request Agent Input/Output Data, MIM and Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204609451/SNMP+Request+Agent+Input+Output+Data+MIM+and+Events
**Categories:** chunks_index.json

Open Input/Output Data Input Data SampleUDR and FlatSampleUDR SnmpRequestUDR Meta Information Model For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . The agent does not publish nor access any MIM parameters. SNMP Request Agent Events Agent Message Events The agent does not produce any message events. However, you can configure the Analysis agent with APL code to produce message events. See Log and Notification Functions for more information about available functions. Debug Events The agent does not itself produce any debug events. However, you can configure the Analysis agent with APL code to produce debug events. See Log and Notification Functions for more information about available functions.

---

# Document 893: Categorized Grouping Meta Information Model and Debug Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204606599/Categorized+Grouping+Meta+Information+Model+and+Debug+Events
**Categories:** chunks_index.json

Meta Information Model For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Parameter Description Using Clean Storage This MIM parameter is used if an inconsistency is detected in the persistent storage and the agent is configured to continue with a clean state. The MIM can be true during the first transaction of an activation but will always be false on the subsequent transactions within the same activation. Using Clean Storage is of the boolean type and is defined as a header MIM context type. Accesses MIM Parameter Description Source File Count Received from the collector. This is accessed if Close on Deactivation is enabled. Note! APL offers the possibility of both publishing and accessing MIM resources and values. For a listing of general MIM p arameters , see Administration and Management . Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event .

---

# Document 894: UDP Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205034867/UDP+Agent+Configuration
**Categories:** chunks_index.json

To open the UDP agent's configuration, click Build  New Configuration. Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime. Click Add agent and select UDP from the Collection tab of the Agent Selection dialog. UDP Tab Open UDP agent configuration dialog - UDP tab Setting Description Setting Description Host The IP address or hostname to which the UDP collector will bind. If the host is bound the port must also be bound. If this field is left empty, the UDP collector binds to all IP addresses available on the system. Port The port number from which the data is received, by default set to 3310. Make sure the port is not used by other applications. Decoder Tab Open UDP agent configuration - Decoder tab Setting Description Setting Description Decoder List holding available decoders introduced via the Ultra Format Editor . The decoders are named according to the following syntax: <decoder> (<module>) The option MZ Format Tagged UDRs indicate that the expected UDRs are stored in one of the built-in formats. If the compressed format is used, the decoder will automatically detect this. Select this option to make the Tagged UDR Type list accessible for configuration. If this option is selected, the Tagged UDR Type list will be enabled. Tagged UDR Type List of available internal UDR formats stored in the Ultra and Code servers. The formats are named according to the following syntax: <internal> (<module>) If the decoder is to reprocess UDRs of an internal format, the Decoder MZ format tagged UDRs has to be selected to enable this list. Once enabled, the internal format may be selected. Full Decode If enabled, the UDR will be fully decoded before inserted into the workflow. This may have a negative impact on performance since all fields may not be accessed in the workflow, making decoding of all fields unnecessary. If disabled (default), the amount of work needed for decoding is minimized, using a "lazy" method decoding sub fields. This means the actual decoding work may not be done until later in the workflow, when the field values are accessed for the first time. Corrupt data (that is, data for which decoding fails) may not be detected during the decoding stage and could cause the UDR to be discarded at a later processing stage.

---

# Document 895: GIT Support - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204639122/GIT+Support
**Categories:** chunks_index.json

The following section provides information on how Git is used to handle different system export scenarios when Git is used as a remote target. How Git is working in our product Git is a distributed version control system, more about Git can be read here . MediationZone only supports connecting to a remote repository that is accessible with an HTTP/HTTPS URL and a Token. When the File System Profile of type Git is saved, a clone of the Repository is created in a directory in the Platform. This directory is $MZHOME/gitrepos by default. It can be changed by setting the property mz.git.basePath to some other path accessible from the Platform. Each File System Profile will have its directory with its clone of the corresponding repository. The local copy of the repository is only updated to match the remote repository when these events occur: An import is performed A user clicks the Refresh button in the System Importer. After an export is performed, regardless of if it is successful or failed. When an export is performed to Git the system will: Delete all files in the selected target folder Perform an export to the selected folder Commit the changes to the local repository Push the commit to the remote repository This means that even if just a few configurations have been updated a full export of what was in the target folder from before needs to be made in order not to lose files in the repository version. During step 4, when the export is pushed to the remote repository, Git can detect if the target folder on the remote repository is updated compared to the local target folder. If an update on the remote repository is detected, the system will evaluate if it will cause any conflicting scenarios (described below). So, if the local repository is updated before an export (events described above) i.e. the local repository has the same status as the remote the system will not detect any conflict, and the system will overwrite the content in the target folder. Open Recommendations for the structuring of configuration The structure of the folders in the Git repository is not decided by the system. Several developers working in parallel on the same deployment calls for structured configuration to avoid too many merge conflicts. The recommendation therefore is to have a separate folder in the Git repository for each use case. Export Scenarios When exporting configurations to Git multiple scenarios can happen because the remote repository is updated by another System or person. Different scenarios are described below. Update in a different directory When there have been updates to any other target folder in the same repository the export will be successful. Open Export log when Git Repository is updated in other directory Updated configurations in the same export folder, no conflict When there has been updates in the remote repository to Configurations on other places than current System have modified in the same export folder the System Export will fail. The System Exporter will perform an Import of the Configurations that was changed on remote. They will be imported with a suffix named _ merge. The action needed by you now is: Move the change from the imported configurations to your local version of the same configurations. Then remove the configurations with the suffix. Alternatively you can remove your local version and rename the imported version to the same name as before if there has not been any updates to the local version. The Imported configuration with Suffix will have a different so called Key than the original. So if the Key is important for any other configuration referring to the configuration in question, or for the configuration itself you need to do the first option. This means that it is always recommended that in these cases the local version is retained whenever possible. Example: If the configuration is an Aggregation Profile with File Storage then the Aggregation Storage directory on disk is based on the Key of the Profile. When the actions mention above is performed a new export can be executed. Open Export log when same directory in Git is updated with no conflict Update same configuration in the same Git Directory When there has been updates in the remote repository to the same Configurations as the current System have modified in the same target folder the System Export will fail. The System Exporter will perform an Import of the conflicting Configurations. They will be imported with a suffix named _ conflict. The action needed by you now is: Move the change from the imported configurations to your local version of the same configurations. Then remove the configurations with the suffix. When the actions mention above is performed a new export can be executed. Open Export log when same configuration is updated in Git Configuration added, removed or renamed on Git in same Git Directory The scenarios when configurations is added and removed or renamed will look the same during an export. Configurations that are added in the same export folder by any other system will be imported with the suffix of _added. Actions need by you is: Rename the configurations with suffix _added , by removing the suffix. Configurations that are removed in the same export folder by any other system but exist in current system will be copied to a new configuration with suffix _removed. Actions need by you is: Remove the configurations with suffix _ remove and the local configuration (if it shall be removed) with the same name but without the suffix. When the actions mention above is performed a new export can be executed. Open Export log Configuration is added and removed or renamed

---

# Document 896: Creating a Workflow Group Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204736807/Creating+a+Workflow+Group+Configuration
**Categories:** chunks_index.json

You create a new workflow group configuration from the Build view of the Desktop User Interface. To open the workflow group configuration, click Build  New Configuration . Select Workflow Group from the Configurations dialog. To open an existing workflow group configuration, click Build and select the workflow group configuration in the Configuration Browser, or right-click a workflow configuration and then select View Configuration . Open A Workflow Group configuration Workflow Group Buttons The contents of the button panel may change depending on the opened which configuration type in the currently displayed tab. There is a set of standard buttons that are visible for all configurations and these buttons are described in the Common Configuration Buttons . Button Description Button Description Open This allows you to include or exclude the following from the Available To Add list: Workflow Groups Workflows Batch- and Task Workflows Real-time workflows Workflow Group Tabs A workflow group configuration has three different tabs: Members , Execution , and Scheduling , the two latter of which are described in Managing a Workflow Group . Members Item Description Item Description Available To Add Upper pane: Displays a tree view of the workflows and workflow groups that are saved within their respective configurations. These are available for you to add as members when creating a new workflow group. Lower pane: Displays a list of workflows that are included in the workflow configuration that you select from the upper pane. Note! A workflow group can be a member of another workflow group. Group Members Shows the currently added workflow group members.

---

# Document 897: Oracle - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204737654/Oracle
**Categories:** chunks_index.json

This section contains information that is specific to the database type Oracle. Supported Functions The Oracle database can be used with: Audit Profile Callable Statements (APL) Database Bulk Lookup Functions (APL) Database Table Related Functions (APL) Database Collection/Forwarding Agents Data Masking Profile Event Notifications Prepared Statements (APL) Data Veracity Shared Table Profile SQL Collection/Forwarding Agents Task Workflows Agents (SQL) Properties When selecting the Oracle database type, you can configure the following properties using the Properties tab in the Database profile: oracle.pool.connectionwaittimeout oracle.pool.maxlimit oracle.net.encryption_client oracle.net.encryption_types_client oracle.net.crypto_checksum_client oracle.net.crypto_checksum_types_client Preparations A database driver is required to connect to an Oracle database. This driver must be stored in the Platform Container. Follow these if Oracle was not set up during the installation of the Platform Container: Download the JDBC driver ( ojdbc<version>.jar ) for the appropriate Oracle database version, and the ONS.jar driver in the case you use Oracle RAC. Copy the downloaded file(s) to the directory MZ_HOME/3pp in the Platform Container. Restart the Platform and ECs for the change to take effect. You should be able to select the Oracle option from the Database profile after this step. Advanced Connection Configuration for Oracle RAC The Advanced Connection Setup is used for Oracle RAC Configurations. To make the Connection String text area and the Notification Service text field appear, select the General radio button. The Username , Password, and Database Type fields will remain. If MediationZone is installed with the Oracle database, the Oracle RAC functionality Fast Connection Failover (FCF) is available. MediationZone supports FCF thus, the expectation is that there will normally be some exceptions generated during RAC instance failover. When FCF is configured, the system detects a lost connection, clears the database connection pool, and reinitializes the connection pool. During a RAC instance failover, you might experience exceptions, for example, when database transactions such as updates and inserts are done. Database exceptions are logged in the system. The Platform and Execution Contexts support the failover behavior. However, note that neither database collection nor forwarding agents support FCF. These agents have different types of database connection pool implementation. Open Database Profile Configuration - Advanced Connection Setup Setting Description Setting Description Connection String In the text field, a connection string can be entered. The connection string can contain a SID or a service name. The string added will not be modified by the underlying system. If a connection string is longer than the text area spaces a vertical scroll bar will be displayed to enable viewing and editing of the connection string. Notification Service Enter the Configuration that enables the Oracle Notification Service daemon (ONS) to establish a Fast Connection Failover (FCF). The ONS string that you enter should at least specify the agent ONS configuration attribute, which is made up of a comma-separated list of host:port pairs. The hosts and ports represent the remote ONS daemons that are available on the RAC agents. For further information see the Installation guidelines for Oracle RAC in the Installation Instructions .

---

# Document 898: Desktop User Interface - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204670149
**Categories:** chunks_index.json

The Desktop Interface enables interaction with MediationZone deployments. http://localhost:9001/desktop/ After a successful login, the Desktop start page is displayed . Open The Dashboard shows the splash screen. From here you can access the main sections: The top menu panel lists the main options available to the users, subdivided into the Build View and Manage View screens. Clicking on the MediationZone logotype takes you back to the start page. On the top right, there is the User Settings List . You can access the dropdown menu by clicking on the icon. Top Menu Bar The Desktop user interface has a top menu bar consisting of the following options: Item Description Item Description Screen Selection The Screen Selection is located at the top left in Desktop. You can select to open the Build screen or the Manage screen and MediationZone icon takes you back to the start screen. Each selection is displayed in a full-screen vi ew. Open Screen Selection List Search Allows you to search the Desktop for any configuration or content. For more information on using search terms to better aid your querying, refer to Search . User The User menu is located on the top right in Desktop and contains various options for your account or deployment. See User Settings for more information. Open User icon Accessibility Support Across all views, there is screen reader support that enables Accessibility access. In addition, the overall display theme can be selected for easier viewing. For more information, see Desktop Accessibility Options . The Desktop interface is redesigned to be easily accessible using web browsers. This all-new experience has many advantages over the Legacy Desktop as it is more intuitive to use. However not all functionality from the Legacy Desktop is available yet, for some advanced features and functionality you still have to use the Legacy variant. F or more information, see the Legacy Desktop . This section contains the following subsections: Build View Manage View

---

# Document 899: FTPS Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607660/FTPS+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An agent message is an information message sent from the agent, stated according to the configurations made in the Event Notification Editor. For further information about the agent message event type, see Agent Event . Ready with file: filename Reported, along with the name of the source file, when the file has been collected and inserted into the workflow. File cancelled: filename Reported, along with the name of the current file, when a Cancel Batch message is received. This assumes the workflow is not aborted when a Cancel Batch message is received. See FTPS Collection Agent Transaction Behavior for further information. Debug Events Debug messages are dispatched in debug mode. During execution, the messages are displayed in the Workflow Monitor. You can configure Event Notifications that are triggered when a debug message is dispatched. For further information about the debug event type, see Debug Event . The agent produces the following debug events: Command trace A printout of the control channel trace either in the Workflow Monitor or in a file.

---

# Document 900: Syslog Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654266/Syslog+Collection+Agent+Configuration
**Categories:** chunks_index.json

To open the Syslog Collection agent configuration dialog from a workflow configuration, you can do either one of the following: double-click the agent icon select the agent icon and click the Open Edit button The Agent Configuration contains the following settings: Open Syslog Collection agent configuration Setting Description Setting Description Host The IP address or hostname to which the Syslog Collection agent will bind. If left empty the agent binds to all IP addresses available on the system. Port Enter the port number to which the data is received. Make sure the port is not used by other applications. Note! On Unix/Linux systems any port below 1024 can only be opened by the root-user because of security limitations. It is recommended that you redirect Syslog messages to a port that is available to the MZ_HOME owner. Info! The port number must be within the range of 0 to 65535. Package Size (B) This value controls the maximum size of the incoming messages. If the size of a message exceeds the value in Package Size (B) , it will be truncated. For the best possible performance, enter a value, in bytes, that is just large enough to accommodate the incoming messages. Info! The Package Size must be at least 512. Receive Buffer Size (B) Enter the maximum size of the UDP buffer. In case of a buffer overflow, the agent will not receive new incoming messages. Note! In order to prevent UDR datagram loss, you may need to the increase the size of the UDR receiver buffer of the operating system. The required size of the UDP receive buffer depends on many factors, such as: The number of Syslog collection workflows on each EC host. The number of workflow threads The number of processors The amount of memory The speed of the hard disk for virtual memory For information about how to increase the UDP receive buffer, see the documentation of your operating system. Info! The Receive Buffer Size must be at least 2048.

---

# Document 901: Python Connector UDR Types - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204740039/Python+Connector+UDR+Types
**Categories:** chunks_index.json

The UDR types used by the Python Connector agent can be viewed in the UDR Internal Format Browser . To open the browser, open an APL Editor , in the editing area. Right-click and select UDR Assistance... and the browser opens. Open Python UDRs ConnectorConnect The agent outputs this UDR when the Use Connection Control option is selected and a new connection is being established. The ConnectorConnect UDR contains the connection details and allows the workflow to accept or discard the connection. The UDR must be routed back to the agent for the connection to be allowed or discarded. The following fields are included in the ConnectorConnect UDR: Field Description Field Description Allow (boolean) This field determines whether the connection is permitted or not, defaults to true. ConnectionId (int) This field contains the connection ID of the connection. Reason (string) This field can be used to explain why the connection is not permitted. TLSInformation (TLSInformation (python)) This field contains TLS information for the connection. ConnectorData The agent outputs this UDR when the Use per Connection Routing option is selected and a data is routed from an exploration tool. If you route a ConnectorData UDR back to the Python Connector agent, the data can be consumed from the specified connection only, otherwise the data can be consumed from all connections. The following fields are included in the ConnectorData UDR: Field Description Field Description ConnectionId (int) This field contains the connection ID. Data (any) This field contains the routed data. The data can be of any accepted type, such as a bytearray or UDR. ConnectorDisconnect The agent outputs this UDR when the Use Connection Control option is selected and a connection has been closed. The following fields are included in the ConnectorDisconnect UDR: Field Description Field Description ConnectionId (int) This field contains the connection ID. TLSInformation The TLSInformation UDR contains the details required to use TLS for connection between the Python Connector agent and an exploration tool using Python. The following fields are included in the TLSInformation UDR: Field Description Field Description CertificateChain (list <X509Certificate (python)>) This field contains the list of X.509 certificates in the certificate chain that is presented by the client. For further information, see the section below, X509Certificate. CipherSuite (string) This field contains the cipher suite. Protocol (string) This field contains the protocol. X509Certificate The X509Certificate UDR contains the certificate details. The following fields are included in the X509Certificate UDR: Field Description Field Description IssuerDN (string) This field contains the distinguished name (DN) of the issuer of the certificate. NotAfter (date) This field contains the expiry date of the certificate, i e the date after which the certificate is no longer valid. NotBefore (date) This field contains the date from which the certificate is valid. Signature (bytearray) This field contains the signature for the certificate. SubjectDN (string) This field contains the distinguished name (DN) of the subject of the certificate.

---

# Document 902: GCP Storage Collection Agent Events - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033302/GCP+Storage+Collection+Agent+Events
**Categories:** chunks_index.json

Agent Message Events An information message from the agent, stated according to the configuration done in the Event Notification Editor. Ready with file: filename Reported along with the name of the source file that has been collected and inserted into the workflow. File cancelled: filename Reported along with the name of the current file, each time a Cancel Batch message is received. This assumes the workflow is not aborted; refer to GCP Storage Collection Agent Input/Output Data and MIM . Debug Events There are no debug events for this agent

---

# Document 903: Always Available - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204646366/Always+Available
**Categories:** chunks_index.json

Command line tool commands that are available even when the Platform is not running, vary both in argument requirements and in behavior, depending on the logged on user's user permissions. This section includes a detailed description of each command, its required or optional arguments, and its operation. The following commands are available: desktop encryptpassword exit help kill pcreate pcommit picoversion quit restart shutdown startup status version For information about the commands that or only available when the Platform is running, see Available When the Platform Is Running .

---

# Document 904: Conditional Trace (CT) - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205783624/Conditional+Trace+CT
**Categories:** chunks_index.json

Conditional Trace makes it possible to download and examine selected records from running workflows, making it a very powerful tool for troubleshooting in production environments. The main advantage is that there is no need to restart the workflows - you can at any time create the rules and turn on tracing for a set of workflows. The configuration is a three step procedure: Defining a Trace Template: Templates define which workflows and UDR types to target, including search criteria to find the particular record/records. Starting a Trace: Traces are started from the Desktop. Examining a Trace: Trace output records can be examined directly in the Desktop. You can also choose to save trace records in a file, or use a trace output examination tool (like Jaeger). Open Output from a Conditional Trace configuration

---

# Document 905: FTP Forwarding Agent Memory Management - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205033082/FTP+Forwarding+Agent+Memory+Management
**Categories:** chunks_index.json

A global memory buffer is allocated for each EC. The size of the buffer is specified using an Execution Context property in the EC's configuration file located in the container. Note that this global backlog memory buffer is used and shared by this and any other forwarding agent that transfers files to a remote server. The same memory buffer is used for all ongoing transactions on the same EC. When several workflows are scheduled to run simultaneously, and the forwarding agents are assigned with the backlog function, there is a risk that the buffer may be too small. In such cases, it is recommended that you increase the property size. Example - Setting a property to increase the maximum memory To increase the maximum memory to 20 MB: mzsh topo set topo://container:<container>/pico:<pico>/val:config.properties.mz.forwarding.backlog.max_memory 20 You must restart the EC for the property to apply. If no property is set the default value of 10 MB will be used. The amount allocated will be printed out in the EC's log file. This memory will not affect the Java heap size and is used by the agent when holding a copy of the file being transferred.

---

# Document 906: GCP BigQuery Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739033/GCP+BigQuery+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The input/output data is the type of data an agent expects and delivers. The agent consumes the selected UDR Type from the Target tab. The agent emits commands that change the state of the file currently processed. Command Description Command Description Cancel Batch Emitted if any error occurs during insertion of rows into Data table or error when mapping the UDR to rows. The agent retrieves commands from other agents and based on them generates a state change of the file currently processed. Command Description Command Description Begin Batch Retrieves a Transaction ID. End Batch Updates the Batch status to indicate the successful insert of all the UDRs into the Data table. This status will be loaded to the Batch Status table. Cancel Batch Updates the Batch status to indicate UDRs insertion has been canceled. This status will be loaded to the Batch Status table. MIM For information about the MIM and a list of the general MIM parameters, see MIM . Publishes MIM Parameter Description MIM Parameter Description Rows Loaded This MIM parameter contains the number of rows that have been successfully inserted into the Data table. Rows Loaded is of the long type and is defined as a global MIM context type. Accesses This agent does not access any MIMs.

---

# Document 907: GTP' Agent Input/Output Data and MIM - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204607875/GTP+Agent+Input+Output+Data+and+MIM
**Categories:** chunks_index.json

Input/Output Data The Input/Output data is the type of data that an agent expects and delivers. The agent produces UDR types in accordance with the Decoder tab settings. MIM For information about the MIM and a list of the general MIM parameters, see Administration and Management in Legacy Desktop . Publishes MIM Value Description Cancel Data Count (long) The number of received Cancel Data Record Packet requests. Data Record Count (long) The number of received Send Data Record Packet requests. Duplicate Message Count (long) The number of received duplicates. Last Request Timestamp (long) This MIM value contains the timestamp for the last received packet. Latest Message Arrival (list) This MIM value contains a list of the server port, message type, and arrival timestamp contained in GTPMessageStat UDRs. Message Error Count (long) The number of received erroneous messages. Out of Sequence Count (long) The number of received records that are not in sequence. Possible Duplicate Count (long) The number of received Send possibly duplicated Data Record Packet requests. Release Data Count (long) The number of received Release Data Record Packet requests. Accesses The agent does not access any MIM parameters.

---

# Document 908: Legacy Kafka Real-Time Collector - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/301138608/Legacy+Kafka+Real-Time+Collector
**Categories:** chunks_index.json

The Kafka real-time collection agent consumes messages from the topic and partitions stated in the Kafka collection agent's configuration, which contains the following settings: Setting Description Setting Description Profile Browse and select the profile the agent should use, as defined in Legacy Kafka Profile . All Select this option if you want messages to be collected from all of the partitions. Range Select this option if you want messages to be collected from the range that you specify. Specific Select this option if you want messages to be collected from the specified partition(s). This is a comma-separated list. Offset You must determine from which offset you want to start collecting. Start at Beginning Select this option to collect messages from the first offset. If you select this option, there is a risk that messages will be processed multiple times after a restart. Start at End Select this option to select messages from the last offset from when the workflow was started. If you select this option, there is a risk that data can be lost after a restart. Start at Requested Select this option to start collection from the given offset on the incoming route specified in the UDRs. You set the offset using the KafkaOffsetUDR in an Analysis agent, see Legacy KafkaOffsetUDR . Using this offset option reduces the risk of data loss, and prevents messages from being processed multiple times after a restart. See the example in the KafkaOffsetUDR linked above. Managed by kafka (auto commit) This function should be used in the case you have a Kafka profile using the Group id, to enable load sharing between several Kafka collectors. Use this in combination with enable.auto.commit=true in the Kafka profile, in the Advanced , Consumer tab. If you select this option, the consumed messages will committed intermittently which may result in lost or duplicate data. To avoid this, select the Start at Requested option instead. Input/Output Data Input Data Messages from the Kafka Log. Legacy KafkaOffsetUDR Output Data Legacy KafkaUDR

---

# Document 909: TCP/IP Collection Agent Configuration - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205654316
**Categories:** chunks_index.json

To open the TCP/IP collection agent configuration, click Build  New Configuration . Select Workflow from the Configurations dialog. When prompted to Select workflow type , select Realtime . Click Add agent and select Tcp Ip from the Agent Selection dialog. Double-click the agent icon or right-click the icon and select Edit agent , to display the Agent Configuration dialog. TCP/IP Tab Open TCP/IP collection agent configuration dialog - TCP/IP tab Setting Description Setting Description Host The IP address or hostname to which the TCP collector connects. If the host is connected the port must also be connected. If left empty, the TCP collector connects to all IP-addresses available in the system. Note! This setting can be dynamically updated. Port The port number from which the data is received. Make sure the port is not used by other applications. Note! The port can also be dynamically updated while the agent is running. Double-click the agent in the Workflow Editor in monitor mode and modify it. Save the workflow to trigger the agent to use the new port. For further information about updating agent configurations while a workflow is running, see Dynamic Update in Administration and Management in Legacy Desktop . Allow Multiple Connections Select this checkbox to allow several TCP/IP connections simultaneously. If cleared, only one connection is allowed. Number of Connections Allowed If Allow Multiple Connections is selected, enter a number between 2 and 65000 to specify the number of allowed connections. Send Response Select this checkbox to have the collector send a response back to the source. If Allow Multiple Connections is selected, the collector expects a UDR extended with the default TCPIPUDR as a reply. If the checkbox is cleared, it expects a bytearray. Note! Drag and drop in the opposite direction in the workflow to create a response route between the agents. The TCP/IP agent must be connected to an agent utilizing APL, since responses are created using APL commands. Open For a description of the differences between single or multiple connections, see A TCP/IP Example . Send TCPIPStateUDR Select this checkbox to track the connection state of the client and have the collection agent send a UDR indicating the status of the client connection each time it changes state. See TCPIPStateUDR in TCP/IP Related UDR Types . Note! This information is not detected if a client is powered down or if its network cable is removed. Decoder Tab The Decoder tab contains settings related to decoding of the collected data. Open TCP/IP collection agent configuration dialog - Decoder tab Extra System Properties This section describes the extra system properties that you can use to configure the TCP/IP Collection Agent. Setting Description Setting Description mz.tcpip.logginginterval_ms This property allows you to limit the number of exceptions from the TCP Collection Agent to one exception per specified interval (in milliseconds). Only change the property if support has expressly recommended it. It is normally not recommend to set this property.

---

# Document 910: Known Issues - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/205881617/Known+Issues
**Categories:** chunks_index.json

Open In MediationZone 9.3, there are the following known issues: 1 Installation and Upgrade Issues 1.1 Unable to Change Platform Database After Installation 1.2 Installation is Reported as Successful Even if the Platform Fails to Start 1.3 Warning Messages Displayed During Installation 1.4 Error Occurs When Performing Multiple Installations on a Host with Different UNIX User IDs 1.5 Configuration History Loss After Upgrade from MediationZone 8 1.6 Deleted Configuration History Unavailable After Upgrade from MediationZone 8 1.7 Only the first EC is started when multiple ECs are specified 2 User Interface issues 2.1 New Desktop Issues 2.1.1 Database Agent Assignment Tab Value Are Not Cleared 2.1.2 Database Agent MIM Browser Need To Click Twice To Close It 2.1.3 External Reference Profile Datalist Can Be Altered in Read-Only Mode 2.1.4 Login Web UI Will Go To Last Page That Last User Accessed 2.1.5 Type Assigned Indication not Shown in MIM Browser 2.1.6 Idle Timeout Only Warns the User Once 2.1.7 Data Veracity Repair Jobs Show in One Line 2.2 Accessibility Issues 2.3 Code Editor Issues 2.4 Known Differences Between Desktops 3 Data Veracity issues 3.1 Data Veracity Search Filtering For Some Fields Are Case-Sensitive 3.2 Data Veracity Filter's Full Query Is Not Available on Desktop 3.3 Adding an Empty Query Group With A Parent Condition In Data Veracity Search & Repair Filter Will Result In An Error 3.4 Importing Old Data Veracity Collection Agent Workflow Configurations Returned Validation Errors 3.5 Data Veracity Filters Are Not Refreshed Automatically 3.6 Data Veracity New Restricted Fields That Were Used For An Existing Repair Rule Is Not Disabled 3.7 Data Veracity Not Working Well in Workflow Packages 4 Duplicate UDR issues 4.1 Duplicate UDR Inspector Stuck Loading When Attempting To Delete Records 4.2 System Log Shows Unknown User On Deleting A Record In Duplicate UDR Inspector 4.3 Duplicate UDR Inspector Allows To Delete Records Without Profile Locking 4.4 Duplicate UDR Inspector Didnt Return Records Properly 4.5 Duplicate UDR MIMs Shows 0 4.6 Incorrect Handling When Duplicate UDR Process Old Data With System Arrival Time 4.7 Duplicate UDR Inspector Table Header Row Count Is Not Updated When Records Found Is 0 4.8 Rename Duplicate UDR Profile Doesnt Auto Reflect In Duplicate UDR Agent 4.9 Exception Thrown When Truncating Previous Tables After Changing DB Profile In Duplicate UDR Profile 4.10 File Directory Is Not Empty When Delete Duplicate UDR File Storage Profile 5 Reference Data Management 5.1 Reference Data Management Table Shows No Data And All Functions Are Locked 5.2 Import/Export Ongoing Process Are Not Aborted When Navigating Away From Reference Data Management 6 SAPCC Rest Agent 6.1 Unable To Add Additional Parameter In Desktop Online Workflow Page 6.2 Workflow Unable To Rollback To Default Values 7 Other issues 7.1 Logging Issue 7.2 Error Thrown When SAPCC Agent Not Added With Any Host 7.3 Inappropriate Validation Handling On Workflow Group Scheduling 7.4 Database Forwarding Agent Validation Message 7.5 External References Can Be Removed when a Workflow is Dependent on It 7.6 Keystore Information Is Gone After Imported Notification Workflow From MZ8.3 7.7 Overwrite After Collection Disabled in Workflow Table of Workflow Properties 7.8 ECS Tables Missing Indexes 7.9 SAP RFC Workflow Hangs when Setting Date Data Type 7.10 ORA-1000 For Oracle Database 7.11 Incorrect OpenAPI Specifications in Operations REST Interface Installation and Upgrade Issues Unable to Change Platform Database After Installation It is not possible to change platform database after the installation is completed which used to be a feature that available since 8.x. This issue will be fixed in future releases. Installation is Reported as Successful Even if the Platform Fails to Start In some circumstances during installation the installer may report as successful when the platform has not started. Warning Messages Displayed During Installation When the user performs  setup.sh prepare , a warning message  setSecurityManager will be removed in a future release " is displayed. This does not impact functionality of MZ 9 installation and will be handled in future versions of MZ. Error Occurs When Performing Multiple Installations on a Host with Different UNIX User IDs The /tmp/syslog/syslog-debug.log file is created when a host machine contains an MZ 9 installation. This file will be overwritten by any subsequent MZ 9 installations that are performed on the same host machine with a different UNIX user ID. However, the attempt to overwrite this file will fail and cause errors to the subsequent installations. To solve this issue, modify the platform.xml file setting by using the mzsh topo open platform command. Edit the platform.xml file, and change the directory to use another directory using the following command: <property name="mz.syslog.debuglogfile.filedir" value="[the directory]"/> . Configuration History Loss After Upgrade from MediationZone 8 After upgrading from MediationZone 8, existing configuration history will no longer be available. This issue will be fixed in future releases. Deleted Configuration History Unavailable After Upgrade from MediationZone 8 After upgrading from MediationZone 8, previously deleted configuration history in MediationZone 8 is no longer accessible in the Configuration Tracer. This issue will be fixed in future releases. Only the first EC is started when multiple ECs are specified When specifying multiple ECs during installation using a comma-delimited format (e.g., ec1,ec2,ec3), the installation completes successfully. However, only the first EC (ec1) is started post-installation. The remaining ECs are not started as expected. User Interface issues New Desktop Issues Database Agent Assignment Tab Value Are Not Cleared When Value Type is set to "To UDR" or "NULL", the Value column are not cleared. Database Agent MIM Browser Need To Click Twice To Close It The MIM browser open from database agent, need to close it by hitting twice on cancel button or the X button to close it. External Reference Profile Datalist Can Be Altered in Read-Only Mode When certain procedures are made, it is possible to alter the data list found in the external reference profile while the read-only mode is used. This causes a system-wide DRRCPException to be thrown. Login Web UI Will Go To Last Page That Last User Accessed User A log out from web UI, and User B login, it will automatically go to the page last accessed by User A. Type Assigned Indication not Shown in MIM Browser This can be viewed in the Legacy Desktop if required. Idle Timeout Only Warns the User Once Inactive users will be logged out after a configurable period, following a warning to that effect. If a user is active after the warning they won't be logged out, but if they are inactive again then they will be logged out with no warning. Data Veracity Repair Jobs Show in One Line When viewing the repair jobs in the new UI they are displayed in one line which is hard to read. Accessibility Issues The new web-native desktop aims to be fully accessible, but there are currently some outstanding issues regarding this. These include: In some interfaces, the keyboard navigation doesn't follow the ideal order or convention Incorrect behavior of radio buttons Code Editor Issues The code editor in the desktop provides code completion and syntax highlighting there are some known issues with syntax highlighting not being correct, it is not possible to add tabs to code and code completion cannot be activated on a Mac if multiple keyboard layouts are enabled. When attempting to use the code completion function in agents or profiles, for example, the Analysis agent , the built-in keyboard shortcut of Command + Space on a Mac computer will not trigger the code completion dialog box. Known Differences Between Desktops There are some known, minor issues with differences between the two Desktops. Data Veracity issues Data Veracity Search Filtering For Some Fields Are Case-Sensitive The filtering for the source_node and message fields in the Data Veracity Search & Repair table are case-sensitive. Data Veracity Filter's Full Query Is Not Available on Desktop The full filter query for Data Veracity is not available for viewing on the Desktop. A partial display of the query can be seen in the filter listing under Data Veracity > Filter listing. Open Adding an Empty Query Group With A Parent Condition In Data Veracity Search & Repair Filter Will Result In An Error Attempting to add an empty query group with a parent condition will cause an error to be thrown upon clicking OK . A sample of the query setup is shown below. Open Importing Old Data Veracity Collection Agent Workflow Configurations Returned Validation Errors If you have an old Data Veracity Collection Agent Workflow configuration from version 8.x, it is possible that validation errors about invalid Analysis Agents may occur. Example error during import: The following agents returned validation errors. Analysis_1 is invalid. The UDR type DataVeracityUDR (DataVeracity) doesn't exist Example error when viewing the invalid workflow Analysis Agent configuration: Error: Failed to set UDR Types Message: Unable to create UDR type description from DataVeracityUDR (DataVeracity). To resolve this, you will need to re-add the Data Veracity UDR for Analysis Agent input UDR Types or use the Set To Input button if applicable. After adding the appropriate Analysis UDR Types, save the workflow configuration. Data Veracity Filters Are Not Refreshed Automatically The Filter page is not always the latest, after creating a new Filter from the Data Veracity Search page, you may need to click the Refresh button to view it. Data Veracity New Restricted Fields That Were Used For An Existing Repair Rule Is Not Disabled If setting a new restricted field that was used in a previously created Repair Rule, it is expected that the rule on the said field should be grayed out when viewing the saved Repair Rule and be ignored when applying repair tasks. This is not the case as the rule on the said field still appears to be editable in the old Saved Repair Rule. Data Veracity Not Working Well in Workflow Packages There are classloading issues in both inspectors and for Data Veracity it is not even possible to access the data since the profile is not selectable. For both ECS Inspector and DV Inspector, it is impossible to view the UDRs containing the Request (generated from Diameter Application Profile) due to classloading issues. Duplicate UDR issues Duplicate UDR Inspector Stuck Loading When Attempting To Delete Records When a Duplicate UDR profile is being used by a running workflow, attempting to delete any records for that profile in Duplicate UDR Inspector would cause the Inspector to be stuck loading. System Log Shows Unknown User On Deleting A Record In Duplicate UDR Inspector After deleting any records in Duplicate UDR Inspector, the log message in the System Log, will show the Username as <Unknown>. Duplicate UDR Inspector Allows To Delete Records Without Profile Locking Currently, when executing a workflow using the Duplicate UDR profile, the profile remains unlocked, allowing for the deletion of records from the Duplicate UDR Inspector. Duplicate UDR Inspector Didnt Return Records Properly With certain search date criteria in Duplicate UDR Inspector, some records are not returned. Duplicate UDR MIMs Shows 0 Duplicate UDR Inspector shows these MIMs as 0 even when there are Duplicate UDR caught. Dup_UDR_1.dups UDRs Dup_UDR_1.Inbound UDRs Dup_UDR_1.Outbound UDRs Dup_UDR_1.r_3 UDRs Incorrect Handling When Duplicate UDR Process Old Data With System Arrival Time With the SQL Storage Duplicate UDR profile set to use System Arrival Time , UDRs with dates older than the system time on the Platform will not be handled properly. The Duplicate UDR Inspector will not display any data despite the UDRs being processed. When using File Storage Duplicate UDR profile with System Arrival Time as well, you might see Records and Duplicates as 0 in the legacy desktop Duplicate UDR Inspector. Duplicate UDR Inspector Table Header Row Count Is Not Updated When Records Found Is 0 When searching a profile that has no records after previously searching with a profile that has any records, the table header would still display previous row count. Rename Duplicate UDR Profile Doesnt Auto Reflect In Duplicate UDR Agent When renaming a Duplicate UDR profile in the configuration browser, the Duplicate UDR agent that is using this profile is does not refresh with the new name automatically. This does not affect the workflow when executing with the profile. Exception Thrown When Truncating Previous Tables After Changing DB Profile In Duplicate UDR Profile Upon saving a copy of Duplicate UDR profile A to another Duplicate UDR profile. Changing the DB profile to a different schema and clicking on Yes at the confirmation message to truncate previous tables would lead to an exception error. File Directory Is Not Empty When Delete Duplicate UDR File Storage Profile When deleting a Duplicate UDR File Storage profile and choosing Yes to delete relational data, the system will fail to empty the File Directory. Reference Data Management Reference Data Management Table Shows No Data And All Functions Are Locked After searching in Reference Data Management with any number of results on the table, navigate away from Reference Data Management to another page. Navigate back to Reference Data Management and click on Get Started. Once the Query dialog is opened, refresh the page, and click on Query to select the same profile and table and the apply, you would find that table is now empty, and all functions are locked. Import/Export Ongoing Process Are Not Aborted When Navigating Away From Reference Data Management Running import/export processes are not aborted when navigating away from the Reference Data Management page. SAPCC Rest Agent Unable To Add Additional Parameter In Desktop Online Workflow Page In Desktop Online Add/Edit Workflow page, click on the Add button for Additional Parameters field, we will see the pop up missing Key and Value fields, so currently user is unable to add Additional Parameters in Workflow page. However, user can add Additional Parameters in Legacy Desktop. Workflow Unable To Rollback To Default Values Set agent fields to Default in Workflow Properties, and enter some data into lets say a textfield and save it. Then edit again to remove data in this textfield, workflow template will throws validation error that the workflow is invalid. This happened to checkbox and some other fields as well. Other issues Logging Issue When logging out of the legacy desktop desktop_current.log has unnecessary logs added Error Thrown When SAPCC Agent Not Added With Any Host In the SAP CC Batch agent, do not add any host, click the Save As button, and you will find the error shown below: java.lang.NullPointerException: Cannot invoke "String.matches(String)" because "hosts" is null Inappropriate Validation Handling On Workflow Group Scheduling In Workflow Group Scheduling, when adding a duplicate execution day plan, it prompts the wrong validation message content and if the user continues to save, scheduling data will be wiped off. Database Forwarding Agent Validation Message In Database Forwarding Agent, when we have a field NULL/To UDR, the validation message that prompted does not properly describe the problem. External References Can Be Removed when a Workflow is Dependent on It External references of the type Database Properties can be removed even if they are in use. Any workflows using these references will become invalid. Keystore Information Is Gone After Imported Notification Workflow From MZ8.3 System Import Notification Workflow from MZ8.3, found that Keystore information in the SAP CC Notification agent is gone. Overwrite After Collection Disabled in Workflow Table of Workflow Properties Users might encounter this issue when using a workflow configuration that uses SFTP Agent exported from MediationZone versions prior to MediationZone 9.3.0.4. This impacts SFTP Agents configured with the Default Collection Strategy and with the After Collection option set to either Move To or Rename. This issue occurs for workflow configurations that have not been modified since import. To resolve this, edit the workflow configuration (e.g. move an agent slightly) and then save the configuration. The Overwrite After Collection workflow property will now be configurable. ECS Tables Missing Indexes ECS Tables have found some indexes were missing during the migration from mz8 to mz9. We are planning to add it back in MZ 9.4.0.0 which will be release at 28th April 2025. Follow by MZ 9.3.2.0 SAP RFC Workflow Hangs when Setting Date Data Type Users may encounter suspended workflows when setting SAP RFC functions with the Date data type using the SAP Java Connector (JCo) library release 3.1.4. This issue happens specifically with the handling of date fields. It is recommended to upgrade to SAP Java Connector (JCo) library release 3.1.11 or later to avoid this issue. The newer releases contain fixes that prevent the workflow from hanging when handling Date data types. ORA-1000 For Oracle Database You will encounter an ORA-01000: maximum open cursors exceeded error for versions 9.3.0.0 onwards until 9.3.0.4 and 9.3.1.3 when using MediationZone with Oracle database. The issue is fixed in MediationZone versions 9.3.0.4 and 9.3.1.3 onwards. Incorrect OpenAPI Specifications in Operations REST Interface The OpenAPI specification files generated for the Operations REST Interface contain inaccuracies. This affects the following endpoints: http(s)://<platform server>:<platform port>/ops/mz/wf/v1/openapi.yaml  Workflows API http(s)://<platform server>:<platform port>/ops/mz/wfg/v1/openapi.yaml  Workflow Groups API http(s)://<platform server>:<platform port>/ops/extref/v1/openapi.yaml  External References API http(s)://<platform server>:<platform port>/ops/mz/host/v1/openapi.yaml - Host API http(s)://<platform server>:<platform port>/ops/mz/pico/v1/openapi.yaml - Pico API These inaccuracies may lead to failed validation when creating an Open API Profile based on the provided OpenAPI specification. The issue will be fixed in MediationZone 9.4.0.0.

---

# Document 911: Execution Container Installation - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204996848
**Categories:** chunks_index.json

This chapter describes how to prepare and install an Execution Container. Note! You need to make a Platform Container Installation prior to this Execution Container Installation . The preparations and installation are done in three steps: General Preparations which include configuration of the software environment and setting values for predefined parameters that may vary between installations. For further information, see General Preparations Execution Context . Software installation. For further information, see Execution Container Software Installation . This chapter includes the following sections: General Preparations Execution Context Execution Container Software Installation

---

# Document 912: AWS S3 Support for External Reference Profile - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204998598/AWS+S3+Support+for+External+Reference+Profile
**Categories:** chunks_index.json

MediationZone is able to access the properties file that is stored on the Amazon Web Service S3 bucket. Selecting the S3 properties file option will open up additional properties to be configured. These properties pertain to the connection details and location of the External Reference file that sits in the Amazon Web Service S3 bucket. Open External Reference profile dialog - S3 Properties File Setting Description Setting Description External Reference Type From the drop-down list select the External Reference source type. The following types are available: S3 Properties File - A file on an Amazon Web Service S3 bucket Properties File - A file on the Platform Container host Environment Variable - Exported environment variables from the Platform's startup shell. Use IAM Roles and Policies Enabling this checkbox will allow the user to utilize the AWS Identity and Access Management credentials to access the S3 bucket. The platform will acquire the IAM credentials from an AWS configuration file, created using the AWS CLI command. Secret Key The secret key for the S3 bucket. Part of the connection credentials required for the platform to connect to the S3 bucket to acquire the property file. Access Key The access key for the S3 bucket. Part of the connection credentials required for the platform to connect to the S3 bucket to acquire the property file. Regions The region that the S3 bucket is located in. Bucket Name The name of the S3 bucket. File Path Name Enter the path and name of the properties file. Local Key The name of the External Reference. Properties File Key The name of the External Reference in the properties file or environment variable. Value The current value of the External Reference in the properties file or environment variable. To update the value from the Properties file, use the Refresh button. If values are not displayed, make sure that a properties file is available in the specified path or that the environment variables are set. Note! Environment variables that are set after Platform start are not available in the External Reference profile.

---

# Document 913: Kafka Real-Time Forwarding Agent - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/607977549/Kafka+Real-Time+Forwarding+Agent
**Categories:** chunks_index.json

The Kafka real-time forwarding agent sends messages to Kafka but has no configuration settings of its own. For this reason, it must be placed after an Analysis or Aggregation agent (i.e., an agent that allows you to write APL), which populates KafkaRecord UDRs with the messages. The system does not automatically populate Kafka-specific fields such as the topic, message key, or valueyou must set these manually using APL. The only default behavior is that the partition is set to 0 if not explicitly specified. You must also handle the response from the Forwarding agent. It always returns a KafkaRecord, even if the message was successfully inserted into Kafka. You can check whether the insert failed by inspecting the errorMessage field: if it is not null , an error occurred. You can then add any appropriate error-handling logic. In the example below, the data is only validated, and no actions are taken beyond displaying the error for debugging purposes. Workflow Example Open Workflow with a Kafka real-time forwarding agent Workflow Design Agent Configuration Agent Configuration Pulse_1 This is our data simulator agent used for demos and tests. It produces data at regular intervals. Analysis_1 Populates the KafkaRecord. This is where you enter data to send, including topic, headers, etc. Nothing is populated automatically. Kafka_forwarding Sends data to Kafka. Analysis_2 Validates reply from Kafka to see if the insert was successful or not. Analysis Agent In the first Analysis agent (Analysis_1), the following code is used to generate KafkaRecord UDRs, map the content of the input UDR, and specify the Kafka topic where the data should be sent. consume { kafka.KafkaRecord consRec = udrCreate(kafka.KafkaRecord); consRec.value = input.Data; consRec.topic = "example"; debug("Forwarding record to Kafka..."); udrRoute(consRec); } In the second Analysis agent (Analysis_2), the following code validates if the insert was successful. consume { if (input.errorMessage != null) { debug("Error..."); } else { debug("Success"); } } Kafka Profile The https://infozone.atlassian.net/wiki/x/lwDzEQ defines the broker to which the messages should be forwarded. You need to create the Kafka profile before it can be selected in the agent.

---

# Document 914: HDFS Forwarding Agent MultiForwardingUDR Input - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204739164
**Categories:** chunks_index.json

When the agent is set to use MultiForwardingUDR input, it accepts input of the UDR type MultiForwardingUDR declared in the package FNT. The declaration follows: internal MultiForwardingUDR { // Entire file content byte[] content; // Target filename and directory FNTUDR fntSpecification; }; The content of the MultiForwardingUDR will be stored at the path that you have set in the fntSpecification field. Use the APL functions fntAddString and fntAddDirDelimiter to set the value of this field. For further information about these functions, see FNTUDR Functions in APL Reference Guide . When the files are received they are written to temp files in the DR_TMP_DIR directory situated in the root output folder. The files are moved to their final destination when an end batch message is received. A runtime error will occur if any of the fields have a null value or if the path is invalid on the target file system. A UDR of the type MultiForwardingUDR which has a target filename that is not identical to its precedent is saved in a new output file. Note After a target filename that is not identical to its precedent is saved, you cannot use the first filename again. For example: Saving filename B after saving filename A, prevents you from using A again. Instead, you should first save all the A filenames, then all the B filenames, and so forth. Non-existing directories will be created if the Create Non-Existing Directories check box under the Filename Template tab is selected. If not selected, a runtime error will occur if a previously unknown directory exists in the FNTUDR of an incoming MultiForwardingUDR . Every configuration option referring to bytearray input is ignored when MultiForwardingUDRs are expected. Example - APL code to send MultiForwardingUDRs This example shows the APL code used in an Analysis agent connected to a forwarding agent expecting input of type MultiForwardingUDRs . import ultra.FNT; MultiForwardingUDR createMultiForwardingUDR (string dir, string file, bytearray fileContent){ //Create the FNTUDR FNTUDR fntudr = udrCreate(FNTUDR); fntAddString(fntudr, dir); fntAddDirDelimiter(fntudr);//Add a directory fntAddString(fntudr, file);//Add a file MultiForwardingUDR multiForwardingUDR = udrCreate(MultiForwardingUDR); multiForwardingUDR.fntSpecification = fntudr; multiForwardingUDR.content = fileContent; return multiForwardingUDR; } consume { bytearray file1Content; strToBA (file1Content, "file nr 1 content"); bytearray file2Content; strToBA (file2Content, "file nr 2 content"); //Send MultiForwardingUDRs to the forwarding agent udrRoute(createMultiForwardingUDR ("dir1", "file1", file1Content)); udrRoute(createMultiForwardingUDR ("dir2", "file2", file2Content)); } The Analysis agent mentioned in this example sends two MultiForwardingUDRs to the forwarding agent. Two files with different contents are placed in two separate sub folders in the root directory. The Create Non-Existing Directories check box in the Filename Template tab in the configuration of the forwarding agent must be selected if the directories do not previously exist.

---

# Document 915: DirectoryStorage for Testing - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204638130/DirectoryStorage+for+Testing
**Categories:** chunks_index.json

For testing purposes, you can store the data repository locally on the Execution Zone. If you use this setup, you can only have one Execution Zone since the data is not shared. To use DirectoryStorage , you must edit the pcc.properties file. Select a directory where DirectoryStorage will store its data by editing the following: mz.pcc.directorystorage.directory=${mz.home}/tmp/pcc Configure the properties mz.pcc.config.storage.class and mz.pcc.bucket.storage.class: config.storage.class=com.digitalroute.pcc.storage.config.directory.DirectoryStorage bucket.storage.class=com.digitalroute.pcc.buckets.storage.memory.MemoryBucketStorage Save the pcc.properties file. In order for the ECs to be able to locate the pcc.properties file, you must set the Execution Context property mz.pcc.properties . mzsh topo set topo://container:main1/pico:ec1/obj:config.properties.mz.pcc '{ properties=<path> }' Example - Setting mz.pcc.properties mzsh topo set topo://container:main1/pico:ec1/obj:config.properties.mz.pcc '{ properties=${mz.home}"/etc/pcc.properties" }' Since buckets are stored in memory with this setup, the bucket data will be lost if ECs are restarted. This setup must not be used in production.

---

# Document 916: Build View - MediationZone Documentation 9.3 - InfoZone

**Source:** ratanon/mz93-documentation
**URL:** https://infozone.atlassian.net/wiki/spaces/MD93/pages/204997180/Build+View
**Categories:** chunks_index.json



---
**End of Part 40** - Continue to next part for more content.
