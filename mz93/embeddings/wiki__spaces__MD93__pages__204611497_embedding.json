{
  "id": "wiki__spaces__MD93__pages__204611497",
  "title": "Starting Clusters and Creating Topics - MediationZone Documentation 9.3 - InfoZone",
  "url": "https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611497",
  "categories": [
    "space:MD93"
  ],
  "text": "Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to the Spark cluster. Before you can create the topics you must start Zookeeper and Kafka. Prerequisites: Prepare scripts according to Preparing and Creating Scripts for KPI Management Starting Clusters To start a cluster follow the steps: Start Zookeeper and Kafka To start Zookeeper, run the following: bin/zookeeper-server-start.sh config/zookeeper.properties To start Kafka, run: bin/kafka-server-start.sh config/server.properties Create Kafka topics and partitions using the scripts included in the Kafka installation. The names of the topics must correspond to the Spark application configuration. In order for the Spark KPI Application to work, the required number of partitions for each topic must be equal to the setting of the property spark.default.parallelism in the Spark application configuration. Use a replication factor that is greater than one (1) to make sure that data is replicated between Kafka brokers. This decreases the risk of losing data in case of issues with the brokers. This is how to create topics, assuming the current working directory is the Kafka software folder: $ ./bin/kafka-topics.sh --create --topic <input topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <output topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <alarm topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> Example - Creating Kafka Topics ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 Example - Creating Kafka topics, overriding retention settings ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 --config retention.ms=86400000 Run the following command to start Spark: $ start_master_workers.sh ... To submit the app to the Spark cluster . Submit the app: $ submit.sh kpiapp ... You can now confirm the status of the Spark cluster. Open a browser and go to http://<master host>:8080 . Open Spark UI",
  "chunks": [
    {
      "chunk_id": 1,
      "text": "Spark applications must be configured with a set of Kafka topics that are either shared between multiple applications or dedicated to specific applications. The assigned topics must be created before you submit an application to the Spark cluster. Before you can create the topics you must start Zookeeper and Kafka. Prerequisites: Prepare scripts according to Preparing and Creating Scripts for KPI Management Starting Clusters To start a cluster follow the steps: Start Zookeeper and Kafka To start Zookeeper, run the following: bin/zookeeper-server-start.sh config/zookeeper.properties To start Kafka, run: bin/kafka-server-start.sh config/server.properties Create Kafka topics and partitions using the scripts included in the Kafka installation. The names of the topics must correspond to the Spark application configuration. In order for the Spark KPI Application to work, the required number of partitions for each topic must be equal to the setting of the property spark.default.parallelism in the Spark application configuration. Use a replication factor that is greater than one (1) to make sure that data is replicated between Kafka brokers. This decreases the risk of losing data in case of issues with the brokers. This is how to create topics, assuming the current working directory is the Kafka software folder: $ ./bin/kafka-topics.sh --create --topic <input topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <output topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> $ ./bin/kafka-topics.sh --create --topic <alarm topic> --bootstrap-server  localhost:9092 --partitions <number of partitions> --replication-factor <number of replicas> Example - Creating Kafka Topics ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 Example - Creating Kafka topics, overriding retention settings ./bin/kafka-topics.sh --create --topic kpi-output --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-input --partitions 6 --replication-factor 1 --config retention.ms=86400000 ./bin/kafka-topics.sh --create --topic kpi-alarm --partitions 6 --replication-factor 1 --config retention.ms=86400000 Run the following command to start Spark: $ start_master_workers.sh ... To submit the app to the Spark cluster . Submit the app: $ submit.sh kpiapp ... You can now confirm the status of the Spark cluster. Open a browser and go to http://<master host>:8080 . Open Spark UI",
      "title": "Starting Clusters and Creating Topics - MediationZone Documentation 9.3 - InfoZone",
      "url": "https://infozone.atlassian.net/wiki/spaces/MD93/pages/204611497",
      "word_count": 359,
      "char_count": 2814
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-24T04:20:49.656992",
    "word_count": 359,
    "char_count": 2814,
    "chunk_count": 1
  }
}