{
  "id": "wiki__spaces__MD93__pages__204742761",
  "title": "KPI Management - Distributed Processing - MediationZone Documentation 9.3 - InfoZone",
  "url": "https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742761",
  "categories": [
    "space:MD93"
  ],
  "text": "The following scheme demonstrates a KPI Management provisioning and processing scenario. Open Provisioning and processing The steps used (the subsequent pages will describe this in more detail) A user provisions a service model configuration via a KPI profile in the Desktop. The user then submits an application, which performs the KPI calculations, to the Spark cluster. Input data is received by the KPI Cluster In agent as KDR UDRs. The agent then transfers the content through Kafka to the Spark Cluster. The Spark cluster periodically polls the input topic and performs the KPI calculations that are based on the service model and the input data. The polling interval depends on the duration of the Spark batch intervals. When the timestamps of the input data indicate that a configurable time period has elapsed, the Spark cluster sends the calculated KPIs to a dedicated output topic. There is also a separate topic for alarm output. If the service model has been configured to produce immediate alarms, the Spark cluster sends the KPIs that hit an alarm level, within a Spark batch, potentially before their KPI period closes. The data on the output and alarm topics are collected via KPI Cluster Out agents extracts and decodes the KPI data to KPIAggregatedOutput UDRs. This chapter includes the following sections: KPI Management Quick-Start Guide - Distributed Processing Preparing and Creating Scripts for KPI Management KPI Management - External Software KPI Management Agents KPI Management Service Model Deployment - Distributed Processing Managing the Spark Cluster KPI Management Scaling Considerations",
  "chunks": [
    {
      "chunk_id": 1,
      "text": "The following scheme demonstrates a KPI Management provisioning and processing scenario. Open Provisioning and processing The steps used (the subsequent pages will describe this in more detail) A user provisions a service model configuration via a KPI profile in the Desktop. The user then submits an application, which performs the KPI calculations, to the Spark cluster. Input data is received by the KPI Cluster In agent as KDR UDRs. The agent then transfers the content through Kafka to the Spark Cluster. The Spark cluster periodically polls the input topic and performs the KPI calculations that are based on the service model and the input data. The polling interval depends on the duration of the Spark batch intervals. When the timestamps of the input data indicate that a configurable time period has elapsed, the Spark cluster sends the calculated KPIs to a dedicated output topic. There is also a separate topic for alarm output. If the service model has been configured to produce immediate alarms, the Spark cluster sends the KPIs that hit an alarm level, within a Spark batch, potentially before their KPI period closes. The data on the output and alarm topics are collected via KPI Cluster Out agents extracts and decodes the KPI data to KPIAggregatedOutput UDRs. This chapter includes the following sections: KPI Management Quick-Start Guide - Distributed Processing Preparing and Creating Scripts for KPI Management KPI Management - External Software KPI Management Agents KPI Management Service Model Deployment - Distributed Processing Managing the Spark Cluster KPI Management Scaling Considerations",
      "title": "KPI Management - Distributed Processing - MediationZone Documentation 9.3 - InfoZone",
      "url": "https://infozone.atlassian.net/wiki/spaces/MD93/pages/204742761",
      "word_count": 253,
      "char_count": 1620
    }
  ],
  "metadata": {
    "scraped_at": "2025-06-24T04:20:42.007618",
    "word_count": 253,
    "char_count": 1620,
    "chunk_count": 1
  }
}